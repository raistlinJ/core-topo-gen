from __future__ import annotations

import os
import sys
import re
import shutil
import subprocess
import io
import json
import datetime
import time
import uuid
import threading
import csv
import logging
import zipfile
import tarfile
import tempfile
import secrets
import socket
import hashlib
import socketserver
import select
import contextlib
import textwrap
import shlex
import posixpath
import fnmatch
import copy
from urllib.parse import urlparse, parse_qs

from collections import deque

from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Iterator, TextIO, Iterable

try:
    import psutil  # type: ignore
except ImportError:  # pragma: no cover - psutil is optional for tests
    psutil = None  # type: ignore
from collections import defaultdict
from types import SimpleNamespace
import xml.etree.ElementTree as ET

from flask import Flask, render_template, request, redirect, url_for, flash, send_file, Response, jsonify, session, g, has_request_context, abort
from werkzeug.utils import secure_filename
from werkzeug.security import generate_password_hash, check_password_hash

try:
    from proxmoxer import ProxmoxAPI  # type: ignore
except ImportError:  # pragma: no cover - optional dependency
    ProxmoxAPI = None  # type: ignore

try:
    import paramiko  # type: ignore
except ImportError:  # pragma: no cover - optional dependency
    paramiko = None  # type: ignore

try:
    from cryptography.fernet import Fernet, InvalidToken  # type: ignore
except ImportError:  # pragma: no cover - optional dependency
    Fernet = None  # type: ignore

    class InvalidToken(Exception):
        """Fallback InvalidToken if cryptography is unavailable."""
        pass
from lxml import etree as LET  # XML validation
ALLOWED_EXTENSIONS = {'xml'}


_SSE_MARKER_PREFIX = '__SSE_EVENT__'


def _write_sse_marker(log_handle, event: str, payload) -> None:
    """Write a marker line into the run log that /stream/<run_id> can translate
    into a typed SSE event.

    This avoids adding additional shared state between the runner thread and the
    streaming endpoint.
    """
    if log_handle is None:
        return
    try:
        safe_event = re.sub(r'[^a-zA-Z0-9_\-]+', '', str(event or 'phase')) or 'phase'
        data = json.dumps(payload or {}, ensure_ascii=False, separators=(',', ':'))
        log_handle.write(f"{_SSE_MARKER_PREFIX} {safe_event} {data}\n")
    except Exception:
        return


def _env_flag(name: str, default: bool = False) -> bool:
    value = os.environ.get(name)
    if value is None:
        return default
    return value.strip().lower() in {'1', 'true', 'yes', 'on'}


HITL_DISABLE_HOST_ENUM = _env_flag('HITL_DISABLE_HOST_ENUM', False)
NON_PHYSICAL_INTERFACE_NAMES = {
    'lo',
    'lo0',
    'loopback',
}
NON_PHYSICAL_INTERFACE_PREFIXES = (
    'awdl',
    'bridge',
    'br-',
    'docker',
    'gif',
    'ham',
    'llw',
    'p2p',
    'rmnet',
    'stf',
    'tap',
    'tailscale',
    'tun',
    'utun',
    'vboxnet',
    'veth',
    'virbr',
    'vmnet',
    'wg',
    'zt',
)
NON_PHYSICAL_INTERFACE_SUBSTRINGS = (
    'tailscale',
    'tunnel',
    'zerotier',
)

FULL_PREVIEW_ARTIFACT_VERSION = 2

_HITL_ATTACHMENT_ALLOWED = {
    "existing_router",
    "existing_switch",
    "new_router",
    "proxmox_vm",
}

_DEFAULT_HITL_ATTACHMENT = "existing_router"

_CORE_FIELD_KEYS = (
    'host',
    'port',
    'ssh_enabled',
    'ssh_host',
    'ssh_port',
    'ssh_username',
    'ssh_password',
    'venv_bin',
    'venv_user_override',
)

DEFAULT_CORE_VENV_BIN = '/opt/core/venv/bin'
CORE_DAEMON_START_COMMAND = 'sudo systemctl start core-daemon'
PYTHON_EXECUTABLE_NAMES = ('core-python', 'python3', 'python')
REMOTE_BASE_DIR_ENV = os.environ.get('CORE_REMOTE_BASE_DIR', '/tmp/core-topo-gen')
REMOTE_STATIC_REPO_ENV = os.environ.get('CORE_REMOTE_STATIC_REPO', '/tmp/core-topo-gen')
REMOTE_RUNS_SUBDIR = 'runs'
REMOTE_LOG_CHUNK_SIZE = 8192

REPO_PUSH_EXCLUDE_DIRS = {
    '.git',
    '.hg',
    '.svn',
    '.mypy_cache',
    '.pytest_cache',
    '.ruff_cache',
    '.venv',
    'venv',
    'env',
    '__pycache__',
    'node_modules',
    'dist',
    'build',
    'outputs',
    'uploads',
    'tmp_hitl',
}
REPO_PUSH_EXCLUDE_PATTERNS = ('*.pyc', '*.pyo', '*.pyd', '*.log', '*.tmp', '*.swp', '*.swo')

_SESSION_HITL_CACHE: Dict[str, Dict[str, Any]] = {}

_REPO_PUSH_PROGRESS: Dict[str, Dict[str, Any]] = {}
_REPO_PUSH_PROGRESS_LOCK = threading.Lock()
_REPO_PUSH_PROGRESS_TTL_SECONDS = 600.0

# Best-effort cancellation context for long-running remote repo finalize.
# Stored in-memory only; entries are removed when finalize completes/errors/cancels.
_REPO_PUSH_CANCEL_CTX: Dict[str, Dict[str, Any]] = {}
_REPO_PUSH_CANCEL_CTX_LOCK = threading.Lock()


def _set_repo_push_cancel_ctx(progress_id: Optional[str], ctx: Dict[str, Any]) -> None:
    if not progress_id:
        return
    with _REPO_PUSH_CANCEL_CTX_LOCK:
        _REPO_PUSH_CANCEL_CTX[progress_id] = dict(ctx or {})


def _get_repo_push_cancel_ctx(progress_id: Optional[str]) -> Optional[Dict[str, Any]]:
    if not progress_id:
        return None
    with _REPO_PUSH_CANCEL_CTX_LOCK:
        payload = _REPO_PUSH_CANCEL_CTX.get(progress_id)
        return dict(payload) if isinstance(payload, dict) else None


def _pop_repo_push_cancel_ctx(progress_id: Optional[str]) -> Optional[Dict[str, Any]]:
    if not progress_id:
        return None
    with _REPO_PUSH_CANCEL_CTX_LOCK:
        payload = _REPO_PUSH_CANCEL_CTX.pop(progress_id, None)
        return dict(payload) if isinstance(payload, dict) else None


def _schedule_repo_push_to_remote(progress_id: str, core_cfg: Dict[str, Any], *, logger: Optional[logging.Logger] = None) -> None:
    log = logger or getattr(app, 'logger', logging.getLogger(__name__))

    def _worker() -> None:
        try:
            # _push_repo_to_remote will update progress_id throughout packaging/uploading
            # and then queue remote finalization (also progress tracked).
            _push_repo_to_remote(
                core_cfg,
                logger=log,
                progress_id=progress_id,
                finalize_async=True,
            )
        except Exception as exc:
            try:
                log.exception('[core.push_repo] background sync failed: %s', exc)
            except Exception:
                pass
            _update_repo_push_progress(progress_id, status='error', stage='error', detail=str(exc))

    try:
        threading.Thread(target=_worker, daemon=True, name=f'core-repo-push-{progress_id[:8]}').start()
    except Exception as exc:
        _update_repo_push_progress(progress_id, status='error', stage='error', detail=f'Failed to schedule repo push: {exc}')


def _init_repo_push_progress(
    progress_id: str,
    *,
    stage: str,
    detail: str,
    status: str = 'initializing',
    percent: Optional[float] = None,
) -> None:
    now = time.time()
    payload = {
        'progress_id': progress_id,
        'status': status,
        'stage': stage,
        'detail': detail,
        'percent': percent,
        'created_at': now,
        'updated_at': now,
    }
    with _REPO_PUSH_PROGRESS_LOCK:
        _REPO_PUSH_PROGRESS[progress_id] = payload


def _update_repo_push_progress(progress_id: Optional[str], **fields: Any) -> None:
    if not progress_id:
        return
    now = time.time()
    with _REPO_PUSH_PROGRESS_LOCK:
        entry = _REPO_PUSH_PROGRESS.get(progress_id)
        if not entry:
            entry = {
                'progress_id': progress_id,
                'created_at': now,
            }
            _REPO_PUSH_PROGRESS[progress_id] = entry
        entry.update(fields)
        entry['updated_at'] = now


def _expire_repo_push_progress() -> None:
    cutoff = time.time() - _REPO_PUSH_PROGRESS_TTL_SECONDS
    with _REPO_PUSH_PROGRESS_LOCK:
        stale = [
            progress_id
            for progress_id, payload in _REPO_PUSH_PROGRESS.items()
            if payload.get('status') in ('complete', 'error') and payload.get('updated_at', 0) < cutoff
        ]
        for progress_id in stale:
            _REPO_PUSH_PROGRESS.pop(progress_id, None)


def _get_repo_push_progress(progress_id: str) -> Optional[Dict[str, Any]]:
    _expire_repo_push_progress()
    with _REPO_PUSH_PROGRESS_LOCK:
        payload = _REPO_PUSH_PROGRESS.get(progress_id)
        if not payload:
            return None
        return dict(payload)


def _sanitize_venv_bin_path(path_value: Any) -> Optional[str]:
    if path_value in (None, '', False):
        return None
    try:
        text = str(path_value)
    except Exception:
        return None
    candidate = os.path.expanduser(text.strip())
    if not candidate:
        return None
    sanitized = candidate.rstrip('/\\')
    return sanitized or None


def _find_python_in_venv_bin(bin_dir: str) -> Optional[str]:
    for exe_name in PYTHON_EXECUTABLE_NAMES:
        candidate = os.path.join(bin_dir, exe_name)
        try:
            if os.path.isfile(candidate) and os.access(candidate, os.X_OK):
                return candidate
        except Exception:
            continue
    return None


def _venv_is_explicit(core_cfg: Dict[str, Any], preferred_venv_bin: Optional[str]) -> bool:
    sanitized = _sanitize_venv_bin_path(preferred_venv_bin)
    if not sanitized:
        return False

        # If the session has a file path but it's not accessible locally (common when the
        # webapp is not running on the CORE VM), prefer fetching the current session XML via
        # gRPC rather than using any stored mapping-by-session-id (which can be stale).
        sid = session.get('id')
        sid_int: Optional[int] = None
        if sid not in (None, ''):
            try:
                sid_int = int(sid)
            except Exception:
                sid_int = None
        if core_cfg and sid_int is not None and file_path:
            try:
                # Only do this when the provided file path doesn't exist locally.
                fp = str(file_path)
                if fp and (not os.path.exists(fp)):
                    out_dir = os.path.join(_outputs_dir(), 'core-sessions')
                    saved = _grpc_save_current_session_xml_with_config(core_cfg, out_dir, session_id=str(sid_int))
                    if saved and os.path.exists(saved):
                        try:
                            _update_xml_session_mapping(
                                saved,
                                sid_int,
                                scenario_name=session.get('scenario_name') or None,
                                core_host=core_cfg.get('host', CORE_HOST) if isinstance(core_cfg, dict) else None,
                                core_port=core_cfg.get('port', CORE_PORT) if isinstance(core_cfg, dict) else None,
                            )
                        except Exception:
                            pass
                        session['file'] = saved
                        session['_hitl_source'] = 'grpc.save_xml'
                        return _hitl_details_from_path(saved)
            except Exception:
                pass
        store = session_store if isinstance(session_store, dict) else _load_core_sessions_store()


class _SSHTunnelError(RuntimeError):
    """Raised when SSH tunneling operations fail."""


class _ForwardServer(socketserver.ThreadingTCPServer):
    daemon_threads = True
    allow_reuse_address = True


class RemoteRepoMissingError(RuntimeError):
    """Raised when the remote CORE repo directory does not exist."""

    def __init__(self, repo_path: str):
        self.repo_path = repo_path
        super().__init__(f'Remote repo not found at {repo_path}')


class CoreDaemonError(RuntimeError):
    """Base exception for remote core-daemon orchestration failures."""


class CoreDaemonMissingError(CoreDaemonError):
    def __init__(
        self,
        message: str,
        *,
        can_auto_start: bool = False,
        start_command: Optional[str] = None,
    ):
        super().__init__(message)
        self.can_auto_start = can_auto_start
        self.start_command = start_command or CORE_DAEMON_START_COMMAND


class CoreDaemonConflictError(CoreDaemonError):
    def __init__(self, message: str, *, pids: Optional[List[int]] = None):
        super().__init__(message)
        self.pids = list(pids or [])


def _ensure_paramiko_available() -> None:
    if paramiko is None:  # pragma: no cover - dependency missing
        raise RuntimeError('SSH tunneling requires the paramiko package. Install paramiko and retry.')


def _make_forward_handler(transport: Any, remote_host: str, remote_port: int):
    class _ForwardHandler(socketserver.BaseRequestHandler):
        def handle(self) -> None:
            logger = getattr(app, 'logger', logging.getLogger(__name__))
            try:
                if transport is None or not transport.is_active():
                    raise _SSHTunnelError('SSH transport is not active')
                chan = transport.open_channel(
                    kind='direct-tcpip',
                    dest_addr=(remote_host, remote_port),
                    src_addr=self.request.getpeername(),
                )
            except Exception as exc:
                raise _SSHTunnelError(f'Failed to open SSH channel to {remote_host}:{remote_port}: {exc}') from exc
            if chan is None:
                raise _SSHTunnelError('SSH channel creation returned None')
            try:
                while True:
                    rlist, _, _ = select.select([self.request, chan], [], [])
                    if self.request in rlist:
                        data = self.request.recv(1024)
                        if not data:
                            break
                        chan.sendall(data)
                    if chan in rlist:
                        data = chan.recv(1024)
                        if not data:
                            break
                        self.request.sendall(data)
            finally:
                try:
                    chan.close()
                except Exception:
                    logger.debug('Failed closing SSH channel', exc_info=True)
                try:
                    self.request.close()
                except Exception:
                    logger.debug('Failed closing local socket', exc_info=True)

    return _ForwardHandler


class _SshTunnel:
    def __init__(
        self,
        *,
        ssh_host: str,
        ssh_port: int,
        username: str,
        password: str | None,
        remote_host: str,
        remote_port: int,
        timeout: float = 15.0,
    ) -> None:
        _ensure_paramiko_available()
        self.ssh_host = ssh_host
        self.ssh_port = ssh_port
        self.username = username
        self.password = password or ''
        self.remote_host = remote_host
        self.remote_port = remote_port
        self.timeout = max(1.0, float(timeout))
        self.client: Any | None = None
        self.transport: Any | None = None
        self.server: _ForwardServer | None = None
        self.thread: threading.Thread | None = None
        self.local_port: int | None = None

    def start(self) -> Tuple[str, int]:
        logger = getattr(app, 'logger', logging.getLogger(__name__))
        self.client = paramiko.SSHClient()  # type: ignore[assignment]
        self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
        try:
            self.client.connect(
                hostname=self.ssh_host,
                port=int(self.ssh_port),
                username=self.username,
                password=self.password,
                look_for_keys=False,
                allow_agent=False,
                timeout=self.timeout,
                banner_timeout=self.timeout,
                auth_timeout=self.timeout,
            )
        except Exception as exc:
            raise _SSHTunnelError(f'Failed to establish SSH connection to {self.ssh_host}:{self.ssh_port}: {exc}') from exc
        self.transport = self.client.get_transport()
        if self.transport is None or not self.transport.is_active():
            self.close()
            raise _SSHTunnelError('SSH transport unavailable after connect')
        try:
            self.transport.set_keepalive(30)
        except Exception:
            pass
        handler = _make_forward_handler(self.transport, self.remote_host, self.remote_port)
        try:
            self.server = _ForwardServer(('127.0.0.1', 0), handler)
        except Exception as exc:
            self.close()
            raise _SSHTunnelError(f'Failed to create local forwarding server: {exc}') from exc
        self.local_port = int(self.server.server_address[1])
        self.thread = threading.Thread(target=self.server.serve_forever, name='core-ssh-forward', daemon=True)
        self.thread.start()
        logger.info('Established SSH tunnel %s@%s:%s -> %s:%s (local %s)', self.username, self.ssh_host, self.ssh_port, self.remote_host, self.remote_port, self.local_port)
        return '127.0.0.1', self.local_port

    def close(self) -> None:
        logger = getattr(app, 'logger', logging.getLogger(__name__))
        if self.server:
            try:
                self.server.shutdown()
            except Exception:
                logger.debug('Tunnel server shutdown failed', exc_info=True)
            try:
                self.server.server_close()
            except Exception:
                logger.debug('Tunnel server close failed', exc_info=True)
        self.server = None
        if self.client:
            try:
                self.client.close()
            except Exception:
                logger.debug('SSH client close failed', exc_info=True)
        self.client = None
        self.transport = None
        self.thread = None
        self.local_port = None


class _RemoteProcessHandle:
    """Popen-like wrapper for an SSH channel running the CLI remotely."""

    def __init__(self, *, channel: Any, client: Any) -> None:
        self.channel = channel
        self.client = client
        self._returncode: Optional[int] = None
        self._cleanup_done = False
        self._lock = threading.Lock()
        self._output_thread: threading.Thread | None = None

    def attach_output_thread(self, thread: threading.Thread) -> None:
        self._output_thread = thread

    def poll(self) -> Optional[int]:
        if self._returncode is not None:
            return self._returncode
        if self.channel is None:
            return self._returncode
        try:
            if self.channel.exit_status_ready():
                self._returncode = self.channel.recv_exit_status()
                self._cleanup()
        except Exception:
            self._returncode = self._returncode or -1
            self._cleanup()
        return self._returncode

    def wait(self, timeout: float | None = None) -> int:
        start = time.time()
        while True:
            rc = self.poll()
            if rc is not None:
                return rc
            if timeout is not None and (time.time() - start) >= timeout:
                raise TimeoutError('Remote process wait timed out')
            time.sleep(0.2)

    def terminate(self) -> None:
        with self._lock:
            try:
                if self.channel and not self.channel.closed:
                    self.channel.close()
            except Exception:
                pass
            self._returncode = self._returncode if self._returncode is not None else -1
            self._cleanup()

    def kill(self) -> None:
        self.terminate()

    def _cleanup(self) -> None:
        if self._cleanup_done:
            return
        self._cleanup_done = True
        try:
            if self._output_thread and self._output_thread.is_alive():
                self._output_thread.join(timeout=2.0)
        except Exception:
            pass
        try:
            if self.client:
                self.client.close()
        except Exception:
            pass
        self.client = None
        self.channel = None


def _relay_remote_channel_to_log(channel: Any, log_handle: Any) -> None:
    """Stream bytes from an SSH channel into the local log file."""

    try:
        while True:
            try:
                if channel.recv_ready():
                    chunk = channel.recv(REMOTE_LOG_CHUNK_SIZE)
                elif channel.exit_status_ready():
                    chunk = channel.recv(REMOTE_LOG_CHUNK_SIZE)
                    if not chunk:
                        break
                else:
                    time.sleep(0.2)
                    continue
            except Exception:
                break
            if not chunk:
                if channel.exit_status_ready():
                    break
                continue
            try:
                text = chunk.decode('utf-8', 'replace')
            except Exception:
                text = chunk.decode('latin-1', 'replace')
            log_handle.write(text)
    finally:
        try:
            log_handle.flush()
        except Exception:
            pass


def _exec_ssh_command(
    client: Any,
    command: str,
    *,
    timeout: float | None = 120.0,
    cancel_check: Any = None,
    check: bool = False,
) -> tuple[int, str, str]:
    """Execute a command over SSH and capture stdout/stderr.

    Uses a wall-clock timeout to avoid hanging on blocking reads.
    When check=True, raises RuntimeError on non-zero exit codes.
    """

    if command:
        _append_core_ui_log('DEBUG', f'[ssh.exec] COMMAND START\n{command}')

    start = time.time()
    stdin, stdout, stderr = client.exec_command(command)
    channel = getattr(stdout, 'channel', None)

    stdout_chunks: list[bytes] = []
    stderr_chunks: list[bytes] = []

    try:
        if channel is not None:
            try:
                channel.settimeout(1.0)
            except Exception:
                pass
        while True:
            if channel is None:
                break
            try:
                if cancel_check is not None and bool(cancel_check()):
                    try:
                        if not channel.closed:
                            channel.close()
                    except Exception:
                        pass
                    raise TimeoutError('SSH command cancelled')
            except TimeoutError:
                raise
            except Exception:
                # Never allow cancel_check bugs to wedge execution.
                pass
            try:
                if channel.recv_ready():
                    stdout_chunks.append(channel.recv(REMOTE_LOG_CHUNK_SIZE))
                if channel.recv_stderr_ready():
                    stderr_chunks.append(channel.recv_stderr(REMOTE_LOG_CHUNK_SIZE))
            except Exception:
                # Don't wedge on transient read errors.
                pass

            try:
                if channel.exit_status_ready():
                    # Drain remaining buffered output then exit.
                    if not channel.recv_ready() and not channel.recv_stderr_ready():
                        break
            except Exception:
                break

            if timeout is not None:
                if (time.time() - start) >= max(0.1, float(timeout)):
                    try:
                        if channel is not None and not channel.closed:
                            channel.close()
                    except Exception:
                        pass
                    raise TimeoutError(f'SSH command timed out after {timeout:.0f}s')

            time.sleep(0.15)
    finally:
        try:
            stdin.close()
        except Exception:
            pass

    exit_code = 0
    try:
        if channel is not None:
            exit_code = int(channel.recv_exit_status())
    except Exception:
        exit_code = 0

    def _decode(blob: Any) -> str:
        if isinstance(blob, bytes):
            return blob.decode('utf-8', 'ignore')
        return str(blob or '')

    stdout_text = _decode(b''.join(stdout_chunks))
    stderr_text = _decode(b''.join(stderr_chunks))

    log_lines = [f'[ssh.exec] EXIT {exit_code}']
    stdout_tag = 'stdout (empty)' if not stdout_text else f'stdout:\n{stdout_text}'
    stderr_tag = 'stderr (empty)' if not stderr_text else f'stderr:\n{stderr_text}'
    log_lines.append(stdout_tag)
    log_lines.append(stderr_tag)
    log_level = 'WARN' if exit_code != 0 else 'DEBUG'
    _append_core_ui_log(log_level, '\n'.join(log_lines))

    if check and exit_code != 0:
        detail = stderr_text.strip() or stdout_text.strip() or f'Exit {exit_code}'
        raise RuntimeError(f'SSH command failed: {detail}')

    return exit_code, stdout_text, stderr_text


def _is_repo_push_cancel_requested(progress_id: Optional[str]) -> bool:
    if not progress_id:
        return False
    payload = _get_repo_push_progress(progress_id)
    if not payload:
        return False
    if payload.get('status') == 'cancelled':
        return True
    return bool(payload.get('cancel_requested'))


def _open_ssh_client(core_cfg: Dict[str, Any]) -> Any:
    cfg = _require_core_ssh_credentials(core_cfg)
    _ensure_paramiko_available()
    client = paramiko.SSHClient()  # type: ignore[assignment]
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
    client.connect(
        hostname=cfg.get('ssh_host') or cfg.get('host') or 'localhost',
        port=int(cfg.get('ssh_port') or 22),
        username=cfg.get('ssh_username'),
        password=cfg.get('ssh_password') or '',
        look_for_keys=False,
        allow_agent=False,
        timeout=30.0,
        banner_timeout=30.0,
        auth_timeout=30.0,
    )
    return client


def _remote_expand_path(sftp: Any, path: str | None) -> str:
    raw = (path or '').strip() or '.'
    try:
        if raw.startswith('~/'):
            home = sftp.normalize('.')
            return posixpath.normpath(posixpath.join(home, raw[2:]))
        if raw.startswith('~'):
            return posixpath.normpath(sftp.normalize(raw))
        if raw.startswith('/'):
            return posixpath.normpath(raw)
        home = sftp.normalize('.')
        return posixpath.normpath(posixpath.join(home, raw))
    except Exception:
        return raw


def _remote_base_dir(sftp: Any) -> str:
    return _remote_expand_path(sftp, REMOTE_BASE_DIR_ENV)


def _remote_static_repo_dir(sftp: Any) -> str:
    return _remote_expand_path(sftp, REMOTE_STATIC_REPO_ENV)


def _remote_path_join(*parts: str) -> str:
    cleaned = [p for p in parts if p not in (None, '', '.')]
    if not cleaned:
        return '/'
    return posixpath.normpath(posixpath.join(*cleaned))


def _remote_mkdirs(client: Any, path: str) -> None:
    quoted = shlex.quote(path)
    _exec_ssh_command(client, f"mkdir -p {quoted}")


def _remote_remove_path(client: Any, path: str) -> None:
    quoted = shlex.quote(path)
    _exec_ssh_command(client, f"rm -rf {quoted}")


def _should_exclude_repo_member(rel_path: str) -> bool:
    if not rel_path:
        return False
    parts = [part for part in Path(rel_path).parts if part not in ('', '.')]
    if not parts:
        return False
    for part in parts:
        if part in REPO_PUSH_EXCLUDE_DIRS:
            return True
    leaf = parts[-1]
    for pattern in REPO_PUSH_EXCLUDE_PATTERNS:
        if fnmatch.fnmatch(leaf, pattern):
            return True
    return False


def _create_local_repo_archive(src_dir: str, dest_basename: str) -> str:
    base_name = dest_basename.strip('/') or 'core-topo-gen'
    tmp_fd, tmp_path = tempfile.mkstemp(prefix='coretg_repo_', suffix='.tar.gz')
    os.close(tmp_fd)

    def _filter(member: tarfile.TarInfo) -> tarfile.TarInfo | None:
        rel_name = member.name
        prefix = f"{base_name}/"
        if rel_name.startswith(prefix):
            rel_name = rel_name[len(prefix):]
        elif rel_name == base_name:
            rel_name = ''
        if rel_name and _should_exclude_repo_member(rel_name):
            return None
        return member

    with tarfile.open(tmp_path, 'w:gz') as tar:
        tar.add(src_dir, arcname=base_name, filter=_filter)
    return tmp_path


def _push_repo_to_remote(
    core_cfg: Dict[str, Any],
    *,
    logger: Optional[logging.Logger] = None,
    progress_id: Optional[str] = None,
    finalize_async: bool = False,
) -> Dict[str, Any]:
    cfg = _require_core_ssh_credentials(core_cfg)
    repo_root = _get_repo_root()
    log = logger or getattr(app, 'logger', logging.getLogger(__name__))
    _update_repo_push_progress(progress_id, status='packaging', stage='packaging', percent=2.0, detail='Creating repository archive…')
    client = _open_ssh_client(cfg)
    sftp = None
    archive_path = None
    try:
        sftp = client.open_sftp()
        remote_repo = _remote_static_repo_dir(sftp)
        remote_parent = posixpath.dirname(remote_repo.rstrip('/')) or '/'
        base_name = os.path.basename(remote_repo.rstrip('/')) or 'core-topo-gen'
        archive_path = _create_local_repo_archive(repo_root, base_name)
        _update_repo_push_progress(progress_id, status='packaging', stage='packaging', percent=8.0, detail='Repository archive ready.')
        remote_archive = _remote_path_join(remote_parent, f"{uuid.uuid4().hex}.tar.gz")
        _update_repo_push_progress(progress_id, status='uploading', stage='uploading', percent=12.0, detail='Uploading snapshot to CORE host…')
        sftp.put(archive_path, remote_archive)
        _update_repo_push_progress(progress_id, status='uploading', stage='uploaded', percent=40.0, detail='Upload complete; preparing remote finalize…')
        if finalize_async:
            _update_repo_push_progress(progress_id, status='finalizing', stage='remote', percent=45.0, detail='Remote finalization queued…')
            _schedule_remote_repo_finalize(
                progress_id,
                cfg,
                remote_repo=remote_repo,
                remote_parent=remote_parent,
                remote_archive=remote_archive,
                logger=log,
            )
            return {'repo_path': remote_repo, 'progress_id': progress_id, 'finalizing': True}
        extract_script = (
            f"set -euo pipefail; mkdir -p {shlex.quote(remote_parent)}; "
            f"rm -rf {shlex.quote(remote_repo)}; "
            f"tar -xzf {shlex.quote(remote_archive)} -C {shlex.quote(remote_parent)}; "
            f"rm -f {shlex.quote(remote_archive)}"
        )
        _update_repo_push_progress(progress_id, status='finalizing', stage='remote', percent=60.0, detail='Extracting snapshot on CORE host…')
        _exec_ssh_command(client, f"bash -lc {shlex.quote(extract_script)}", timeout=None, check=True)
        _update_repo_push_progress(progress_id, status='finalizing', stage='remote', percent=95.0, detail='Cleaning temporary archive…')
        log.info('[remote-sync] Repository uploaded to %s', remote_repo)
        _update_repo_push_progress(progress_id, status='complete', stage='complete', percent=100.0, detail='Repository ready on remote host.')
        return {'repo_path': remote_repo, 'progress_id': progress_id}
    finally:
        try:
            if archive_path and os.path.exists(archive_path):
                os.remove(archive_path)
        except Exception:
            pass
        try:
            if sftp:
                sftp.close()
        except Exception:
            pass
        try:
            client.close()
        except Exception:
            pass


def _schedule_remote_repo_finalize(
    progress_id: Optional[str],
    core_cfg: Dict[str, Any],
    *,
    remote_repo: str,
    remote_parent: str,
    remote_archive: str,
    logger: Optional[logging.Logger] = None,
) -> None:
    if not progress_id:
        # No async tracking requested; fall back to synchronous finalize.
        extract_script = (
            f"set -euo pipefail; mkdir -p {shlex.quote(remote_parent)}; "
            f"rm -rf {shlex.quote(remote_repo)}; "
            f"tar -xzf {shlex.quote(remote_archive)} -C {shlex.quote(remote_parent)}; "
            f"rm -f {shlex.quote(remote_archive)}"
        )
        try:
            client = _open_ssh_client(core_cfg)
            try:
                _exec_ssh_command(client, f"bash -lc {shlex.quote(extract_script)}", timeout=None, check=True)
            finally:
                client.close()
        except Exception:
            pass
        return

    def _worker() -> None:
        client: Any | None = None
        pidfile: Optional[str] = None
        try:
            if _is_repo_push_cancel_requested(progress_id):
                _update_repo_push_progress(progress_id, status='cancelled', stage='cancelled', detail='Cancelled by user.')
                return
            _update_repo_push_progress(progress_id, status='finalizing', stage='cleanup', percent=55.0, detail='Removing previous repository…')
            client = _open_ssh_client(core_cfg)

            # Track the remote tar PID so a cancel request can kill it.
            try:
                pidfile = _remote_path_join(
                    posixpath.dirname(remote_archive.rstrip('/')) or remote_parent or '/',
                    f"coretg_finalize_{progress_id}.pid" if progress_id else f"coretg_finalize_{uuid.uuid4().hex}.pid",
                )
            except Exception:
                pidfile = _remote_path_join(remote_parent, f"coretg_finalize_{uuid.uuid4().hex}.pid")

            try:
                _set_repo_push_cancel_ctx(
                    progress_id,
                    {
                        'core_cfg': dict(core_cfg),
                        'remote_pidfile': pidfile,
                        'remote_archive': remote_archive,
                        'remote_parent': remote_parent,
                        'remote_repo': remote_repo,
                    },
                )
            except Exception:
                pass

            _exec_ssh_command(
                client,
                f"mkdir -p {shlex.quote(remote_parent)}",
                timeout=None,
                cancel_check=lambda: _is_repo_push_cancel_requested(progress_id),
                check=True,
            )
            _exec_ssh_command(
                client,
                f"rm -rf {shlex.quote(remote_repo)}",
                timeout=None,
                cancel_check=lambda: _is_repo_push_cancel_requested(progress_id),
                check=True,
            )
            _update_repo_push_progress(progress_id, status='finalizing', stage='extract', percent=75.0, detail='Extracting new snapshot on CORE host…')

            # Run tar under a tracked pidfile.
            tar_script = (
                "set -e; "
                f"pidfile={shlex.quote(pidfile or '')}; "
                "rm -f -- \"$pidfile\" 2>/dev/null || true; "
                f"( tar -xzf {shlex.quote(remote_archive)} -C {shlex.quote(remote_parent)} ) & "
                "pid=$!; "
                "echo \"$pid\" > \"$pidfile\"; "
                "wait \"$pid\"; "
                "rc=$?; "
                "rm -f -- \"$pidfile\" 2>/dev/null || true; "
                "exit \"$rc\""
            )
            _exec_ssh_command(
                client,
                f"sh -lc {shlex.quote(tar_script)}",
                timeout=None,
                cancel_check=lambda: _is_repo_push_cancel_requested(progress_id),
                check=True,
            )
            _update_repo_push_progress(progress_id, status='finalizing', stage='cleanup', percent=90.0, detail='Cleaning temporary archive…')
            _exec_ssh_command(
                client,
                f"rm -f {shlex.quote(remote_archive)}",
                timeout=None,
                cancel_check=lambda: _is_repo_push_cancel_requested(progress_id),
                check=True,
            )
            _update_repo_push_progress(progress_id, status='complete', stage='complete', percent=100.0, detail='Repository ready on remote host.')
            if logger:
                logger.info('[remote-sync] Repository finalized at %s', remote_repo)
        except TimeoutError as exc:
            # Used both for true timeouts and cooperative cancel.
            if 'cancelled' in str(exc).lower():
                _update_repo_push_progress(progress_id, status='cancelled', stage='cancelled', detail='Cancelled by user.')
                try:
                    if client:
                        _exec_ssh_command(client, f"rm -f {shlex.quote(remote_archive)}", timeout=30.0)
                except Exception:
                    pass
                return
            raise
        except Exception as exc:
            try:
                if _is_repo_push_cancel_requested(progress_id):
                    _update_repo_push_progress(progress_id, status='cancelled', stage='cancelled', detail='Cancelled by user.')
                    try:
                        if client and pidfile:
                            _exec_ssh_command(client, f"rm -f {shlex.quote(pidfile)}", timeout=10.0)
                    except Exception:
                        pass
                    return
            except Exception:
                pass
            if logger:
                logger.exception('[remote-sync] finalize failed: %s', exc)
            _update_repo_push_progress(progress_id, status='error', stage='error', detail=str(exc))
            try:
                if client:
                    _exec_ssh_command(client, f"rm -f {shlex.quote(remote_archive)}", timeout=60.0)
            except Exception:
                pass
        finally:
            try:
                _pop_repo_push_cancel_ctx(progress_id)
            except Exception:
                pass
            if client:
                try:
                    client.close()
                except Exception:
                    pass

    try:
        threading.Thread(target=_worker, daemon=True).start()
    except Exception:
        _update_repo_push_progress(progress_id, status='error', stage='error', detail='Failed to schedule remote finalization')


def _iter_values_by_key(obj: Any, keys: set[str]) -> Iterator[Any]:
    if isinstance(obj, dict):
        for k, v in obj.items():
            if k in keys:
                yield v
            yield from _iter_values_by_key(v, keys)
    elif isinstance(obj, list):
        for item in obj:
            yield from _iter_values_by_key(item, keys)


def _extract_flow_artifact_dirs_from_plan(preview_plan_path: str) -> List[str]:
    """Extract local artifact directories referenced by the plan.

    We currently support Flow flag generators by looking for `artifacts_dir` keys.
    """
    try:
        with open(preview_plan_path, 'r', encoding='utf-8') as f:
            payload = json.load(f)
    except Exception:
        return []

    full = payload.get('full_preview') if isinstance(payload, dict) else None
    root = full if isinstance(full, dict) else payload

    dirs: List[str] = []
    for v in _iter_values_by_key(root, {'artifacts_dir'}):
        if isinstance(v, str) and v:
            dirs.append(v)

    seen: set[str] = set()
    out: List[str] = []
    for d in dirs:
        if d not in seen:
            seen.add(d)
            out.append(d)
    return out


def _upload_flow_artifacts_for_plan_to_remote(
    *,
    client: Any,
    sftp: Any,
    preview_plan_path: str,
    log_handle: Any,
) -> None:
    """Upload any locally-generated flow artifacts to the CORE VM.

    The preview plan references absolute paths (typically under /tmp/vulns/...).
    When running the CLI on a remote CORE VM, those directories must exist on
    the remote filesystem or bind mounts / docker cp will see nothing.
    """
    artifact_dirs = _extract_flow_artifact_dirs_from_plan(preview_plan_path)
    if not artifact_dirs:
        try:
            log_handle.write('[remote] No flow artifact dirs referenced in preview plan\n')
        except Exception:
            pass
        return

    allowed_prefixes = ('/tmp/vulns',)
    upload_dirs = [d for d in artifact_dirs if any(d == p or d.startswith(p + '/') for p in allowed_prefixes)]
    skipped = [d for d in artifact_dirs if d not in upload_dirs]
    if skipped:
        try:
            log_handle.write(f"[remote] Skipping {len(skipped)} artifact dirs (outside /tmp/vulns)\n")
        except Exception:
            pass
    if not upload_dirs:
        return

    made_dirs: set[str] = set()
    copied_files = 0
    copied_bytes = 0
    for local_dir in upload_dirs:
        if not os.path.isdir(local_dir):
            try:
                log_handle.write(f"[remote] flow.artifacts.upload skip (missing): {local_dir}\n")
            except Exception:
                pass
            continue

        for root, dirs, files in os.walk(local_dir):
            rel = os.path.relpath(root, local_dir)
            rel = '' if rel == '.' else rel
            remote_root = local_dir if not rel else _remote_path_join(local_dir, rel)
            if remote_root not in made_dirs:
                try:
                    _remote_mkdirs(client, remote_root)
                    made_dirs.add(remote_root)
                except Exception:
                    pass

            for dn in dirs:
                rp_dir = _remote_path_join(remote_root, dn)
                if rp_dir in made_dirs:
                    continue
                try:
                    _remote_mkdirs(client, rp_dir)
                    made_dirs.add(rp_dir)
                except Exception:
                    pass

            for fn in files:
                lp = os.path.join(root, fn)
                if not os.path.isfile(lp):
                    continue
                rp = _remote_path_join(remote_root, fn)
                try:
                    sftp.put(lp, rp)
                    copied_files += 1
                    try:
                        copied_bytes += int(os.path.getsize(lp))
                    except Exception:
                        pass
                except Exception:
                    try:
                        log_handle.write(f"[remote] flow.artifacts.upload failed: {lp} -> {rp}\n")
                    except Exception:
                        pass

        try:
            log_handle.write(f"[remote] flow.artifacts.uploaded dir={local_dir}\n")
        except Exception:
            pass

    try:
        log_handle.write(f"[remote] flow.artifacts.upload complete files={copied_files} bytes={copied_bytes}\n")
    except Exception:
        pass


def _prepare_remote_cli_context(
    *,
    client: Any,
    run_id: str,
    xml_path: str,
    preview_plan_path: str | None,
    log_handle: Any,
) -> Dict[str, Any]:
    """Upload required artifacts before starting the remote CLI."""

    sftp = client.open_sftp()
    try:
        base_dir = _remote_base_dir(sftp)
        run_dir = _remote_path_join(base_dir, REMOTE_RUNS_SUBDIR, run_id)
        _remote_mkdirs(client, run_dir)
        repo_dir = _remote_static_repo_dir(sftp)
        try:
            sftp.stat(repo_dir)
        except Exception as exc:
            raise RemoteRepoMissingError(repo_dir) from exc
        package_dir = _remote_path_join(repo_dir, 'core_topo_gen')
        package_init = _remote_path_join(package_dir, '__init__.py')
        try:
            sftp.stat(package_dir)
            sftp.stat(package_init)
        except Exception as exc:
            raise RemoteRepoMissingError(repo_dir) from exc
        try:
            log_handle.write(f"[remote] Using repo at {repo_dir}\n")
        except Exception:
            pass
        # Ensure reports/outputs/uploads directories exist for CLI outputs
        for subdir in ('reports', 'outputs', 'uploads'):
            _remote_mkdirs(client, _remote_path_join(repo_dir, subdir))
        remote_xml_path = _remote_path_join(run_dir, os.path.basename(xml_path))
        sftp.put(xml_path, remote_xml_path)
        remote_preview_plan = None
        if preview_plan_path:
            remote_preview_plan = _remote_path_join(run_dir, os.path.basename(preview_plan_path))
            sftp.put(preview_plan_path, remote_preview_plan)
            # If the preview/flow plan references local /tmp/vulns artifact directories,
            # upload them to the CORE VM so the remote run can use them.
            _upload_flow_artifacts_for_plan_to_remote(
                client=client,
                sftp=sftp,
                preview_plan_path=preview_plan_path,
                log_handle=log_handle,
            )
        context = {
            'base_dir': base_dir,
            'run_dir': run_dir,
            'repo_dir': repo_dir,
            'xml_path': remote_xml_path,
            'preview_plan_path': remote_preview_plan,
        }
        return context
    finally:
        try:
            sftp.close()
        except Exception:
            pass


def _select_remote_python_interpreter(client: Any, core_cfg: Dict[str, Any]) -> str:
    candidates = _candidate_remote_python_interpreters(core_cfg)
    if not candidates:
        candidates = ['python3', 'python']
    last_error = None
    for candidate in candidates:
        cmd = f"{shlex.quote(candidate)} -V"
        code, _out, err = _exec_ssh_command(client, cmd)
        if code == 0:
            return candidate
        last_error = err or f"{candidate} unavailable"
    detail = f" ({last_error})" if last_error else ''
    raise RuntimeError(f"Unable to locate a python interpreter on the CORE host{detail}")


def _resolve_remote_artifact_path(
    sftp: Any,
    candidate: str,
    *,
    repo_dir: str | None,
    run_dir: str | None,
) -> str:
    path = (candidate or '').strip()
    if not path:
        return ''
    if path.startswith('/'):
        return posixpath.normpath(path)
    options: list[str] = []
    if repo_dir:
        options.append(_remote_path_join(repo_dir, path))
    if run_dir:
        options.append(_remote_path_join(run_dir, path))
    options.append(posixpath.normpath(path))
    for option in options:
        try:
            sftp.stat(option)
            return posixpath.normpath(option)
        except Exception:
            continue
    return posixpath.normpath(options[0])


def _sync_remote_artifacts(meta: Dict[str, Any]) -> None:
    if not meta.get('remote'):
        return
    if meta.get('remote_artifacts_synced'):
        return
    log_path = meta.get('log_path')
    if not log_path or not os.path.exists(log_path):
        return
    try:
        with open(log_path, 'r', encoding='utf-8', errors='ignore') as fh:
            log_text = fh.read()
    except Exception:
        return
    remote_report = _extract_report_path_from_text(log_text, require_exists=False)
    remote_summary = _extract_summary_path_from_text(log_text, require_exists=False)
    if not remote_report and not remote_summary:
        meta['remote_artifacts_synced'] = True
        return
    core_cfg = meta.get('core_cfg') if isinstance(meta.get('core_cfg'), dict) else None
    if not core_cfg:
        return
    try:
        client = _open_ssh_client(core_cfg)
        sftp = client.open_sftp()
    except Exception as exc:
        try:
            app.logger.warning('[remote-sync] SSH connect failed: %s', exc)
        except Exception:
            pass
        return
    downloaded: list[tuple[str, str]] = []
    try:
        repo_dir = meta.get('remote_repo_dir')
        run_dir = meta.get('remote_run_dir')
        if remote_report:
            resolved_report = _resolve_remote_artifact_path(sftp, remote_report, repo_dir=repo_dir, run_dir=run_dir)
            local_report = os.path.join(_reports_dir(), os.path.basename(resolved_report))
            sftp.get(resolved_report, local_report)
            downloaded.append(('report', local_report))
        if remote_summary:
            resolved_summary = _resolve_remote_artifact_path(sftp, remote_summary, repo_dir=repo_dir, run_dir=run_dir)
            local_summary = os.path.join(_reports_dir(), os.path.basename(resolved_summary))
            sftp.get(resolved_summary, local_summary)
            downloaded.append(('summary', local_summary))
    except Exception as exc:
        try:
            app.logger.warning('[remote-sync] Failed copying artifacts: %s', exc)
        except Exception:
            pass
        return
    finally:
        try:
            sftp.close()
        except Exception:
            pass
        try:
            client.close()
        except Exception:
            pass
    if downloaded:
        try:
            with open(log_path, 'a', encoding='utf-8') as fh:
                for kind, local_path in downloaded:
                    if kind == 'report':
                        fh.write(f"Scenario report written to {local_path}\n")
                    elif kind == 'summary':
                        fh.write(f"Scenario summary written to {local_path}\n")
        except Exception:
            pass
    meta['remote_artifacts_synced'] = True


def _cleanup_remote_workspace(meta: Dict[str, Any]) -> None:
    if not meta.get('remote'):
        return
    if meta.get('remote_workspace_cleaned'):
        return
    run_dir = meta.get('remote_run_dir')
    if not run_dir:
        meta['remote_workspace_cleaned'] = True
        return
    core_cfg = meta.get('core_cfg') if isinstance(meta.get('core_cfg'), dict) else None
    if not core_cfg:
        return
    try:
        client = _open_ssh_client(core_cfg)
    except Exception:
        return
    try:
        _remote_remove_path(client, run_dir)
    except Exception:
        pass
    finally:
        try:
            client.close()
        except Exception:
            pass
    meta['remote_workspace_cleaned'] = True

@contextlib.contextmanager
def _core_connection_via_ssh(core_cfg: Dict[str, Any]) -> Iterator[Tuple[str, int]]:
    tunnel: _SshTunnel | None = None
    try:
        tunnel = _SshTunnel(
            ssh_host=str(core_cfg.get('ssh_host') or core_cfg.get('host') or 'localhost'),
            ssh_port=int(core_cfg.get('ssh_port') or 22),
            username=str(core_cfg.get('ssh_username') or ''),
            password=core_cfg.get('ssh_password'),
            remote_host=str(core_cfg.get('host') or 'localhost'),
            remote_port=int(core_cfg.get('port') or 50051),
        )
        host, port = tunnel.start()
        yield host, port
    finally:
        if tunnel:
            tunnel.close()


def _coerce_bool(value: Any) -> bool:
    if isinstance(value, bool):
        return value
    if isinstance(value, (int, float)):
        return bool(value)
    if isinstance(value, str):
        return value.strip().lower() in {'1', 'true', 'yes', 'on', 'y'}
    return False


def _normalize_core_config(raw: Any, *, include_password: bool = True) -> Dict[str, Any]:
    base = raw if isinstance(raw, dict) else {}
    nested = base.get('ssh') if isinstance(base.get('ssh'), dict) else {}
    host = str(base.get('host') or CORE_HOST or 'localhost')
    try:
        port = int(base.get('port') if base.get('port') not in (None, '') else CORE_PORT)
    except Exception:
        port = CORE_PORT
    ssh_enabled = True
    ssh_host = str(base.get('ssh_host') or nested.get('host') or host)
    try:
        ssh_port = int(base.get('ssh_port') if base.get('ssh_port') not in (None, '') else (nested.get('port') or 22))
    except Exception:
        ssh_port = 22
    ssh_username = str(base.get('ssh_username') or nested.get('username') or '')
    ssh_password_val: str | None = None
    if include_password:
        pw_source = base.get('ssh_password')
        if pw_source in (None, '') and isinstance(nested, dict):
            pw_source = nested.get('password')
        ssh_password_val = str(pw_source) if pw_source not in (None, '') else ''
    env_override_raw = os.environ.get('CORE_REMOTE_VENV_BIN')
    env_override = _sanitize_venv_bin_path(env_override_raw)
    default_venv = _sanitize_venv_bin_path(DEFAULT_CORE_VENV_BIN)
    venv_candidate = base.get('venv_bin')
    if venv_candidate in (None, '') and isinstance(nested, dict):
        venv_candidate = nested.get('venv_bin') or nested.get('core_venv_bin')
    if venv_candidate in (None, ''):
        venv_candidate = base.get('core_venv_bin')
    if venv_candidate in (None, ''):
        venv_candidate = env_override or DEFAULT_CORE_VENV_BIN
    venv_bin = _sanitize_venv_bin_path(venv_candidate) or default_venv or DEFAULT_CORE_VENV_BIN
    venv_user_override = bool(base.get('venv_user_override'))
    if not venv_user_override and isinstance(nested, dict):
        venv_user_override = bool(nested.get('venv_user_override'))
    if not venv_user_override:
        if env_override and venv_bin == env_override:
            venv_user_override = True
        elif default_venv and venv_bin == default_venv:
            venv_user_override = False
        else:
            venv_user_override = bool(venv_bin and default_venv and venv_bin != default_venv)
    cfg: Dict[str, Any] = {
        'host': host,
        'port': port,
        'ssh_enabled': ssh_enabled,
        'ssh_host': ssh_host,
        'ssh_port': ssh_port,
        'ssh_username': ssh_username,
        'venv_bin': venv_bin,
        'venv_user_override': bool(venv_user_override),
    }
    if include_password:
        cfg['ssh_password'] = ssh_password_val or ''
    return cfg


def _extract_optional_core_config(raw: Any, *, include_password: bool = False) -> Dict[str, Any] | None:
    if not isinstance(raw, dict):
        return None
    candidate = dict(raw)

    def _has_value(val: Any) -> bool:
        if val is None:
            return False
        if isinstance(val, str):
            return val.strip() != ''
        return True

    significant_keys = ('host', 'port', 'ssh_host', 'ssh_port', 'ssh_username')
    metadata_signal_keys = ('vm_key', 'vm_name', 'vm_node', 'core_secret_id')
    has_signal = any(_has_value(candidate.get(key)) for key in significant_keys)
    if not has_signal:
        nested = candidate.get('ssh') if isinstance(candidate.get('ssh'), dict) else {}
        has_signal = any(_has_value(nested.get(key)) for key in ('host', 'port', 'username', 'password'))
    if not has_signal:
        has_signal = any(_has_value(candidate.get(key)) for key in metadata_signal_keys)
    if include_password and not has_signal:
        has_signal = _has_value(candidate.get('ssh_password')) or (
            isinstance(candidate.get('ssh'), dict) and _has_value(candidate['ssh'].get('password'))
        )
    if not has_signal:
        return None

    provided_fields: Dict[str, Any] = {
        key: candidate.get(key)
        for key in ('host', 'port', 'ssh_host', 'ssh_port', 'ssh_username', 'venv_bin', 'venv_user_override')
    }
    if include_password:
        provided_fields['ssh_password'] = candidate.get('ssh_password')

    normalized = _normalize_core_config(candidate, include_password=include_password)
    extras: Dict[str, Any] = {}
    for key, value in candidate.items():
        if key in {'host', 'port', 'ssh_enabled', 'ssh_host', 'ssh_port', 'ssh_username', 'ssh_password', 'venv_bin', 'venv_user_override', 'ssh'}:
            continue
        extras[key] = value
    for field, original in provided_fields.items():
        if field == 'ssh_password' and not include_password:
            continue
        if not _has_value(original):
            normalized.pop(field, None)
    if extras:
        normalized.update(extras)
    if not include_password:
        normalized.pop('ssh_password', None)
    if not normalized and extras:
        normalized = extras
    return normalized

def _scrub_scenario_core_config(raw: Any) -> Dict[str, Any] | None:
    """Remove sensitive fields while preserving VM metadata for history/logging."""

    if not isinstance(raw, dict):
        return None
    cleaned: Dict[str, Any] = {}
    for key, value in raw.items():
        if key == 'ssh_password':
            continue
        if key == 'ssh' and isinstance(value, dict):
            nested = {k: v for k, v in value.items() if k != 'password'}
            if nested:
                cleaned['ssh'] = nested
            continue
        cleaned[key] = value
    normalized = _normalize_core_config(cleaned, include_password=False)
    extras: Dict[str, Any] = {}
    for key, value in cleaned.items():
        if key in _CORE_FIELD_KEYS:
            continue
        extras[key] = value
    if extras:
        normalized = dict(normalized)
        normalized.update(extras)
    return normalized


def _merge_core_configs(*configs: Any, include_password: bool = True) -> Dict[str, Any]:
    base = _core_backend_defaults(include_password=include_password)
    merged = dict(base)
    extras: Dict[str, Any] = {}
    core_secret_id: Optional[str] = None
    for cfg in configs:
        if cfg is None:
            continue
        normalized = _extract_optional_core_config(cfg, include_password=include_password)
        if not normalized:
            # Password-only override should still apply when include_password requested
            if include_password and isinstance(cfg, dict) and cfg.get('ssh_password') not in (None, ''):
                normalized = {'ssh_password': cfg.get('ssh_password')}
            else:
                continue
        for key, value in normalized.items():
            if key in _CORE_FIELD_KEYS:
                merged[key] = value
            elif key == 'core_secret_id' and value not in (None, ''):
                core_secret_id = str(value)
                extras['core_secret_id'] = core_secret_id
            else:
                extras[key] = value
    if include_password and (not merged.get('ssh_password')) and core_secret_id:
        try:
            secret_record = _load_core_credentials(core_secret_id)
        except RuntimeError:
            secret_record = None
        if secret_record:
            password_plain = secret_record.get('ssh_password_plain') or secret_record.get('password_plain') or ''
            if password_plain and not merged.get('ssh_password'):
                merged['ssh_password'] = password_plain
            for field in ('host', 'port', 'grpc_host', 'grpc_port', 'ssh_host', 'ssh_port', 'ssh_username', 'venv_bin'):
                stored_val = secret_record.get(field)
                if stored_val in (None, ''):
                    continue
                target_field = 'host' if field == 'grpc_host' else 'port' if field == 'grpc_port' else field
                if target_field in {'host', 'port'}:
                    if not merged.get(target_field):
                        merged[target_field] = stored_val
                else:
                    if merged.get(target_field) in (None, '', 0):
                        merged[target_field] = stored_val
            extras.setdefault('core_secret_id', core_secret_id)
            for meta_field in ('vm_key', 'vm_name', 'vm_node', 'vmid', 'proxmox_secret_id', 'proxmox_target'):
                if meta_field in extras:
                    continue
                stored_meta_val = secret_record.get(meta_field)
                if stored_meta_val in (None, ''):
                    continue
                extras[meta_field] = stored_meta_val
    merged = _normalize_core_config(merged, include_password=include_password)
    if extras:
        merged.update(extras)
    return merged


def _core_backend_defaults(*, include_password: bool = True) -> Dict[str, Any]:
    env_cfg = {
        'host': os.environ.get('CORE_HOST', CORE_HOST),
        'port': os.environ.get('CORE_PORT', CORE_PORT),
        'ssh_enabled': os.environ.get('CORE_SSH_ENABLED'),
        'ssh_host': os.environ.get('CORE_SSH_HOST'),
        'ssh_port': os.environ.get('CORE_SSH_PORT'),
        'ssh_username': os.environ.get('CORE_SSH_USERNAME'),
    }
    if include_password:
        env_cfg['ssh_password'] = os.environ.get('CORE_SSH_PASSWORD')
    cfg = _normalize_core_config(env_cfg, include_password=include_password)
    cfg['ssh_enabled'] = True
    return cfg


def _normalize_history_core_value(raw: Any) -> Dict[str, Any]:
    if isinstance(raw, dict):
        normalized = _extract_optional_core_config(raw, include_password=False)
        if normalized is not None:
            return normalized
        return _normalize_core_config({}, include_password=False)
    if isinstance(raw, str):
        text = raw.strip()
        if not text:
            return _normalize_core_config({}, include_password=False)
        try:
            parsed = json.loads(text)
            if isinstance(parsed, dict):
                return _normalize_core_config(parsed, include_password=False)
        except Exception:
            pass
        host_match = re.search(r'([A-Za-z0-9_.-]+)\s*:\s*(\d+)', text)
        host = host_match.group(1) if host_match else None
        try:
            port = int(host_match.group(2)) if host_match else None
        except Exception:
            port = None
        lower = text.lower()
        data: Dict[str, Any] = {}
        if host:
            data['host'] = host
        if port:
            data['port'] = port
        if 'ssh' in lower:
            disabled_markers = ('disabled', 'off', 'false', '0', 'no')
            ssh_enabled = not any(marker in lower for marker in disabled_markers)
            data['ssh_enabled'] = ssh_enabled
        return _normalize_core_config(data, include_password=False)
    return _normalize_core_config({}, include_password=False)


def _normalize_run_history_entry(entry: Any) -> Dict[str, Any]:
    if not isinstance(entry, dict):
        return {}
    normalized = dict(entry)

    # Per-scenario invariant: run history entries represent a single scenario.
    # Prefer explicit scenario_name; otherwise fall back to scenario_names[0].
    try:
        scenario_name = normalized.get('scenario_name')
        if isinstance(scenario_name, str):
            scenario_name = scenario_name.strip()
        else:
            scenario_name = ''
    except Exception:
        scenario_name = ''
    if not scenario_name:
        try:
            names = normalized.get('scenario_names')
            if isinstance(names, list) and names:
                first = names[0]
                scenario_name = first.strip() if isinstance(first, str) else str(first).strip()
        except Exception:
            scenario_name = ''
    if scenario_name:
        normalized['scenario_name'] = scenario_name
        normalized['scenario_names'] = [scenario_name]
    else:
        # Normalize legacy non-list forms and then truncate to one.
        sn = normalized.get('scenario_names')
        if not isinstance(sn, list):
            if sn is None:
                sn_list: list[str] = []
            elif isinstance(sn, str):
                if '||' in sn:
                    sn_list = [s for s in sn.split('||') if s]
                else:
                    sn_list = [s.strip() for s in sn.split(',') if s.strip()]
            else:
                sn_list = []
            normalized['scenario_names'] = sn_list
        if isinstance(normalized.get('scenario_names'), list) and len(normalized['scenario_names']) > 1:
            normalized['scenario_names'] = [normalized['scenario_names'][0]]
    try:
        normalized['core'] = _normalize_history_core_value(normalized.get('core'))
    except Exception:
        normalized['core'] = _normalize_core_config({}, include_password=False)
    if 'core_cfg_public' in normalized:
        try:
            normalized['core_cfg_public'] = _normalize_history_core_value(normalized.get('core_cfg_public'))
        except Exception:
            normalized['core_cfg_public'] = _normalize_core_config({}, include_password=False)
    if 'scenario_core' in normalized and normalized.get('scenario_core') is not None:
        try:
            normalized['scenario_core'] = _normalize_history_core_value(normalized.get('scenario_core'))
        except Exception:
            normalized['scenario_core'] = _normalize_core_config({}, include_password=False)
    return normalized


def _require_core_ssh_credentials(core_cfg: Dict[str, Any]) -> Dict[str, Any]:
    """Normalize and validate CORE SSH configuration, enforcing credential presence."""

    source = core_cfg if isinstance(core_cfg, dict) else {}
    extras: Dict[str, Any] = {}
    if isinstance(source, dict):
        for key, value in source.items():
            if key in _CORE_FIELD_KEYS or key in {'ssh', 'ssh_password'}:
                continue
            extras[key] = value

    cfg = _normalize_core_config(source, include_password=True)
    # All backend interactions with the CORE daemon **must** traverse an SSH tunnel.
    # This helper normalizes and validates the credentials up-front so callers do
    # not accidentally bypass the tunnel when invoking gRPC helpers or CLI flows.
    username = str(cfg.get('ssh_username') or '').strip()
    if not username:
        raise _SSHTunnelError('SSH username is required (SSH tunnel enforced for CORE gRPC)')
    password_raw = cfg.get('ssh_password')
    if password_raw is None:
        password = ''
    elif isinstance(password_raw, str):
        password = password_raw.strip()
    else:
        password = str(password_raw).strip()
    if not password:
        raise _SSHTunnelError('SSH password is required (SSH tunnel enforced for CORE gRPC)')
    cfg['ssh_username'] = username
    cfg['ssh_password'] = password
    if extras:
        cfg.update(extras)
    return cfg


@contextlib.contextmanager
def _core_connection(core_cfg: Dict[str, Any]) -> Iterator[Tuple[str, int]]:
    cfg = _require_core_ssh_credentials(core_cfg)
    logger = getattr(app, 'logger', logging.getLogger(__name__))
    logger.info('Opening CORE SSH tunnel: %s@%s:%s → %s:%s', cfg['ssh_username'], cfg['ssh_host'], cfg['ssh_port'], cfg['host'], cfg['port'])
    with _core_connection_via_ssh(cfg) as forwarded:
        yield forwarded


def _build_python_probe_command(host: str, port: int, timeout: float, *, interpreter: str = 'python3') -> str:
    host_literal = json.dumps(host)
    interpreter_safe = shlex.quote(interpreter)
    script = textwrap.dedent(
        f"""{interpreter_safe} - <<'PY'
import socket
import sys
host = {host_literal}
port = {int(port)}
timeout = {timeout:.2f}
try:
    sock = socket.create_connection((host, port), timeout=timeout)
except Exception as exc:
    print("ERROR:", exc)
    sys.exit(1)
else:
    sock.close()
    print("OK")
PY
"""
    )
    return script


def _candidate_remote_python_interpreters(core_cfg: Dict[str, Any]) -> List[str]:
    venv_bin = str(core_cfg.get('venv_bin') or '').strip()
    candidates: List[str] = []
    if venv_bin:
        sanitized = venv_bin.rstrip('/\\')
        if sanitized:
            candidates.append(os.path.join(sanitized, 'python3'))
            candidates.append(os.path.join(sanitized, 'python'))
    candidates.append('core-python')
    candidates.extend(['python3', 'python'])
    ordered: List[str] = []
    seen: set[str] = set()
    for entry in candidates:
        normalized = str(entry or '').strip()
        if not normalized or normalized in seen:
            continue
        seen.add(normalized)
        ordered.append(normalized)
    return ordered


def _compose_remote_python_command(interpreter: str, script: str, activate_path: Optional[str] = None) -> str:
    activation = ''
    if activate_path:
        activation = f". {shlex.quote(activate_path)} >/dev/null 2>&1 || true; "
    return f"{activation}{shlex.quote(interpreter)} - <<'PY'\n{script}\nPY"


def _summarize_for_log(text: str, *, limit: int = 320, collapse_ws: bool = True) -> str:
    if not text:
        return ''
    sample = text.strip()
    if not sample:
        return ''
    if collapse_ws:
        sample = re.sub(r'\s+', ' ', sample)
    if len(sample) > limit:
        sample = sample[:limit - 1].rstrip() + '…'
    return sample


def _current_core_ui_logs() -> list:
    if not has_request_context():
        return []
    try:
        return list(getattr(g, 'core_ui_logs', []) or [])
    except Exception:
        return []


def _append_core_ui_log(level: str, message: str) -> None:
    lvl = (level or 'INFO').upper()
    msg = message if isinstance(message, str) else str(message)
    try:
        logger = getattr(app, 'logger', logging.getLogger(__name__))
    except Exception:
        logger = logging.getLogger(__name__)
    log_fn = getattr(logger, lvl.lower(), logger.info)
    try:
        log_fn(msg)
    except Exception:
        logger.info(msg)
    if not has_request_context():
        return
    try:
        buf = getattr(g, 'core_ui_logs', None)
        if buf is None:
            buf = []
            g.core_ui_logs = buf
        buf.append({'level': lvl, 'message': msg})
    except Exception:
        pass


def _remote_core_sessions_script(address: str) -> str:
    address_literal = json.dumps(address)
    template = textwrap.dedent(
        """
import json
import traceback

from core.api.grpc.client import CoreGrpcClient

ADDRESS = __ADDRESS_LITERAL__


def serialize(session):
    state_obj = getattr(session, 'state', None)
    state = getattr(state_obj, 'name', None) if state_obj is not None else None
    if state is None:
        state = state_obj or getattr(session, 'state', None)
    return {
        'id': getattr(session, 'id', None),
        'state': state,
        'nodes': getattr(session, 'nodes', None),
        'file': getattr(session, 'file', None),
        'dir': getattr(session, 'dir', None),
    }


def main():
    result = {'sessions': [], 'error': None, 'traceback': None}
    client = None
    try:
        client = CoreGrpcClient(address=ADDRESS)
        client.connect()
        raw_sessions = client.get_sessions()
        items = []
        for sess in raw_sessions:
            entry = serialize(sess)
            sid = entry.get('id')
            needs_nodes = entry.get('nodes') in (None, 0)
            if sid is not None and needs_nodes:
                getter = getattr(client, 'get_nodes', None)
                if callable(getter):
                    try:
                        nodes = getter(int(sid))
                        if nodes is not None:
                            entry['nodes'] = len(nodes)
                    except Exception:
                        pass
            items.append(entry)
        result['sessions'] = items
    except Exception as exc:
        result['error'] = f"{exc.__class__.__name__}: {exc}"
        result['traceback'] = traceback.format_exc()
    finally:
        try:
            if client:
                client.close()
        except Exception:
            pass
    print(json.dumps(result))


if __name__ == '__main__':
    main()
"""
    )
    return template.replace('__ADDRESS_LITERAL__', address_literal)


def _remote_core_open_xml_script(address: str, xml_path: str, auto_start: bool = True) -> str:
    address_literal = json.dumps(address)
    xml_literal = json.dumps(xml_path)
    auto_literal = 'True' if auto_start else 'False'
    template = textwrap.dedent(
        """
import json
import traceback
from pathlib import Path

from core.api.grpc.client import CoreGrpcClient
try:
    from core.api.grpc import core_pb2 as _core_pb2
except Exception:
    _core_pb2 = None


class _SessionShim:
    __slots__ = ('id', 'session_id')

    def __init__(self, session_id):
        sid = int(session_id)
        self.id = sid
        self.session_id = sid

    def to_proto(self):
        if _core_pb2 is None:
            raise AttributeError('core_pb2 unavailable')
        msg = _core_pb2.Session()
        msg.id = int(self.id)
        return msg


def _start_session(client, session_id):
    session = None
    try:
        opener = getattr(client, 'open_session', None)
        if callable(opener):
            maybe = opener(int(session_id))
            if maybe is not None:
                session = maybe
    except Exception:
        pass
    if session is None:
        try:
            sessions = client.get_sessions()
            for sess in sessions or []:
                sid = getattr(sess, 'id', None) or getattr(sess, 'session_id', None)
                if sid is not None and int(sid) == int(session_id):
                    session = sess
                    break
        except Exception:
            session = None
    if session is not None:
        try:
            client.start_session(session=session)
            return
        except TypeError:
            pass
        try:
            client.start_session(session)
            return
        except TypeError:
            pass
    shim = _SessionShim(session_id)
    try:
        client.start_session(session=shim)
        return
    except Exception:
        pass
    try:
        client.start_session(shim)
        return
    except Exception:
        pass
    client.start_session(session_id)


def main():
    payload = {}
    client = None
    try:
        client = CoreGrpcClient(address=__ADDRESS_LITERAL__)
        connector = getattr(client, 'connect', None)
        if callable(connector):
            connector()
        success, session_id = client.open_xml(Path(__XML_PATH_LITERAL__), start=False)
        payload['result'] = bool(success)
        if session_id is not None:
            payload['session_id'] = int(session_id)
        if __AUTO_START_LITERAL__ and session_id is not None and success:
            _start_session(client, int(session_id))
    except Exception as exc:
        payload['error'] = str(exc)
        payload['traceback'] = traceback.format_exc()
    finally:
        try:
            if client is not None:
                closer = getattr(client, 'close', None)
                if callable(closer):
                    closer()
        except Exception:
            pass
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    )
    script = template.replace('__ADDRESS_LITERAL__', address_literal)
    script = script.replace('__XML_PATH_LITERAL__', xml_literal)
    script = script.replace('__AUTO_START_LITERAL__', auto_literal)
    return script


def _remote_core_session_action_script(address: str, action: str, session_id: int) -> str:
    address_literal = json.dumps(address)
    session_literal = json.dumps(int(session_id))
    action_literal = json.dumps(action)
    template = textwrap.dedent(
        """
import json
import traceback

from core.api.grpc.client import CoreGrpcClient
try:
    from core.api.grpc import core_pb2 as _core_pb2
except Exception:
    _core_pb2 = None


def _session_identifier(item):
    return getattr(item, 'id', None) or getattr(item, 'session_id', None)


class _SessionShim:
    __slots__ = ('id', 'session_id')

    def __init__(self, session_id):
        sid = int(session_id)
        self.id = sid
        self.session_id = sid

    def to_proto(self):
        if _core_pb2 is None:
            raise AttributeError('core_pb2 unavailable')
        msg = _core_pb2.Session()
        msg.id = int(self.id)
        return msg


def _resolve_session(client, session_id):
    try:
        opener = getattr(client, 'open_session', None)
        if callable(opener):
            session = opener(int(session_id))
            if session is not None:
                return session
    except Exception:
        pass
    try:
        sessions = client.get_sessions()
        for sess in sessions or []:
            sid = _session_identifier(sess)
            if sid is not None and int(sid) == int(session_id):
                return sess
    except Exception:
        pass
    return None


def _start_session(client, session_id):
    session = _resolve_session(client, session_id)
    if session is not None:
        try:
            client.start_session(session=session)
            return
        except TypeError:
            pass
        try:
            client.start_session(session)
            return
        except TypeError:
            pass
    shim = _SessionShim(session_id)
    try:
        client.start_session(session=shim)
        return
    except Exception:
        pass
    try:
        client.start_session(shim)
        return
    except Exception:
        pass
    client.start_session(session_id)


def main():
    payload = {}
    client = None
    try:
        client = CoreGrpcClient(address=__ADDRESS_LITERAL__)
        connector = getattr(client, 'connect', None)
        if callable(connector):
            connector()
        session_id = int(__SESSION_LITERAL__)
        action = __ACTION_LITERAL__
        if action == 'start':
            _start_session(client, session_id)
        elif action == 'stop':
            client.stop_session(session_id)
        elif action == 'delete':
            client.delete_session(session_id)
        else:
            raise ValueError(f'Unsupported action: {action}')
        payload['status'] = 'ok'
        payload['session_id'] = session_id
    except Exception as exc:
        payload['error'] = str(exc)
        payload['traceback'] = traceback.format_exc()
    finally:
        try:
            if client is not None:
                closer = getattr(client, 'close', None)
                if callable(closer):
                    closer()
        except Exception:
            pass
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    )
    script = template.replace('__ADDRESS_LITERAL__', address_literal)
    script = script.replace('__SESSION_LITERAL__', session_literal)
    script = script.replace('__ACTION_LITERAL__', action_literal)
    return script


def _remote_core_save_xml_script(address: str, session_id: Optional[str], dest_path: str) -> str:
    address_literal = json.dumps(address)
    dest_literal = json.dumps(dest_path)
    session_literal = 'None' if session_id is None else json.dumps(str(session_id))
    template = textwrap.dedent(
        """
import json
import traceback
from pathlib import Path

from core.api.grpc.client import CoreGrpcClient


def _session_identifier(item):
    return getattr(item, 'id', None) or getattr(item, 'session_id', None)


def main():
    payload = {}
    client = None
    try:
        client = CoreGrpcClient(address=__ADDRESS_LITERAL__)
        connector = getattr(client, 'connect', None)
        if callable(connector):
            connector()
        sessions = client.get_sessions() or []
        if not sessions:
            raise RuntimeError('No CORE sessions available')
        requested = __SESSION_LITERAL__
        target_id = None
        if requested is not None:
            for sess in sessions:
                sid = _session_identifier(sess)
                if sid is not None and str(sid) == str(requested):
                    target_id = sid
                    break
        if target_id is None:
            target_id = _session_identifier(sessions[0])
        if target_id is None:
            raise RuntimeError('Unable to determine session id to save')
        try:
            client.open_session(target_id)
        except Exception:
            pass
        out_path = Path(__DEST_LITERAL__)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        client.save_xml(session_id=target_id, file_path=out_path)
        payload['session_id'] = str(target_id)
        payload['output_path'] = str(out_path)
    except Exception as exc:
        payload['error'] = str(exc)
        payload['traceback'] = traceback.format_exc()
    finally:
        try:
            if client is not None:
                closer = getattr(client, 'close', None)
                if callable(closer):
                    closer()
        except Exception:
            pass
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    )
    script = template.replace('__ADDRESS_LITERAL__', address_literal)
    script = script.replace('__DEST_LITERAL__', dest_literal)
    script = script.replace('__SESSION_LITERAL__', session_literal)
    return script


def _run_remote_python_json(
    core_cfg: Dict[str, Any],
    script: str,
    *,
    logger: logging.Logger,
    label: str,
    meta: Optional[Dict[str, Any]] = None,
    command_desc: Optional[str] = None,
    timeout: float = 120.0,
) -> Dict[str, Any]:
    cfg = _normalize_core_config(core_cfg, include_password=True)
    if meta is not None and command_desc:
        meta['grpc_command'] = command_desc
        try:
            g.last_core_grpc_command = command_desc  # type: ignore[attr-defined]
        except Exception:
            pass
    try:
        client = _open_ssh_client(cfg)
    except Exception as exc:
        logger.warning('[core.remote] SSH connection failed for %s: %s', label, exc)
        raise
    try:
        interpreter_candidates = _candidate_remote_python_interpreters(cfg)
        if not interpreter_candidates:
            interpreter_candidates = ['python3', 'python']
        venv_bin = str(cfg.get('venv_bin') or '').strip()
        activate_path = posixpath.join(venv_bin, 'activate') if venv_bin else None
        last_error: Optional[str] = None
        last_stdout_text: str = ''
        last_stderr_text: str = ''
        last_interpreter: str = ''
        for interpreter in interpreter_candidates:
            command = _compose_remote_python_command(interpreter, script, activate_path)
            logger.info('[core.remote] (%s) executing via %s', label, interpreter)
            exit_code, stdout, stderr = _exec_ssh_command(client, command, timeout=timeout)
            stdout_text = (stdout or '').strip()
            stderr_text = (stderr or '').strip()
            last_stdout_text = stdout_text
            last_stderr_text = stderr_text
            last_interpreter = interpreter
            if stderr_text:
                logger.debug('[core.remote] (%s) stderr (%s): %s', label, interpreter, stderr_text)
            if exit_code != 0:
                last_error = f"interpreter={interpreter} exit={exit_code} stdout={_summarize_for_log(stdout_text)} stderr={_summarize_for_log(stderr_text)}"
                continue
            if not stdout_text:
                last_error = f"interpreter={interpreter} empty stdout"
                continue
            try:
                return json.loads(stdout_text)
            except json.JSONDecodeError as exc:
                logger.warning('[core.remote] (%s) invalid JSON via %s: %s', label, interpreter, exc)
                logger.debug('[core.remote] (%s) raw stdout: %s', label, stdout_text)
                last_error = f'json decode error: {exc}'
                continue
        detail = f' ({last_error})' if last_error else ''
        # Include a small amount of last output to aid troubleshooting (passwords are not logged).
        out_detail = ''
        if last_stdout_text or last_stderr_text:
            out_detail = (
                f" last_interpreter={last_interpreter}"
                f" last_stdout={_summarize_for_log(last_stdout_text)}"
                f" last_stderr={_summarize_for_log(last_stderr_text)}"
            )
        raise RuntimeError(f'Remote execution failed for {label}{detail}{out_detail}')
    finally:
        try:
            client.close()
        except Exception:
            pass


def _sftp_ensure_dir(sftp: Any, directory: str) -> None:
    directory = posixpath.normpath(directory)
    if not directory or directory == '/':
        return
    parts: list[str] = []
    current = directory
    while current not in ('', '/'):
        parts.append(current)
        current = posixpath.dirname(current)
    for path in reversed(parts):
        try:
            sftp.stat(path)
        except Exception:
            try:
                sftp.mkdir(path)
            except Exception:
                pass


def _upload_file_to_core_host(core_cfg: Dict[str, Any], local_path: str, *, remote_dir: str = '/tmp/core-topo-gen/uploads') -> str:
    if not os.path.exists(local_path):
        raise FileNotFoundError(local_path)
    cfg = _normalize_core_config(core_cfg, include_password=True)
    client = _open_ssh_client(cfg)
    sftp = None
    try:
        sftp = client.open_sftp()
        _sftp_ensure_dir(sftp, remote_dir)
        unique = f"{uuid.uuid4().hex}_{os.path.basename(local_path)}"
        remote_path = posixpath.normpath(posixpath.join(remote_dir, unique))
        sftp.put(local_path, remote_path)
        return remote_path
    finally:
        try:
            if sftp:
                sftp.close()
        except Exception:
            pass
        try:
            client.close()
        except Exception:
            pass


def _remove_remote_file(core_cfg: Dict[str, Any], remote_path: str) -> None:
    if not remote_path:
        return
    cfg = _normalize_core_config(core_cfg, include_password=True)
    try:
        client = _open_ssh_client(cfg)
    except Exception:
        return
    try:
        sftp = client.open_sftp()
    except Exception:
        try:
            client.close()
        except Exception:
            pass
        return
    try:
        try:
            sftp.remove(remote_path)
        except FileNotFoundError:
            pass
    finally:
        try:
            sftp.close()
        except Exception:
            pass
        try:
            client.close()
        except Exception:
            pass


def _download_remote_file(core_cfg: Dict[str, Any], remote_path: str, local_path: str) -> None:
    cfg = _normalize_core_config(core_cfg, include_password=True)
    client = _open_ssh_client(cfg)
    sftp = None
    try:
        sftp = client.open_sftp()
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        sftp.get(remote_path, local_path)
    finally:
        try:
            if sftp:
                sftp.close()
        except Exception:
            pass
        try:
            client.close()
        except Exception:
            pass


def _collect_remote_core_daemon_pids(
    client: Any,
    *,
    log_handle: Optional[TextIO] = None,
    log_prefix: str = '[remote] ',
    stage: str = 'probe',
) -> List[int]:
    # Use `timeout` to avoid SSH reads hanging indefinitely if the remote shell blocks.
    command = "sh -c 'timeout 5s pgrep -x core-daemon 2>/dev/null || true'"
    try:
        stdin, stdout, stderr = client.exec_command(command, timeout=8.0)
    except Exception:
        return []
    try:
        stdout_data = stdout.read()
        stderr_data = stderr.read()
    finally:
        try:
            stdin.close()
        except Exception:
            pass
    try:
        exit_code = stdout.channel.recv_exit_status() if hasattr(stdout, 'channel') else 0
    except Exception:
        exit_code = 0
    try:
        if log_handle is not None:
            out_preview = (
                stdout_data.decode('utf-8', 'ignore')
                if isinstance(stdout_data, (bytes, bytearray))
                else str(stdout_data or '')
            ).strip()
            err_preview = (
                stderr_data.decode('utf-8', 'ignore')
                if isinstance(stderr_data, (bytes, bytearray))
                else str(stderr_data or '')
            ).strip()
            # Keep logs readable.
            if len(out_preview) > 300:
                out_preview = out_preview[:300] + '…'
            if len(err_preview) > 300:
                err_preview = err_preview[:300] + '…'
            log_handle.write(
                f"{log_prefix}{stage}: {command} -> exit={exit_code} stdout={out_preview} stderr={err_preview}\n"
            )
            _write_sse_marker(
                log_handle,
                'phase',
                {
                    'stage': stage,
                    'kind': 'ssh',
                    'command': command,
                    'exit': exit_code,
                    'stdout': out_preview,
                    'stderr': err_preview,
                },
            )
    except Exception:
        pass
    if exit_code not in (0, 1):
        return []
    text = ''
    if isinstance(stdout_data, bytes):
        text = stdout_data.decode('utf-8', 'ignore')
    else:
        text = str(stdout_data or '')
    pids: List[int] = []
    for token in text.strip().split():
        try:
            pids.append(int(token))
        except Exception:
            continue
    return pids


def _start_remote_core_daemon(client: Any, sudo_password: str | None, logger: logging.Logger) -> tuple[int, str, str]:
    # Avoid hanging on an interactive sudo prompt: if no password is available,
    # use -n so sudo fails fast instead of blocking forever.
    if sudo_password:
        # Wrap in `timeout` so systemctl can't block forever.
        sudo_cmd = "sudo -S -p '' sh -c 'timeout 20s systemctl start core-daemon'"
    else:
        sudo_cmd = "sudo -n sh -c 'timeout 20s systemctl start core-daemon'"
    stdin = stdout = stderr = None
    try:
        stdin, stdout, stderr = client.exec_command(sudo_cmd, timeout=20.0, get_pty=True)
        if sudo_password:
            try:
                stdin.write(str(sudo_password) + '\n')
                stdin.flush()
            except Exception:
                pass
        stdout_data = stdout.read()
        stderr_data = stderr.read()
        exit_code = stdout.channel.recv_exit_status() if hasattr(stdout, 'channel') else 0
        out_text = stdout_data.decode('utf-8', 'ignore') if isinstance(stdout_data, bytes) else str(stdout_data or '')
        err_text = stderr_data.decode('utf-8', 'ignore') if isinstance(stderr_data, bytes) else str(stderr_data or '')
        logger.info('[core] auto-start daemon executed: exit=%d stdout=%s stderr=%s', exit_code, out_text.strip(), err_text.strip())
        return exit_code, out_text, err_text
    finally:
        try:
            if stdin:
                stdin.close()
        except Exception:
            pass


def _stop_remote_core_daemon_conflict(
    client: Any,
    *,
    sudo_password: str | None,
    pids: List[int],
    logger: logging.Logger,
) -> Dict[str, Any]:
    """Stop duplicate core-daemon processes.

    Implementation strategy:
    - Stop the systemd service (best-effort)
    - Kill the discovered PIDs
    - Start the service again

    This intentionally errs on the side of producing a single clean daemon.
    """

    if not pids:
        return {'status': 'noop', 'detail': 'no pids provided'}
    if not sudo_password:
        raise RuntimeError('Stopping core-daemon requires sudo; provide an SSH password.')

    def _sudo(cmd: str, *, timeout: float = 30.0) -> tuple[int, str, str]:
        wrapped = f"sudo -S -p '' sh -c {shlex.quote(cmd)}"
        stdin = stdout = stderr = None
        try:
            stdin, stdout, stderr = client.exec_command(wrapped, timeout=timeout, get_pty=True)
            try:
                stdin.write(str(sudo_password) + '\n')
                stdin.flush()
            except Exception:
                pass
            out = stdout.read() if stdout else b''
            err = stderr.read() if stderr else b''
            try:
                exit_code = stdout.channel.recv_exit_status() if (stdout and hasattr(stdout, 'channel')) else 0
            except Exception:
                exit_code = 0
            out_text = out.decode('utf-8', 'ignore') if isinstance(out, (bytes, bytearray)) else str(out or '')
            err_text = err.decode('utf-8', 'ignore') if isinstance(err, (bytes, bytearray)) else str(err or '')
            return exit_code, out_text, err_text
        finally:
            try:
                if stdin:
                    stdin.close()
            except Exception:
                pass

    pid_args = ' '.join(str(int(pid)) for pid in pids if str(pid).strip().isdigit())
    if not pid_args:
        return {'status': 'noop', 'detail': 'no valid pids provided'}

    steps: List[Dict[str, Any]] = []
    for label, cmd in (
        ('systemctl_stop', 'timeout 15s systemctl stop core-daemon || true'),
        ('kill_term', f'kill -TERM {pid_args} 2>/dev/null || true'),
        ('sleep', 'sleep 1'),
        ('kill_kill', f'kill -KILL {pid_args} 2>/dev/null || true'),
        ('systemctl_start', 'timeout 20s systemctl start core-daemon || true'),
    ):
        code, out, err = _sudo(cmd, timeout=35.0)
        steps.append({'step': label, 'command': cmd, 'exit': code, 'stdout': (out or '').strip(), 'stderr': (err or '').strip()})
        try:
            logger.info('[core] stop duplicate daemons: %s exit=%s', label, code)
        except Exception:
            pass
    return {'status': 'attempted', 'pids': pids, 'steps': steps}


def _local_custom_services_dir() -> str:
    repo_root = _get_repo_root()
    return os.path.join(repo_root, 'on_core_machine', 'custom_services')


def _local_custom_service_files() -> list[str]:
    base = _local_custom_services_dir()
    try:
        if not os.path.isdir(base):
            return []
    except Exception:
        return []
    out: list[str] = []
    try:
        for name in os.listdir(base):
            if not name.endswith('.py'):
                continue
            if name.startswith('.'):
                continue
            ap = os.path.join(base, name)
            if os.path.isfile(ap):
                out.append(ap)
    except Exception:
        return []
    out.sort(key=lambda p: os.path.basename(p).lower())
    return out


def _install_custom_services_to_core_vm(
    ssh_client: Any,
    *,
    sudo_password: str | None,
    logger: logging.Logger,
) -> dict:
    """Copy repo-provided CORE custom services to the remote CORE VM.

    - Uploads python service modules under on_core_machine/custom_services.
    - Installs them into the remote core.services package directory.
    - Restarts (or starts) core-daemon.
    - Verifies the modules import successfully.
    """

    local_files = _local_custom_service_files()
    if not local_files:
        raise RuntimeError(f'No custom services found under {_local_custom_services_dir()}')
    module_names = [os.path.splitext(os.path.basename(p))[0] for p in local_files]

    def _exec(cmd: str, *, timeout: float = 25.0) -> tuple[int, str, str]:
        stdin = stdout = stderr = None
        try:
            stdin, stdout, stderr = ssh_client.exec_command(cmd, timeout=timeout, get_pty=True)
            out = stdout.read() if stdout else b''
            err = stderr.read() if stderr else b''
            try:
                exit_code = stdout.channel.recv_exit_status() if (stdout and hasattr(stdout, 'channel')) else 0
            except Exception:
                exit_code = 0
            out_text = out.decode('utf-8', 'ignore') if isinstance(out, (bytes, bytearray)) else str(out or '')
            err_text = err.decode('utf-8', 'ignore') if isinstance(err, (bytes, bytearray)) else str(err or '')
            return exit_code, out_text, err_text
        finally:
            try:
                if stdin:
                    stdin.close()
            except Exception:
                pass

    def _sudo(cmd: str, *, timeout: float = 45.0) -> tuple[int, str, str]:
        if not sudo_password:
            # Installing into site-packages requires root on typical CORE installs.
            raise RuntimeError('Installing custom services requires sudo; provide an SSH password in Step 2.')
        wrapped = f"sudo -S -p '' sh -c {shlex.quote(cmd)}"
        stdin = stdout = stderr = None
        try:
            stdin, stdout, stderr = ssh_client.exec_command(wrapped, timeout=timeout, get_pty=True)
            try:
                stdin.write(str(sudo_password) + '\n')
                stdin.flush()
            except Exception:
                pass
            out = stdout.read() if stdout else b''
            err = stderr.read() if stderr else b''
            try:
                exit_code = stdout.channel.recv_exit_status() if (stdout and hasattr(stdout, 'channel')) else 0
            except Exception:
                exit_code = 0
            out_text = out.decode('utf-8', 'ignore') if isinstance(out, (bytes, bytearray)) else str(out or '')
            err_text = err.decode('utf-8', 'ignore') if isinstance(err, (bytes, bytearray)) else str(err or '')
            return exit_code, out_text, err_text
        finally:
            try:
                if stdin:
                    stdin.close()
            except Exception:
                pass

    # Discover the remote core.services directory (where CORE loads service modules).
    probe = (
        "python3 -c \"import os, core.services; print(os.path.dirname(core.services.__file__))\" 2>/dev/null "
        "|| python -c \"import os, core.services; print(os.path.dirname(core.services.__file__))\" 2>/dev/null"
    )
    code, out, err = _exec(f"sh -c {shlex.quote(probe)}", timeout=20.0)
    services_dir = (out or '').strip().splitlines()[-1].strip() if (out or '').strip() else ''
    if not services_dir:
        raise RuntimeError(f'Failed to locate remote core.services directory (probe exit={code}): {(err or out or "").strip()}')

    logger.info('[core] Installing custom services into %s', services_dir)

    # Upload files to a temp directory first.
    tmp_dir = '/tmp/coretg_custom_services'
    _exec(f"sh -c {shlex.quote(f'mkdir -p {tmp_dir}')}", timeout=15.0)
    sftp = None
    try:
        sftp = ssh_client.open_sftp()
        for lp in local_files:
            rp = f"{tmp_dir}/{os.path.basename(lp)}"
            sftp.put(lp, rp)
    finally:
        try:
            if sftp:
                sftp.close()
        except Exception:
            pass

    # Install into core.services directory.
    install_cmd = f"install -m 0644 {tmp_dir}/*.py {shlex.quote(services_dir)}/"
    code, out, err = _sudo(install_cmd, timeout=45.0)
    if code != 0:
        raise RuntimeError(f'Failed installing custom services (exit={code}): {(err or out or "").strip()}')

    # Restart (or start) core-daemon.
    restart_cmd = "systemctl restart core-daemon || systemctl start core-daemon"
    code, out, err = _sudo(restart_cmd, timeout=45.0)
    if code != 0:
        raise RuntimeError(f'Failed restarting core-daemon after install (exit={code}): {(err or out or "").strip()}')

    # Verify CORE discovers these services (not just that files exist).
    # We import the installed modules, extract CoreService subclasses and their `name`s,
    # then ensure those names are present in the discoverable scan of `core.services`.
    module_list_literal = json.dumps(module_names)
    verify_script = textwrap.dedent(
        f"""
        import importlib
        import inspect
        import json
        import pkgutil
        import sys

        import core.services

        try:
            from core.services.base import CoreService  # type: ignore
        except Exception:  # pragma: no cover - remote execution
            from core.services.coreservices import CoreService  # type: ignore

        custom_modules = json.loads({module_list_literal!r})

        def service_names_from_module(mod):
            names = []
            for _name, obj in inspect.getmembers(mod, inspect.isclass):
                try:
                    is_service = issubclass(obj, CoreService) and obj is not CoreService
                except Exception:
                    continue
                if not is_service:
                    continue
                svc_name = getattr(obj, 'name', None)
                if isinstance(svc_name, str) and svc_name.strip():
                    names.append(svc_name.strip())
            # keep deterministic output
            return sorted(set(names))

        module_service_names = {{}}
        missing_modules = []
        modules_without_services = []
        custom_names = set()

        for mod_name in custom_modules:
            fq = f"core.services.{{mod_name}}"
            try:
                mod = importlib.import_module(fq)
            except Exception as exc:
                missing_modules.append({{'module': mod_name, 'error': repr(exc)}})
                continue
            names = service_names_from_module(mod)
            module_service_names[mod_name] = names
            if not names:
                modules_without_services.append(mod_name)
            custom_names.update(names)

        # Discoverable scan across core.services.
        all_names = set()
        for m in pkgutil.iter_modules(core.services.__path__):
            name = getattr(m, 'name', None)
            if not name:
                continue
            try:
                mod = importlib.import_module(f"core.services.{{name}}")
            except Exception:
                continue
            for svc_name in service_names_from_module(mod):
                all_names.add(svc_name)

        custom_names_missing_from_scan = sorted([n for n in custom_names if n not in all_names])

        result = {{
            'custom_modules': custom_modules,
            'module_service_names': module_service_names,
            'custom_service_names': sorted(custom_names),
            'missing_modules': missing_modules,
            'modules_without_services': modules_without_services,
            'all_service_names_count': len(all_names),
            'custom_names_missing_from_scan': custom_names_missing_from_scan,
        }}
        print('::SERVICESCHECK::' + json.dumps(result, sort_keys=True))

        if missing_modules or modules_without_services or custom_names_missing_from_scan:
            sys.exit(4)
        """
    ).strip()

    verify_cmd_py3 = textwrap.dedent(
        f"""
        cat <<'PY' | python3 -
        {verify_script}
        PY
        """
    ).strip()
    verify_cmd_py = textwrap.dedent(
        f"""
        cat <<'PY' | python -
        {verify_script}
        PY
        """
    ).strip()
    verify_cmd = f"{verify_cmd_py3} 2>/dev/null || {verify_cmd_py}"
    code, out, err = _exec(f"sh -c {shlex.quote(verify_cmd)}", timeout=35.0)
    marker = '::SERVICESCHECK::'
    payload_line = ''
    for line in (out or '').splitlines():
        if marker in line:
            payload_line = line.strip()
    if not payload_line:
        raise RuntimeError(f'Custom services verification did not produce expected output (exit={code}): {(err or out or "").strip()}')
    try:
        verify_payload = json.loads(payload_line.split(marker, 1)[1])
    except Exception as exc:
        raise RuntimeError(f'Custom services verification output could not be parsed: {exc}: {payload_line}')
    if code != 0:
        raise RuntimeError(f'Custom services failed CORE discovery verification: {json.dumps(verify_payload, indent=2)}')

    return {
        'services_dir': services_dir,
        'modules': module_names,
        'service_names': verify_payload.get('custom_service_names') if isinstance(verify_payload, dict) else None,
        'module_service_names': verify_payload.get('module_service_names') if isinstance(verify_payload, dict) else None,
    }


def _ensure_remote_core_daemon_ready(
    client: Any,
    *,
    core_cfg: Dict[str, Any],
    auto_start_allowed: bool,
    sudo_password: str | None,
    logger: logging.Logger,
    log_handle: Optional[TextIO] = None,
    log_prefix: str = '[remote] ',
) -> int:
    """Verify that exactly one core-daemon process is running, auto-starting if permitted."""

    def _log_daemon_probe(stage: str, attempt_idx: int, pids: List[int]) -> None:
        if not log_handle:
            return
        if pids:
            pid_text = ', '.join(str(pid) for pid in pids)
            summary = f'PID(s) {pid_text}'
        else:
            summary = 'no process found'
        try:
            log_handle.write(f"{log_prefix}{stage} probe {attempt_idx + 1}: {summary}\n")
            _write_sse_marker(
                log_handle,
                'phase',
                {
                    'stage': f'core-daemon.{stage}.probe',
                    'attempt': attempt_idx + 1,
                    'pids': pids,
                    'summary': summary,
                },
            )
        except Exception:
            pass

    def _poll_for_single_daemon(max_attempts: int, delay: float, stage: str) -> Optional[int]:
        last_pids: List[int] = []
        for attempt in range(max(1, max_attempts)):
            if log_handle:
                try:
                    log_handle.write(f"{log_prefix}{stage}: checking for core-daemon (attempt {attempt + 1}/{max(1, max_attempts)})\n")
                    _write_sse_marker(
                        log_handle,
                        'phase',
                        {
                            'stage': f'core-daemon.{stage}.poll',
                            'attempt': attempt + 1,
                            'max_attempts': max(1, max_attempts),
                        },
                    )
                except Exception:
                    pass
            poll_pids = _collect_remote_core_daemon_pids(
                client,
                log_handle=log_handle,
                log_prefix=log_prefix,
                stage=f"{stage}.pgrep",
            )
            _log_daemon_probe(stage, attempt, poll_pids)
            last_pids = poll_pids
            if len(poll_pids) > 1:
                raise CoreDaemonConflictError(
                    'Multiple core-daemon processes are running on the CORE VM.',
                    pids=poll_pids,
                )
            if len(poll_pids) == 1:
                if log_handle:
                    try:
                        _write_sse_marker(
                            log_handle,
                            'phase',
                            {
                                'stage': 'core-daemon.ready',
                                'pid': poll_pids[0],
                                'source': stage,
                            },
                        )
                    except Exception:
                        pass
                return poll_pids[0]
            if attempt < max_attempts - 1:
                try:
                    time.sleep(delay)
                except Exception:
                    pass
        return None

    pid = _poll_for_single_daemon(max_attempts=1, delay=0.5, stage='initial')
    if pid is not None:
        if log_handle:
            try:
                log_handle.write(f"{log_prefix}core-daemon already running (PID {pid})\n")
            except Exception:
                pass
        return pid
    if not auto_start_allowed:
        raise CoreDaemonMissingError(
            'core-daemon is not running on the CORE VM.',
            can_auto_start=True,
            start_command=CORE_DAEMON_START_COMMAND,
        )
    exit_code = -1
    stdout_text = ''
    stderr_text = ''
    try:
        try:
            logger.info('[core] Auto-starting core-daemon via `%s`', CORE_DAEMON_START_COMMAND)
        except Exception:
            pass
        if log_handle:
            try:
                log_handle.write(
                    f"{log_prefix}core-daemon not detected; attempting auto-start with command: {CORE_DAEMON_START_COMMAND}\n"
                )
                if not sudo_password:
                    log_handle.write(
                        f"{log_prefix}NOTE: No SSH sudo password provided; using non-interactive sudo (-n). If sudo requires a password, auto-start will fail quickly instead of hanging.\n"
                    )
            except Exception:
                pass
        try:
            logger.info('[core] auto-starting core-daemon via `%s`', CORE_DAEMON_START_COMMAND)
        except Exception:
            pass
        exit_code, stdout_text, stderr_text = _start_remote_core_daemon(client, sudo_password, logger)
        # If sudo cannot run non-interactively, fail fast with a clear message.
        if exit_code != 0 and not sudo_password:
            err_lower = (stderr_text or '').lower()
            if 'password' in err_lower or 'sudo' in err_lower:
                raise CoreDaemonMissingError(
                    'core-daemon auto-start failed: sudo requires a password (none provided).',
                    can_auto_start=False,
                    start_command=CORE_DAEMON_START_COMMAND,
                )
        if log_handle:
            try:
                log_handle.write(
                    f"{log_prefix}core-daemon auto-start command '{CORE_DAEMON_START_COMMAND}' exit={exit_code} stdout={stdout_text.strip()} stderr={stderr_text.strip()}\n"
                )
                _write_sse_marker(
                    log_handle,
                    'phase',
                    {
                        'stage': 'core-daemon.start',
                        'kind': 'ssh',
                        'command': CORE_DAEMON_START_COMMAND,
                        'exit': exit_code,
                        'stdout': (stdout_text or '').strip()[:1500],
                        'stderr': (stderr_text or '').strip()[:1500],
                    },
                )
            except Exception:
                pass
    except Exception as exc:
        raise CoreDaemonMissingError(
            f'core-daemon auto-start failed: {exc}',
            can_auto_start=False,
            start_command=CORE_DAEMON_START_COMMAND,
        ) from exc
    try:
        status_cmd = 'sh -c "timeout 10s systemctl is-active core-daemon"'
        status_code, status_out, status_err = _exec_ssh_command(client, status_cmd, timeout=15.0)
        if log_handle:
            try:
                log_handle.write(
                    f"{log_prefix}{status_cmd} -> exit={status_code} stdout={status_out.strip()} stderr={status_err.strip()}\n"
                )
                _write_sse_marker(
                    log_handle,
                    'phase',
                    {
                        'stage': 'core-daemon.systemctl.is-active',
                        'kind': 'ssh',
                        'command': status_cmd,
                        'exit': status_code,
                        'stdout': (status_out or '').strip()[:1500],
                        'stderr': (status_err or '').strip()[:1500],
                    },
                )
            except Exception:
                pass
        # Capture a small amount of extra detail for operator visibility.
        if log_handle:
            try:
                status_detail_cmd = "sh -c \"timeout 15s systemctl status core-daemon --no-pager -l | tail -n 40\""
                detail_code, detail_out, detail_err = _exec_ssh_command(client, status_detail_cmd, timeout=20.0)
                out_clean = (detail_out or '').rstrip()
                err_clean = (detail_err or '').rstrip()
                log_handle.write(
                    f"{log_prefix}{status_detail_cmd} -> exit={detail_code}\n"
                )
                if out_clean:
                    log_handle.write(f"{log_prefix}systemctl status (tail)\n{out_clean}\n")
                if err_clean:
                    log_handle.write(f"{log_prefix}systemctl status stderr\n{err_clean}\n")
                _write_sse_marker(
                    log_handle,
                    'phase',
                    {
                        'stage': 'core-daemon.systemctl.status',
                        'kind': 'ssh',
                        'command': status_detail_cmd,
                        'exit': detail_code,
                        'stdout': out_clean[:2500] if out_clean else '',
                        'stderr': err_clean[:1500] if err_clean else '',
                    },
                )
            except Exception:
                pass
        if exit_code != 0 and log_handle:
            try:
                journal_cmd = "sh -c \"timeout 15s journalctl -u core-daemon -n 40 --no-pager || true\""
                j_code, j_out, j_err = _exec_ssh_command(client, journal_cmd, timeout=20.0)
                out_clean = (j_out or '').rstrip()
                err_clean = (j_err or '').rstrip()
                log_handle.write(f"{log_prefix}{journal_cmd} -> exit={j_code}\n")
                if out_clean:
                    log_handle.write(f"{log_prefix}journalctl (tail)\n{out_clean}\n")
                if err_clean:
                    log_handle.write(f"{log_prefix}journalctl stderr\n{err_clean}\n")
                _write_sse_marker(
                    log_handle,
                    'phase',
                    {
                        'stage': 'core-daemon.journalctl',
                        'kind': 'ssh',
                        'command': journal_cmd,
                        'exit': j_code,
                        'stdout': out_clean[:2500] if out_clean else '',
                        'stderr': err_clean[:1500] if err_clean else '',
                    },
                )
            except Exception:
                pass
    except Exception:
        pass
    pid = _poll_for_single_daemon(max_attempts=10, delay=0.5, stage='post-start')
    if pid is not None:
        return pid
    raise CoreDaemonMissingError(
        'core-daemon auto-start did not stabilize within the expected time.',
        can_auto_start=False,
        start_command=CORE_DAEMON_START_COMMAND,
    )


def _check_remote_daemon_before_setup(
    client: Any,
    *,
    core_cfg: Dict[str, Any],
    auto_start_allowed: bool,
    log_handle: Optional[TextIO],
    log_prefix: str,
) -> int:
    logger = getattr(app, 'logger', logging.getLogger(__name__))
    pid = _ensure_remote_core_daemon_ready(
        client=client,
        core_cfg=core_cfg,
        auto_start_allowed=auto_start_allowed,
        sudo_password=core_cfg.get('ssh_password'),
        logger=logger,
        log_handle=log_handle,
        log_prefix=log_prefix,
    )
    if log_handle:
        try:
            log_handle.write(f"{log_prefix}core-daemon PID {pid} confirmed before run setup\n")
        except Exception:
            pass
    return pid

def _execute_remote_core_session_action(
    core_cfg: Dict[str, Any],
    action: str,
    session_id: int,
    *,
    logger: logging.Logger,
    meta: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    address = f"{cfg.get('host') or CORE_HOST}:{cfg.get('port') or CORE_PORT}"
    script = _remote_core_session_action_script(address, action, session_id)
    command_desc = (
        f"remote ssh {ssh_user}@{ssh_host} -> CoreGrpcClient.{action}_session {address} (session={session_id})"
    )
    payload = _run_remote_python_json(
        cfg,
        script,
        logger=logger,
        label=f'core.{action}_session',
        meta=meta,
        command_desc=command_desc,
        timeout=120.0,
    )
    error_text = payload.get('error')
    if error_text:
        tb = payload.get('traceback')
        if tb:
            logger.debug('[core.%s_session] traceback: %s', action, tb)
        raise RuntimeError(error_text)
    return payload


def _list_active_core_sessions_via_remote_python(
    core_cfg: Dict[str, Any],
    *,
    errors: Optional[List[str]] = None,
    meta: Optional[Dict[str, Any]] = None,
    logger: Optional[logging.Logger] = None,
) -> list[dict]:
    cfg = _normalize_core_config(core_cfg, include_password=True)
    log = logger or getattr(app, 'logger', logging.getLogger(__name__))
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    target_host = cfg.get('host') or 'localhost'
    try:
        target_port = int(cfg.get('port') or CORE_PORT)
    except Exception:
        target_port = CORE_PORT
    if meta is not None:
        meta['grpc_command'] = (
            f"remote ssh {ssh_user}@{ssh_host} -> CoreGrpcClient.get_sessions {target_host}:{target_port}"
        )
    address = f"{target_host}:{target_port}"
    script = _remote_core_sessions_script(address)
    command_desc = meta['grpc_command'] if meta and 'grpc_command' in meta else f'get_sessions {address}'
    try:
        payload = _run_remote_python_json(
            cfg,
            script,
            logger=log,
            label='core.get_sessions',
            meta=meta,
            command_desc=command_desc,
            timeout=120.0,
        )
    except Exception as exc:
        msg = f'Remote CORE session fetch failed: {exc}'
        log.warning('[core.grpc] %s', msg)
        if errors is not None:
            errors.append(msg)
        return []
    error_text = payload.get('error')
    if error_text:
        log.warning('[core.grpc] remote CORE session fetch error: %s', error_text)
        if errors is not None:
            errors.append(f'Remote CORE session fetch failed: {error_text}')
        tb = payload.get('traceback')
        if tb:
            log.debug('[core.grpc] remote traceback: %s', tb)
        return []
    sessions = payload.get('sessions') or []
    log.info('[core.grpc] remote returned %d session(s)', len(sessions))
    return sessions


def _exec_ssh_python_probe(client: Any, command: str, *, timeout: float) -> Tuple[int, str, str]:
    stdin, stdout, stderr = client.exec_command(command, timeout=timeout)
    try:
        stdout_data = stdout.read()
        stderr_data = stderr.read()
    finally:
        try:
            stdin.close()
        except Exception:
            pass
    exit_code = stdout.channel.recv_exit_status() if hasattr(stdout, 'channel') else 0
    if isinstance(stdout_data, bytes):
        stdout_text = stdout_data.decode('utf-8', 'ignore')
    else:
        stdout_text = str(stdout_data)
    if isinstance(stderr_data, bytes):
        stderr_text = stderr_data.decode('utf-8', 'ignore')
    else:
        stderr_text = str(stderr_data)
    return exit_code, stdout_text, stderr_text


def _ensure_core_daemon_listening(core_cfg: Dict[str, Any], *, timeout: float = 5.0) -> None:
    cfg = _require_core_ssh_credentials(core_cfg)
    _ensure_paramiko_available()
    daemon_host = str(cfg.get('host') or CORE_HOST or 'localhost').strip() or 'localhost'
    if daemon_host in {'0.0.0.0', '::', '::0', '[::]'}:
        daemon_host = '127.0.0.1'
    daemon_port = int(cfg.get('port') or CORE_PORT)
    ssh_host = str(cfg.get('ssh_host') or cfg.get('host') or 'localhost')
    ssh_port = int(cfg.get('ssh_port') or 22)
    username = str(cfg.get('ssh_username') or '').strip()
    password = cfg.get('ssh_password') or ''
    logger = getattr(app, 'logger', logging.getLogger(__name__))
    client = paramiko.SSHClient()  # type: ignore[assignment]
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
    try:
        client.connect(
            hostname=ssh_host,
            port=ssh_port,
            username=username,
            password=password,
            look_for_keys=False,
            allow_agent=False,
            timeout=max(timeout, 5.0),
            banner_timeout=max(timeout, 5.0),
            auth_timeout=max(timeout, 5.0),
        )
        probe_errors: List[str] = []
        interpreter_candidates = _candidate_remote_python_interpreters(cfg)
        if not interpreter_candidates:
            interpreter_candidates = ['python3', 'python']
        for interpreter in interpreter_candidates:
            command = _build_python_probe_command(daemon_host, daemon_port, timeout, interpreter=interpreter)
            exit_code, stdout_text, stderr_text = _exec_ssh_python_probe(client, command, timeout=timeout + 2.0)
            stdout_clean = stdout_text.strip()
            stderr_clean = stderr_text.strip()
            combined = ' '.join(part for part in (stdout_clean, stderr_clean) if part).strip()
            if exit_code == 0 and 'OK' in stdout_clean:
                logger.info('[core] Verified core-daemon listening at %s:%s via %s', daemon_host, daemon_port, interpreter)
                return
            if not combined:
                combined = f'exit status {exit_code}'
            probe_errors.append(f'{interpreter}: {combined}')
            lower = combined.lower()
            if 'not found' in lower or 'command not found' in lower:
                continue
        error_detail = '; '.join(probe_errors) if probe_errors else 'probe failed'
        raise RuntimeError(f'core-daemon did not respond on {daemon_host}:{daemon_port} ({error_detail})')
    finally:
        try:
            client.close()
        except Exception:
            pass


def _run_core_connection_advanced_checks(
    cfg: Dict[str, Any],
    *,
    adv_fix_docker_daemon: bool = False,
    adv_run_core_cleanup: bool = False,
    adv_check_core_version: bool = False,
    adv_restart_core_daemon: bool = False,
    adv_start_core_daemon: bool = False,
    adv_auto_kill_sessions: bool = False,
) -> Dict[str, Dict[str, Any]]:
    """Run optional advanced checks against the CORE VM.

    These checks mirror the ones available in the Execute Scenario modal.
    Returns a dict keyed by adv_* flag names with shape:
      { enabled: bool, ok: bool|None, message: str }

    Notes:
    - This function is best-effort: when a check is enabled, it will attempt the action
      and report failure in its own result without raising (except for totally unexpected errors).
    - Some checks require sudo (SSH password). If missing, the check is marked failed.
    """

    results: Dict[str, Dict[str, Any]] = {}

    def _set(key: str, *, enabled: bool, ok: Optional[bool], message: str) -> None:
        results[key] = {
            'enabled': bool(enabled),
            'ok': ok,
            'message': str(message or '').strip(),
        }

    enabled_any = any(
        bool(v)
        for v in (
            adv_fix_docker_daemon,
            adv_run_core_cleanup,
            adv_check_core_version,
            adv_restart_core_daemon,
            adv_start_core_daemon,
            adv_auto_kill_sessions,
        )
    )
    if not enabled_any:
        # Keep keys present but disabled for stable UI.
        _set('adv_check_core_version', enabled=False, ok=None, message='')
        _set('adv_fix_docker_daemon', enabled=False, ok=None, message='')
        _set('adv_run_core_cleanup', enabled=False, ok=None, message='')
        _set('adv_restart_core_daemon', enabled=False, ok=None, message='')
        _set('adv_start_core_daemon', enabled=False, ok=None, message='')
        _set('adv_auto_kill_sessions', enabled=False, ok=None, message='')
        return results

    logger = getattr(app, 'logger', logging.getLogger(__name__))
    cfg = _normalize_core_config(cfg, include_password=True)
    if paramiko is None:
        # Can't run these without SSH.
        for key, enabled in (
            ('adv_check_core_version', adv_check_core_version),
            ('adv_fix_docker_daemon', adv_fix_docker_daemon),
            ('adv_run_core_cleanup', adv_run_core_cleanup),
            ('adv_restart_core_daemon', adv_restart_core_daemon),
            ('adv_start_core_daemon', adv_start_core_daemon),
            ('adv_auto_kill_sessions', adv_auto_kill_sessions),
        ):
            if enabled:
                _set(key, enabled=True, ok=False, message='Paramiko unavailable; cannot run this check.')
            else:
                _set(key, enabled=False, ok=None, message='')
        return results

    def _check_core_version(client: Any, required: str = '9.2.1') -> str:
        candidates = [
            "sh -c 'timeout 6s core-daemon --version 2>/dev/null || true'",
            "sh -c 'timeout 6s core-daemon -v 2>/dev/null || true'",
            "sh -c 'timeout 6s dpkg-query -W -f=\"${Version}\" core-daemon 2>/dev/null || true'",
            "sh -c 'timeout 6s rpm -q --qf \"%{VERSION}-%{RELEASE}\" core-daemon 2>/dev/null || true'",
            "sh -c 'timeout 6s core --version 2>/dev/null || true'",
        ]
        raw = ''
        for cmd in candidates:
            try:
                _code, out, err = _exec_ssh_command(client, cmd, timeout=12.0)
            except Exception:
                continue
            text = (out or '').strip() or (err or '').strip()
            if text:
                raw = text
                break
        if not raw:
            raise RuntimeError('Unable to determine CORE version on remote host')
        found = None
        try:
            m = re.search(r"(\d+\.\d+\.\d+)", raw)
            if m:
                found = m.group(1)
        except Exception:
            found = None
        if not found:
            raise RuntimeError(f'Unable to parse CORE version from: {raw}')
        if found != required:
            raise RuntimeError(f'CORE version mismatch: expected {required}, found {found}')
        return found

    def _sudo_exec(client: Any, cmd: str, *, timeout: float = 45.0) -> tuple[int, str, str]:
        sudo_password = cfg.get('ssh_password')
        # Wrap in timeout to avoid hanging.
        wrapped = f"sh -c 'timeout {int(max(5, timeout))}s {cmd}'"
        if sudo_password:
            sudo_cmd = f"sudo -S -p '' {wrapped}"
        else:
            sudo_cmd = f"sudo -n {wrapped}"
        stdin = stdout = stderr = None
        try:
            stdin, stdout, stderr = client.exec_command(sudo_cmd, timeout=timeout + 5.0, get_pty=True)
            if sudo_password:
                try:
                    stdin.write(str(sudo_password) + '\n')
                    stdin.flush()
                except Exception:
                    pass
            stdout_data = stdout.read() if stdout else b''
            stderr_data = stderr.read() if stderr else b''
            try:
                exit_code = stdout.channel.recv_exit_status() if (stdout and hasattr(stdout, 'channel')) else 0
            except Exception:
                exit_code = 0
            out_text = stdout_data.decode('utf-8', 'ignore') if isinstance(stdout_data, (bytes, bytearray)) else str(stdout_data or '')
            err_text = stderr_data.decode('utf-8', 'ignore') if isinstance(stderr_data, (bytes, bytearray)) else str(stderr_data or '')
            return exit_code, out_text, err_text
        finally:
            try:
                if stdin:
                    stdin.close()
            except Exception:
                pass

    def _maybe_fix_docker_daemon(client: Any) -> None:
        desired = {'bridge': 'none', 'iptables': False}
        existing: dict[str, Any] = {}
        try:
            _code, out, _err = _exec_ssh_command(client, "sh -c 'timeout 5s cat /etc/docker/daemon.json 2>/dev/null || true'", timeout=10.0)
            text = (out or '').strip()
            if text:
                try:
                    existing = json.loads(text)
                except Exception:
                    existing = {}
        except Exception:
            existing = {}
        merged = dict(existing) if isinstance(existing, dict) else {}
        merged.update(desired)
        payload = json.dumps(merged, indent=2, sort_keys=True) + "\n"

        tmp_local = None
        remote_tmp = None
        try:
            import tempfile

            with tempfile.NamedTemporaryFile('w', delete=False, encoding='utf-8') as tf:
                tf.write(payload)
                tmp_local = tf.name
            remote_tmp = _upload_file_to_core_host(cfg, tmp_local, remote_dir='/tmp/core-topo-gen/uploads')

            exit_code, _out, err = _sudo_exec(client, 'install -d -m 0755 /etc/docker', timeout=20.0)
            if exit_code != 0:
                err_lower = (err or '').lower()
                if (not cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                    raise RuntimeError('Fix docker daemon failed: sudo requires a password (none provided).')
                raise RuntimeError('Fix docker daemon failed: could not create /etc/docker')
            exit_code, _out, err = _sudo_exec(client, f"install -m 0644 {shlex.quote(remote_tmp)} /etc/docker/daemon.json", timeout=25.0)
            if exit_code != 0:
                err_lower = (err or '').lower()
                if (not cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                    raise RuntimeError('Fix docker daemon failed: sudo requires a password (none provided).')
                raise RuntimeError('Fix docker daemon failed: could not write /etc/docker/daemon.json')

            rc, _o, _e = _sudo_exec(client, 'systemctl restart docker || service docker restart || true', timeout=40.0)
            if rc != 0:
                raise RuntimeError('Fix docker daemon failed: docker restart did not succeed')
        finally:
            try:
                if tmp_local and os.path.exists(tmp_local):
                    os.remove(tmp_local)
            except Exception:
                pass
            try:
                if remote_tmp:
                    _remove_remote_file(cfg, remote_tmp)
            except Exception:
                pass

    def _maybe_core_cleanup(client: Any) -> None:
        # Prefer system-provided core-cleanup if available; otherwise do safe stale /tmp/pycore.* purge.
        try:
            _code, out, _err = _exec_ssh_command(client, "sh -c 'command -v core-cleanup >/dev/null 2>&1; echo $?'", timeout=10.0)
            has_core_cleanup = (out or '').strip() == '0'
        except Exception:
            has_core_cleanup = False
        if has_core_cleanup:
            exit_code, _out, err = _sudo_exec(client, 'core-cleanup', timeout=70.0)
            if exit_code != 0:
                err_lower = (err or '').lower()
                if (not cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                    raise RuntimeError('core-cleanup failed: sudo requires a password (none provided).')
                raise RuntimeError('core-cleanup failed')
            return

        # Fallback: remove stale pycore directories not in active session ids.
        try:
            target_host = str(cfg.get('host') or CORE_HOST)
            target_port = int(cfg.get('port') or CORE_PORT)
        except Exception:
            target_host = str(cfg.get('host') or CORE_HOST)
            target_port = CORE_PORT
        try:
            sessions = _list_active_core_sessions(target_host, int(target_port), cfg, errors=[], meta={})
        except Exception:
            sessions = []
        active_ids: set[int] = set()
        for entry in sessions:
            try:
                sid = entry.get('id')
                if sid is None:
                    continue
                active_ids.add(int(str(sid).strip()))
            except Exception:
                continue
        active_json = json.dumps(sorted(active_ids))
        cleanup_cmd = (
            "python3 - <<'PY'\n"
            "import os, json, glob, shutil, time\n"
            "active=set(json.loads(os.environ.get('ACTIVE_IDS','[]')))\n"
            "removed=[]\nkept=[]\nnow=time.time()\n"
            "for p in glob.glob('/tmp/pycore.*'):\n"
            "  base=os.path.basename(p)\n"
            "  try: sid=int(base.split('.')[-1])\n"
            "  except Exception: kept.append(p); continue\n"
            "  if sid in active: kept.append(p); continue\n"
            "  try: age=now-os.stat(p).st_mtime\n"
            "  except Exception: age=999\n"
            "  if age < 30: kept.append(p); continue\n"
            "  try: shutil.rmtree(p); removed.append(p)\n"
            "  except Exception: kept.append(p)\n"
            "print(json.dumps({'removed':removed,'kept':kept,'active_session_ids':sorted(active)}))\n"
            "PY"
        )
        shell_cmd = f"ACTIVE_IDS={shlex.quote(active_json)} {cleanup_cmd}"
        _exec_ssh_command(client, f"sh -c {shlex.quote(shell_cmd)}", timeout=30.0)

    def _maybe_restart_core_daemon(client: Any) -> None:
        exit_code, _out, err = _sudo_exec(client, 'systemctl restart core-daemon', timeout=35.0)
        if exit_code != 0:
            err_lower = (err or '').lower()
            if (not cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                raise RuntimeError('Restart core-daemon failed: sudo requires a password (none provided).')
            raise RuntimeError('Restart core-daemon failed')

    def _maybe_start_core_daemon(client: Any) -> str:
        """Try starting core-daemon if not running. Returns status message."""
        # First check if daemon is running
        exit_code, out, _err = _sudo_exec(client, 'systemctl is-active core-daemon', timeout=15.0)
        if exit_code == 0 and (out or '').strip().lower() == 'active':
            return 'already running'
        
        # Try to start it
        exit_code, _out, err = _sudo_exec(client, 'systemctl start core-daemon', timeout=35.0)
        if exit_code != 0:
            err_lower = (err or '').lower()
            if (not cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                raise RuntimeError('Start core-daemon failed: sudo requires a password (none provided).')
            raise RuntimeError(f'Start core-daemon failed: {err or "unknown error"}')
        
        # Verify it started
        exit_code, out, _err = _sudo_exec(client, 'systemctl is-active core-daemon', timeout=15.0)
        if exit_code == 0 and (out or '').strip().lower() == 'active':
            return 'started successfully'
        return 'start attempted, status unclear'

    def _maybe_kill_active_sessions() -> tuple[list[int], list[str]]:
        deleted: list[int] = []
        errors: list[str] = []
        try:
            target_host = str(cfg.get('host') or CORE_HOST)
            target_port = int(cfg.get('port') or CORE_PORT)
        except Exception:
            target_host = str(cfg.get('host') or CORE_HOST)
            target_port = CORE_PORT
        try:
            sessions = _list_active_core_sessions(target_host, int(target_port), cfg, errors=[], meta={})
        except Exception as exc:
            errors.append(f"Failed listing active CORE sessions: {exc}")
            return deleted, errors
        ids: list[int] = []
        for entry in sessions:
            sid = entry.get('id')
            if sid in (None, ''):
                continue
            try:
                ids.append(int(str(sid).strip()))
            except Exception:
                continue
        seen: set[int] = set()
        ordered: list[int] = []
        for sid in ids:
            if sid in seen:
                continue
            seen.add(sid)
            ordered.append(sid)
        for sid in ordered:
            try:
                _execute_remote_core_session_action(cfg, 'delete', sid, logger=logger)
                deleted.append(sid)
            except Exception as exc:
                errors.append(f"Failed deleting session {sid}: {exc}")
        return deleted, errors

    _ensure_paramiko_available()
    client = paramiko.SSHClient()  # type: ignore[assignment]
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
    try:
        client.connect(
            hostname=str(cfg.get('ssh_host') or cfg.get('host') or 'localhost'),
            port=int(cfg.get('ssh_port') or 22),
            username=str(cfg.get('ssh_username') or ''),
            password=cfg.get('ssh_password') or '',
            look_for_keys=False,
            allow_agent=False,
            timeout=15.0,
            banner_timeout=15.0,
            auth_timeout=15.0,
        )

        if adv_check_core_version:
            try:
                ver = _check_core_version(client, '9.2.1')
                _set('adv_check_core_version', enabled=True, ok=True, message=f'found {ver}')
            except Exception as exc:
                _set('adv_check_core_version', enabled=True, ok=False, message=str(exc))
        else:
            _set('adv_check_core_version', enabled=False, ok=None, message='')

        if adv_fix_docker_daemon:
            try:
                _maybe_fix_docker_daemon(client)
                _set('adv_fix_docker_daemon', enabled=True, ok=True, message='applied /etc/docker/daemon.json and restarted docker')
            except Exception as exc:
                _set('adv_fix_docker_daemon', enabled=True, ok=False, message=str(exc))
        else:
            _set('adv_fix_docker_daemon', enabled=False, ok=None, message='')

        if adv_run_core_cleanup:
            try:
                _maybe_core_cleanup(client)
                _set('adv_run_core_cleanup', enabled=True, ok=True, message='completed')
            except Exception as exc:
                _set('adv_run_core_cleanup', enabled=True, ok=False, message=str(exc))
        else:
            _set('adv_run_core_cleanup', enabled=False, ok=None, message='')

        if adv_restart_core_daemon:
            try:
                _maybe_restart_core_daemon(client)
                _set('adv_restart_core_daemon', enabled=True, ok=True, message='requested')
            except Exception as exc:
                _set('adv_restart_core_daemon', enabled=True, ok=False, message=str(exc))
        else:
            _set('adv_restart_core_daemon', enabled=False, ok=None, message='')

        if adv_start_core_daemon:
            try:
                status_msg = _maybe_start_core_daemon(client)
                _set('adv_start_core_daemon', enabled=True, ok=True, message=status_msg)
            except Exception as exc:
                _set('adv_start_core_daemon', enabled=True, ok=False, message=str(exc))
        else:
            _set('adv_start_core_daemon', enabled=False, ok=None, message='')

    finally:
        try:
            client.close()
        except Exception:
            pass

    # Auto-kill sessions is done via gRPC calls (remote python) and does not require an open SSHClient.
    if adv_auto_kill_sessions:
        try:
            deleted, errs = _maybe_kill_active_sessions()
            if errs:
                _set('adv_auto_kill_sessions', enabled=True, ok=False, message='; '.join(errs)[:500])
            else:
                detail = f"deleted {len(deleted)} session(s)" if deleted else 'no sessions deleted'
                _set('adv_auto_kill_sessions', enabled=True, ok=True, message=detail)
        except Exception as exc:
            _set('adv_auto_kill_sessions', enabled=True, ok=False, message=str(exc))
    else:
        _set('adv_auto_kill_sessions', enabled=False, ok=None, message='')

    return results


def _normalize_hitl_attachment(raw_value: Any) -> str:
    if isinstance(raw_value, str):
        candidate = raw_value.strip()
        if candidate in _HITL_ATTACHMENT_ALLOWED:
            return candidate
        normalized = candidate.lower().replace('-', '_').replace(' ', '_')
        if normalized in _HITL_ATTACHMENT_ALLOWED:
            return normalized
        synonyms = {
            "router": "existing_router",
            "existing": "existing_router",
            "existingrouter": "existing_router",
            "existing_router": "existing_router",
            "existing-switch": "existing_switch",
            "existing switch": "existing_switch",
            "existing_switch": "existing_switch",
            "switch": "existing_switch",
            "newrouter": "new_router",
            "new_router": "new_router",
            "new router": "new_router",
            "router_new": "new_router",
            "proxmox_vm": "proxmox_vm",
            "proxmoxvm": "proxmox_vm",
            "proxmox-vm": "proxmox_vm",
            "proxmox vm": "proxmox_vm",
            "vm": "proxmox_vm",
            "external_vm": "proxmox_vm",
            "externalvm": "proxmox_vm",
        }
        if normalized in synonyms:
            return synonyms[normalized]
    return _DEFAULT_HITL_ATTACHMENT


def _normalize_hitl_interface_name(raw_value: Any) -> str:
    """Strip any legacy hitl- prefix so CORE sees the raw device name."""
    if isinstance(raw_value, str):
        candidate = raw_value.strip()
    elif raw_value is None:
        candidate = ''
    else:
        candidate = str(raw_value).strip()
    if not candidate:
        return ''
    prefix = 'hitl-'
    if candidate.lower().startswith(prefix):
        return candidate[len(prefix):]
    return candidate


def _slugify_hitl_name(raw_value: Any, fallback: str) -> str:
    value = ''
    if isinstance(raw_value, str):
        value = raw_value.strip().lower()
    elif raw_value is not None:
        value = str(raw_value).strip().lower()
    if not value:
        value = fallback.lower()
    cleaned = []
    for ch in value:
        if ch.isalnum():
            cleaned.append(ch)
        elif ch in {'-', '_'}:
            cleaned.append(ch)
        else:
            cleaned.append('-')
    slug = ''.join(cleaned).strip('-_')
    if not slug:
        slug = fallback.lower().strip('-_') or 'iface'
    return slug[:48]


def _stable_hitl_preview_router_id(scenario_key: str, slug: str, idx: int) -> int:
    key = f"hitl-router|{scenario_key or '__default__'}|{slug}|{idx}"
    digest = hashlib.sha256(key.encode('utf-8', 'replace')).hexdigest()
    return 700_000 + (int(digest[:10], 16) % 200_000)


def _build_hitl_preview_router(
    scenario_key: str,
    iface: Dict[str, Any],
    slug: str,
    ordinal: int,
    ip_info: Dict[str, Any],
) -> Dict[str, Any]:
    node_id = _stable_hitl_preview_router_id(scenario_key, slug, ordinal)
    new_router_ip = ip_info.get('new_router_ip4')
    link_network = ip_info.get('network_cidr') or ip_info.get('network')
    prefix_len = ip_info.get('prefix_len')
    new_router_ip_cidr = None
    if new_router_ip:
        if prefix_len and '/' not in str(new_router_ip):
            new_router_ip_cidr = f"{new_router_ip}/{prefix_len}"
        else:
            new_router_ip_cidr = str(new_router_ip)
    r2r_interfaces: Dict[str, Any] = {}
    metadata = {
        'hitl_preview': True,
        'hitl_interface_name': iface.get('name'),
        'hitl_attachment': iface.get('attachment'),
        'hitl_slug': slug,
        'link_network': link_network,
        'rj45_ip4': ip_info.get('rj45_ip4'),
        'existing_router_ip4': ip_info.get('existing_router_ip4'),
        'new_router_ip4': new_router_ip,
        'prefix_len': ip_info.get('prefix_len'),
        'netmask': ip_info.get('netmask'),
    }
    preview_router = {
        'node_id': node_id,
        'name': f"hitl-router-{slug}",
        'role': 'router',
        'kind': 'router',
        'ip4': new_router_ip_cidr,
        'r2r_interfaces': r2r_interfaces,
        'vulnerabilities': [],
        'is_base_bridge': False,
        'metadata': metadata,
    }
    return preview_router


def _sanitize_hitl_config(hitl_config: Any, scenario_name: Optional[str], xml_basename: Optional[str]) -> Dict[str, Any]:
    def _normalize_list(value: Any) -> List[str]:
        if isinstance(value, list):
            return [str(v).strip() for v in value if str(v).strip()]
        if isinstance(value, str):
            return [part.strip() for part in value.split(',') if part.strip()]
        return []

    cfg = hitl_config if isinstance(hitl_config, dict) else {}
    enabled = bool(cfg.get('enabled'))
    raw_interfaces = cfg.get('interfaces')

    if isinstance(raw_interfaces, list):
        iterable = raw_interfaces
    elif isinstance(raw_interfaces, str) and raw_interfaces.strip():
        iterable = [{'name': raw_interfaces.strip()}]
    elif isinstance(raw_interfaces, dict) and raw_interfaces.get('name'):
        iterable = [raw_interfaces]
    else:
        iterable = []

    sanitized: List[Dict[str, Any]] = []
    for entry in iterable:
        if entry is None:
            continue
        if isinstance(entry, str):
            name = _normalize_hitl_interface_name(entry)
            if name:
                sanitized.append({'name': name, 'attachment': _DEFAULT_HITL_ATTACHMENT})
            continue
        if not isinstance(entry, dict):
            continue
        clone = dict(entry)
        name_candidate = clone.get('name') or clone.get('interface') or clone.get('iface')
        if not isinstance(name_candidate, str):
            name_candidate = str(name_candidate or '').strip()
        else:
            name_candidate = name_candidate.strip()
        normalized_name = _normalize_hitl_interface_name(name_candidate)
        if not normalized_name:
            continue
        clone['name'] = normalized_name
        alias_candidate = clone.get('alias') or clone.get('description') or clone.get('display') or clone.get('summary')
        if isinstance(alias_candidate, str) and alias_candidate.strip():
            clone['alias'] = alias_candidate.strip()
        if 'display' in clone and not clone.get('description'):
            disp_val = clone.get('display')
            if isinstance(disp_val, str) and disp_val.strip():
                clone['description'] = disp_val.strip()
        clone['attachment'] = _normalize_hitl_attachment(clone.get('attachment'))
        if 'ipv4' in clone:
            clone['ipv4'] = _normalize_list(clone.get('ipv4'))
        if 'ipv6' in clone:
            clone['ipv6'] = _normalize_list(clone.get('ipv6'))
        sanitized.append(clone)

    scenario_key = ''
    candidate = cfg.get('scenario_key')
    if isinstance(candidate, str) and candidate.strip():
        scenario_key = candidate.strip()
    elif isinstance(scenario_name, str) and scenario_name.strip():
        scenario_key = scenario_name.strip()
    elif isinstance(xml_basename, str) and xml_basename.strip():
        scenario_key = xml_basename.strip()
    else:
        scenario_key = '__default__'

    sanitized_cfg = {
        'enabled': enabled,
        'interfaces': sanitized,
        'scenario_key': scenario_key,
    }
    core_cfg = _extract_optional_core_config(cfg.get('core'), include_password=False)
    if core_cfg:
        sanitized_cfg['core'] = core_cfg
    _enrich_hitl_interfaces_with_ips(sanitized_cfg)
    return sanitized_cfg


def _enrich_hitl_interfaces_with_ips(hitl_cfg: Dict[str, Any]) -> None:
    interfaces = hitl_cfg.get('interfaces') or []
    scenario_key = hitl_cfg.get('scenario_key') or '__default__'
    preview_routers: List[Dict[str, Any]] = []
    used_hitl_link_networks: set[str] = set()
    total_interfaces = len(interfaces)
    for idx, iface in enumerate(list(interfaces)):
        if not isinstance(iface, dict):
            continue
        attachment = _normalize_hitl_attachment(iface.get('attachment'))
        iface['attachment'] = attachment
        slug = _slugify_hitl_name(iface.get('name'), f"iface-{idx+1}")
        iface['slug'] = slug
        iface['ordinal'] = idx
        iface['interface_count'] = total_interfaces
        ip_info: Optional[Dict[str, Any]] = None
        if attachment in {'new_router', 'existing_router'}:
            ip_info = predict_hitl_link_ips_unique(scenario_key, iface.get('name'), idx, used_hitl_link_networks)
        if attachment in {'new_router', 'existing_router'} and ip_info:
            iface['link_network'] = ip_info.get('network')
            iface['link_network_cidr'] = ip_info.get('network_cidr') or ip_info.get('network')
            iface['prefix_len'] = ip_info.get('prefix_len')
            iface['netmask'] = ip_info.get('netmask')
            iface['existing_router_ip4'] = ip_info.get('existing_router_ip4')
            iface['new_router_ip4'] = ip_info.get('new_router_ip4')
            iface['rj45_ip4'] = ip_info.get('rj45_ip4')
            ipv4_current = iface.get('ipv4') if isinstance(iface.get('ipv4'), list) else []
            rj45_ip = iface.get('rj45_ip4')
            if rj45_ip:
                ordered = [rj45_ip] + [ip for ip in ipv4_current if ip != rj45_ip]
                iface['ipv4'] = ordered
        if attachment != 'new_router':
            continue
        if not ip_info:
            continue
        preview_router = _build_hitl_preview_router(scenario_key, iface, slug, idx, ip_info)
        preview_metadata = preview_router.setdefault('metadata', {})
        preview_metadata['scenario_key'] = scenario_key
        preview_metadata['ordinal'] = idx
        preview_metadata['interface_count'] = total_interfaces
        iface['preview_router'] = preview_router
        preview_routers.append(preview_router)
    if preview_routers:
        hitl_cfg['preview_routers'] = preview_routers


def _deterministic_hitl_peer_index(
    scenario_key: str,
    iface_name: str,
    ordinal: int,
    total_ifaces: int,
    candidate_count: int,
) -> Optional[int]:
    if candidate_count <= 0:
        return None
    total = total_ifaces if total_ifaces and total_ifaces > 0 else candidate_count
    seed = f"{scenario_key or '__default__'}|{iface_name or ordinal}|{ordinal}|{total}"
    try:
        base_digest = hashlib.sha256(seed.encode('utf-8', 'replace')).digest()
        counter_bytes = (0).to_bytes(8, 'little', signed=False)
        digest = hashlib.sha256(base_digest + counter_bytes).digest()
        value = int.from_bytes(digest[:8], 'big') / float(1 << 64)
        index = int(value * candidate_count) % candidate_count
        return index
    except Exception:
        return 0


def _wire_hitl_preview_routers(full_preview: Dict[str, Any], hitl_cfg: Dict[str, Any]) -> None:
    routers_list = full_preview.get('routers')
    if not isinstance(routers_list, list) or not routers_list:
        return
    interfaces = hitl_cfg.get('interfaces') or []
    preview_interfaces = [iface for iface in interfaces if isinstance(iface, dict) and iface.get('preview_router')]
    if not preview_interfaces:
        return
    base_routers = [router for router in routers_list if not (router.get('metadata', {}) or {}).get('hitl_preview')]
    if not base_routers:
        return
    scenario_key = hitl_cfg.get('scenario_key') or '__default__'
    total_ifaces = len(preview_interfaces)
    edges_list = full_preview.setdefault('r2r_edges_preview', [])
    existing_edge_pairs: set[tuple[int, int]] = set()
    normalized_edges: List[tuple[int, int]] = []
    for edge in list(edges_list):
        try:
            a, b = edge
            pair = tuple(sorted((int(a), int(b))))
            normalized_edges.append(pair)
            existing_edge_pairs.add(pair)
        except Exception:
            continue
    if normalized_edges:
        edges_list[:] = normalized_edges
    else:
        edges_list.clear()
    links_list = full_preview.setdefault('r2r_links_preview', [])
    existing_edge_id = max(
        (
            detail.get('edge_id', 0)
            for detail in links_list
            if isinstance(detail, dict) and isinstance(detail.get('edge_id'), int)
        ),
        default=0,
    )
    next_edge_id = existing_edge_id + 1
    degree_map = full_preview.get('r2r_degree_preview')
    if not isinstance(degree_map, dict):
        degree_map = {}
        full_preview['r2r_degree_preview'] = degree_map
    policy_preview = full_preview.get('r2r_policy_preview')
    policy_degree = None
    if isinstance(policy_preview, dict):
        policy_degree = policy_preview.setdefault('degree_sequence', {})
    router_lookup = {router.get('node_id'): router for router in routers_list if isinstance(router, dict)}
    for iface in preview_interfaces:
        preview_router = iface.get('preview_router')
        if not isinstance(preview_router, dict):
            continue
        metadata = preview_router.setdefault('metadata', {})
        if metadata.get('hitl_peer_wired'):
            continue
        new_router_id = preview_router.get('node_id')
        if new_router_id is None:
            continue
        candidate_count = len(base_routers)
        if candidate_count <= 0:
            continue
        iface_name = iface.get('name') or metadata.get('hitl_interface_name') or iface.get('slug') or f"iface-{iface.get('ordinal', 0)}"
        ordinal = iface.get('ordinal') if isinstance(iface.get('ordinal'), int) else metadata.get('ordinal') or 0
        total_count = iface.get('interface_count') if isinstance(iface.get('interface_count'), int) else metadata.get('interface_count') or total_ifaces
        peer_index = _deterministic_hitl_peer_index(
            scenario_key,
            str(iface_name),
            int(ordinal),
            int(total_count or total_ifaces or 1),
            candidate_count,
        ) or 0
        peer_router = base_routers[peer_index % candidate_count]
        peer_id = peer_router.get('node_id')
        if peer_id is None:
            continue
        prefix_len = iface.get('prefix_len') or metadata.get('prefix_len')
        new_ip = iface.get('new_router_ip4') or metadata.get('new_router_ip4')
        existing_ip = iface.get('existing_router_ip4') or metadata.get('existing_router_ip4')
        subnet = (
            iface.get('link_network_cidr')
            or metadata.get('link_network')
            or iface.get('link_network')
        )
        if subnet and prefix_len and '/' not in str(subnet):
            subnet = f"{subnet}/{prefix_len}"

        def _fmt_ip(ip: Any) -> Optional[str]:
            if not ip:
                return None
            ip_str = str(ip)
            if '/' in ip_str:
                return ip_str
            if prefix_len:
                return f"{ip_str}/{prefix_len}"
            return ip_str

        new_ip_cidr = _fmt_ip(new_ip)
        existing_ip_cidr = _fmt_ip(existing_ip)
        preview_iface_map = preview_router.setdefault('r2r_interfaces', {})
        peer_iface_map = peer_router.setdefault('r2r_interfaces', {})
        if new_ip_cidr:
            preview_iface_map[str(peer_id)] = new_ip_cidr
        else:
            preview_iface_map.setdefault(str(peer_id), '')
        if existing_ip_cidr:
            peer_iface_map[str(new_router_id)] = existing_ip_cidr
        else:
            peer_iface_map.setdefault(str(new_router_id), '')
        metadata['peer_router_node_id'] = peer_id
        metadata['peer_router_name'] = peer_router.get('name')
        metadata['hitl_peer_wired'] = True
        iface['peer_router_node_id'] = peer_id
        iface['target_router_id'] = peer_id
        layout_positions = full_preview.get('layout_positions')
        if isinstance(layout_positions, dict):
            routers_positions = layout_positions.setdefault('routers', {})
            if isinstance(routers_positions, dict):
                peer_pos = routers_positions.get(str(peer_id)) or routers_positions.get(peer_id)
                offset_x = 90 + 15 * (int(metadata.get('ordinal') or 0))
                offset_y = 60 + 10 * (int(metadata.get('ordinal') or 0))
                if isinstance(peer_pos, dict):
                    base_x = peer_pos.get('x', 0)
                    base_y = peer_pos.get('y', 0)
                else:
                    base_x = 200 + 120 * (int(metadata.get('ordinal') or 0))
                    base_y = 200 + 90 * (int(metadata.get('ordinal') or 0))
                routers_positions[str(new_router_id)] = {
                    'x': int(base_x) + offset_x,
                    'y': int(base_y) + offset_y,
                }
        edge_pair = tuple(sorted((int(peer_id), int(new_router_id))))
        if edge_pair not in existing_edge_pairs:
            existing_edge_pairs.add(edge_pair)
            edges_list.append(edge_pair)
            link_detail = {
                'edge_id': next_edge_id,
                'routers': [
                    {'id': peer_id, 'ip': existing_ip_cidr},
                    {'id': new_router_id, 'ip': new_ip_cidr},
                ],
                'subnet': subnet,
                'hitl_preview': True,
            }
            links_list.append(link_detail)
            if subnet:
                subnets_list = full_preview.setdefault('r2r_subnets', [])
                if subnet not in subnets_list:
                    subnets_list.append(subnet)
            degree_map[peer_id] = degree_map.get(peer_id, 0) + 1
            degree_map[new_router_id] = degree_map.get(new_router_id, 0) + 1
            next_edge_id += 1
            metadata['peer_router_node_id'] = peer_id
            iface['peer_router_node_id'] = peer_id
        else:
            degree_map.setdefault(peer_id, degree_map.get(peer_id, 0))
            degree_map.setdefault(new_router_id, degree_map.get(new_router_id, 0))
        if policy_degree is not None:
            policy_degree[str(peer_id)] = degree_map.get(peer_id, 0)
            policy_degree[str(new_router_id)] = degree_map.get(new_router_id, 0)
    if degree_map:
        values = [int(v) for v in degree_map.values() if isinstance(v, int)]
        if values:
            full_preview['r2r_stats_preview'] = {
                'min': min(values),
                'max': max(values),
                'avg': round(sum(values) / len(values), 2),
            }


def _augment_hitl_existing_router_interfaces(full_preview: Dict[str, Any], hitl_cfg: Dict[str, Any]) -> None:
    if not isinstance(full_preview, dict) or not isinstance(hitl_cfg, dict):
        return
    routers_list = full_preview.get('routers')
    if not isinstance(routers_list, list) or not routers_list:
        return
    interfaces = hitl_cfg.get('interfaces') or []
    existing_router_ifaces = [
        iface for iface in interfaces
        if isinstance(iface, dict) and _normalize_hitl_attachment(iface.get('attachment')) == 'existing_router'
    ]
    if not existing_router_ifaces:
        return
    base_router_entries = [
        router for router in routers_list
        if isinstance(router, dict) and not (router.get('metadata', {}) or {}).get('hitl_preview')
    ]
    if not base_router_entries:
        return
    scenario_key = hitl_cfg.get('scenario_key') or '__default__'
    total_ifaces = len(existing_router_ifaces)
    router_lookup: Dict[Any, Dict[str, Any]] = {}
    for router in routers_list:
        if not isinstance(router, dict):
            continue
        node_id = router.get('node_id')
        if node_id is not None:
            router_lookup[node_id] = router
    links_list = full_preview.setdefault('r2r_links_preview', [])
    existing_edge_id = max(
        (
            detail.get('edge_id', 0)
            for detail in links_list
            if isinstance(detail, dict) and isinstance(detail.get('edge_id'), int)
        ),
        default=0,
    )
    next_edge_id = existing_edge_id + 1
    existing_link_keys: set[tuple[Any, Any]] = set()
    for link in list(links_list):
        if not isinstance(link, dict):
            continue
        routers = link.get('routers')
        if not isinstance(routers, list) or len(routers) < 2:
            continue
        ra = routers[0].get('id') if isinstance(routers[0], dict) else None
        rb = routers[1].get('id') if isinstance(routers[1], dict) else None
        if ra is None or rb is None:
            continue
        existing_link_keys.add((ra, rb))
        existing_link_keys.add((rb, ra))
    global_overlay = full_preview.setdefault('hitl_existing_router_interfaces', [])
    overlay_keys = {
        (entry.get('router_id'), entry.get('slug'))
        for entry in global_overlay
        if isinstance(entry, dict)
    }

    def _compose_ip_with_prefix(ip_val: Any, prefix_len: Any) -> Optional[str]:
        if not ip_val:
            return None
        ip_str = str(ip_val)
        if '/' in ip_str:
            return ip_str
        if prefix_len:
            try:
                return f"{ip_str}/{int(prefix_len)}"
            except Exception:
                return f"{ip_str}/{prefix_len}"
        return ip_str

    for iface in existing_router_ifaces:
        slug = iface.get('slug')
        if not isinstance(slug, str) or not slug:
            slug = _slugify_hitl_name(iface.get('name'), f"iface-{(iface.get('ordinal') or 0) + 1}")
            iface['slug'] = slug
        ordinal = iface.get('ordinal') if isinstance(iface.get('ordinal'), int) else existing_router_ifaces.index(iface)
        total_count = iface.get('interface_count') if isinstance(iface.get('interface_count'), int) else total_ifaces
        target_router_id = iface.get('target_router_id') if iface.get('target_router_id') in router_lookup else None
        if target_router_id is None:
            if not base_router_entries:
                continue
            iface_name = iface.get('name') or slug or f"iface-{ordinal}"
            peer_index = _deterministic_hitl_peer_index(
                scenario_key,
                str(iface_name),
                int(ordinal or 0),
                int(total_count or total_ifaces or 1),
                len(base_router_entries),
            ) or 0
            chosen_router = base_router_entries[peer_index % len(base_router_entries)]
            target_router_id = chosen_router.get('node_id')
        if target_router_id is None or target_router_id not in router_lookup:
            continue
        iface['target_router_id'] = target_router_id
        iface['peer_router_node_id'] = target_router_id
        router_entry = router_lookup[target_router_id]
        prefix_len = iface.get('prefix_len')
        existing_ip_cidr = _compose_ip_with_prefix(iface.get('existing_router_ip4'), prefix_len)
        rj45_ip_cidr = _compose_ip_with_prefix(iface.get('rj45_ip4'), prefix_len)
        iface['existing_router_ip4_cidr'] = existing_ip_cidr
        iface['rj45_ip4_cidr'] = rj45_ip_cidr
        peer_key = slug
        iface['hitl_peer_key'] = peer_key
        router_iface_map = router_entry.setdefault('r2r_interfaces', {})
        if existing_ip_cidr:
            router_iface_map[peer_key] = existing_ip_cidr
        else:
            router_iface_map.setdefault(peer_key, '')
        router_metadata = router_entry.setdefault('metadata', {})
        router_overlay_list = router_metadata.setdefault('hitl_existing_router_interfaces', [])
        router_overlay_keys = {entry.get('slug') for entry in router_overlay_list if isinstance(entry, dict)}
        overlay_entry = {
            'slug': slug,
            'interface_name': iface.get('name'),
            'router_id': target_router_id,
            'router_name': router_entry.get('name'),
            'ip': existing_ip_cidr,
            'rj45_ip': rj45_ip_cidr,
            'network': iface.get('link_network_cidr') or iface.get('link_network'),
            'hitl_preview': True,
            'hitl_attachment': 'existing_router',
            'hitl_peer_key': peer_key,
            'scenario_key': scenario_key,
        }
        if slug not in router_overlay_keys:
            router_overlay_list.append(dict(overlay_entry))
        global_key = (target_router_id, slug)
        if global_key not in overlay_keys:
            global_overlay.append(dict(overlay_entry))
            overlay_keys.add(global_key)
        link_key = (target_router_id, peer_key)
        if link_key not in existing_link_keys:
            link_detail = {
                'edge_id': next_edge_id,
                'routers': [
                    {'id': target_router_id, 'ip': existing_ip_cidr},
                    {'id': peer_key, 'ip': rj45_ip_cidr},
                ],
                'subnet': iface.get('link_network_cidr') or iface.get('link_network'),
                'hitl_preview': True,
                'hitl_attachment': 'existing_router',
                'hitl_interface_slug': slug,
            }
            links_list.append(link_detail)
            existing_link_keys.add(link_key)
            existing_link_keys.add((peer_key, target_router_id))
            next_edge_id += 1


def _merge_hitl_preview_with_full_preview(full_preview: Dict[str, Any], hitl_cfg: Dict[str, Any]) -> None:
    if not isinstance(full_preview, dict) or not isinstance(hitl_cfg, dict):
        return
    preview_routers = hitl_cfg.get('preview_routers') or []
    routers_list = full_preview.get('routers')
    if not isinstance(routers_list, list):
        routers_list = []
        full_preview['routers'] = routers_list
    existing_ids = set()
    for entry in routers_list:
        if isinstance(entry, dict):
            node_id = entry.get('node_id')
            if node_id is not None:
                existing_ids.add(node_id)
    appended_ids: List[Any] = []
    for router in preview_routers:
        if not isinstance(router, dict):
            continue
        node_id = router.get('node_id')
        if node_id in existing_ids:
            continue
        routers_list.append(router)
        existing_ids.add(node_id)
        appended_ids.append(node_id)
    if appended_ids:
        # Keep routers sorted by node_id for deterministic previews
        try:
            def _router_sort_key(entry: Any) -> tuple[int, int]:
                if not isinstance(entry, dict):
                    return (1, 0)
                node_id = entry.get('node_id')
                if isinstance(node_id, int):
                    return (0, node_id)
                sort_val = 0
                if node_id is not None:
                    try:
                        sort_val = int(str(node_id))
                    except Exception:
                        digest = hashlib.sha256(str(node_id).encode('utf-8', 'replace')).hexdigest()
                        sort_val = int(digest[:8], 16)
                return (0, sort_val)

            routers_list.sort(key=_router_sort_key)
        except Exception:
            pass
        hitl_router_ids = full_preview.get('hitl_router_ids')
        if not isinstance(hitl_router_ids, list):
            hitl_router_ids = []
            full_preview['hitl_router_ids'] = hitl_router_ids
        for node_id in appended_ids:
            hitl_router_ids.append(node_id)
        try:
            seen = set()
            deduped = []
            for nid in hitl_router_ids:
                if nid in seen:
                    continue
                seen.add(nid)
                deduped.append(nid)
            full_preview['hitl_router_ids'] = deduped
            hitl_router_ids = deduped
        except Exception:
            pass
        full_preview['hitl_router_count'] = len([nid for nid in hitl_router_ids if nid is not None])
    _wire_hitl_preview_routers(full_preview, hitl_cfg)
    _augment_hitl_existing_router_interfaces(full_preview, hitl_cfg)

"""Flask web backend for core-topo-gen.

Augmented to guarantee the in-repo version of core_topo_gen is imported
instead of any globally installed distribution so new planning modules
like planning.full_preview are always available.
"""

# Ensure repository root (parent directory) precedes any site-packages version & purge shadowed installs
try:
    _THIS_DIR = os.path.abspath(os.path.dirname(__file__))
    _REPO_ROOT = os.path.abspath(os.path.join(_THIS_DIR, '..'))
    if _REPO_ROOT not in sys.path:
        sys.path.insert(0, _REPO_ROOT)
    # Purge any pre-imported site-packages version of core_topo_gen so we always load in-repo
    import sys as _sys
    for k in list(_sys.modules.keys()):
        if k == 'core_topo_gen' or k.startswith('core_topo_gen.'):
            del _sys.modules[k]
except Exception:
    pass

try:
    from core_topo_gen.parsers.hitl import parse_hitl_info
except ModuleNotFoundError as exc:
    raise RuntimeError(
        "core_topo_gen package is not available from this context. "
        "Run webapp commands from the repository root so the in-repo package is importable."
    ) from exc

from core_topo_gen.utils.hitl import predict_hitl_link_ips, predict_hitl_link_ips_unique

# Proactively ensure the in-repo planning.full_preview module is available even if an
# older site-packages installation of core_topo_gen (without that module) is first on sys.path.
def _ensure_full_preview_module():  # safe no-op if already present
    try:
        import importlib, sys as _sys
        try:
            # Fast path: module already importable
            import core_topo_gen.planning.full_preview  # type: ignore
            try:
                app.logger.debug('[full_preview] already importable (fast path)')
            except Exception:
                pass
            return True
        except ModuleNotFoundError:
            # Force reload planning package from repo root then load file directly
            repo_root = _REPO_ROOT
            planning_dir = os.path.join(repo_root, 'core_topo_gen', 'planning')
            candidate = os.path.join(planning_dir, 'full_preview.py')
            if not os.path.exists(candidate):
                try:
                    app.logger.error('[full_preview] candidate missing at %s', candidate)
                except Exception:
                    pass
                return False
            import importlib.util
            spec = importlib.util.spec_from_file_location('core_topo_gen.planning.full_preview', candidate)
            if not spec or not spec.loader:
                try:
                    app.logger.error('[full_preview] spec/loader missing for %s', candidate)
                except Exception:
                    pass
                return False
            module = importlib.util.module_from_spec(spec)
            _sys.modules['core_topo_gen.planning.full_preview'] = module
            try:
                spec.loader.exec_module(module)  # type: ignore
            except Exception:
                try:
                    import traceback, io as _io
                    buf = _io.StringIO(); traceback.print_exc(file=buf)
                    app.logger.error('[full_preview] exec_module failed: %s', buf.getvalue())
                except Exception:
                    pass
                return False
            # Attach as attribute of planning package for attribute-based access patterns
            try:
                import core_topo_gen.planning as planning_pkg  # type: ignore
                setattr(planning_pkg, 'full_preview', module)
            except Exception:
                pass
            try:
                app.logger.info('[full_preview] dynamically loaded from %s', candidate)
            except Exception:
                pass
            return True
    except Exception:
        return False

# Attempt early so later endpoints succeed
try:
    if not _ensure_full_preview_module():
        # Will try again lazily in the endpoint if needed
        pass
except Exception:
    pass

app = Flask(__name__)
app.secret_key = os.environ.get('FLASK_SECRET', 'coretopogenweb')
_log_level_name = os.environ.get('WEBAPP_LOG_LEVEL', 'INFO').strip().upper()
try:
    app.logger.setLevel(getattr(logging, _log_level_name, logging.INFO))
except Exception:
    pass

def _enumerate_host_interfaces(include_down: bool = False) -> List[Dict[str, Any]]:
    """Return host network interfaces available for Hardware-in-the-Loop selection."""
    results: List[Dict[str, Any]] = []
    logger = getattr(app, 'logger', logging.getLogger(__name__))
    if HITL_DISABLE_HOST_ENUM:
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug('[hitl] host interface enumeration skipped (HITL_DISABLE_HOST_ENUM=1)')
        return results
    if psutil is None:
        logger.warning('[hitl] psutil not available; host interface enumeration skipped')
        try:
            logger.warning('[hitl] psutil import failed under interpreter: %s', sys.executable)
            logger.debug('[hitl] sys.path=%s', sys.path)
            logger.debug('[hitl] PATH=%s', os.environ.get('PATH'))
            logger.debug('[hitl] PYTHONPATH=%s', os.environ.get('PYTHONPATH'))
        except Exception:
            pass
        return results
    try:
        logger.debug('[hitl] enumerating host interfaces (include_down=%s)', include_down)
        stats = psutil.net_if_stats()
        addrs = psutil.net_if_addrs()
    except Exception as exc:
        logger.error('[hitl] interface enumeration failed while retrieving stats: %s', exc, exc_info=True)
        return results

    link_families = set()
    for attr in ('AF_LINK',):
        fam = getattr(psutil, attr, None)
        if fam is not None:
            link_families.add(fam)
    for attr in ('AF_PACKET', 'AF_LINK'):
        fam = getattr(socket, attr, None)
        if fam is not None:
            link_families.add(fam)

    total_seen = 0
    skipped_down = 0
    skipped_loopback = 0
    skipped_other = 0
    def _is_non_physical(ifname: str) -> Optional[str]:
        name_normalized = ifname.lower()
        if name_normalized in NON_PHYSICAL_INTERFACE_NAMES:
            return 'exact'
        for prefix in NON_PHYSICAL_INTERFACE_PREFIXES:
            if name_normalized.startswith(prefix):
                return f'prefix:{prefix}'
        for needle in NON_PHYSICAL_INTERFACE_SUBSTRINGS:
            if needle in name_normalized:
                return f'substr:{needle}'
        # Treat typical virtual adapters that expose no MAC and no IPv4 address as non-physical
        return None

    for name, addr_list in addrs.items():
        total_seen += 1
        stat = stats.get(name)
        is_up = bool(getattr(stat, 'isup', False)) if stat else False
        if not include_down and not is_up:
            skipped_down += 1
            logger.debug('[hitl] skipping interface %s: interface is down', name)
            continue

        ipv4: List[str] = []
        ipv6: List[str] = []
        mac_addr: Optional[str] = None
        is_loopback = False

        for addr in addr_list:
            fam = addr.family
            if fam == socket.AF_INET:
                if addr.address:
                    ipv4.append(addr.address)
                    if addr.address.startswith('127.'):
                        is_loopback = True
            elif fam == getattr(socket, 'AF_INET6', None):
                if addr.address:
                    address = addr.address.split('%')[0]
                    ipv6.append(address)
                    if address == '::1':
                        is_loopback = True
            elif fam in link_families:
                if addr.address and addr.address != '00:00:00:00:00:00':
                    mac_addr = addr.address

        name_lc = name.lower()
        if name_lc.startswith('lo') or name == 'lo0':
            is_loopback = True

        if is_loopback:
            skipped_loopback += 1
            logger.debug('[hitl] skipping interface %s: loopback detected', name)
            continue

        non_physical_reason = _is_non_physical(name)
        if non_physical_reason is not None:
            skipped_other += 1
            logger.debug('[hitl] skipping interface %s: marked non-physical (%s)', name, non_physical_reason)
            continue

        entry: Dict[str, Any] = {
            'name': name,
            'display': name,
            'mac': mac_addr,
            'ipv4': ipv4,
            'ipv6': ipv6,
            'mtu': getattr(stat, 'mtu', None) if stat else None,
            'speed': getattr(stat, 'speed', None) if stat else None,
            'is_up': is_up,
        }
        flags = getattr(stat, 'flags', None)
        if isinstance(flags, str):
            entry['flags'] = [flag for flag in flags.replace(',', ' ').split() if flag]
        elif isinstance(flags, (list, tuple, set)):
            entry['flags'] = list(flags)

        results.append(entry)
        logger.debug('[hitl] captured interface %s: mac=%s ipv4=%s ipv6=%s is_up=%s',
                     name, mac_addr, ','.join(ipv4) or '-', ','.join(ipv6) or '-', is_up)

    logger.debug(
        '[hitl] host interface enumeration complete: total_seen=%d exported=%d skipped_down=%d skipped_loopback=%d skipped_other=%d',
        total_seen,
        len(results),
        skipped_down,
        skipped_loopback,
        skipped_other,
    )


def _normalize_mac_value(value: Any) -> str:
    if not value:
        return ''
    text = str(value).strip().lower()
    return ''.join(ch for ch in text if ch.isalnum())


def _enumerate_core_vm_interfaces_via_ssh(
    *,
    ssh_host: str,
    ssh_port: int,
    username: str,
    password: str,
    prox_interfaces: Optional[List[Dict[str, Any]]] = None,
    include_down: bool = False,
    vm_context: Optional[Dict[str, Any]] = None,
    timeout: float = 10.0,
) -> List[Dict[str, Any]]:
    _ensure_paramiko_available()
    logger = getattr(app, 'logger', logging.getLogger(__name__))
    client = paramiko.SSHClient()  # type: ignore[assignment]
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
    try:
        client.connect(
            hostname=ssh_host,
            port=int(ssh_port),
            username=username,
            password=password,
            look_for_keys=False,
            allow_agent=False,
            timeout=timeout,
            banner_timeout=timeout,
            auth_timeout=timeout,
        )
    except Exception as exc:  # pragma: no cover - network dependent
        raise _SSHTunnelError(f'Failed to establish SSH session to {ssh_host}:{ssh_port}: {exc}') from exc
    try:
        physical_ifaces: Optional[set[str]] = None
        phys_cmd = 'for I in $(ls -1 /sys/class/net 2>/dev/null); do if [ -e "/sys/class/net/$I/device" ]; then printf "%s\n" "$I"; fi; done'
        try:
            _, phys_stdout, _ = client.exec_command(phys_cmd, timeout=timeout)
            phys_data = phys_stdout.read()
            phys_status = phys_stdout.channel.recv_exit_status() if hasattr(phys_stdout, 'channel') else 0
            if phys_status == 0:
                if isinstance(phys_data, bytes):
                    phys_text = phys_data.decode('utf-8', 'ignore')
                else:
                    phys_text = str(phys_data)
                physical_list = [line.strip() for line in phys_text.splitlines() if line.strip()]
                if physical_list:
                    physical_ifaces = {name for name in physical_list if name and not name.lower().startswith('lo')}
        except Exception:  # pragma: no cover - best effort only
            app.logger.debug('[hitl] unable to enumerate physical interface set from CORE VM', exc_info=True)

        command = 'LANG=C ip -json address show'
        try:
            stdin, stdout, stderr = client.exec_command(command, timeout=timeout)
            stdout_data = stdout.read()
            stderr_data = stderr.read()
            exit_status = stdout.channel.recv_exit_status() if hasattr(stdout, 'channel') else 0
        except Exception as exc:  # pragma: no cover - remote command failure
            raise RuntimeError(f'Failed to execute "{command}" on CORE VM: {exc}') from exc
        if isinstance(stdout_data, bytes):
            stdout_text = stdout_data.decode('utf-8', 'ignore')
        else:
            stdout_text = str(stdout_data)
        if exit_status != 0:
            err_text = stderr_data.decode('utf-8', 'ignore') if isinstance(stderr_data, bytes) else str(stderr_data)
            raise RuntimeError(f'CORE VM rejected interface query ({exit_status}): {err_text.strip() or "unknown error"}')
        try:
            parsed = json.loads(stdout_text.strip() or '[]')
        except json.JSONDecodeError as exc:
            raise RuntimeError('Unable to parse interface data from CORE VM (ip -json output invalid)') from exc
        if not isinstance(parsed, list):
            raise RuntimeError('Unexpected interface payload from CORE VM')

        prox_context = vm_context if isinstance(vm_context, dict) else {}
        prox_map: Dict[str, Dict[str, Any]] = {}
        if prox_interfaces and isinstance(prox_interfaces, list):
            for entry in prox_interfaces:
                if not isinstance(entry, dict):
                    continue
                mac_norm = _normalize_mac_value(entry.get('macaddr') or entry.get('mac') or entry.get('hwaddr'))
                if not mac_norm:
                    continue
                prox_info: Dict[str, Any] = {
                    'id': entry.get('id') or entry.get('interface_id') or entry.get('name') or entry.get('label'),
                    'macaddr': entry.get('macaddr') or entry.get('mac') or entry.get('hwaddr'),
                    'bridge': entry.get('bridge'),
                    'model': entry.get('model'),
                    'raw': entry,
                }
                prox_info.update({
                    'vm_key': prox_context.get('vm_key'),
                    'vm_name': prox_context.get('vm_name'),
                    'vm_node': prox_context.get('vm_node'),
                    'vmid': prox_context.get('vmid'),
                })
                prox_map[mac_norm] = prox_info

        results: List[Dict[str, Any]] = []
        for item in parsed:
            if not isinstance(item, dict):
                continue
            ifname = item.get('ifname') or item.get('name')
            if not ifname:
                continue
            name_lc = str(ifname).lower()
            if name_lc.startswith('lo'):
                continue
            if physical_ifaces is not None:
                base_name = str(ifname).split('@', 1)[0]
                base_name = base_name.split(':', 1)[0]
                root_name = base_name.split('.', 1)[0]
                if base_name not in physical_ifaces and root_name not in physical_ifaces:
                    continue
            flags = item.get('flags') if isinstance(item.get('flags'), list) else []
            operstate = str(item.get('operstate') or '').strip().upper()
            is_up = operstate == 'UP' or 'UP' in [str(flag).upper() for flag in flags]
            if not include_down and not is_up:
                continue
            mac_addr = item.get('address') or item.get('mac')
            mac_norm = _normalize_mac_value(mac_addr)
            addr_info = item.get('addr_info') if isinstance(item.get('addr_info'), list) else []
            ipv4: List[str] = []
            ipv6: List[str] = []
            for addr in addr_info:
                if not isinstance(addr, dict):
                    continue
                family = str(addr.get('family') or '').lower()
                value = addr.get('local') or addr.get('address')
                if not value:
                    continue
                if family == 'inet':
                    ipv4.append(str(value))
                elif family == 'inet6':
                    ipv6.append(str(value).split('%')[0])
            entry: Dict[str, Any] = {
                'name': ifname,
                'display': ifname,
                'mac': mac_addr,
                'ipv4': ipv4,
                'ipv6': ipv6,
                'mtu': item.get('mtu'),
                'speed': item.get('link_speed') or None,
                'is_up': is_up,
                'flags': flags,
            }
            prox_entry = prox_map.get(mac_norm)
            if prox_entry:
                entry['proxmox'] = prox_entry
                bridge_val = prox_entry.get('bridge')
                if bridge_val:
                    entry['bridge'] = bridge_val
            results.append(entry)

        results.sort(key=lambda rec: rec.get('name') or '')
        logger.info('[hitl] enumerated %d interfaces via CORE VM SSH (host=%s)', len(results), ssh_host)
        return results
    finally:
        try:
            client.close()
        except Exception:
            logger.debug('SSH client close failed after interface enumeration', exc_info=True)


def _enumerate_core_vm_interfaces_from_secret(
    secret_id: str,
    *,
    prox_interfaces: Optional[List[Dict[str, Any]]] = None,
    include_down: bool = False,
    vm_context: Optional[Dict[str, Any]] = None,
) -> List[Dict[str, Any]]:
    if not secret_id:
        raise ValueError('CORE credential identifier is required')
    record = _load_core_credentials(secret_id)
    if not record:
        raise ValueError('Stored CORE credentials not found')
    password = record.get('ssh_password_plain') or record.get('password_plain') or ''
    if not password:
        raise ValueError('Stored CORE credentials are missing password material')
    username = str(record.get('ssh_username') or '').strip()
    if not username:
        raise ValueError('Stored CORE credentials are missing SSH username')
    ssh_host = str(record.get('ssh_host') or record.get('host') or '').strip()
    if not ssh_host:
        raise ValueError('Stored CORE credentials are missing SSH host')
    try:
        ssh_port = int(record.get('ssh_port') or 22)
    except Exception:
        ssh_port = 22
    vm_meta = vm_context if isinstance(vm_context, dict) else {
        'vm_key': record.get('vm_key'),
        'vm_name': record.get('vm_name'),
        'vm_node': record.get('vm_node'),
        'vmid': record.get('vmid'),
    }
    return _enumerate_core_vm_interfaces_via_ssh(
        ssh_host=ssh_host,
        ssh_port=ssh_port,
        username=username,
        password=password,
        prox_interfaces=prox_interfaces,
        include_down=include_down,
        vm_context=vm_meta,
    )

    results.sort(key=lambda item: item['name'])
    return results

# ----------------------- Basic Path Helpers (restored) -----------------------
def _get_repo_root() -> str:
    """Return absolute repository root (directory containing this webapp folder)."""
    try:
        return _REPO_ROOT
    except Exception:
        return os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

def _outputs_dir() -> str:
    d = os.path.join(_get_repo_root(), 'outputs')
    os.makedirs(d, exist_ok=True)
    return d

def _uploads_dir() -> str:
    d = os.path.join(_get_repo_root(), 'uploads')
    os.makedirs(d, exist_ok=True)
    return d

def _reports_dir() -> str:
    d = os.path.join(_get_repo_root(), 'reports')
    os.makedirs(d, exist_ok=True)
    return d


def _try_resolve_latest_outputs_xml(xml_path: str) -> Optional[str]:
    """Best-effort recovery for stale saved XML paths.

    The web UI stores `result_path` pointing to an XML under `outputs/scenarios-<ts>/...`.
    Those folders can be deleted or moved (e.g., scenario deletion/purge), leaving a stale
    path in localStorage. When that happens, try to find the newest file with the same
    basename under `outputs/scenarios-*`.

    Returns an absolute path if found, else None.
    """
    try:
        if not xml_path:
            return None
        abs_path = os.path.abspath(xml_path)
        if os.path.exists(abs_path):
            return abs_path
        base = os.path.basename(abs_path)
        if not base.lower().endswith('.xml'):
            return None
        outputs_dir = os.path.abspath(_outputs_dir())
        import glob
        candidates = glob.glob(os.path.join(outputs_dir, 'scenarios-*', base))
        candidates = [p for p in candidates if p and os.path.exists(p)]
        if not candidates:
            return None
        candidates.sort(key=lambda p: os.path.getmtime(p), reverse=True)
        best = os.path.abspath(candidates[0])
        try:
            if os.path.commonpath([best, outputs_dir]) != outputs_dir:
                return None
        except Exception:
            return None
        return best
    except Exception:
        return None


def _derive_default_seed(xml_hash: str) -> int:
    try:
        seed_val = int(xml_hash[:12], 16)
        seed_val %= (2**31 - 1)
        if seed_val <= 0:
            seed_val = 97531
        return seed_val
    except Exception:
        return 1357911

# Additional helper dirs (stubs restored after accidental removal)
def _traffic_dir() -> str:
    d = os.path.join(_outputs_dir(), 'traffic')
    os.makedirs(d, exist_ok=True)
    return d

def _segmentation_dir() -> str:
    d = os.path.join(_outputs_dir(), 'segmentation')
    os.makedirs(d, exist_ok=True)
    return d

def _vuln_base_dir() -> str:
    d = os.path.join(_outputs_dir(), 'vulns')
    os.makedirs(d, exist_ok=True)
    return d

def _vuln_repo_subdir() -> str:
    return 'repo'


def _ensure_private_dir(path: str, mode: int = 0o700) -> str:
    os.makedirs(path, exist_ok=True)
    if os.name != 'nt':
        try:
            os.chmod(path, mode)
        except Exception:
            pass
    return path


def _ensure_private_file(path: str, mode: int = 0o600) -> None:
    if os.name != 'nt':
        try:
            os.chmod(path, mode)
        except Exception:
            pass


def _sanitize_secret_slug(raw: str, fallback: str = 'entry') -> str:
    cleaned = ''.join(ch.lower() if ch.isalnum() else '-' for ch in (raw or ''))
    cleaned = re.sub(r'-{2,}', '-', cleaned).strip('-')
    return cleaned[:48] or fallback


def _proxmox_secret_dir() -> str:
    base = _ensure_private_dir(os.path.join(_outputs_dir(), 'secrets'))
    prox_dir = os.path.join(base, 'proxmox')
    return _ensure_private_dir(prox_dir)


def _proxmox_secret_key_path() -> str:
    return os.path.join(_proxmox_secret_dir(), '.key')


def _load_or_create_proxmox_key() -> bytes:
    if Fernet is None:  # pragma: no cover - dependency missing
        raise RuntimeError('cryptography package is required for secure Proxmox credential storage')
    env_key = os.environ.get('PROXMOX_SECRET_KEY')
    if env_key:
        key_bytes = env_key.encode('utf-8')
        try:
            Fernet(key_bytes)
            return key_bytes
        except Exception as exc:  # pragma: no cover - misconfigured env
            logging.getLogger(__name__).warning('Invalid PROXMOX_SECRET_KEY provided: %s', exc)
    key_path = _proxmox_secret_key_path()
    if os.path.exists(key_path):
        try:
            with open(key_path, 'rb') as fh:
                key_bytes = fh.read().strip()
            if key_bytes:
                Fernet(key_bytes)  # validate
                return key_bytes
        except Exception:
            logging.getLogger(__name__).warning('Existing Proxmox secret key invalid; regenerating')
    key_bytes = Fernet.generate_key()
    tmp_path = key_path + '.tmp'
    with open(tmp_path, 'wb') as fh:
        fh.write(key_bytes)
    os.replace(tmp_path, key_path)
    _ensure_private_file(key_path)
    return key_bytes


def _get_proxmox_cipher():
    if Fernet is None:  # pragma: no cover - dependency missing
        raise RuntimeError('cryptography package is required for secure Proxmox credential storage')
    key = _load_or_create_proxmox_key()
    return Fernet(key)


def _sanitize_proxmox_slug(raw: str, fallback: str = 'scenario') -> str:
    return _sanitize_secret_slug(raw, fallback)


def _derive_proxmox_identifier(
    scenario_name: str,
    scenario_index: Optional[int],
    url: str,
    username: str,
) -> str:
    slug = _sanitize_proxmox_slug(scenario_name or '')
    index_part = f"{scenario_index:02d}-" if isinstance(scenario_index, int) and scenario_index >= 0 else ''
    fingerprint_src = f"{url}|{username}"
    fingerprint = hashlib.sha256(fingerprint_src.encode('utf-8', 'ignore')).hexdigest()[:12]
    return f"{index_part}{slug}-{fingerprint}"


def _proxmox_secret_path(identifier: str) -> str:
    safe = _sanitize_proxmox_slug(identifier, 'proxmox')
    return os.path.join(_proxmox_secret_dir(), f"{safe}.json")


def _save_proxmox_credentials(payload: Dict[str, Any]) -> Dict[str, Any]:
    cipher = _get_proxmox_cipher()
    scenario_name = str(payload.get('scenario_name') or '').strip()
    scenario_index = payload.get('scenario_index')
    url = str(payload.get('url') or '').strip()
    username = str(payload.get('username') or '').strip()
    password = payload.get('password') or ''
    port = int(payload.get('port') or 8006)
    verify_ssl = bool(payload.get('verify_ssl', False))
    identifier = _derive_proxmox_identifier(scenario_name, scenario_index if isinstance(scenario_index, int) else None, url, username)
    encrypted_password = cipher.encrypt(password.encode('utf-8')).decode('utf-8')
    record = {
        'identifier': identifier,
        'scenario_name': scenario_name,
        'scenario_index': scenario_index if isinstance(scenario_index, int) else None,
        'url': url,
        'port': port,
        'username': username,
        'password': encrypted_password,
        'verify_ssl': verify_ssl,
        'stored_at': datetime.datetime.now(datetime.UTC).isoformat(),
    }
    path = _proxmox_secret_path(identifier)
    tmp_path = path + '.tmp'
    with open(tmp_path, 'w', encoding='utf-8') as fh:
        json.dump(record, fh, indent=2)
    os.replace(tmp_path, path)
    _ensure_private_file(path)
    return {
        'identifier': identifier,
        'url': url,
        'port': port,
        'username': username,
        'verify_ssl': verify_ssl,
        'stored_at': record['stored_at'],
    }


def _load_proxmox_credentials(identifier: str) -> Optional[Dict[str, Any]]:
    path = _proxmox_secret_path(identifier)
    if not os.path.exists(path):
        return None
    try:
        with open(path, 'r', encoding='utf-8') as fh:
            data = json.load(fh)
        cipher = _get_proxmox_cipher()
        enc = data.get('password')
        password = ''
        if isinstance(enc, str) and enc:
            try:
                password = cipher.decrypt(enc.encode('utf-8')).decode('utf-8')
            except InvalidToken:
                password = ''
        data['password_plain'] = password
        return data
    except Exception:
        logging.getLogger(__name__).exception('Failed to load Proxmox credentials for %s', identifier)
        return None


def _delete_proxmox_credentials(identifier: str) -> bool:
    if not isinstance(identifier, str) or not identifier.strip():
        return False
    path = _proxmox_secret_path(identifier)
    try:
        if os.path.exists(path):
            os.remove(path)
            return True
    except Exception:
        logging.getLogger(__name__).exception('Failed to delete Proxmox credentials for %s', identifier)
        raise
    return False


def _core_secret_dir() -> str:
    base = _ensure_private_dir(os.path.join(_outputs_dir(), 'secrets'))
    core_dir = os.path.join(base, 'core')
    return _ensure_private_dir(core_dir)


def _core_secret_key_path() -> str:
    return os.path.join(_core_secret_dir(), '.key')


def _load_or_create_core_key() -> bytes:
    if Fernet is None:  # pragma: no cover - dependency missing
        raise RuntimeError('cryptography package is required for secure CORE credential storage')
    env_key = os.environ.get('CORE_SECRET_KEY')
    if env_key:
        key_bytes = env_key.encode('utf-8')
        try:
            Fernet(key_bytes)
            return key_bytes
        except Exception as exc:  # pragma: no cover - misconfigured env
            logging.getLogger(__name__).warning('Invalid CORE_SECRET_KEY provided: %s', exc)
    key_path = _core_secret_key_path()
    if os.path.exists(key_path):
        try:
            with open(key_path, 'rb') as fh:
                key_bytes = fh.read().strip()
            if key_bytes:
                Fernet(key_bytes)
                return key_bytes
        except Exception:
            logging.getLogger(__name__).warning('Existing CORE secret key invalid; regenerating')
    key_bytes = Fernet.generate_key()
    tmp_path = key_path + '.tmp'
    with open(tmp_path, 'wb') as fh:
        fh.write(key_bytes)
    os.replace(tmp_path, key_path)
    _ensure_private_file(key_path)
    return key_bytes


def _get_core_cipher():
    if Fernet is None:  # pragma: no cover - dependency missing
        raise RuntimeError('cryptography package is required for secure CORE credential storage')
    key = _load_or_create_core_key()
    return Fernet(key)


def _derive_core_identifier(
    scenario_name: str,
    scenario_index: Optional[int],
    host: str,
    ssh_username: str,
) -> str:
    slug = _sanitize_secret_slug(scenario_name or '', 'core')
    index_part = f"{scenario_index:02d}-" if isinstance(scenario_index, int) and scenario_index >= 0 else ''
    fingerprint_src = f"{host}|{ssh_username}"
    fingerprint = hashlib.sha256(fingerprint_src.encode('utf-8', 'ignore')).hexdigest()[:12]
    return f"{index_part}{slug}-{fingerprint}"


def _save_core_credentials(payload: Dict[str, Any]) -> Dict[str, Any]:
    cipher = _get_core_cipher()
    scenario_name = str(payload.get('scenario_name') or '').strip()
    raw_index = payload.get('scenario_index')
    scenario_index: Optional[int]
    try:
        scenario_index = int(raw_index)
    except Exception:
        scenario_index = None
    grpc_host = str(payload.get('grpc_host') or payload.get('host') or '').strip()
    if not grpc_host:
        raise ValueError('gRPC host is required to persist CORE credentials')
    try:
        grpc_port = int(payload.get('grpc_port') or payload.get('port') or 50051)
    except Exception:
        grpc_port = 50051
    ssh_host = str(payload.get('ssh_host') or grpc_host).strip()
    try:
        ssh_port = int(payload.get('ssh_port') or 22)
    except Exception:
        ssh_port = 22
    ssh_username = str(payload.get('ssh_username') or '').strip()
    ssh_password_raw = payload.get('ssh_password') or ''
    if not isinstance(ssh_password_raw, str):
        ssh_password_raw = str(ssh_password_raw)
    if not ssh_username:
        raise ValueError('SSH username is required to persist CORE credentials')
    if not ssh_password_raw:
        raise ValueError('SSH password is required to persist CORE credentials')
    ssh_enabled = bool(payload.get('ssh_enabled', True))
    venv_bin_raw = payload.get('venv_bin')
    if venv_bin_raw in (None, ''):
        venv_bin = DEFAULT_CORE_VENV_BIN
    else:
        venv_bin = str(venv_bin_raw).strip() or DEFAULT_CORE_VENV_BIN
    identifier = _derive_core_identifier(scenario_name, scenario_index, grpc_host or ssh_host, ssh_username)
    encrypted_password = cipher.encrypt(ssh_password_raw.encode('utf-8')).decode('utf-8')
    vm_key = str(payload.get('vm_key') or '').strip()
    vm_name = str(payload.get('vm_name') or '').strip()
    vm_node = str(payload.get('vm_node') or '').strip()
    vmid_raw = payload.get('vmid')
    vmid = ''
    if isinstance(vmid_raw, (str, int)):
        vmid = str(vmid_raw).strip()
    prox_secret_raw = payload.get('proxmox_secret_id') or payload.get('secret_id')
    prox_secret_id = str(prox_secret_raw).strip() if prox_secret_raw not in (None, '') else None
    prox_target = None
    raw_target = payload.get('proxmox_target')
    if isinstance(raw_target, dict):
        target_slim: Dict[str, Any] = {}
        for key in ('node', 'vmid', 'interface_id', 'macaddr', 'bridge', 'model', 'vm_name', 'label'):
            if key in raw_target:
                target_slim[key] = raw_target.get(key)
        prox_target = target_slim or None
    record = {
        'identifier': identifier,
        'scenario_name': scenario_name,
        'scenario_index': scenario_index,
        'host': grpc_host,
        'port': grpc_port,
        'grpc_host': grpc_host,
        'grpc_port': grpc_port,
        'ssh_host': ssh_host,
        'ssh_port': ssh_port,
        'ssh_username': ssh_username,
        'ssh_enabled': ssh_enabled,
        'venv_bin': venv_bin,
        'password': encrypted_password,
        'vm_key': vm_key,
        'vm_name': vm_name,
        'vm_node': vm_node,
    'vmid': vmid if vmid else None,
        'proxmox_secret_id': prox_secret_id,
        'proxmox_target': prox_target,
        'stored_at': datetime.datetime.now(datetime.UTC).isoformat(),
    }
    path = os.path.join(_core_secret_dir(), f"{identifier}.json")
    tmp_path = path + '.tmp'
    with open(tmp_path, 'w', encoding='utf-8') as fh:
        json.dump(record, fh, indent=2)
    os.replace(tmp_path, path)
    _ensure_private_file(path)
    return {
        'identifier': identifier,
        'scenario_name': scenario_name,
        'scenario_index': scenario_index,
        'host': grpc_host,
        'port': grpc_port,
        'grpc_host': grpc_host,
        'grpc_port': grpc_port,
        'ssh_host': ssh_host,
        'ssh_port': ssh_port,
        'ssh_username': ssh_username,
        'ssh_enabled': ssh_enabled,
        'venv_bin': venv_bin,
        'vm_key': vm_key,
        'vm_name': vm_name,
        'vm_node': vm_node,
    'vmid': vmid if vmid else None,
        'proxmox_secret_id': prox_secret_id,
        'proxmox_target': prox_target,
        'stored_at': record['stored_at'],
    }


def _load_core_credentials(identifier: str) -> Optional[Dict[str, Any]]:
    path = os.path.join(_core_secret_dir(), f"{identifier}.json")
    if not os.path.exists(path):
        return None
    try:
        with open(path, 'r', encoding='utf-8') as fh:
            data = json.load(fh)
        cipher = _get_core_cipher()
        encrypted = data.get('password')
        password_plain = ''
        if isinstance(encrypted, str) and encrypted:
            try:
                password_plain = cipher.decrypt(encrypted.encode('utf-8')).decode('utf-8')
            except InvalidToken:
                password_plain = ''
        data['ssh_password_plain'] = password_plain
        return data
    except Exception:
        logging.getLogger(__name__).exception('Failed to load CORE credentials for %s', identifier)
        return None


def _delete_core_credentials(identifier: str) -> bool:
    if not isinstance(identifier, str) or not identifier.strip():
        return False
    path = os.path.join(_core_secret_dir(), f"{identifier}.json")
    try:
        if os.path.exists(path):
            os.remove(path)
            return True
    except Exception:
        logging.getLogger(__name__).exception('Failed to delete CORE credentials for %s', identifier)
        raise
    return False


def _connect_proxmox_from_secret(identifier: str, *, timeout: float = 8.0) -> tuple[Any, Dict[str, Any]]:
    if ProxmoxAPI is None:  # pragma: no cover - dependency missing
        raise RuntimeError('Proxmox integration unavailable: install proxmoxer package')
    if not isinstance(identifier, str) or not identifier.strip():
        raise ValueError('Proxmox secret identifier is required')
    record = _load_proxmox_credentials(identifier)
    if not record:
        raise ValueError('Stored Proxmox credentials not found')
    password = record.get('password_plain') or ''
    if not password:
        raise ValueError('Stored Proxmox credentials are missing password material')
    url_raw = str(record.get('url') or '').strip()
    parsed = urlparse(url_raw) if url_raw else None
    host = parsed.hostname if parsed and parsed.hostname else url_raw
    if not host:
        raise ValueError('Stored Proxmox URL is invalid')
    try:
        port = int(record.get('port') or (parsed.port if parsed else 8006) or 8006)
    except Exception:
        port = 8006
    verify_ssl = bool(record.get('verify_ssl', True))
    backend = 'https'
    if parsed and parsed.scheme:
        backend = 'https' if parsed.scheme.lower() == 'https' else 'http'
    client = ProxmoxAPI(  # type: ignore[call-arg]
        host=host,
        user=record.get('username'),
        password=password,
        port=port,
        verify_ssl=verify_ssl,
        timeout=max(2.0, min(float(timeout), 30.0)),
        backend=backend,
    )
    return client, record


def _parse_proxmox_net_config(raw: Any) -> Dict[str, Any]:
    result: Dict[str, Any] = {}
    if not isinstance(raw, str) or not raw:
        return result
    tokens = [tok.strip() for tok in raw.split(',') if tok.strip()]
    for idx, token in enumerate(tokens):
        if '=' not in token:
            continue
        key, value = token.split('=', 1)
        key = key.strip().lower()
        value = value.strip()
        if idx == 0 and key in {'virtio', 'e1000', 'rtl8139', 'vmxnet3', 'ne2k_isa', 'i82551'}:
            result['model'] = key
            result['macaddr'] = value
            continue
        if key == 'macaddr' and 'macaddr' in result:
            # Keep first mac address discovered from model token
            continue
        result[key] = value
    return result


_BRIDGE_NAME_SANITIZER = re.compile(r'[^a-z0-9_-]+')


def _normalize_internal_bridge_name(raw: Any) -> str:
    if raw is None:
        raise ValueError('Internal bridge name is required')
    candidate = str(raw).strip().lower()
    if not candidate:
        raise ValueError('Internal bridge name is required')
    candidate = _BRIDGE_NAME_SANITIZER.sub('-', candidate).strip('-_')
    candidate = re.sub(r'[-_]{2,}', '-', candidate)
    if not candidate:
        raise ValueError('Internal bridge name is invalid')
    if len(candidate) > 10:
        candidate = candidate[:10].rstrip('-_')
        if not candidate:
            raise ValueError('Internal bridge name is invalid')
    if not re.fullmatch(r'[a-z0-9][a-z0-9_-]*', candidate):
        raise ValueError('Internal bridge name may contain only lowercase letters, numbers, hyphen, and underscore, and must start with an alphanumeric character')
    return candidate


def _rewrite_bridge_in_net_config(config_value: str, bridge_name: str) -> tuple[str, bool, Optional[str]]:
    if not isinstance(config_value, str) or not config_value.strip():
        raise ValueError('Proxmox network configuration string is required')
    tokens = [tok.strip() for tok in config_value.split(',') if tok.strip()]
    previous_bridge: Optional[str] = None
    updated = False
    for idx, token in enumerate(tokens):
        if '=' not in token:
            continue
        key, value = token.split('=', 1)
        if key.strip().lower() == 'bridge':
            previous_bridge = value.strip()
            if previous_bridge == bridge_name:
                return config_value, False, previous_bridge
            tokens[idx] = f'bridge={bridge_name}'
            updated = True
            break
    if not updated:
        tokens.append(f'bridge={bridge_name}')
        updated = True
    new_config = ','.join(tokens)
    return new_config, updated, previous_bridge


def _parse_proxmox_vm_key(vm_key: str) -> tuple[str, int]:
    if not isinstance(vm_key, str):
        raise ValueError('VM key must be a string')
    parts = vm_key.split('::', 1)
    if len(parts) != 2:
        raise ValueError('VM key must be in the format "node::vmid"')
    node = parts[0].strip()
    vmid_raw = parts[1].strip()
    if not node or not vmid_raw:
        raise ValueError('VM key is missing node or VM ID information')
    try:
        vmid = int(vmid_raw)
    except Exception as exc:  # pragma: no cover - defensive
        raise ValueError(f'VM ID must be an integer (received {vmid_raw!r})') from exc
    return node, vmid


def _ensure_proxmox_bridge(client: Any, node: str, bridge_name: str, *, comment: Optional[str] = None) -> Dict[str, Any]:
    """Validate that the requested bridge already exists on the target node.

    The HITL workflow now assumes operators have pre-created the internal
    bridge (for example, ``<username>`` truncated to 10 characters) outside of apply time. We still
    enumerate the node's network devices so we can return a consistent
    metadata structure, but we no longer attempt to create or reload the
    bridge automatically. If the bridge is missing, surface a clear error so
    users can provision it manually before retrying.
    """

    try:
        networks = client.nodes(node).network.get()
    except Exception as exc:  # pragma: no cover - network enumeration failure
        raise RuntimeError(f'Failed to enumerate network devices on node {node}: {exc}') from exc

    for entry in networks or []:
        if not isinstance(entry, dict):
            continue
        iface = str(entry.get('iface') or '').strip()
        if iface == bridge_name:
            return {
                'created': False,
                'already_exists': True,
                'reload_invoked': False,
                'reload_ok': True,
                'reload_error': None,
            }

    raise RuntimeError(
        f'Bridge {bridge_name} not found on node {node}. Create the bridge manually before applying HITL mappings.'
    )


def _enumerate_proxmox_vms(identifier: str) -> Dict[str, Any]:
    client, record = _connect_proxmox_from_secret(identifier)
    inventory: List[Dict[str, Any]] = []
    nodes = []
    try:
        nodes = client.nodes.get()  # type: ignore[assignment]
    except Exception as exc:
        raise RuntimeError(f'Failed to list Proxmox nodes: {exc}') from exc
    for node in nodes or []:
        node_name = node.get('node') if isinstance(node, dict) else None
        if not node_name:
            continue
        try:
            vms = client.nodes(node_name).qemu.get()
        except Exception as exc:
            logging.getLogger(__name__).warning('Failed to enumerate VMs for node %s: %s', node_name, exc)
            continue
        for vm in vms or []:
            if not isinstance(vm, dict):
                continue
            vmid = vm.get('vmid')
            if vmid is None:
                continue
            template_flag = vm.get('template')
            if template_flag in (True, 1, '1', 'true', 'TRUE'):
                continue
            try:
                vmid_int = int(vmid)
            except Exception:
                vmid_int = vmid
            vm_name = vm.get('name') or f'VM {vmid_int}'
            vm_status = vm.get('status')
            config: Dict[str, Any] = {}
            try:
                config = client.nodes(node_name).qemu(vmid_int).config.get()
            except Exception as exc:
                logging.getLogger(__name__).warning('Failed to fetch config for VM %s on node %s: %s', vmid_int, node_name, exc)
            interfaces: List[Dict[str, Any]] = []
            for key, value in (config or {}).items():
                if not isinstance(key, str) or not key.lower().startswith('net'):
                    continue
                parsed = _parse_proxmox_net_config(value)
                iface_entry = {
                    'id': key,
                    'macaddr': parsed.get('macaddr') or parsed.get('hwaddr') or '',
                    'bridge': parsed.get('bridge') or '',
                    'model': parsed.get('model') or parsed.get('modeltype') or '',
                    'tag': parsed.get('tag') or parsed.get('vlan') or '',
                    'firewall': parsed.get('firewall') or '',
                    'raw': value,
                }
                interfaces.append(iface_entry)
            inventory.append({
                'node': node_name,
                'vmid': vmid_int,
                'name': vm_name,
                'status': vm_status,
                'interfaces': interfaces,
            })
    return {
        'fetched_at': datetime.datetime.now(datetime.UTC).isoformat(),
        'url': record.get('url'),
        'username': record.get('username'),
        'verify_ssl': record.get('verify_ssl', True),
        'vms': inventory,
    }

# ---------------- User persistence helpers (restored) ----------------
def _users_db_path() -> str:
    override = os.environ.get('CORE_TOPO_GEN_USERS_DB_PATH')
    if override:
        return os.path.abspath(override)
    # During pytest, isolate persisted state so local dev credentials don't break tests.
    if os.environ.get('PYTEST_CURRENT_TEST') or ('pytest' in sys.modules):
        base = os.path.join(tempfile.gettempdir(), 'core_topo_gen_test_users')
        os.makedirs(base, exist_ok=True)
        return os.path.join(base, 'users.json')
    base = os.path.join(_outputs_dir(), 'users')
    os.makedirs(base, exist_ok=True)
    return os.path.join(base, 'users.json')


def _base_upload_state_path() -> str:
    return os.path.join(_outputs_dir(), 'base_upload.json')


def _sanitize_base_upload_meta(meta: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    if not isinstance(meta, dict):
        return None
    out: Dict[str, Any] = {}
    path = meta.get('path') or meta.get('filepath')
    if isinstance(path, str) and path:
        out['path'] = path
    display = meta.get('display_name') or meta.get('name')
    if isinstance(display, str) and display:
        out['display_name'] = display
    if 'valid' in meta:
        out['valid'] = bool(meta.get('valid'))
    if 'exists' in meta:
        out['exists'] = bool(meta.get('exists'))
    if not out.get('path'):
        return None
    return out


def _load_base_upload_state() -> Optional[Dict[str, Any]]:
    path = _base_upload_state_path()
    if not os.path.exists(path):
        return None
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return _sanitize_base_upload_meta(data)
    except Exception:
        return None


def _save_base_upload_state(meta: Dict[str, Any]) -> None:
    clean = _sanitize_base_upload_meta(meta)
    if not clean:
        return
    clean = dict(clean)
    clean['updated_at'] = datetime.datetime.now(datetime.UTC).isoformat()
    try:
        with open(_base_upload_state_path(), 'w', encoding='utf-8') as f:
            json.dump(clean, f, indent=2)
    except Exception:
        pass


def _clear_base_upload_state() -> None:
    path = _base_upload_state_path()
    try:
        if os.path.exists(path):
            os.remove(path)
    except Exception:
        pass


def _hydrate_base_upload_from_disk(payload: Dict[str, Any]) -> None:
    if payload.get('base_upload'):
        return
    meta = _load_base_upload_state()
    if not meta:
        return
    meta = dict(meta)
    path = meta.get('path') or ''
    exists = bool(path) and os.path.exists(path)
    meta['exists'] = exists
    if path and exists:
        ok, _errs = _validate_core_xml(path)
        meta['valid'] = bool(ok)
        if 'display_name' not in meta or not meta['display_name']:
            meta['display_name'] = os.path.basename(path)
    payload['base_upload'] = meta
    scen_list = payload.get('scenarios') or []
    if scen_list and isinstance(scen_list[0], dict):
        base_section = scen_list[0].setdefault('base', {})
        if path and not base_section.get('filepath'):
            base_section['filepath'] = path
        display = meta.get('display_name')
        if display and not base_section.get('display_name'):
            base_section['display_name'] = display

def _load_users() -> dict:
    p = _users_db_path()
    if not os.path.exists(p):
        return { 'users': [] }
    try:
        with open(p, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, dict) and isinstance(data.get('users'), list):
                return data
    except Exception:
        pass
    return { 'users': [] }

def _save_users(data: dict) -> None:
    p = _users_db_path(); tmp = p + '.tmp'
    try:
        os.makedirs(os.path.dirname(p), exist_ok=True)
        with open(tmp, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2)
        os.replace(tmp, p)
    except Exception:
        try:
            if os.path.exists(tmp): os.remove(tmp)
        except Exception: pass

def _ensure_admin_user() -> None:
    db = _load_users(); users = db.get('users', [])
    if not users:
        users = [{ 'username': 'coreadmin', 'password_hash': generate_password_hash('coreadmin'), 'role': 'admin' }]
        db['users'] = users; _save_users(db)
        try: app.logger.warning("Seeded default admin user 'coreadmin' / 'coreadmin'. Change immediately.")
        except Exception: pass
        return
    if not any(u.get('role') == 'admin' for u in users):
        import secrets as _secrets
        pwd = os.environ.get('ADMIN_PASSWORD') or _secrets.token_urlsafe(10)
        users.append({ 'username': 'admin', 'password_hash': generate_password_hash(pwd), 'role': 'admin' })
        db['users'] = users; _save_users(db)
        try: app.logger.warning("No admin found; created 'admin' user with generated password: %s", pwd)
        except Exception: pass

_ensure_admin_user()

def _current_user_record() -> Optional[dict]:
    if not has_request_context():
        return None
    cached = getattr(g, '_current_user_record', _G_USER_RECORD_SENTINEL)
    if cached is not _G_USER_RECORD_SENTINEL:
        return cached
    record: Optional[dict] = None
    user = _current_user()
    if user and user.get('username'):
        username = user.get('username')
        db = _load_users()
        for entry in db.get('users', []):
            if entry.get('username') != username:
                continue
            record = dict(entry)
            record.pop('password_hash', None)
            record['role'] = _normalize_role_value(entry.get('role'))
            record['scenarios'] = _normalize_scenario_assignments(entry.get('scenarios'))
            break
    setattr(g, '_current_user_record', record)
    return record


def _current_user_assigned_scenarios() -> list[str]:
    record = _current_user_record()
    if not record:
        return []
    scenarios = record.get('scenarios')
    if isinstance(scenarios, list):
        return list(scenarios)
    return []


def _assigned_scenarios_for_user(user: Optional[dict]) -> list[str]:
    if not user or not isinstance(user, dict):
        return []
    username = user.get('username') or ''
    if not isinstance(username, str):
        try:
            username = str(username)
        except Exception:
            username = ''
    username = username.strip()
    if not username:
        return []
    if has_request_context():
        current = _current_user()
        if current and current.get('username') == username:
            return _current_user_assigned_scenarios()
    db = _load_users()
    users = db.get('users', []) if isinstance(db, dict) else []
    for entry in users:
        if not isinstance(entry, dict):
            continue
        if entry.get('username') != username:
            continue
        return _normalize_scenario_assignments(entry.get('scenarios'))
    return []


def _builder_allowed_norms(user: Optional[dict] = None) -> Optional[set[str]]:
    """Return assigned scenario norms for roles that must be restricted.

    Historically this function applied only to the builder role.
    Participants are also restricted to their assigned scenarios across the UI,
    so we include them here to keep scenario visibility consistent.
    """
    effective_user = user
    if effective_user is None and has_request_context():
        effective_user = _current_user()
    if not effective_user:
        return None
    role = _normalize_role_value(effective_user.get('role'))
    if role not in {'builder', 'participant'}:
        return None
    assigned = _assigned_scenarios_for_user(effective_user)
    return {norm for norm in assigned if norm}


def _builder_placeholder_scenario(name: str, participant_url: str = '') -> Dict[str, Any]:
    """Return a minimal scenario structure for builder-only views."""
    template = copy.deepcopy(_default_scenarios_payload()['scenarios'][0])
    template['name'] = name or template.get('name') or 'Scenario'
    hitl_meta = template.get('hitl') if isinstance(template.get('hitl'), dict) else {}
    hitl_meta = dict(hitl_meta)
    if participant_url:
        for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
            hitl_meta[key] = participant_url
    else:
        for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
            hitl_meta.pop(key, None)
    template['hitl'] = hitl_meta
    return template


def _parse_iso_datetime(raw: Any) -> Optional[datetime.datetime]:
    if not raw:
        return None
    try:
        text = str(raw).strip()
    except Exception:
        return None
    if not text:
        return None
    try:
        # Accept trailing Z
        if text.endswith('Z'):
            text = text[:-1] + '+00:00'
        dt = datetime.datetime.fromisoformat(text)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=datetime.timezone.utc)
        return dt
    except Exception:
        return None


def _select_builder_hitl_fallback(hints: dict[str, Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    """Pick a best-effort admin HITL validation hint for builder scenarios.

    Builder users (and admin users previewing builder mode) cannot validate or select
    CORE VMs themselves, but they still need to see admin-managed connectivity.
    """
    best: Optional[Dict[str, Any]] = None
    best_dt: Optional[datetime.datetime] = None
    for value in (hints or {}).values():
        if not isinstance(value, dict):
            continue
        prox = value.get('proxmox') if isinstance(value.get('proxmox'), dict) else {}
        core = value.get('core') if isinstance(value.get('core'), dict) else {}
        has_any = bool(
            (prox.get('secret_id') or prox.get('validated'))
            or (core.get('core_secret_id') or core.get('vm_key') or core.get('validated'))
        )
        if not has_any:
            continue
        dt = (
            _parse_iso_datetime(core.get('stored_at'))
            or _parse_iso_datetime(core.get('last_validated_at'))
            or _parse_iso_datetime(prox.get('stored_at'))
            or _parse_iso_datetime(prox.get('last_validated_at'))
        )
        if best is None:
            best = value
            best_dt = dt
            continue
        if best_dt is None and dt is not None:
            best = value
            best_dt = dt
            continue
        if best_dt is not None and dt is not None and dt > best_dt:
            best = value
            best_dt = dt
    return best


def _builder_catalog_seed_scenarios(
    allowed_norms: set[str],
    assignment_order: Optional[Iterable[str]] = None,
    *,
    user: Optional[dict] = None,
) -> list[Dict[str, Any]]:
    """Hydrate scenario payloads for builders from catalog + assignments."""
    if not allowed_norms:
        return []
    names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(None, user=user)
    hitl_validation_hints = _load_scenario_hitl_validation_from_disk()
    hitl_config_hints = _load_scenario_hitl_config_from_disk()
    builder_hitl_fallback = _select_builder_hitl_fallback(hitl_validation_hints)
    builder_hitl_config_fallback = _select_builder_hitl_fallback(hitl_config_hints) if hitl_config_hints else None
    # Compare permissions using match keys so minor punctuation differences
    # (e.g., "Scenario_1b" vs "Scenario 1b") don't hide scenarios.
    allowed_keys = {k for k in (_scenario_match_key(v) for v in allowed_norms) if k}
    if not allowed_keys:
        return []

    display_by_norm: dict[str, str] = {}
    catalog_norm_by_key: dict[str, str] = {}
    for display in names:
        norm = _normalize_scenario_label(display)
        key = _scenario_match_key(display)
        if norm:
            display_by_norm.setdefault(norm, display)
        if key and norm and key not in catalog_norm_by_key:
            catalog_norm_by_key[key] = norm

    def _humanize(norm_value: str) -> str:
        text = (norm_value or '').replace('_', ' ').strip()
        text = re.sub(r'\s+', ' ', text)
        return text.title() if text else norm_value

    ordered_keys: list[str] = []
    if assignment_order:
        for entry in assignment_order:
            k = _scenario_match_key(entry)
            if k and k in allowed_keys and k not in ordered_keys:
                ordered_keys.append(k)
    for display in names:
        k = _scenario_match_key(display)
        if k and k in allowed_keys and k not in ordered_keys:
            ordered_keys.append(k)
    # Any remaining allowed scenarios (not in catalog/assignment order)
    for k in sorted(allowed_keys):
        if k not in ordered_keys:
            ordered_keys.append(k)

    parsed_cache: dict[str, list[Dict[str, Any]]] = {}
    hydrated: list[Dict[str, Any]] = []

    # Map allowed keys back to a catalog norm if available (preserves exact display names).
    allowed_norm_by_key: dict[str, str] = {}
    for norm in allowed_norms:
        k = _scenario_match_key(norm)
        if k and k not in allowed_norm_by_key:
            allowed_norm_by_key[k] = norm

    for key in ordered_keys:
        norm = catalog_norm_by_key.get(key) or allowed_norm_by_key.get(key) or ''
        display_name = display_by_norm.get(norm) or _humanize(norm or key)
        participant_hint = scenario_url_hints.get(norm, '') if scenario_url_hints else ''
        validation_hint = hitl_validation_hints.get(norm) if hitl_validation_hints else None
        config_hint = hitl_config_hints.get(norm) if hitl_config_hints else None
        scenario_copy: Optional[Dict[str, Any]] = None
        candidate_paths = scenario_paths.get(norm) if scenario_paths else None
        best_path = _select_existing_path(candidate_paths) if candidate_paths else None
        if best_path:
            cached = parsed_cache.get(best_path)
            if cached is None:
                try:
                    parsed = _parse_scenarios_xml(best_path)
                    cached = parsed.get('scenarios', []) if isinstance(parsed, dict) else []
                except Exception:
                    cached = []
                parsed_cache[best_path] = cached
            for scen in cached or []:
                if not isinstance(scen, dict):
                    continue
                scen_norm = _normalize_scenario_label(scen.get('name'))
                if scen_norm == norm:
                    scenario_copy = copy.deepcopy(scen)
                    break
        if not scenario_copy:
            scenario_copy = _builder_placeholder_scenario(display_name, participant_hint)
        else:
            scenario_copy['name'] = display_name
            if participant_hint:
                hitl_meta = scenario_copy.get('hitl') if isinstance(scenario_copy.get('hitl'), dict) else {}
                hitl_meta = dict(hitl_meta)
                for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                    hitl_meta[key] = participant_hint
                scenario_copy['hitl'] = hitl_meta

        # Merge admin-validated HITL hints (safe subset) into builder scenarios.
        if isinstance(validation_hint, dict) and validation_hint:
            hitl_meta = scenario_copy.get('hitl') if isinstance(scenario_copy.get('hitl'), dict) else {}
            hitl_meta = dict(hitl_meta)
            prox_hint = validation_hint.get('proxmox')
            if isinstance(prox_hint, dict) and prox_hint:
                prox_state = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else {}
                prox_state = dict(prox_state)
                prox_state.update(prox_hint)
                hitl_meta['proxmox'] = prox_state
            core_hint = validation_hint.get('core')
            if isinstance(core_hint, dict) and core_hint:
                core_state = hitl_meta.get('core') if isinstance(hitl_meta.get('core'), dict) else {}
                core_state = dict(core_state)
                core_state.update(core_hint)
                hitl_meta['core'] = core_state
            scenario_copy['hitl'] = hitl_meta

        # Builder-only fallback: builders can't validate/select CORE VMs, so fill any missing
        # hint fields (e.g., vm_key) from the best available admin validation hint.
        if builder_hitl_fallback:
            hitl_meta = scenario_copy.get('hitl') if isinstance(scenario_copy.get('hitl'), dict) else {}
            hitl_meta = dict(hitl_meta)
            prox_hint = builder_hitl_fallback.get('proxmox') if isinstance(builder_hitl_fallback.get('proxmox'), dict) else None
            core_hint = builder_hitl_fallback.get('core') if isinstance(builder_hitl_fallback.get('core'), dict) else None

            if isinstance(prox_hint, dict) and prox_hint:
                prox_state = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else {}
                prox_state = dict(prox_state)
                for k, v in prox_hint.items():
                    # Only fill missing values; don't override scenario-specific settings.
                    if k not in prox_state or prox_state.get(k) in (None, '', False):
                        prox_state[k] = v
                hitl_meta['proxmox'] = prox_state

            if isinstance(core_hint, dict) and core_hint:
                core_state = hitl_meta.get('core') if isinstance(hitl_meta.get('core'), dict) else {}
                core_state = dict(core_state)
                for k, v in core_hint.items():
                    if k not in core_state or core_state.get(k) in (None, '', False):
                        core_state[k] = v
                hitl_meta['core'] = core_state

            scenario_copy['hitl'] = hitl_meta

        # Merge admin-managed HITL configuration (enabled/interfaces/mappings) into builder scenarios.
        effective_cfg = config_hint if (isinstance(config_hint, dict) and config_hint) else builder_hitl_config_fallback
        if isinstance(effective_cfg, dict) and effective_cfg:
            hitl_meta = scenario_copy.get('hitl') if isinstance(scenario_copy.get('hitl'), dict) else {}
            hitl_meta = dict(hitl_meta)
            # Config hints are authoritative for builder views.
            try:
                participant_cfg = _normalize_participant_proxmox_url(effective_cfg.get('participant_proxmox_url'))
                if participant_cfg:
                    for k in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                        if k not in hitl_meta or hitl_meta.get(k) in (None, ''):
                            hitl_meta[k] = participant_cfg
            except Exception:
                pass
            if 'enabled' in effective_cfg:
                hitl_meta['enabled'] = bool(effective_cfg.get('enabled'))
            if isinstance(effective_cfg.get('interfaces'), list):
                hitl_meta['interfaces'] = effective_cfg.get('interfaces')
            if isinstance(effective_cfg.get('core'), dict):
                core_state = hitl_meta.get('core') if isinstance(hitl_meta.get('core'), dict) else {}
                core_state = dict(core_state)
                core_state.update(effective_cfg.get('core'))
                hitl_meta['core'] = core_state
            if isinstance(effective_cfg.get('proxmox'), dict):
                prox_state = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else {}
                prox_state = dict(prox_state)
                prox_state.update(effective_cfg.get('proxmox'))
                hitl_meta['proxmox'] = prox_state
            scenario_copy['hitl'] = hitl_meta
        hydrated.append(scenario_copy)
    return hydrated


def _builder_filter_report_scenarios(
    scenario_names: list[str],
    scenario_norm: str,
    *,
    user: Optional[dict] = None,
) -> tuple[list[str], str, Optional[set[str]]]:
    allowed_norms = _builder_allowed_norms(user)
    if allowed_norms is None:
        return scenario_names, scenario_norm, None
    allowed_keys = {key for key in (_scenario_match_key(v) for v in allowed_norms) if key}
    filtered = [
        name for name in scenario_names or []
        if _scenario_match_key(name) in allowed_keys
    ]
    normalized_selection = scenario_norm if (not scenario_norm or _scenario_match_key(scenario_norm) in allowed_keys) else ''
    return filtered, normalized_selection, allowed_norms

# Diagnostic endpoint for environment/module troubleshooting
@app.route('/diag/modules')
def diag_modules():
    out = {}
    # core_topo_gen package file
    try:
        import core_topo_gen as ctg  # type: ignore
        out['core_topo_gen.__file__'] = getattr(ctg, '__file__', None)
    except Exception as e:
        out['core_topo_gen_error'] = str(e)
    # planning package
    try:
        import core_topo_gen.planning as plan_pkg  # type: ignore
        planning_file = getattr(plan_pkg, '__file__', None)
        out['planning_dir'] = os.path.dirname(planning_file) if planning_file else None
        if not planning_file:
            out['planning_file_is_none'] = True
    except Exception as e:
        out['planning_import_error'] = str(e)

def _current_user() -> dict | None:
    user = session.get('user')
    if isinstance(user, dict) and user.get('username'):
        return user
    return None


def _set_current_user(user: dict | None) -> None:
    if user:
        session['user'] = {
            'username': user.get('username'),
            'role': _normalize_role_value(user.get('role'))
        }
    else:
        session.pop('user', None)


@app.before_request
def _inject_current_user() -> None:
    try:
        g.current_user = _current_user()
    except Exception:
        g.current_user = None


@app.before_request
def _bind_ui_view_mode() -> None:
    try:
        g.ui_view_mode = _current_ui_view_mode()
    except Exception:
        g.ui_view_mode = _UI_VIEW_DEFAULT


@app.context_processor
def _inject_template_user() -> dict:
    try:
        user = _current_user()
        if user:
            return {
                'current_user': SimpleNamespace(
                    username=user.get('username'),
                    role=_normalize_role_value(user.get('role')),
                    is_authenticated=True,
                )
            }
    except Exception:
        pass
    return {
        'current_user': SimpleNamespace(
            username=None,
            role=None,
            is_authenticated=False,
        )
    }


@app.context_processor
def _inject_nav_participant_link() -> dict:
    try:
        # If the current page is scoped to a scenario, the navbar Participant UI link
        # should reflect THAT scenario only. If that scenario has no participant URL,
        # hide the nav item instead of falling back to some other scenario that does.
        scenario_norm = ''
        scenario_label = ''
        try:
            scenario_label = (request.args.get('scenario') or '').strip() if has_request_context() else ''
            scenario_norm = _normalize_scenario_label(scenario_label)
        except Exception:
            scenario_norm = ''
            scenario_label = ''

        if scenario_norm:
            user = _current_user()
            scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(None, user=user)
            mapping = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
            url_value = mapping.get(scenario_norm, '')
            return {
                'nav_participant_url': url_value or '',
                'nav_participant_scenario': scenario_label or '',
            }

        # If the page is not scoped to a scenario (no `?scenario=`), avoid "guessing" a participant URL
        # for admin/builder views. Otherwise CORE/Reports can show Participant UI for an unrelated scenario.
        view_mode = getattr(g, 'ui_view_mode', _UI_VIEW_DEFAULT)
        if view_mode != 'participant':
            return {'nav_participant_url': '', 'nav_participant_scenario': ''}

        url_value, scenario_label = _resolve_participant_ui_target()
        return {
            'nav_participant_url': url_value,
            'nav_participant_scenario': scenario_label,
        }
    except Exception:
        return {'nav_participant_url': '', 'nav_participant_scenario': ''}


@app.context_processor
def _inject_ui_view_state() -> dict:
    return {'ui_view_mode': getattr(g, 'ui_view_mode', _UI_VIEW_DEFAULT)}


@app.route('/ui-view', methods=['POST'])
def set_ui_view_mode():
    user = _current_user()
    if not user or not _is_admin_view_role(user.get('role')):
        abort(403)
    requested = (request.form.get('mode') or '').strip().lower()
    if requested not in _UI_VIEW_ALLOWED:
        requested = _UI_VIEW_DEFAULT
    role = _normalize_role_value(user.get('role'))
    if role == 'builder' and requested == 'admin':
        requested = 'builder'
    if role not in _ADMIN_VIEW_ROLES:
        requested = _UI_VIEW_DEFAULT
    session[_UI_VIEW_SESSION_KEY] = requested
    target = request.form.get('next') or request.referrer
    scenario_hint = ''
    if target:
        try:
            parsed_target = urlparse(target)
            query_params = parse_qs(parsed_target.query or '')
            scenario_hint = (query_params.get('scenario', [''])[0] or '').strip()
        except Exception:
            scenario_hint = ''
    if not scenario_hint:
        try:
            scenario_hint = (request.form.get('scenario') or request.args.get('scenario') or '').strip()
        except Exception:
            scenario_hint = ''
    if requested == 'participant':
        redirect_target = url_for('participant_ui_page', scenario=scenario_hint) if scenario_hint else url_for('participant_ui_page')
    else:
        redirect_target = _resolve_ui_view_redirect_target(target)
    return redirect(redirect_target)


@app.route('/participant-ui')
def participant_ui_page():
    state = _participant_ui_state()
    url_value = state.get('selected_url', '')
    scenario_label = state.get('selected_label', '')
    nearest_gateway = state.get('selected_nearest_gateway', '')
    override = _normalize_participant_proxmox_url(request.args.get('url')) if request.args.get('url') else ''
    participant_url = override or url_value
    return render_template(
        'participant_ui.html',
        participant_url=participant_url,
        participant_scenario_label=scenario_label,
        participant_nearest_gateway=nearest_gateway,
        participant_scenarios=state.get('listing', []),
        participant_scenarios_heading=state.get('listing_heading'),
        participant_scenarios_hint=state.get('listing_hint'),
        participant_scenarios_empty=state.get('listing_empty_message'),
        participant_restricted=state.get('restrict_to_assigned', False),
        participant_active_norm=state.get('selected_norm', ''),
        participant_has_assignments=state.get('has_assignments', False),
    )


@app.route('/participant-ui/gateway')
def participant_ui_gateway_api():
    state = _participant_ui_state()
    scenario_norm = _normalize_scenario_label(request.args.get('scenario') or '')
    if not scenario_norm:
        scenario_norm = state.get('selected_norm', '')

    # Enforce assignment-based access: do not allow restricted users to query
    # gateway details for scenarios outside their assigned list.
    try:
        if state.get('restrict_to_assigned'):
            allowed_norms = {row.get('norm') for row in (state.get('listing') or []) if isinstance(row, dict) and row.get('norm')}
            if scenario_norm and scenario_norm not in allowed_norms:
                return jsonify({'ok': False, 'error': 'Scenario not assigned.'}), 403
    except Exception:
        pass

    gateway = ''

    # Prefer the most recent session XML for this scenario (matches core.html HITL gateway logic).
    try:
        history = _load_run_history()
        last_run = _latest_run_history_for_scenario(scenario_norm, history)
    except Exception:
        last_run = None
    session_xml_path = None
    if isinstance(last_run, dict):
        session_xml_path = last_run.get('session_xml_path') or last_run.get('post_xml_path')
    if session_xml_path:
        try:
            hitl = _hitl_details_from_path(str(session_xml_path))
            first = hitl[0] if isinstance(hitl, list) and hitl else None
            ips = first.get('ips') if isinstance(first, dict) else None
            if isinstance(ips, list) and ips:
                gateway = str(ips[0]).split('/', 1)[0]
        except Exception:
            gateway = ''

    # Fallback to participant state / saved XML mapping if we couldn't derive it from the last session XML.
    if not gateway:
        if scenario_norm and scenario_norm == state.get('selected_norm', ''):
            gateway = state.get('selected_nearest_gateway', '')
        else:
            try:
                _names, scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(None, user=_current_user())
            except Exception:
                scenario_paths = {}
            gateway = _nearest_gateway_address_for_scenario(scenario_norm, scenario_paths=scenario_paths)

    return jsonify({'ok': True, 'scenario_norm': scenario_norm, 'nearest_gateway': gateway or ''})


def _parse_iso_ts(ts: Any) -> float:
    if not ts:
        return 0.0
    if not isinstance(ts, str):
        ts = str(ts)
    text = ts.strip()
    if not text:
        return 0.0
    try:
        return datetime.datetime.fromisoformat(text.replace('Z', '+00:00')).timestamp()
    except Exception:
        return 0.0


def _latest_run_history_for_scenario(scenario_norm: str, history: Optional[list[dict]] = None) -> Optional[dict]:
    scenario_norm = _normalize_scenario_label(scenario_norm)
    if not scenario_norm:
        return None
    if history is None:
        history = _load_run_history()
    filtered = _filter_history_by_scenario(history, scenario_norm)
    if not filtered:
        return None
    best: Optional[dict] = None
    best_ts = -1.0
    for entry in filtered:
        if not isinstance(entry, dict):
            continue
        ts_val = _parse_iso_ts(entry.get('timestamp'))
        if ts_val > best_ts:
            best_ts = ts_val
            best = entry
    return best


def _load_summary_counts(summary_path: Optional[str]) -> dict:
    if not summary_path:
        return {}
    try:
        ap = os.path.abspath(str(summary_path))
    except Exception:
        ap = str(summary_path)
    if not ap or not os.path.exists(ap):
        return {}
    try:
        with open(ap, 'r', encoding='utf-8') as f:
            payload = json.load(f)
        counts = payload.get('counts') if isinstance(payload, dict) else None
        return counts if isinstance(counts, dict) else {}
    except Exception:
        return {}


def _load_summary_metadata(summary_path: Optional[str]) -> dict:
    if not summary_path:
        return {}
    try:
        ap = os.path.abspath(str(summary_path))
    except Exception:
        ap = str(summary_path)
    if not ap or not os.path.exists(ap):
        return {}
    try:
        with open(ap, 'r', encoding='utf-8') as f:
            payload = json.load(f)
        meta = payload.get('metadata') if isinstance(payload, dict) else None
        return meta if isinstance(meta, dict) else {}
    except Exception:
        return {}


def _seed_from_preview_plan(preview_plan_path: Optional[str]) -> Optional[int]:
    if not preview_plan_path:
        return None
    try:
        ap = os.path.abspath(str(preview_plan_path))
    except Exception:
        ap = str(preview_plan_path)
    if not ap or not os.path.exists(ap):
        return None
    try:
        with open(ap, 'r', encoding='utf-8') as f:
            payload = json.load(f)
        if not isinstance(payload, dict):
            return None
        meta = payload.get('metadata') if isinstance(payload.get('metadata'), dict) else {}
        seed = meta.get('seed')
        if seed is None and isinstance(payload.get('full_preview'), dict):
            seed = payload['full_preview'].get('seed')
        if seed is None:
            return None
        return int(seed)
    except Exception:
        return None


def _switch_names_from_session_xml(session_xml_path: Optional[str]) -> list[str]:
    if not session_xml_path:
        return []
    try:
        ap = os.path.abspath(str(session_xml_path))
    except Exception:
        ap = str(session_xml_path)
    if not ap or not os.path.exists(ap):
        return []
    try:
        summary = _analyze_core_xml(ap)
    except Exception:
        summary = {}
    switches = summary.get('switches') if isinstance(summary, dict) else None
    if not isinstance(switches, list):
        return []
    out: list[str] = []
    for s in switches:
        name = str(s).strip() if s is not None else ''
        if name and name not in out:
            out.append(name)
    return out


def _subnet_cidrs_from_session_xml(session_xml_path: Optional[str]) -> list[str]:
    if not session_xml_path:
        return []
    try:
        ap = os.path.abspath(str(session_xml_path))
    except Exception:
        ap = str(session_xml_path)
    if not ap or not os.path.exists(ap):
        return []

    try:
        import ipaddress
    except Exception:
        return []

    try:
        summary = _analyze_core_xml(ap)
    except Exception:
        summary = {}
    nodes = summary.get('nodes') if isinstance(summary, dict) else None
    if not isinstance(nodes, list):
        return []

    networks: set[str] = set()

    def _add_ipv4(ipv4: Any, ipv4_mask: Any = None) -> None:
        if not ipv4:
            return
        ip_text = str(ipv4).strip()
        if not ip_text:
            return
        try:
            if '/' in ip_text:
                iface = ipaddress.ip_interface(ip_text)
            else:
                mask_text = str(ipv4_mask).strip() if ipv4_mask else ''
                if not mask_text:
                    return
                iface = ipaddress.ip_interface(f"{ip_text}/{mask_text}")
            net = iface.network
            # Filter out host routes; we only want meaningful subnetworks.
            if getattr(net, 'prefixlen', 32) >= 32:
                return
            networks.add(str(net))
        except Exception:
            return

    for node in nodes:
        if not isinstance(node, dict):
            continue
        ifaces = node.get('interfaces')
        if not isinstance(ifaces, list):
            continue
        for iface in ifaces:
            if not isinstance(iface, dict):
                continue
            _add_ipv4(iface.get('ipv4'), iface.get('ipv4_mask'))

    return sorted(networks, key=lambda s: (s.split('/', 1)[0], int(s.split('/', 1)[1]) if '/' in s else 999))


def _vulnerability_ipv4s_from_session_xml(session_xml_path: Optional[str]) -> list[str]:
    """Best-effort vulnerability IPv4 addresses from a CORE session XML.

    We treat Docker-backed nodes as "vulnerabilities" for Participant UI purposes.
    """

    if not session_xml_path:
        return []
    try:
        ap = os.path.abspath(str(session_xml_path))
    except Exception:
        ap = str(session_xml_path)
    if not ap or not os.path.exists(ap):
        return []

    try:
        import ipaddress
    except Exception:
        return []

    # NOTE: We intentionally parse the XML directly instead of relying on
    # _analyze_core_xml() because its node list is filtered for UI convenience
    # and can drop docker/compose nodes when interface/link info is missing.
    try:
        root = LET.parse(ap).getroot()
    except Exception:
        return []

    def _local(tag: str) -> str:
        if not tag:
            return ''
        if '}' in tag:
            return tag.split('}', 1)[1]
        return tag

    def _iter_local(el, lname: str):
        lname = lname.lower()
        for e in el.iter():
            if _local(getattr(e, 'tag', '')).lower() == lname:
                yield e

    def _looks_like_vuln(dev) -> bool:
        try:
            t = str(dev.get('type') or '').strip().lower()
        except Exception:
            t = ''
        try:
            cls = str(dev.get('class') or '').strip().lower()
        except Exception:
            cls = ''
        try:
            comp = str(dev.get('compose') or '').strip()
        except Exception:
            comp = ''
        try:
            comp_name = str(dev.get('compose_name') or '').strip()
        except Exception:
            comp_name = ''
        if 'docker' in t or 'docker' in cls:
            return True
        if comp or comp_name:
            return True
        # fall back to services, if present
        try:
            for svc in dev.findall('.//service'):
                nm = (svc.get('name') or (svc.text or '')).strip().lower()
                if 'docker' in nm:
                    return True
        except Exception:
            pass
        return False

    vuln_ids: set[str] = set()
    # session exports sometimes use <node> instead of <device>
    for dev in list(_iter_local(root, 'device')) + list(_iter_local(root, 'node')):
        if not _looks_like_vuln(dev):
            continue
        did = str(dev.get('id') or '').strip()
        if did:
            vuln_ids.add(did)

    if not vuln_ids:
        return []

    ips: set[str] = set()

    def _add_ipv4(ip_value: Any) -> None:
        if not ip_value:
            return
        ip_raw = str(ip_value).strip()
        if not ip_raw:
            return
        ip_text = ip_raw.split('/', 1)[0].strip()
        if not ip_text:
            return
        try:
            ip_obj = ipaddress.ip_address(ip_text)
        except Exception:
            return
        if getattr(ip_obj, 'version', None) != 4:
            return
        if ip_obj.is_loopback or ip_obj.is_unspecified or ip_obj.is_link_local:
            return
        ips.add(str(ip_obj))

    def _maybe_add_ipv4_from_attrib(attrib: dict) -> None:
        if not attrib:
            return
        for k, v in attrib.items():
            key = str(k or '').strip().lower().replace('-', '_')
            if not key:
                continue
            # Common encodings seen across CORE XML exports.
            if key in {'ip4', 'ipv4', 'ip', 'addr', 'address'}:
                _add_ipv4(v)
                continue
            if key.endswith('_ip4') or key.endswith('_ipv4') or key.endswith('_ip'):
                _add_ipv4(v)
                continue

    # Collect addresses from link endpoints.
    for link in _iter_local(root, 'link'):
        try:
            n1 = str(link.get('node1') or link.get('node1_id') or '').strip()
            n2 = str(link.get('node2') or link.get('node2_id') or '').strip()
        except Exception:
            n1, n2 = '', ''

        # Common CORE format: iface1 belongs to node1; iface2 belongs to node2.
        try:
            if n1 and n1 in vuln_ids:
                iface1 = next(_iter_local(link, 'iface1'), None)
                if iface1 is not None:
                    _maybe_add_ipv4_from_attrib(getattr(iface1, 'attrib', {}) or {})
            if n2 and n2 in vuln_ids:
                iface2 = next(_iter_local(link, 'iface2'), None)
                if iface2 is not None:
                    _maybe_add_ipv4_from_attrib(getattr(iface2, 'attrib', {}) or {})
        except Exception:
            pass

        # Additional address encodings
        try:
            for child in list(link):
                tag = _local(getattr(child, 'tag', '')).lower()
                if tag not in ('iface', 'interface', 'addr', 'address'):
                    continue
                target = str(child.get('node') or child.get('node_id') or child.get('device') or '').strip()
                if target and target not in vuln_ids:
                    continue
                _maybe_add_ipv4_from_attrib(getattr(child, 'attrib', {}) or {})
                _add_ipv4(child.get('value') or (child.text or '').strip())
        except Exception:
            pass

    # Also attempt to read any interfaces nested under the vuln node itself.
    for dev in list(_iter_local(root, 'device')) + list(_iter_local(root, 'node')):
        did = str(dev.get('id') or '').strip()
        if not did or did not in vuln_ids:
            continue
        try:
            _maybe_add_ipv4_from_attrib(getattr(dev, 'attrib', {}) or {})
            for iface in list(_iter_local(dev, 'interface')) + list(_iter_local(dev, 'iface')):
                _maybe_add_ipv4_from_attrib(getattr(iface, 'attrib', {}) or {})
        except Exception:
            pass

        # Fallback: scan any descendant elements for common address attributes.
        try:
            for child in dev.iter():
                _maybe_add_ipv4_from_attrib(getattr(child, 'attrib', {}) or {})
                tag = _local(getattr(child, 'tag', '')).lower()
                if tag in {'ip', 'ip4', 'ipv4', 'addr', 'address'}:
                    _add_ipv4((child.text or '').strip())
        except Exception:
            pass

    try:
        return sorted(ips, key=lambda s: int(ipaddress.ip_address(s)))
    except Exception:
        return sorted(ips)


def _counts_from_session_xml(session_xml_path: Optional[str]) -> dict:
    """Best-effort {nodes, routers, switches} from a CORE session XML.

    This intentionally parses the XML directly (instead of _analyze_core_xml()) so
    that counts are not affected by any UI-oriented filtering.
    """
    if not session_xml_path:
        return {}
    try:
        ap = os.path.abspath(str(session_xml_path))
    except Exception:
        ap = str(session_xml_path)
    if not ap or not os.path.exists(ap):
        return {}
    try:
        root = LET.parse(ap).getroot()
    except Exception:
        return {}

    def _local(tag: str) -> str:
        if not tag:
            return ''
        if '}' in tag:
            return tag.split('}', 1)[1]
        return tag

    def _iter_local(el, lname: str):
        lname = lname.lower()
        for e in el.iter():
            if _local(getattr(e, 'tag', '')).lower() == lname:
                yield e

    candidates = list(_iter_local(root, 'device')) + list(_iter_local(root, 'node'))
    devices: list[Any] = []
    seen_ids: set[str] = set()
    for cand in candidates:
        ident = cand.get('id') or cand.get('name')
        key = str(ident).strip() if ident is not None else ''
        if key and key in seen_ids:
            continue
        if key:
            seen_ids.add(key)
        devices.append(cand)

    def _dev_type(dev) -> str:
        try:
            t = str(dev.get('type') or '').strip()
        except Exception:
            t = ''
        if not t and hasattr(dev, 'find'):
            try:
                type_el = dev.find('./type') or dev.find('./model') or dev.find('./icon')
                if type_el is not None and getattr(type_el, 'text', None):
                    t = type_el.text.strip()
            except Exception:
                t = ''
        return t

    routers = 0
    switches_device = 0
    switch_ids: set[str] = set()
    for idx, dev in enumerate(devices, start=1):
        try:
            did = str((dev.get('id') or dev.get('name') or f"device_{idx}") or '').strip() or f"device_{idx}"
        except Exception:
            did = f"device_{idx}"
        t = _dev_type(dev).lower()
        if 'router' in t:
            routers += 1
        if t == 'switch':
            switches_device += 1
            if did:
                switch_ids.add(did)

    # Some session exports represent switches as <network type="switch"> entries.
    extra_switches = 0
    try:
        for net in _iter_local(root, 'network'):
            ntype = str(net.get('type') or '').strip().lower()
            if 'switch' not in ntype:
                continue
            nid = str(net.get('id') or net.get('name') or '').strip()
            if nid and nid in switch_ids:
                continue
            extra_switches += 1
    except Exception:
        extra_switches = 0

    out: dict[str, Any] = {}
    out['nodes'] = len(devices)
    out['routers'] = routers
    out['switches'] = switches_device + extra_switches
    return out


def _recent_session_id_for_scenario(
    scenario_norm: str,
    *,
    scenario_paths: dict[str, set[str]],
) -> tuple[Optional[int], float]:
    """Return (session_id, last_seen_epoch) from the newest known CORE session XML mapping."""
    scenario_norm = _normalize_scenario_label(scenario_norm)
    if not scenario_norm:
        return None, 0.0
    try:
        store = _load_core_sessions_store()
    except Exception:
        store = {}
    best_sid: Optional[int] = None
    best_mtime: float = 0.0
    for path, entry in (store or {}).items():
        if not path:
            continue
        stored_norm = _session_store_entry_scenario_norm(entry)
        if stored_norm and stored_norm != scenario_norm:
            continue
        if not stored_norm and not _path_matches_scenario(path, scenario_norm, scenario_paths):
            continue
        try:
            ap = os.path.abspath(str(path))
        except Exception:
            ap = str(path)
        if not ap or not os.path.exists(ap):
            continue
        try:
            mtime = os.path.getmtime(ap)
        except Exception:
            mtime = 0.0
        sid = _session_store_entry_session_id(entry)
        if sid is None:
            continue
        if best_sid is None or mtime > best_mtime:
            best_sid = sid
            best_mtime = mtime
    return best_sid, best_mtime


def _live_core_session_status_for_scenario(
    scenario_norm: str,
    *,
    history: list[dict],
    scenario_names: list[str],
    scenario_paths: dict[str, set[str]],
) -> Optional[dict]:
    """Best-effort live CORE session status for a scenario.

    Returns None if CORE cannot be queried (no credentials / remote failure).
        Otherwise returns: {running: bool|None, session_id: int|None, state: str}.

        Notes:
        - running=True means we could positively associate an active CORE session
            with the selected scenario.
        - running=False means we could query CORE and there are no active sessions.
        - running=None means we could query CORE and there are active sessions, but
            we could not confidently associate any of them with the selected scenario.

    This does not expose host/port; it only uses them server-side.
    """

    scenario_norm = _normalize_scenario_label(scenario_norm)
    if not scenario_norm:
        return None

    try:
        mapping = _load_core_sessions_store()
    except Exception:
        mapping = {}

    scenario_session_ids = _session_ids_for_scenario(mapping, scenario_norm, scenario_paths)

    try:
        core_cfg = _select_core_config_for_page(scenario_norm, history, include_password=True)
        host = core_cfg.get('host', CORE_HOST)
        port = int(core_cfg.get('port', CORE_PORT))
    except Exception:
        return None

    errors: list[str] = []
    meta: dict[str, Any] = {}
    try:
        sessions = _list_active_core_sessions(host, port, core_cfg, errors=errors, meta=meta)
    except Exception:
        return None
    if not isinstance(sessions, list):
        return None

    def _is_active(sess: dict) -> bool:
        state_raw = str(sess.get('state') or '').strip().lower()
        return state_raw not in {'shutdown'}

    def _session_id_int(sess: dict) -> Optional[int]:
        sid = sess.get('id')
        try:
            return int(sid) if sid is not None else None
        except Exception:
            return None

    def _matches(sess: dict) -> bool:
        # 1) session_id linkage via stored XML mapping
        sid_int = _session_id_int(sess)
        if sid_int is not None and sid_int in scenario_session_ids:
            return True
        # 2) CORE-provided scenario_name match (if present)
        label = _normalize_scenario_label(sess.get('scenario_name')) if sess.get('scenario_name') else ''
        if label and label == scenario_norm:
            return True
        # 3) file/dir path match
        if _path_matches_scenario(sess.get('file'), scenario_norm, scenario_paths):
            return True
        if _path_matches_scenario(sess.get('dir'), scenario_norm, scenario_paths):
            return True
        return False

    active_sessions = [s for s in sessions if isinstance(s, dict) and _is_active(s)]
    matching_active = [s for s in active_sessions if _matches(s)]

    for sess in matching_active:
        if not isinstance(sess, dict):
            continue
        state_raw = str(sess.get('state') or '').strip().lower()
        sid_int = _session_id_int(sess)
        return {
            'running': True,
            'session_id': sid_int,
            'state': state_raw,
        }

    # Queried successfully but no matching active sessions.
    # If there are active sessions but none match the selected scenario, prefer
    # Unknown (None) over incorrectly marking the selected scenario as Running.
    if active_sessions:
        # Conservative fallback: if there is exactly one scenario in scope and
        # there are active CORE sessions, treat the first active session as the
        # selected scenario's session. This avoids reporting Unknown in fresh
        # runs where session->scenario linkage isn't available yet.
        try:
            if isinstance(scenario_names, list) and len(scenario_names) == 1:
                only_norm = _normalize_scenario_label(scenario_names[0])
                if only_norm and only_norm == scenario_norm:
                    first = active_sessions[0] if active_sessions else None
                    if isinstance(first, dict):
                        return {
                            'running': True,
                            'session_id': _session_id_int(first),
                            'state': str(first.get('state') or '').strip().lower(),
                        }
        except Exception:
            pass
        return {
            'running': None,
            'session_id': None,
            'state': '',
        }

    return {
        'running': False,
        'session_id': None,
        'state': '',
    }


@app.route('/participant-ui/details')
def participant_ui_details_api():
    state = _participant_ui_state()
    scenario_norm = _normalize_scenario_label(request.args.get('scenario') or '')
    if not scenario_norm:
        scenario_norm = _normalize_scenario_label(state.get('selected_norm', ''))

    listing = state.get('listing', [])
    listing_entry: Optional[dict] = None
    if isinstance(listing, list) and scenario_norm:
        for entry in listing:
            if isinstance(entry, dict) and (entry.get('norm') or '') == scenario_norm:
                listing_entry = entry
                break

    display = ''
    has_url = False
    assigned = False
    placeholder = False
    if isinstance(listing_entry, dict):
        display = str(listing_entry.get('display') or '')
        has_url = bool(listing_entry.get('has_url'))
        assigned = bool(listing_entry.get('assigned'))
        placeholder = bool(listing_entry.get('placeholder'))

    # Global open stats
    stats = _load_participant_ui_stats()
    scenarios_stats = stats.get('scenarios') if isinstance(stats.get('scenarios'), dict) else {}
    scenario_stats = scenarios_stats.get(scenario_norm, {}) if scenario_norm else {}
    if not isinstance(scenario_stats, dict):
        scenario_stats = {}

    # Execute history
    history = _load_run_history()
    last_run = _latest_run_history_for_scenario(scenario_norm, history)
    last_execute_ts = (last_run or {}).get('timestamp') if isinstance(last_run, dict) else ''
    returncode = (last_run or {}).get('returncode') if isinstance(last_run, dict) else None
    try:
        returncode_int = int(returncode) if returncode is not None else None
    except Exception:
        returncode_int = None
    last_execute_ok = (returncode_int == 0) if returncode_int is not None else None

    summary_path = (last_run or {}).get('summary_path') if isinstance(last_run, dict) else None
    summary_counts = _load_summary_counts(summary_path)
    summary_meta = _load_summary_metadata(summary_path)

    # Counts
    nodes_total = summary_counts.get('total_nodes')
    routers_total = summary_counts.get('routers')
    switches_total = summary_counts.get('switches')
    try:
        nodes_total = int(nodes_total) if nodes_total is not None else None
    except Exception:
        nodes_total = None
    try:
        routers_total = int(routers_total) if routers_total is not None else None
    except Exception:
        routers_total = None
    try:
        switches_total = int(switches_total) if switches_total is not None else None
    except Exception:
        switches_total = None

    # Subnetworks (best-effort: CIDR networks from session XML interface addresses)
    session_xml_path = None
    if isinstance(last_run, dict):
        session_xml_path = last_run.get('session_xml_path') or last_run.get('post_xml_path')
    subnetworks = _subnet_cidrs_from_session_xml(session_xml_path)

    vulnerability_ips = _vulnerability_ipv4s_from_session_xml(session_xml_path)

    # Vulnerabilities count:
    # - Prefer actual session XML-derived value when we can parse a real file.
    # - Otherwise fall back to the planned additive count from the summary.
    vuln_total: Optional[int] = None
    xml_exists = False
    try:
        xml_exists = bool(session_xml_path and os.path.exists(str(session_xml_path)))
    except Exception:
        xml_exists = False
    if xml_exists:
        vuln_total = len(vulnerability_ips)
    else:
        planned = summary_meta.get('vuln_total_planned_additive') if isinstance(summary_meta, dict) else None
        try:
            vuln_total = int(planned) if planned is not None else None
        except Exception:
            vuln_total = None

    # Gateway: prefer the scenario's last session XML (matches core.html HITL gateway logic)
    gateway = ''
    if session_xml_path:
        try:
            hitl = _hitl_details_from_path(str(session_xml_path))
            first = hitl[0] if isinstance(hitl, list) and hitl else None
            ips = first.get('ips') if isinstance(first, dict) else None
            if isinstance(ips, list) and ips:
                gateway = str(ips[0]).split('/', 1)[0]
        except Exception:
            gateway = ''

    # Fallback to participant state / saved XML mapping if session XML doesn't yield a gateway.
    if not gateway:
        if scenario_norm and scenario_norm == _normalize_scenario_label(state.get('selected_norm', '')):
            gateway = str(state.get('selected_nearest_gateway') or '')
        else:
            try:
                _names, scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(None, user=_current_user())
            except Exception:
                scenario_paths = {}
            gateway = _nearest_gateway_address_for_scenario(scenario_norm, scenario_paths=scenario_paths) if scenario_norm else ''

    # Prefer counts from session XML when available (authoritative for Participant UI).
    xml_counts = _counts_from_session_xml(session_xml_path)
    if isinstance(xml_counts.get('nodes'), int):
        nodes_total = xml_counts.get('nodes')
    if isinstance(xml_counts.get('routers'), int):
        routers_total = xml_counts.get('routers')
    if isinstance(xml_counts.get('switches'), int):
        switches_total = xml_counts.get('switches')

    # Session status: prefer live CORE query; fall back to unknown instead of guessing.
    session_running: Optional[bool] = None
    session_state = ''
    session_id: Optional[int] = None
    # Catalog paths/names for live lookup.
    scenario_names_live: list[str]
    scenario_paths_live: dict[str, set[str]]
    try:
        scenario_names_live, scenario_paths_live, _scenario_url_hints_live = _scenario_catalog_for_user(history, user=_current_user())
    except Exception:
        scenario_names_live, scenario_paths_live = [], {}

    live = _live_core_session_status_for_scenario(
        scenario_norm,
        history=history,
        scenario_names=scenario_names_live,
        scenario_paths=scenario_paths_live,
    )

    if isinstance(live, dict):
        running_val = live.get('running')
        session_running = running_val if isinstance(running_val, bool) else None
        session_state = str(live.get('state') or '')
        sid_val = live.get('session_id')
        try:
            session_id = int(sid_val) if sid_val is not None else None
        except Exception:
            session_id = None
    else:
        session_running = None

    if session_id is None:
        # Best-effort show the most recent known session id from saved XML mapping.
        try:
            session_id, _last_seen = _recent_session_id_for_scenario(scenario_norm, scenario_paths=scenario_paths_live)
        except Exception:
            session_id = None

    return jsonify({
        'ok': True,
        'scenario_norm': scenario_norm,
        'scenario': {
            'display': display,
            'assigned': assigned,
            'placeholder': placeholder,
            'participant_link_configured': bool(has_url),
        },
        'gateway': gateway or '',
        'open_stats': {
            'open_count': int(scenario_stats.get('open_count') or 0),
            'last_open_ts': str(scenario_stats.get('last_open_ts') or ''),
        },
        'execute': {
            'last_execute_ts': str(last_execute_ts or ''),
            'returncode': returncode_int,
            'ok': last_execute_ok,
        },
        'session': {
            'session_id': session_id,
            'running': session_running,
            'state': session_state,
        },
        'counts': {
            'nodes': nodes_total,
            'routers': routers_total,
            'switches': switches_total,
            'vulnerabilities': vuln_total,
        },
        'subnetworks': subnetworks,
        'vulnerability_ips': vulnerability_ips,
    })


def _core_xml_device_summaries(xml_path: str) -> list[dict[str, str]]:
    """Return a best-effort list of {id,name,type} from CORE XML."""
    try:
        root = LET.parse(xml_path).getroot()
    except Exception:
        return []

    def _local(tag: str) -> str:
        if not tag:
            return ''
        if '}' in tag:
            return tag.split('}', 1)[1]
        return tag

    out: list[dict[str, str]] = []
    seen: set[str] = set()
    for dev in list(root.iter()):
        try:
            lname = _local(getattr(dev, 'tag', '')).lower()
        except Exception:
            lname = ''
        if lname not in {'device', 'node'}:
            continue
        did = (dev.get('id') or dev.get('name') or '').strip()
        if not did:
            continue
        if did in seen:
            continue
        seen.add(did)
        name_val = (dev.get('name') or '').strip()
        if not name_val:
            try:
                name_el = dev.find('./name')
                if name_el is not None and getattr(name_el, 'text', None):
                    name_val = str(name_el.text).strip()
            except Exception:
                name_val = ''
        type_val = (dev.get('type') or '').strip()
        if not type_val:
            try:
                type_el = dev.find('./type') or dev.find('./model')
                if type_el is not None and getattr(type_el, 'text', None):
                    type_val = str(type_el.text).strip()
            except Exception:
                type_val = ''
        out.append({
            'id': did,
            'name': name_val or did,
            'type': type_val or '',
        })
    return out


def _core_xml_network_summaries(xml_path: str) -> list[dict[str, str]]:
    """Return a best-effort list of CORE network objects as {id,name,type} from CORE XML."""
    try:
        root = LET.parse(xml_path).getroot()
    except Exception:
        return []

    def _local(tag: str) -> str:
        if not tag:
            return ''
        if '}' in tag:
            return tag.split('}', 1)[1]
        return tag

    out: list[dict[str, str]] = []
    seen: set[str] = set()
    for el in list(root.iter()):
        try:
            lname = _local(getattr(el, 'tag', '')).lower()
        except Exception:
            lname = ''
        if lname != 'network':
            continue
        nid = (el.get('id') or el.get('name') or '').strip()
        if not nid or nid in seen:
            continue
        seen.add(nid)
        name_val = (el.get('name') or '').strip() or nid
        type_val = (el.get('type') or '').strip()
        if not type_val:
            try:
                type_el = el.find('./type') or el.find('./model')
                if type_el is not None and getattr(type_el, 'text', None):
                    type_val = str(type_el.text).strip()
            except Exception:
                type_val = ''
        out.append({'id': nid, 'name': name_val, 'type': type_val or ''})
    return out


def _core_xml_link_summaries(xml_path: str, id_to_name: dict[str, str] | None = None) -> list[dict[str, str]]:
    """Return a best-effort list of link endpoints from CORE XML.

    Important: CORE often represents host↔LAN connections as node↔network links.
    This helper keeps those endpoints rather than filtering to only device IDs.
    """
    try:
        root = LET.parse(xml_path).getroot()
    except Exception:
        return []

    def _local(tag: str) -> str:
        if not tag:
            return ''
        if '}' in tag:
            return tag.split('}', 1)[1]
        return tag

    def _norm(value: Any) -> str:
        return str(value).strip() if value is not None else ''

    name_map = id_to_name or {}
    out: list[dict[str, str]] = []
    seen_pairs: set[tuple[str, str]] = set()

    for link in list(root.iter()):
        try:
            lname = _local(getattr(link, 'tag', '')).lower()
        except Exception:
            lname = ''
        if lname != 'link':
            continue

        n1 = _norm(getattr(link, 'get', lambda *_: None)('node1') or getattr(link, 'get', lambda *_: None)('node1_id'))
        n2 = _norm(getattr(link, 'get', lambda *_: None)('node2') or getattr(link, 'get', lambda *_: None)('node2_id'))
        if not n1 or not n2:
            try:
                if not n1 and hasattr(link, 'find'):
                    iface1 = link.find('.//iface1') or link.find('.//interface1')
                    if iface1 is not None:
                        n1 = _norm(iface1.get('node') or iface1.get('device') or iface1.get('node_id'))
                if not n2 and hasattr(link, 'find'):
                    iface2 = link.find('.//iface2') or link.find('.//interface2')
                    if iface2 is not None:
                        n2 = _norm(iface2.get('node') or iface2.get('device') or iface2.get('node_id'))
            except Exception:
                pass
        if not n1 or not n2 or n1 == n2:
            continue
        ordered = tuple(sorted((n1, n2)))
        if ordered in seen_pairs:
            continue
        seen_pairs.add(ordered)
        out.append({
            'node1': ordered[0],
            'node2': ordered[1],
            'node1_name': str(name_map.get(ordered[0], ordered[0])),
            'node2_name': str(name_map.get(ordered[1], ordered[1])),
        })
    return out


@app.route('/participant-ui/topology')
def participant_ui_topology_api():
    state = _participant_ui_state()
    scenario_norm = _normalize_scenario_label(request.args.get('scenario') or '')
    if not scenario_norm:
        scenario_norm = _normalize_scenario_label(state.get('selected_norm', ''))

    # Enforce assignment-based access for restricted users.
    try:
        if state.get('restrict_to_assigned'):
            allowed_norms = {row.get('norm') for row in (state.get('listing') or []) if isinstance(row, dict) and row.get('norm')}
            if scenario_norm and scenario_norm not in allowed_norms:
                return jsonify({'ok': False, 'error': 'Scenario not assigned.'}), 403
    except Exception:
        pass

    # Resolve latest session XML.
    try:
        history = _load_run_history()
        last_run = _latest_run_history_for_scenario(scenario_norm, history)
    except Exception:
        last_run = None
    session_xml_path = None
    if isinstance(last_run, dict):
        session_xml_path = last_run.get('session_xml_path') or last_run.get('post_xml_path')

    if not session_xml_path:
        return jsonify({
            'ok': True,
            'scenario_norm': scenario_norm,
            'status': 'No session XML available for this scenario yet.',
            'nodes': [],
            'links': [],
            'subnets': [],
            'vulnerability_ips': [],
        })

    try:
        ap = os.path.abspath(str(session_xml_path))
    except Exception:
        ap = str(session_xml_path)

    try:
        app.logger.info("[participant-ui.topology] scenario=%s session_xml=%s", scenario_norm, ap)
    except Exception:
        pass
    if not ap or not os.path.exists(ap):
        return jsonify({
            'ok': True,
            'scenario_norm': scenario_norm,
            'status': 'Session XML path missing on disk.',
            'nodes': [],
            'links': [],
            'subnets': [],
            'vulnerability_ips': [],
        })

    # Parse topology summary (used for enriched node details like services/interfaces).
    try:
        summary = _analyze_core_xml(ap)
    except Exception:
        summary = {}

    # Best-effort attach flow/chain metadata so the Participant UI graph can
    # label sequence nodes with Roman numerals (mirrors Preview graph view).
    flow_meta: Optional[dict] = None
    try:
        scenario_norm_for_flow = _normalize_scenario_label(scenario_norm)
        if scenario_norm_for_flow:
            flow_plan_path = _latest_flow_plan_for_scenario_norm(scenario_norm_for_flow)
            if flow_plan_path:
                with open(flow_plan_path, 'r', encoding='utf-8') as f:
                    flow_payload = json.load(f) or {}
                if isinstance(flow_payload, dict):
                    meta = flow_payload.get('metadata') if isinstance(flow_payload.get('metadata'), dict) else {}
                    candidate = (meta or {}).get('flow') or flow_payload.get('flow')
                    if isinstance(candidate, dict):
                        flow_meta = candidate
    except Exception:
        flow_meta = None

    raw_nodes = summary.get('nodes') if isinstance(summary, dict) else None
    nodes_list: list[dict] = raw_nodes if isinstance(raw_nodes, list) else []

    # IMPORTANT: do NOT use summary.links_detail here.
    # The summary path intentionally filters/prunes to "important" nodes and can
    # drop host↔LAN/switch edges, making it look like only routers are connected.
    # Instead, extract raw links directly from the XML (including network nodes).
    all_devices = _core_xml_device_summaries(ap)
    all_networks = _core_xml_network_summaries(ap)
    name_map: dict[str, str] = {}
    type_map: dict[str, str] = {}
    for row in (all_devices or []):
        if not isinstance(row, dict):
            continue
        rid = str(row.get('id') or '').strip()
        if not rid:
            continue
        name_map.setdefault(rid, str(row.get('name') or rid))
        type_map.setdefault(rid, str(row.get('type') or ''))
    for row in (all_networks or []):
        if not isinstance(row, dict):
            continue
        rid = str(row.get('id') or '').strip()
        if not rid:
            continue
        name_map.setdefault(rid, str(row.get('name') or rid))
        type_map.setdefault(rid, str(row.get('type') or ''))
    links_list: list[dict] = _core_xml_link_summaries(ap, id_to_name=name_map)

    # Ensure we include *all* devices AND networks from the XML (even if filtered from summary).
    by_id: dict[str, dict] = {}
    for n in nodes_list:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if nid:
            by_id[nid] = n
    for dev in all_devices:
        did = str(dev.get('id') or '').strip()
        if not did or did in by_id:
            continue
        by_id[did] = {
            'id': did,
            'name': dev.get('name') or did,
            'type': dev.get('type') or '',
            'services': [],
            'interfaces': [],
            'linked_nodes': [],
        }

    # Add networks as graph nodes so node↔network links can be drawn.
    for net in all_networks:
        nid = str(net.get('id') or '').strip()
        if not nid or nid in by_id:
            continue
        raw_type = str(net.get('type') or '').strip()
        # Render network objects as "switch" by default to match graph styling.
        type_hint = (raw_type or '').lower()
        name_hint = str(net.get('name') or '').strip().lower()
        is_hitl = False
        # Preserve RJ45/HITL network objects so the graph can style them correctly.
        # The Details page exposes these via summary.hitl_network_nodes.
        try:
            import re
            name_looks_like_iface = bool(re.match(r'^(ens|enp|eth)\d', name_hint))
        except Exception:
            name_looks_like_iface = False

        if name_looks_like_iface or 'rj45' in type_hint or 'rj-45' in type_hint or 'hitl' in type_hint or 'tap' in type_hint:
            coerced_type = 'hitl'
            is_hitl = True
        elif 'wlan' in type_hint or 'wireless' in type_hint:
            coerced_type = 'wlan'
        else:
            coerced_type = 'switch'
        by_id[nid] = {
            'id': nid,
            'name': net.get('name') or nid,
            'type': coerced_type,
            'services': [],
            'interfaces': [],
            'linked_nodes': [],
            'is_hitl': bool(is_hitl),
        }

    # Vulnerability tagging: mark nodes whose ipv4 matches vulnerability IPs.
    vuln_ips = _vulnerability_ipv4s_from_session_xml(ap)
    vuln_set = {str(ip).strip() for ip in vuln_ips if ip}

    # Subnet tagging.
    subnets = _subnet_cidrs_from_session_xml(ap)

    out_nodes: list[dict[str, Any]] = []
    network_ids: set[str] = set()
    try:
        for net in (all_networks or []):
            if isinstance(net, dict) and net.get('id'):
                network_ids.add(str(net.get('id')).strip())
    except Exception:
        network_ids = set()
    try:
        import ipaddress
    except Exception:
        ipaddress = None

    for nid, n in by_id.items():
        name_val = str(n.get('name') or nid)
        type_val = str(n.get('type') or '')
        services_val = n.get('services') if isinstance(n.get('services'), list) else []
        ifaces = n.get('interfaces') if isinstance(n.get('interfaces'), list) else []
        ipv4s: list[str] = []
        subnet_hits: set[str] = set()
        for iface in ifaces:
            if not isinstance(iface, dict):
                continue
            ip4 = (iface.get('ipv4') or '').strip() if isinstance(iface.get('ipv4'), str) else ''
            mask = (iface.get('ipv4_mask') or '').strip() if isinstance(iface.get('ipv4_mask'), str) else ''
            if ip4:
                ip4_clean = ip4.split('/', 1)[0].strip()
                if ip4_clean:
                    ipv4s.append(ip4_clean)
            if ipaddress is not None and ip4:
                try:
                    if '/' in ip4:
                        net = ipaddress.ip_interface(ip4).network
                    elif mask:
                        net = ipaddress.ip_interface(f"{ip4}/{mask}").network
                    else:
                        net = None
                    if net is not None and getattr(net, 'prefixlen', 32) < 32:
                        subnet_hits.add(str(net))
                except Exception:
                    pass
        ipv4s = [ip for ip in ipv4s if ip]
        # stable unique
        seen_ip: set[str] = set()
        ipv4s_u: list[str] = []
        for ip in ipv4s:
            if ip in seen_ip:
                continue
            seen_ip.add(ip)
            ipv4s_u.append(ip)
        is_vuln = any(ip in vuln_set for ip in ipv4s_u)
        out_nodes.append({
            'id': nid,
            'name': name_val,
            'type': (type_val or '').strip() or 'node',
            'services': services_val,
            'interfaces': ifaces,
            'ipv4s': ipv4s_u,
            'subnets': sorted(subnet_hits) if subnet_hits else [],
            'is_vulnerability': bool(is_vuln),
            'is_hitl': bool(n.get('is_hitl')),
        })

    # Sort nodes for stable output.
    def _type_rank(t: str) -> int:
        tt = (t or '').lower()
        if tt == 'router':
            return 0
        if tt == 'switch':
            return 1
        return 2

    out_nodes.sort(key=lambda r: (_type_rank(str(r.get('type') or '')), str(r.get('name') or '').lower(), str(r.get('id') or '')))

    out_links: list[dict[str, str]] = []
    for l in links_list:
        if not isinstance(l, dict):
            continue
        a = str(l.get('node1') or '').strip()
        b = str(l.get('node2') or '').strip()
        if not a or not b:
            continue
        # Only keep links whose endpoints exist in our node set.
        if a not in by_id or b not in by_id:
            continue
        out_links.append({
            'node1': a,
            'node2': b,
            'node1_name': str(l.get('node1_name') or name_map.get(a) or a),
            'node2_name': str(l.get('node2_name') or name_map.get(b) or b),
        })

    # Improve readability: rename network/LAN nodes to their most common subnet (if any).
    try:
        id_to_node: dict[str, dict[str, Any]] = {str(n.get('id')): n for n in out_nodes if isinstance(n, dict) and n.get('id') is not None}
        adj: dict[str, set[str]] = {}
        for l in out_links:
            a = str(l.get('node1') or '').strip()
            b = str(l.get('node2') or '').strip()
            if not a or not b:
                continue
            adj.setdefault(a, set()).add(b)
            adj.setdefault(b, set()).add(a)
        for net_id in (network_ids or set()):
            if net_id not in id_to_node:
                continue
            # Never rename HITL/RJ45 network nodes to a subnet; keep interface label (e.g., ens19).
            try:
                if str(id_to_node[net_id].get('type') or '').strip().lower() == 'hitl':
                    continue
            except Exception:
                pass
            neighbor_ids = list(adj.get(net_id, set()))
            if not neighbor_ids:
                continue
            counts: dict[str, int] = {}
            for nbr in neighbor_ids:
                node_obj = id_to_node.get(nbr)
                if not node_obj:
                    continue
                # Routers often have interfaces in many subnets and are excluded from subnet boxes.
                # If we use router subnets to rename LAN/switch nodes, the UI can show confusing
                # "CIDR switch" nodes that appear to be alone in their subnet.
                try:
                    if str(node_obj.get('type') or '').strip().lower() == 'router':
                        continue
                except Exception:
                    pass
                for cidr in (node_obj.get('subnets') or []):
                    c = str(cidr).strip()
                    if not c:
                        continue
                    counts[c] = counts.get(c, 0) + 1
            if not counts:
                continue
            best = sorted(counts.items(), key=lambda kv: (-kv[1], kv[0]))[0][0]
            net_obj = id_to_node[net_id]
            net_obj['name'] = best
            net_obj['subnets'] = [best]
    except Exception:
        pass

    out = {
        'ok': True,
        'scenario_norm': scenario_norm,
        'status': '',
        'nodes': out_nodes,
        'links': out_links,
        'subnets': subnets,
        'vulnerability_ips': vuln_ips,
    }
    if isinstance(flow_meta, dict) and flow_meta:
        out['flow'] = flow_meta
    return jsonify(out)


_PARTICIPANT_UI_STATS_PATH = os.path.join(_outputs_dir(), 'participant_ui_stats.json')


def _load_participant_ui_stats() -> dict:
    try:
        if not os.path.exists(_PARTICIPANT_UI_STATS_PATH):
            return {
                'version': 1,
                'totals': {'open_count': 0, 'last_open_ts': ''},
                'scenarios': {},
            }
        with open(_PARTICIPANT_UI_STATS_PATH, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            raise ValueError('participant_ui_stats payload not a dict')
        payload.setdefault('version', 1)
        totals = payload.get('totals')
        if not isinstance(totals, dict):
            payload['totals'] = {'open_count': 0, 'last_open_ts': ''}
        payload.setdefault('scenarios', {})
        if not isinstance(payload.get('scenarios'), dict):
            payload['scenarios'] = {}
        return payload
    except Exception:
        return {
            'version': 1,
            'totals': {'open_count': 0, 'last_open_ts': ''},
            'scenarios': {},
        }


def _save_participant_ui_stats(payload: dict) -> None:
    os.makedirs(os.path.dirname(_PARTICIPANT_UI_STATS_PATH), exist_ok=True)
    tmp_path = _PARTICIPANT_UI_STATS_PATH + '.tmp'
    with open(tmp_path, 'w', encoding='utf-8') as fh:
        json.dump(payload, fh, indent=2)
    try:
        _ensure_private_file(tmp_path)
    except Exception:
        pass
    os.replace(tmp_path, _PARTICIPANT_UI_STATS_PATH)
    try:
        _ensure_private_file(_PARTICIPANT_UI_STATS_PATH)
    except Exception:
        pass


@app.route('/participant-ui/stats')
def participant_ui_stats_api():
    # Returns global stats (all users) for the selected scenario.
    scenario_norm = _normalize_scenario_label(request.args.get('scenario') or '')
    if not scenario_norm:
        try:
            scenario_norm = _normalize_scenario_label(_participant_ui_state().get('selected_norm', ''))
        except Exception:
            scenario_norm = ''
    stats = _load_participant_ui_stats()
    scenarios = stats.get('scenarios') if isinstance(stats.get('scenarios'), dict) else {}
    scenario_stats = scenarios.get(scenario_norm, {}) if scenario_norm else {}
    if not isinstance(scenario_stats, dict):
        scenario_stats = {}
    totals = stats.get('totals') if isinstance(stats.get('totals'), dict) else {}
    return jsonify({
        'ok': True,
        'scenario_norm': scenario_norm,
        'scenario': {
            'open_count': int(scenario_stats.get('open_count') or 0),
            'last_open_ts': scenario_stats.get('last_open_ts') or '',
        },
        'totals': {
            'open_count': int(totals.get('open_count') or 0),
            'last_open_ts': totals.get('last_open_ts') or '',
        },
    })


@app.route('/participant-ui/record-open', methods=['POST'])
def participant_ui_record_open_api():
    # Records a user pressing the Open button (used for global stats).
    payload = request.get_json(silent=True) or {}
    if not isinstance(payload, dict):
        payload = {}
    scenario_norm = _normalize_scenario_label(payload.get('scenario_norm') or '')
    href_raw = (payload.get('href') or '').strip()
    href = _normalize_participant_proxmox_url(href_raw)
    # Allow recording same-origin redirect endpoints (used to avoid exposing participant URLs in HTML).
    if not href and href_raw.startswith('/participant-ui/'):
        href = href_raw
    if not href:
        # Do not record meaningless events.
        return jsonify({'ok': False, 'error': 'missing href'}), 400
    now = datetime.datetime.now(datetime.UTC).replace(microsecond=0).isoformat().replace('+00:00', 'Z')
    stats = _load_participant_ui_stats()
    totals = stats.get('totals') if isinstance(stats.get('totals'), dict) else {}
    totals['open_count'] = int(totals.get('open_count') or 0) + 1
    totals['last_open_ts'] = now
    stats['totals'] = totals
    if scenario_norm:
        scenarios = stats.get('scenarios') if isinstance(stats.get('scenarios'), dict) else {}
        entry = scenarios.get(scenario_norm)
        if not isinstance(entry, dict):
            entry = {}
        entry['open_count'] = int(entry.get('open_count') or 0) + 1
        entry['last_open_ts'] = now
        scenarios[scenario_norm] = entry
        stats['scenarios'] = scenarios
    _save_participant_ui_stats(stats)
    return jsonify({
        'ok': True,
        'scenario_norm': scenario_norm,
        'last_open_ts': now,
        'totals': stats.get('totals', {}),
        'scenario': (stats.get('scenarios', {}) or {}).get(scenario_norm, {}) if scenario_norm else {},
    })


@app.route('/participant-ui/open')
def participant_ui_open_redirect():
    """Open the selected participant console as a top-level navigation.

    This endpoint intentionally redirects to the configured participant URL so the UI can avoid
    embedding or exposing raw participant URLs in HTML for restricted roles.
    """
    scenario_norm = _normalize_scenario_label(request.args.get('scenario') or '')
    state = _participant_ui_state()
    resolved = ''
    try:
        listing = state.get('listing', [])
        if isinstance(listing, list) and scenario_norm:
            for entry in listing:
                if not isinstance(entry, dict):
                    continue
                if (entry.get('norm') or '') == scenario_norm and entry.get('url'):
                    resolved = str(entry.get('url') or '')
                    break
    except Exception:
        resolved = ''
    if not resolved:
        try:
            resolved = str(state.get('selected_url') or '')
        except Exception:
            resolved = ''

    resolved = _normalize_participant_proxmox_url(resolved)
    if not resolved:
        abort(404)
    return redirect(resolved)


_LOGIN_EXEMPT_ENDPOINTS = {
    'login',
    'static',
    'healthz',
}


# ---- Attack Flow / Flag Chain (Flow page) ----


ATTACK_FLOW_EXTENSION_DEFINITION_ID = "extension-definition--fb9c968a-745b-4ade-9b25-c324172197f4"
# NOTE: Some tools (including Attack Flow Builder) try to fetch `extension-definition.schema`.
# The GitHub *web* URL returns HTML; use raw content so the URL is directly fetchable as JSON.
# (The GitHub Pages URL referenced in the schema `$id` has been observed to 404.)
ATTACK_FLOW_SCHEMA_URL = "https://raw.githubusercontent.com/center-for-threat-informed-defense/attack-flow/main/stix/attack-flow-schema-2.0.0.json"
ATTACK_FLOW_SCHEMA_VERSION = "2.0.0"


def _iso_now() -> str:
    try:
        # Attack Flow / STIX 2.1 common properties require timestamps with at least millisecond precision.
        return datetime.datetime.now(datetime.UTC).isoformat(timespec='milliseconds').replace('+00:00', 'Z')
    except Exception:
        return "1970-01-01T00:00:00Z"


def _new_stix_id(stix_type: str) -> str:
    return f"{stix_type}--{uuid.uuid4()}"


def _new_uuid() -> str:
    return str(uuid.uuid4())


def _latest_session_xml_for_scenario_norm(scenario_norm: str) -> str | None:
    try:
        history = _load_run_history()
        last_run = _latest_run_history_for_scenario(scenario_norm, history)
    except Exception:
        last_run = None

    if isinstance(last_run, dict):
        session_xml_path = last_run.get('session_xml_path') or last_run.get('post_xml_path')
        if session_xml_path:
            try:
                ap = os.path.abspath(str(session_xml_path))
            except Exception:
                ap = str(session_xml_path)
            if ap and os.path.exists(ap):
                return ap

    # Fallback: run history can become stale if artifacts under outputs/ are purged.
    # Best-effort: pick the newest matching session XML under outputs/core-sessions/.
    try:
        base = os.path.join(_outputs_dir(), 'core-sessions')
        if os.path.isdir(base) and scenario_norm:
            prefix = str(scenario_norm).strip().lower() + '-'
            best_path: str | None = None
            best_mtime: float = -1.0
            for name in os.listdir(base):
                if not isinstance(name, str):
                    continue
                low = name.lower()
                if not (low.startswith(prefix) and low.endswith('.xml')):
                    continue
                p = os.path.join(base, name)
                try:
                    m = os.path.getmtime(p)
                except Exception:
                    continue
                if m > best_mtime:
                    best_mtime = m
                    best_path = p
            if best_path and os.path.exists(best_path):
                return os.path.abspath(best_path)
    except Exception:
        pass

    return None


def _build_topology_graph_from_session_xml(xml_path: str) -> tuple[list[dict[str, Any]], list[dict[str, str]], dict[str, set[str]]]:
    """Return (nodes, links, adjacency) for the session XML.

    This mirrors the logic in participant_ui_topology_api, including network nodes,
    so reachability checks align with what the UI can draw.
    """
    try:
        summary = _analyze_core_xml(xml_path)
    except Exception:
        summary = {}

    raw_nodes = summary.get('nodes') if isinstance(summary, dict) else None
    nodes_list: list[dict] = raw_nodes if isinstance(raw_nodes, list) else []

    all_devices = _core_xml_device_summaries(xml_path)
    all_networks = _core_xml_network_summaries(xml_path)
    name_map: dict[str, str] = {}
    type_map: dict[str, str] = {}
    device_compose_map: dict[str, dict[str, str]] = {}
    for row in (all_devices or []):
        if not isinstance(row, dict):
            continue
        rid = str(row.get('id') or '').strip()
        if not rid:
            continue
        name_map.setdefault(rid, str(row.get('name') or rid))
        type_map.setdefault(rid, str(row.get('type') or ''))
        try:
            comp = str(row.get('compose') or '').strip()
        except Exception:
            comp = ''
        try:
            comp_name = str(row.get('compose_name') or '').strip()
        except Exception:
            comp_name = ''
        if comp or comp_name:
            device_compose_map[rid] = {
                'compose': comp,
                'compose_name': comp_name,
            }
    for row in (all_networks or []):
        if not isinstance(row, dict):
            continue
        rid = str(row.get('id') or '').strip()
        if not rid:
            continue
        name_map.setdefault(rid, str(row.get('name') or rid))
        type_map.setdefault(rid, str(row.get('type') or ''))

    links_list: list[dict] = _core_xml_link_summaries(xml_path, id_to_name=name_map)

    by_id: dict[str, dict] = {}
    for n in nodes_list:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if nid:
            by_id[nid] = n

    for dev in all_devices:
        did = str(dev.get('id') or '').strip()
        if not did or did in by_id:
            continue
        by_id[did] = {
            'id': did,
            'name': dev.get('name') or did,
            'type': dev.get('type') or '',
            'compose': (device_compose_map.get(did) or {}).get('compose') or '',
            'compose_name': (device_compose_map.get(did) or {}).get('compose_name') or '',
            'services': [],
            'interfaces': [],
            'linked_nodes': [],
        }

    for net in all_networks:
        nid = str(net.get('id') or '').strip()
        if not nid or nid in by_id:
            continue
        raw_type = str(net.get('type') or '').strip()
        type_hint = (raw_type or '').lower()
        name_hint = str(net.get('name') or '').strip().lower()
        is_hitl = False
        try:
            name_looks_like_iface = bool(re.match(r'^(ens|enp|eth)\d', name_hint))
        except Exception:
            name_looks_like_iface = False

        if name_looks_like_iface or 'rj45' in type_hint or 'rj-45' in type_hint or 'hitl' in type_hint or 'tap' in type_hint:
            coerced_type = 'hitl'
            is_hitl = True
        elif 'wlan' in type_hint or 'wireless' in type_hint:
            coerced_type = 'wlan'
        else:
            coerced_type = 'switch'
        by_id[nid] = {
            'id': nid,
            'name': net.get('name') or nid,
            'type': coerced_type,
            'services': [],
            'interfaces': [],
            'linked_nodes': [],
            'is_hitl': bool(is_hitl),
        }

    out_nodes: list[dict[str, Any]] = []
    for nid, n in by_id.items():
        out_nodes.append({
            'id': nid,
            'name': str(n.get('name') or nid),
            'type': (str(n.get('type') or '').strip() or 'node'),
            'compose': str(n.get('compose') or (device_compose_map.get(nid) or {}).get('compose') or ''),
            'compose_name': str(n.get('compose_name') or (device_compose_map.get(nid) or {}).get('compose_name') or ''),
            'interfaces': n.get('interfaces') if isinstance(n.get('interfaces'), list) else [],
            'services': n.get('services') if isinstance(n.get('services'), list) else [],
        })

    out_links: list[dict[str, str]] = []
    for l in links_list:
        if not isinstance(l, dict):
            continue
        a = str(l.get('node1') or '').strip()
        b = str(l.get('node2') or '').strip()
        if not a or not b or a == b:
            continue
        if a not in by_id or b not in by_id:
            continue
        out_links.append({'node1': a, 'node2': b})

    adj: dict[str, set[str]] = {nid: set() for nid in by_id.keys()}
    for l in out_links:
        a = str(l.get('node1') or '').strip()
        b = str(l.get('node2') or '').strip()
        if not a or not b:
            continue
        adj.setdefault(a, set()).add(b)
        adj.setdefault(b, set()).add(a)

    return out_nodes, out_links, adj


def _pick_flag_chain_nodes(nodes: list[dict[str, Any]], adj: dict[str, set[str]], *, length: int) -> list[dict[str, Any]]:
    """Pick an ordered list of nodes to place flags on.

    The chain is considered solvable if each consecutive pair is connected by
    at least one path in the topology graph. We build the chain by:
    - picking two far-apart endpoints (approx. diameter) in the full graph
    - extracting eligible nodes along that path
    """
    length = max(1, min(int(length or 1), 50))

    id_to_node: dict[str, dict[str, Any]] = {}
    eligible_ids: list[str] = []
    for n in nodes:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if not nid:
            continue
        id_to_node[nid] = n
        t = (str(n.get('type') or '').strip().lower())
        # Flow placement eligibility:
        # - flag-generators may be placed on vulnerability nodes and docker-role nodes
        # - flag-node-generators require docker-role slots (enforced elsewhere)
        is_docker = ('docker' in t) or (str(n.get('type') or '').strip().upper() == 'DOCKER')
        is_vuln = bool(n.get('is_vuln'))
        if is_docker or is_vuln:
            eligible_ids.append(nid)

    if not eligible_ids:
        return []

    def bfs_farthest(start: str) -> tuple[str, dict[str, str | None]]:
        parent: dict[str, str | None] = {start: None}
        q = deque([start])
        last = start
        while q:
            cur = q.popleft()
            last = cur
            for nb in adj.get(cur, set()):
                if nb in parent:
                    continue
                parent[nb] = cur
                q.append(nb)
        return last, parent

    # Choose a start node that is likely connected.
    start = eligible_ids[0]
    far1, _p1 = bfs_farthest(start)
    far2, parents = bfs_farthest(far1)

    # Reconstruct full graph path far1 -> far2.
    path_ids: list[str] = []
    cur = far2
    while cur is not None:
        path_ids.append(cur)
        cur = parents.get(cur)
    path_ids.reverse()

    # Extract eligible nodes along the path.
    chain_ids: list[str] = [nid for nid in path_ids if nid in eligible_ids]

    # If not enough eligible nodes on that path, fall back to a BFS walk over eligible nodes.
    if len(chain_ids) < length:
        seen = set()
        q = deque([start])
        chain_ids = []
        while q and len(chain_ids) < length:
            cur = q.popleft()
            if cur in seen:
                continue
            seen.add(cur)
            if cur in eligible_ids:
                chain_ids.append(cur)
            for nb in sorted(adj.get(cur, set())):
                if nb not in seen:
                    q.append(nb)

    chain_ids = chain_ids[:length]
    # Default ordering: randomize the sequence (user can tweak ordering in the UI).
    try:
        import random as _random
        _random.Random().shuffle(chain_ids)
    except Exception:
        pass
    return [id_to_node[nid] for nid in chain_ids if nid in id_to_node]


def _pick_flag_chain_nodes_for_preset(
    nodes: list[dict[str, Any]],
    adj: dict[str, set[str]],
    *,
    steps: list[dict[str, str]],
) -> list[dict[str, Any]]:
    """Pick a chain that satisfies preset step constraints.

    Currently enforced:
    - flag-generator steps: may be placed on vulnerability nodes OR docker-role nodes
    - flag-node-generator steps: must be placed on a non-vulnerability docker-role node
    """
    try:
        length = len(steps or [])
    except Exception:
        length = 0
    length = max(1, min(int(length or 1), 50))

    id_to_node: dict[str, dict[str, Any]] = {}
    eligible_ids: list[str] = []
    docker_ids: set[str] = set()
    for n in nodes or []:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if not nid:
            continue
        id_to_node[nid] = n
        t_raw = str(n.get('type') or '')
        t = t_raw.strip().lower()
        is_docker = ('docker' in t) or (t_raw.strip().upper() == 'DOCKER')
        is_vuln = bool(n.get('is_vuln'))
        if is_docker or is_vuln:
            eligible_ids.append(nid)
        if is_docker and not is_vuln:
            docker_ids.add(nid)

    if not eligible_ids:
        return []
    if length > len(set(eligible_ids)):
        return []

    # Try to keep the chain in a single connected component, starting from a docker node if possible.
    start = None
    if docker_ids:
        start = next(iter(sorted(docker_ids)))
    else:
        start = eligible_ids[0]

    visited: list[str] = []
    try:
        seen: set[str] = set()
        q = deque([start])
        while q:
            cur = q.popleft()
            if cur in seen:
                continue
            seen.add(cur)
            visited.append(cur)
            for nb in sorted(adj.get(cur, set())):
                if nb not in seen:
                    q.append(nb)
    except Exception:
        visited = list(dict.fromkeys(eligible_ids))

    comp_eligible = [nid for nid in visited if nid in set(eligible_ids)]
    if len(comp_eligible) < length:
        # Fallback: allow selecting across components rather than erroring.
        comp_eligible = list(dict.fromkeys(eligible_ids))

    used: set[str] = set()
    chosen: list[str] = []
    for step in (steps or [])[:length]:
        kind = str((step or {}).get('kind') or '').strip()
        need_docker = (kind == 'flag-node-generator')
        pool = [
            nid for nid in comp_eligible
            if nid not in used and (not need_docker or nid in docker_ids)
        ]
        if not pool:
            return []
        pick = pool[0]
        used.add(pick)
        chosen.append(pick)

    if len(chosen) < length:
        return []
    return [id_to_node[nid] for nid in chosen if nid in id_to_node]


def _flow_compose_docker_stats(nodes: list[dict[str, Any]]) -> dict[str, int]:
    """Return counts for debugging Flow eligibility.

    - docker_total: nodes that look like docker nodes
    - compose_backed_total: docker nodes that carry compose metadata
    - vuln_total: nodes that are vulnerability candidates
    - eligible_total: nodes eligible for chain placement (docker role + vuln nodes)

    Additional explicit metrics (new; kept alongside legacy keys):
    - docker_nonvuln_total: docker nodes that are NOT vulnerability candidates
    - flag_generator_eligible_total: nodes eligible for flag-generator steps (docker role + vuln nodes)
    - flag_node_generator_eligible_total: nodes eligible for flag-node-generator steps (non-vuln docker role only)
    """
    docker_total = 0
    docker_nonvuln_total = 0
    vuln_total = 0
    compose_backed_total = 0
    eligible_total = 0
    for n in nodes or []:
        if not isinstance(n, dict):
            continue
        t_raw = str(n.get('type') or '')
        t = t_raw.strip().lower()
        is_docker = ('docker' in t) or (t_raw.strip().upper() == 'DOCKER')
        is_vuln = bool(n.get('is_vuln'))
        if is_docker:
            docker_total += 1
            comp = str(n.get('compose') or '').strip()
            comp_name = str(n.get('compose_name') or '').strip()
            if comp or comp_name:
                compose_backed_total += 1
        if is_vuln:
            vuln_total += 1
        if is_docker and (not is_vuln):
            docker_nonvuln_total += 1
        if is_docker or is_vuln:
            eligible_total += 1
    return {
        'docker_total': docker_total,
        'docker_nonvuln_total': docker_nonvuln_total,
        'vuln_total': vuln_total,
        'compose_backed_total': compose_backed_total,
        'eligible_total': eligible_total,
        'flag_generator_eligible_total': eligible_total,
        'flag_node_generator_eligible_total': docker_nonvuln_total,
    }


def _attack_flow_builder_afb_for_chain(
    *,
    chain_nodes: list[dict[str, Any]],
    scenario_label: str,
    flag_assignments: list[dict[str, Any]] | None = None,
) -> dict[str, Any]:
    """Build an Attack Flow Builder .afb document for a linear chain.

    Attack Flow Builder's native format is a UI graph JSON (".afb") with explicit
    edge objects (e.g., dynamic_line source/target). This avoids tool warnings like
    "edges must connect on both sides" when importing a STIX bundle.
    """
    now = _iso_now()

    assignment_by_node_id: dict[str, dict[str, Any]] = {}
    try:
        for fa in (flag_assignments or []):
            if not isinstance(fa, dict):
                continue
            nid = str(fa.get('node_id') or '').strip()
            if nid and nid not in assignment_by_node_id:
                assignment_by_node_id[nid] = fa
    except Exception:
        assignment_by_node_id = {}

    # Attack Flow Builder v3 expects an OpenChart DiagramViewExport:
    # - root {schema, objects, (optional) theme/layout/camera}
    # - a single root canvas/group export (id="flow") with an `objects` list of child instances
    # - actions are blocks (id="action") and MUST include an `anchors` map (can be empty)
    flow_instance = _new_uuid()
    objects: list[dict[str, Any]] = []
    layout: dict[str, list[int]] = {}

    flow_name = f"Flag Chain" + (f" - {scenario_label}" if scenario_label else '')
    flow_children: list[str] = []

    # Track per-action anchors so we can wire lines between actions.
    action_left_anchor: dict[str, str] = {}
    action_right_anchor: dict[str, str] = {}
    # Keep references to anchor export dicts so we can append latch instances.
    anchor_obj_by_instance: dict[str, dict[str, Any]] = {}

    # Create one action node per chain step.
    for idx, node in enumerate(chain_nodes, start=1):
        node_id = str(node.get('id') or '').strip()
        node_name = str(node.get('name') or node_id)

        fa = assignment_by_node_id.get(node_id)
        gen_id = str((fa or {}).get('id') or '').strip()
        gen_name = str((fa or {}).get('name') or '').strip()
        gen_kind = str((fa or {}).get('type') or '').strip()
        gen_source = str((fa or {}).get('flag_generator') or '').strip()
        gen_catalog = str((fa or {}).get('generator_catalog') or '').strip()

        output_files: list[str] = ['outputs.json', 'hint.txt']
        try:
            outs = (fa or {}).get('outputs')
            if isinstance(outs, list) and any(str(x).strip() == 'flag' for x in outs):
                output_files.append('flag.txt')
        except Exception:
            pass
        try:
            actual = (fa or {}).get('actual_outputs')
            if isinstance(actual, list) and any(str(x).strip() == 'flag' for x in actual):
                if 'flag.txt' not in output_files:
                    output_files.append('flag.txt')
        except Exception:
            pass

        desc_lines: list[str] = [f"Capture the flag placed on node '{node_name}' (id={node_id})."]
        if fa:
            try:
                gen_desc = str((fa or {}).get('description') or '').strip()
            except Exception:
                gen_desc = ''
            if gen_desc:
                desc_lines.append('')
                desc_lines.append(gen_desc)
            try:
                dh = (fa or {}).get('description_hints')
                if isinstance(dh, list):
                    hints = [str(x or '').strip() for x in dh if str(x or '').strip()]
                    if hints:
                        desc_lines.extend(hints)
            except Exception:
                pass
            if gen_name or gen_id:
                desc_lines.append(
                    "Generator: "
                    + (gen_name if gen_name else gen_id)
                    + (f" ({gen_id})" if (gen_name and gen_id) else '')
                )
            if gen_kind:
                desc_lines.append(f"Kind: {gen_kind}")
            if gen_source:
                desc_lines.append(f"Source: {gen_source}")
            if gen_catalog:
                desc_lines.append(f"Catalog: {gen_catalog}")
            try:
                outs = (fa or {}).get('outputs')
                if isinstance(outs, list) and outs:
                    out_keys = [str(x).strip() for x in outs if str(x).strip()]
                    if out_keys:
                        desc_lines.append("Outputs: " + ", ".join(out_keys))
            except Exception:
                pass
            try:
                produces = (fa or {}).get('produces')
                if isinstance(produces, list) and produces:
                    prod_keys = [str(x).strip() for x in produces if str(x).strip()]
                    if prod_keys:
                        desc_lines.append("Artifacts: " + ", ".join(prod_keys))
            except Exception:
                pass
            if output_files:
                desc_lines.append("Output files: " + ", ".join(output_files))
            try:
                hint = str((fa or {}).get('hint') or '').strip()
                if hint:
                    desc_lines.append("Hint: " + hint)
            except Exception:
                pass

        description = "\n".join([x for x in desc_lines if x is not None])

        action_instance = _new_uuid()
        flow_children.append(action_instance)

        # Create two anchors for this action, used for incoming/outgoing edges.
        left_anchor_instance = _new_uuid()
        right_anchor_instance = _new_uuid()
        action_left_anchor[action_instance] = left_anchor_instance
        action_right_anchor[action_instance] = right_anchor_instance

        # Builder uses angle-keyed anchors (e.g., "0", "180") that refer to
        # template IDs like horizontal_anchor/vertical_anchor.
        left_anchor_obj = {
            'id': 'horizontal_anchor',
            'instance': left_anchor_instance,
            'latches': [],
        }
        right_anchor_obj = {
            'id': 'horizontal_anchor',
            'instance': right_anchor_instance,
            'latches': [],
        }
        anchor_obj_by_instance[left_anchor_instance] = left_anchor_obj
        anchor_obj_by_instance[right_anchor_instance] = right_anchor_obj
        objects.append(left_anchor_obj)
        objects.append(right_anchor_obj)

        objects.append({
            'id': 'action',
            'instance': action_instance,
            # NOTE: OpenChart uses ordered entries (JsonEntries) not dicts.
            'properties': [
                ['name', f"Capture Flag {idx}: {node_name}"],
                # The builder template exposes a nested "ttp" mapping. Keep it present
                # but empty so the file loads without requiring ATT&CK mappings.
                ['ttp', [['tactic', None], ['technique', None]]],
                ['description', description],
            ],
            # Required for BlockExport; an empty map lets the importer create
            # default anchors from the template.
            # We provide anchors for the positions we actually use so the semantic
            # analyzer can discover edges (via anchor -> latch -> line).
            'anchors': {
                # Keys must match Builder's expected anchor keys (angle degrees).
                # "180" ≈ left, "0" ≈ right.
                '180': left_anchor_instance,
                '0': right_anchor_instance,
            },
        })

        # Lay actions left-to-right.
        layout[action_instance] = [220 + (idx - 1) * 260, 320]

    # Create explicit edges between consecutive actions.
    # In OpenChart exports, a line references its source/target latches, and an
    # anchor references linked latches. This is how Builder derives the graph.
    for i in range(len(flow_children) - 1):
        src_action = flow_children[i]
        trg_action = flow_children[i + 1]

        src_anchor = action_right_anchor.get(src_action)
        trg_anchor = action_left_anchor.get(trg_action)
        if not src_anchor or not trg_anchor:
            continue

        line_instance = _new_uuid()
        src_latch_instance = _new_uuid()
        trg_latch_instance = _new_uuid()
        handle_instance = _new_uuid()

        # Link latches to anchors.
        try:
            anchor_obj_by_instance[src_anchor]['latches'].append(src_latch_instance)
        except Exception:
            pass
        try:
            anchor_obj_by_instance[trg_anchor]['latches'].append(trg_latch_instance)
        except Exception:
            pass

        # Create latches and handle.
        objects.append({'id': 'generic_latch', 'instance': src_latch_instance})
        objects.append({'id': 'generic_latch', 'instance': trg_latch_instance})
        objects.append({'id': 'generic_handle', 'instance': handle_instance})

        # Builder's exported layout includes latch positions (but not anchors/lines).
        # Place latches near their respective actions.
        try:
            src_pos = layout.get(src_action)
            trg_pos = layout.get(trg_action)
            if isinstance(src_pos, list) and len(src_pos) == 2 and isinstance(trg_pos, list) and len(trg_pos) == 2:
                src_x, src_y = int(src_pos[0]), int(src_pos[1])
                trg_x, trg_y = int(trg_pos[0]), int(trg_pos[1])
                layout[src_latch_instance] = [src_x + 140, src_y]
                layout[trg_latch_instance] = [trg_x - 140, trg_y]
        except Exception:
            pass

        # Create line connecting the two latches.
        objects.append({
            'id': 'dynamic_line',
            'instance': line_instance,
            'source': src_latch_instance,
            'target': trg_latch_instance,
            'handles': [handle_instance],
        })
        flow_children.append(line_instance)

    # Root flow canvas/group export. Must reference children via `objects`.
    objects.append({
        'id': 'flow',
        'instance': flow_instance,
        'properties': [
            ['name', flow_name],
            ['description', 'A linear chain of flags placed on topology nodes.'],
            ['author', [
                ['name', 'CORE TopoGen'],
                ['identity_class', 'organization'],
                ['contact_information', ''],
            ]],
            ['scope', 'incident'],
            ['external_references', []],
            ['created', now],
        ],
        'objects': flow_children,
    })
    layout[flow_instance] = [160, 120]

    return {
        'schema': 'attack_flow_v2',
        'theme': 'dark_theme',
        'objects': objects,
        'layout': layout,
    }


def _flow_strip_ids_from_hint(text: str) -> str:
    """Strip any node id fragments from a rendered hint.

    Historically, hints included "(id=...)" to help debugging. For the user-facing
    experience, we now remove ids from the hint text while still keeping internal
    next_node_id/this_node_id fields for chaining.
    """
    try:
        s = str(text or '')
    except Exception:
        return ''
    if not s.strip():
        return ''
    try:
        # Remove patterns like " (id=abc123)" (case-insensitive, whitespace tolerant).
        s = re.sub(r"\s*\(\s*id\s*=\s*[^)]*\)", "", s, flags=re.IGNORECASE)
    except Exception:
        pass
    # Remove any leftover unexpanded id placeholders.
    for token in ('{{NEXT_NODE_ID}}', '{{THIS_NODE_ID}}'):
        try:
            s = s.replace(token, '')
        except Exception:
            continue
    try:
        s = re.sub(r"[\t ]{2,}", " ", s)
    except Exception:
        pass
    return s.strip()


def _flow_render_hint_template(tpl: str, *, scenario_label: str, id_to_name: dict[str, str], this_id: str, next_id: str) -> str:
    try:
        text = str(tpl or '').strip() or 'Next: {{NEXT_NODE_NAME}}'
        next_id_val = str(next_id or '').strip()
        next_name_val = (id_to_name.get(next_id_val) or '').strip() if next_id_val else ''
        if not next_id_val:
            return "You've completed this sequence of challenges!"
        if not next_name_val:
            next_name_val = next_id_val
        repl = {
            '{{SCENARIO}}': str(scenario_label or ''),
            '{{THIS_NODE_ID}}': str(this_id or ''),
            '{{THIS_NODE_NAME}}': id_to_name.get(str(this_id or '')) or str(this_id or ''),
            '{{NEXT_NODE_ID}}': next_id_val,
            '{{NEXT_NODE_NAME}}': next_name_val,
        }
        for k, v in repl.items():
            try:
                text = text.replace(k, str(v))
            except Exception:
                continue
        return _flow_strip_ids_from_hint(text)
    except Exception:
        return _flow_strip_ids_from_hint(str(tpl or '').strip() or 'Next: {{NEXT_NODE_NAME}}')


def _flow_hint_templates_from_generator(gen: dict[str, Any]) -> list[str]:
    """Return ordered hint templates from a generator view.

    Accepts:
    - gen['hint_templates']: list[str]
    - gen['hint_template']: str or list[str] (legacy / permissive)
    """
    out: list[str] = []
    try:
        ht = gen.get('hint_templates')
        if isinstance(ht, list):
            for x in ht:
                s = str(x or '').strip()
                if s:
                    s2 = _flow_strip_ids_from_hint(s)
                    if s2:
                        out.append(s2)
    except Exception:
        pass
    if out:
        return out
    try:
        ht1 = gen.get('hint_template')
        if isinstance(ht1, list):
            for x in ht1:
                s = str(x or '').strip()
                if s:
                    s2 = _flow_strip_ids_from_hint(s)
                    if s2:
                        out.append(s2)
        else:
            s = str(ht1 or '').strip()
            if s:
                s2 = _flow_strip_ids_from_hint(s)
                if s2:
                    out.append(s2)
    except Exception:
        pass
    if not out:
        out = ['Next: {{NEXT_NODE_NAME}}']
    return out


def _flow_preset_steps(preset: str) -> list[dict[str, str]]:
    p = str(preset or '').strip().lower()
    if p in {'sample_reverse_nfs_ssh', 'sample'}:
        return [
            {'id': 'binary_embed_text', 'kind': 'flag-generator', 'catalog': 'flag_generators'},
            {'id': 'nfs_sensitive_file', 'kind': 'flag-node-generator', 'catalog': 'flag_node_generators'},
            {'id': 'textfile_username_password', 'kind': 'flag-generator', 'catalog': 'flag_generators'},
        ]
    return []


def _flow_compute_flag_assignments_for_preset(
    preview: dict,
    chain_nodes: list[dict[str, Any]],
    scenario_label: str,
    preset: str,
) -> tuple[list[dict[str, Any]], str | None]:
    steps = _flow_preset_steps(preset)
    if not steps:
        return [], 'unknown preset'

    def _preset_stats() -> dict[str, int]:
        try:
            _nodes, _links, _adj = _build_topology_graph_from_preview_plan(preview)
            st = _flow_compose_docker_stats(_nodes)
            return st if isinstance(st, dict) else {}
        except Exception:
            return {}

    def _requirement_message(*, required_total: int, required_nonvuln_docker: int) -> str:
        st = _preset_stats()
        docker_total = int(st.get('docker_total') or 0)
        vuln_total = int(st.get('vuln_total') or 0)
        required_any = max(0, int(required_total) - int(required_nonvuln_docker))
        return f"Requires {required_any} Docker/Vuln and {int(required_nonvuln_docker)} Non-Vuln. Current: Docker: {docker_total}, Vuln: {vuln_total}"

    required_total = len(steps)
    required_nonvuln_docker = sum(1 for s in steps if str((s or {}).get('kind') or '').strip() == 'flag-node-generator')

    if len(chain_nodes) < required_total:
        return [], _requirement_message(required_total=required_total, required_nonvuln_docker=required_nonvuln_docker)

    try:
        gens, _ = _flag_generators_from_enabled_sources()
    except Exception:
        gens = []
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
    except Exception:
        node_gens = []

    by_id: dict[str, dict[str, Any]] = {}
    for g in (gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid and gid not in by_id:
            by_id[gid] = g
    for g in (node_gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid and gid not in by_id:
            by_id[gid] = g

    chain_ids: list[str] = [str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict) and str(n.get('id') or '').strip()]
    id_to_name: dict[str, str] = {}
    for n in chain_nodes:
        try:
            nid = str(n.get('id') or '').strip()
            nm = str(n.get('name') or '').strip()
            if nid:
                id_to_name[nid] = nm or nid
        except Exception:
            pass

    # Prefer v3 plugin contracts for artifact-level chaining semantics.
    try:
        plugins_by_id = _flow_enabled_plugin_contracts_by_id()
    except Exception:
        plugins_by_id = {}

    out: list[dict[str, Any]] = []
    for i, step in enumerate(steps):
        cid = chain_ids[i] if i < len(chain_ids) else ''
        next_id = chain_ids[i + 1] if (i + 1) < len(chain_ids) else ''
        gen_id = str(step.get('id') or '').strip()
        kind = str(step.get('kind') or '').strip() or 'flag-generator'
        catalog = str(step.get('catalog') or '').strip() or ('flag_node_generators' if kind == 'flag-node-generator' else 'flag_generators')

        # Mirror the existing rule: vuln nodes are only assigned flag-generators.
        try:
            node = chain_nodes[i] if i < len(chain_nodes) else {}
            if kind == 'flag-node-generator' and isinstance(node, dict) and bool(node.get('is_vuln')):
                return [], _requirement_message(required_total=required_total, required_nonvuln_docker=required_nonvuln_docker)
        except Exception:
            pass

        gen = by_id.get(gen_id)
        if not isinstance(gen, dict):
            return [], f'generator not found/enabled: {gen_id}'

        hint_templates = _flow_hint_templates_from_generator(gen)
        hint_tpl = hint_templates[0] if hint_templates else 'Next: {{NEXT_NODE_NAME}}'

        # Runtime IO (generator input/output fields).
        # Show required + optional separately (UI), but only required participates in feasibility.
        input_fields_all = sorted([
            str(x.get('name') or '').strip()
            for x in (gen.get('inputs') or [])
            if isinstance(x, dict) and str(x.get('name') or '').strip()
        ])
        input_fields_required = sorted([
            str(x.get('name') or '').strip()
            for x in (gen.get('inputs') or [])
            if isinstance(x, dict) and str(x.get('name') or '').strip() and x.get('required') is not False
        ])
        input_fields_optional = sorted([x for x in input_fields_all if x and x not in set(input_fields_required)])
        output_fields = sorted([
            str(x.get('name') or '').strip()
            for x in (gen.get('outputs') or [])
            if isinstance(x, dict) and str(x.get('name') or '').strip()
        ])

        # Artifact-level contracts (plugin requires/produces).
        requires_artifacts: list[str] = []
        produces_artifacts: list[str] = []
        try:
            plugin = plugins_by_id.get(gen_id)
            if isinstance(plugin, dict):
                req = plugin.get('requires')
                if isinstance(req, list):
                    requires_artifacts = sorted([str(x).strip() for x in req if str(x).strip()])
                prod = plugin.get('produces')
                if isinstance(prod, list):
                    produces_artifacts = sorted([
                        str(it.get('artifact') or '').strip()
                        for it in prod
                        if isinstance(it, dict) and str(it.get('artifact') or '').strip()
                    ])
        except Exception:
            requires_artifacts = []
            produces_artifacts = []

        # If an artifact "requires" token also appears as an optional input field,
        # treat it as optional (exclude from effective chaining requirements).
        try:
            optional_field_set = set(input_fields_optional)
            requires_effective = sorted([x for x in (requires_artifacts or []) if x and x not in optional_field_set])
        except Exception:
            requires_effective = list(requires_artifacts or [])

        # Effective chaining semantics used by ordering validation.
        inputs_effective = sorted(set(requires_effective) | set(input_fields_required))
        outputs_effective = sorted(set(produces_artifacts) | set(output_fields))

        rendered_hints = [
            _flow_render_hint_template(t, scenario_label=scenario_label, id_to_name=id_to_name, this_id=str(cid), next_id=str(next_id))
            for t in (hint_templates or [])
        ]
        out.append({
            'node_id': str(cid),
            'id': gen_id,
            'name': str(gen.get('name') or ''),
            'description': str(gen.get('description') or ''),
            'type': kind,
            'flag_generator': str(gen.get('_source_name') or '').strip() or 'unknown',
            'generator_catalog': catalog,
            'language': str(gen.get('language') or ''),
            'description_hints': list(gen.get('description_hints') or []) if isinstance(gen.get('description_hints'), list) else [],
            # Effective union (used for chaining feasibility / ordering validation).
            'inputs': inputs_effective,
            'outputs': outputs_effective,

            # Split-out views for UI transparency.
            'requires': requires_artifacts,
            'produces': produces_artifacts,
            'input_fields': input_fields_all,
            'input_fields_required': input_fields_required,
            'input_fields_optional': input_fields_optional,
            'output_fields': output_fields,
            'hint_template': hint_tpl,
            'hint_templates': hint_templates,
            'hint': rendered_hints[0] if rendered_hints else _flow_render_hint_template(hint_tpl, scenario_label=scenario_label, id_to_name=id_to_name, this_id=str(cid), next_id=str(next_id)),
            'hints': rendered_hints,
            'next_node_id': str(next_id),
            'next_node_name': str(id_to_name.get(str(next_id)) or ''),
        })

    return out, None


@app.before_request
def _require_login_redirect() -> None | Response:
    try:
        endpoint = request.endpoint or ''
        if not endpoint:
            return None
        if endpoint.startswith('static'):
            return None
        if endpoint in _LOGIN_EXEMPT_ENDPOINTS:
            return None
        if _current_user() is None:
            # For API routes, return a JSON 401 instead of an HTML redirect.
            # This prevents frontend fetch() calls from failing with non-JSON responses.
            try:
                if (request.path or '').startswith('/api/'):
                    return jsonify({'ok': False, 'error': 'Login required'}), 401
            except Exception:
                pass
            return redirect(url_for('login'))
    except Exception:
        return None
    return None


def _require_admin() -> bool:
    user = _current_user()
    if user and (user.get('role') == 'admin'):
        return True
    flash('Admin privileges required')
    return False


@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'GET':
        return render_template('login.html')
    username = (request.form.get('username') or '').strip()
    password = request.form.get('password') or ''
    if not username or not password:
        flash('Username and password required')
        return render_template('login.html', error=True), 400
    db = _load_users()
    users = db.get('users', [])
    user = next((u for u in users if u.get('username') == username), None)
    if user and check_password_hash(user.get('password_hash', ''), password):
        role_value = _normalize_role_value(user.get('role'))
        _set_current_user({'username': user.get('username'), 'role': role_value})
        session.permanent = True
        try:
            session[_UI_VIEW_SESSION_KEY] = _default_ui_view_mode_for_role(role_value)
        except Exception:
            pass
        if _is_participant_role(role_value):
            return redirect(url_for('participant_ui_page'))
        return redirect(url_for('index'))
    flash('Invalid username or password')
    return render_template('login.html', error=True), 401


@app.route('/logout', methods=['POST', 'GET'])
def logout():
    _set_current_user(None)
    return redirect(url_for('login'))


@app.route('/scenarios/flag-sequencing')
def flow_page():
    history = _load_run_history()
    current_user = _current_user()
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
    scenario_participant_urls = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
    participant_url_flags = {
        norm: bool(url)
        for norm, url in scenario_participant_urls.items()
        if isinstance(norm, str) and norm
    }

    scenario_query = request.args.get('scenario', '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    # Enforce per-scenario view: default to the first available scenario when unset/invalid.
    if scenario_names:
        if not scenario_norm or not any(_normalize_scenario_label(n) == scenario_norm for n in scenario_names):
            scenario_norm = _normalize_scenario_label(scenario_names[0])
    active_scenario = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query)

    # Best-effort: provide the saved scenario XML path so the shared Preview button can work from this page.
    active_scenario_xml_path = ''
    try:
        if scenario_norm and isinstance(scenario_paths, dict):
            p = scenario_paths.get(scenario_norm) or scenario_paths.get(active_scenario) or ''
            p2 = _select_existing_path(p)
            if p2:
                active_scenario_xml_path = os.path.abspath(p2)
    except Exception:
        active_scenario_xml_path = ''

    xml_preview = ''
    try:
        if active_scenario_xml_path and os.path.isfile(active_scenario_xml_path):
            with open(active_scenario_xml_path, 'r', encoding='utf-8', errors='ignore') as f:
                xml_preview = f.read()
    except Exception:
        xml_preview = ''

    return render_template(
        'flow.html',
        scenarios=scenario_names,
        active_scenario=active_scenario,
        participant_url_flags=participant_url_flags,
        preview_xml_path=active_scenario_xml_path,
        xml_preview=xml_preview,
    )


@app.route('/scenarios/preview')
def scenarios_preview_page():
    """Scenarios sub-tab: Preview.

    This page renders a lightweight shell and loads the existing full preview page
    into an iframe while showing a loading modal.
    """
    history = _load_run_history()
    current_user = _current_user()
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)

    scenario_query = (request.args.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    if scenario_names:
        if not scenario_norm or not any(_normalize_scenario_label(n) == scenario_norm for n in scenario_names):
            scenario_norm = _normalize_scenario_label(scenario_names[0])
    active_scenario = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query)

    # Prefer explicit xml_path (e.g., coming from Topology editor save). Fallback to the catalog path for the active scenario.
    xml_path = (request.args.get('xml_path') or '').strip()
    xml_path_abs = ''
    try:
        if xml_path:
            xml_path_abs = os.path.abspath(xml_path)
            if not os.path.exists(xml_path_abs):
                xml_path_abs = ''
    except Exception:
        xml_path_abs = ''
    if not xml_path_abs:
        try:
            p = ''
            if scenario_norm and isinstance(scenario_paths, dict):
                p = scenario_paths.get(scenario_norm) or scenario_paths.get(active_scenario) or ''
            p2 = _select_existing_path(p)
            if p2:
                xml_path_abs = os.path.abspath(p2)
        except Exception:
            xml_path_abs = ''

    scenario_xml_by_name: dict[str, str] = {}
    try:
        if isinstance(scenario_names, list) and isinstance(scenario_paths, dict):
            for nm in scenario_names:
                try:
                    nm_norm = _normalize_scenario_label(nm)
                    raw_path = scenario_paths.get(nm_norm) or scenario_paths.get(nm) or ''
                    chosen = _select_existing_path(raw_path) or ''
                    scenario_xml_by_name[str(nm)] = os.path.abspath(chosen) if chosen else ''
                except Exception:
                    scenario_xml_by_name[str(nm)] = ''
    except Exception:
        scenario_xml_by_name = {}

    return render_template(
        'scenarios_preview.html',
        scenarios=scenario_names,
        active_scenario=active_scenario,
        scenario_xml_by_name=scenario_xml_by_name,
        scenario_tab=active_scenario,
        preview_xml_path=xml_path_abs,
    )


def _latest_preview_plan_for_scenario_norm(scenario_norm: str, *, prefer_flow: bool = False) -> Optional[str]:
    """Return absolute path to newest persisted plan for a scenario.

    This is primarily used by Flag Sequencing. Two plan types exist:
    - Preview plans: outputs/plans/plan_from_preview_*.json
    - Flow-modified plans: outputs/plans/plan_from_flow_*.json

    By default, we prefer preview plans. Flow-modified plans may include
    runtime/Flow-only mutations (e.g., promoting nodes to Docker role for
    attachment) which should not affect the topology/eligibility counts
    displayed in Flag Sequencing.

    Important nuance: if a newer preview plan exists (scenario topology changed),
    we return the newer preview plan even when prefer_flow=True. The caller can
    still merge the latest flow metadata (chain/order/assignments) into the
    newer preview payload.
    """
    try:
        scenario_norm = _normalize_scenario_label(scenario_norm)
        if not scenario_norm:
            return None
        plans_dir = Path(_outputs_dir()) / 'plans'
        if not plans_dir.is_dir():
            return None
        def _created_at_ts(meta: Any, *, fallback_path: Optional[Path] = None) -> float:
            try:
                if isinstance(meta, dict):
                    raw = meta.get('created_at')
                else:
                    raw = None
                s = str(raw or '').strip()
                if s:
                    # Handle ISO strings with Z suffix.
                    if s.endswith('Z'):
                        s = s[:-1] + '+00:00'
                    try:
                        from datetime import datetime as _dt
                        dt = _dt.fromisoformat(s)
                        return float(dt.timestamp())
                    except Exception:
                        pass
            except Exception:
                pass
            try:
                if fallback_path is not None:
                    return float(fallback_path.stat().st_mtime)
            except Exception:
                pass
            return 0.0

        def _latest_matching(pattern: str) -> tuple[Optional[Path], float]:
            scanned = 0
            candidates = list(plans_dir.glob(pattern))
            candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)
            for p in candidates:
                scanned += 1
                if scanned > 250:
                    return None, 0.0
                try:
                    with open(p, 'r', encoding='utf-8') as f:
                        payload = json.load(f) or {}
                    meta = payload.get('metadata') if isinstance(payload, dict) else None
                    scen = str((meta or {}).get('scenario') or '').strip()
                    if _normalize_scenario_label(scen) == scenario_norm:
                        return p, _created_at_ts(meta, fallback_path=p)
                except Exception:
                    continue
            return None, 0.0

        latest_flow, latest_flow_ts = _latest_matching('plan_from_flow_*.json')
        latest_preview, latest_preview_ts = _latest_matching('plan_from_preview_*.json')

        if prefer_flow:
            # Prefer flow plan only when no preview exists.
            if latest_preview is not None:
                return str(latest_preview)
            if latest_flow is not None:
                return str(latest_flow)
            return None

        # prefer_flow=False (default): always prefer preview when available.
        if latest_preview is not None:
            return str(latest_preview)
        if latest_flow is not None:
            return str(latest_flow)
        return None
    except Exception:
        return None


def _attach_latest_flow_into_plan_payload(payload: dict[str, Any], *, scenario: str) -> None:
    """Best-effort: merge saved flow metadata into a plan payload.

    This is useful when the newest plan is a preview plan (topology updated) but
    we still want to honor the user's saved Flow chain/order/assignments.
    """
    try:
        if not isinstance(payload, dict):
            return
        scenario_norm = _normalize_scenario_label(scenario or '')
        if not scenario_norm:
            return

        full_prev = payload.get('full_preview')
        if not isinstance(full_prev, dict):
            return

        flow_plan_path = _latest_flow_plan_for_scenario_norm(scenario_norm)
        if not flow_plan_path:
            return
        try:
            with open(flow_plan_path, 'r', encoding='utf-8') as f:
                flow_payload = json.load(f) or {}
        except Exception:
            return

        flow_meta = None
        if isinstance(flow_payload, dict):
            meta = flow_payload.get('metadata') if isinstance(flow_payload.get('metadata'), dict) else {}
            flow_meta = (meta or {}).get('flow')
            if not flow_meta:
                flow_meta = flow_payload.get('flow')
        if not isinstance(flow_meta, dict):
            return

        # Repair saved Flow metadata against the *current* full_preview topology.
        # This prevents stale chains/assignments from pointing at nodes that are no
        # longer eligible (e.g., non-docker/non-vuln hosts).
        try:
            repaired = _flow_repair_saved_flow_for_preview(full_prev, flow_meta)
            if not isinstance(repaired, dict):
                return
            flow_meta = repaired
        except Exception:
            return

        payload.setdefault('metadata', {})
        if isinstance(payload.get('metadata'), dict):
            payload['metadata']['flow'] = flow_meta
            payload['metadata']['flow_plan_path'] = flow_plan_path

        # Keep the same shape expected by the preview graph.
        full_prev.setdefault('metadata', {})
        if isinstance(full_prev.get('metadata'), dict):
            full_prev['metadata']['flow'] = flow_meta
            full_prev['metadata']['flow_plan_path'] = flow_plan_path
        full_prev['flow'] = flow_meta
    except Exception:
        return


def _latest_flow_plan_for_scenario_norm(scenario_norm: str) -> Optional[str]:
    """Return newest flow-modified plan path for scenario (or None)."""
    try:
        scenario_norm = _normalize_scenario_label(scenario_norm)
        if not scenario_norm:
            return None
        plans_dir = Path(_outputs_dir()) / 'plans'
        if not plans_dir.is_dir():
            return None
        candidates = list(plans_dir.glob('plan_from_flow_*.json'))
        candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        for p in candidates[:250]:
            try:
                with open(p, 'r', encoding='utf-8') as f:
                    payload = json.load(f) or {}
                meta = payload.get('metadata') if isinstance(payload, dict) else None
                scen = str((meta or {}).get('scenario') or '').strip()
                if _normalize_scenario_label(scen) == scenario_norm:
                    return str(p)
            except Exception:
                continue
        return None
    except Exception:
        return None


def _attach_latest_flow_into_full_preview(full_prev: dict, scenario: Optional[str]) -> None:
    """Best-effort: merge saved Flag Sequencing (flow) metadata into a full_preview payload.

    Preview plans generated from XML alone do not include flow/chain data; the user's
    sequencing is stored in outputs/plans/plan_from_flow_*.json.

    The Preview graph expects flow chain data under full_preview.metadata.flow (or
    sometimes full_preview.flow) to mark sequence nodes.
    """
    try:
        if not isinstance(full_prev, dict):
            return
        scenario_norm = _normalize_scenario_label(scenario or '')
        if not scenario_norm:
            return
        flow_plan_path = _latest_flow_plan_for_scenario_norm(scenario_norm)
        if not flow_plan_path:
            return
        try:
            with open(flow_plan_path, 'r', encoding='utf-8') as f:
                flow_payload = json.load(f) or {}
        except Exception:
            return
        flow_meta = None
        if isinstance(flow_payload, dict):
            meta = flow_payload.get('metadata') if isinstance(flow_payload.get('metadata'), dict) else {}
            flow_meta = (meta or {}).get('flow')
            if not flow_meta:
                flow_meta = flow_payload.get('flow')
        if not isinstance(flow_meta, dict):
            return

        # Repair saved Flow metadata against the *current* full_preview topology.
        # Preview plans can change over time; keep the UI from marking/assigning
        # flags to nodes that are no longer eligible.
        try:
            repaired = _flow_repair_saved_flow_for_preview(full_prev, flow_meta)
            if not isinstance(repaired, dict):
                return
            flow_meta = repaired
        except Exception:
            return

        # Attach in the shape the front-end already understands.
        full_prev.setdefault('metadata', {})
        if isinstance(full_prev.get('metadata'), dict):
            full_prev['metadata']['flow'] = flow_meta
            full_prev['metadata']['flow_plan_path'] = flow_plan_path
        # Back-compat: also provide top-level alias.
        full_prev['flow'] = flow_meta
    except Exception:
        return


def _flow_node_is_docker_role(node: dict[str, Any] | None) -> bool:
    try:
        if not isinstance(node, dict):
            return False
        t_raw = str(node.get('type') or '')
        t = t_raw.strip().lower()
        if ('docker' in t) or (t_raw.strip().upper() == 'DOCKER'):
            return True
        # Preview graph nodes also carry compose metadata for docker-role hosts.
        comp = str(node.get('compose') or '').strip()
        comp_name = str(node.get('compose_name') or '').strip()
        return bool(comp or comp_name)
    except Exception:
        return False


def _flow_node_is_vuln(node: dict[str, Any] | None) -> bool:
    try:
        if not isinstance(node, dict):
            return False
        return bool(node.get('is_vuln'))
    except Exception:
        return False


def _flow_repair_saved_flow_for_preview(full_prev: dict, flow_meta: dict) -> dict | None:
    """Best-effort: repair a persisted Flow chain/assignments against the current preview.

    This is intentionally conservative: if we cannot build a valid chain of the same
    length, we return None and the caller should skip attaching Flow.

    Placement rules enforced:
    - flag-generator: allowed on vuln nodes OR docker-role nodes
    - flag-node-generator: allowed only on non-vuln docker-role nodes
    """
    if not isinstance(full_prev, dict) or not isinstance(flow_meta, dict):
        return None

    chain_in = flow_meta.get('chain')
    if not isinstance(chain_in, list) or not chain_in:
        return None

    assignments_in = flow_meta.get('flag_assignments')
    assignments: list[dict[str, Any]] | None = None
    if isinstance(assignments_in, list) and assignments_in:
        assignments = [a for a in assignments_in if isinstance(a, dict)]

    nodes, _links, _adj = _build_topology_graph_from_preview_plan(full_prev)
    id_map = {str(n.get('id') or '').strip(): n for n in (nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()}
    if not id_map:
        return None

    # Candidate pools.
    eligible_any: list[dict[str, Any]] = []
    eligible_nonvuln_docker: list[dict[str, Any]] = []
    for n in nodes or []:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if not nid:
            continue
        is_docker = _flow_node_is_docker_role(n)
        is_vuln = _flow_node_is_vuln(n)
        if is_docker or is_vuln:
            eligible_any.append(n)
        if is_docker and (not is_vuln):
            eligible_nonvuln_docker.append(n)
    eligible_any.sort(key=lambda x: str(x.get('id') or ''))
    eligible_nonvuln_docker.sort(key=lambda x: str(x.get('id') or ''))

    used: set[str] = set()
    chain_out: list[dict[str, str]] = []
    assignments_out: list[dict[str, Any]] | None = [] if assignments is not None else None

    for i, step in enumerate(chain_in):
        step_id = str((step or {}).get('id') or '').strip() if isinstance(step, dict) else ''

        kind = 'flag-generator'
        if assignments is not None and i < len(assignments):
            try:
                kind = str((assignments[i] or {}).get('type') or '').strip() or 'flag-generator'
            except Exception:
                kind = 'flag-generator'

        need_nonvuln_docker = (kind == 'flag-node-generator')

        cand = id_map.get(step_id) if step_id else None
        if cand is not None:
            nid = str(cand.get('id') or '').strip()
            ok = bool(nid) and (nid not in used)
            if ok:
                is_docker = _flow_node_is_docker_role(cand)
                is_vuln = _flow_node_is_vuln(cand)
                if need_nonvuln_docker:
                    ok = bool(is_docker and (not is_vuln))
                else:
                    ok = bool(is_docker or is_vuln)
            if not ok:
                cand = None

        if cand is None:
            pool = eligible_nonvuln_docker if need_nonvuln_docker else eligible_any
            pick = None
            for n in pool:
                nid = str(n.get('id') or '').strip()
                if nid and nid not in used:
                    pick = n
                    break
            cand = pick

        if not isinstance(cand, dict):
            return None

        nid = str(cand.get('id') or '').strip()
        if not nid:
            return None
        used.add(nid)

        chain_out.append({
            'id': nid,
            'name': str(cand.get('name') or nid),
            'type': str(cand.get('type') or 'node'),
        })

        if assignments is not None and assignments_out is not None and i < len(assignments):
            a2 = dict(assignments[i] or {})
            a2['node_id'] = nid
            assignments_out.append(a2)

    out = dict(flow_meta)
    out['chain'] = chain_out
    out['length'] = len(chain_out)
    if assignments_out is not None:
        out['flag_assignments'] = assignments_out
    return out


def _build_topology_graph_from_preview_plan(preview: Dict[str, Any]) -> Tuple[list[dict[str, Any]], list[dict[str, str]], dict[str, set[str]]]:
    """Build a lightweight graph from a full_preview payload.

    This is used for Flow generation based on persisted preview artifacts
    (no CORE session XML required).
    """
    nodes_out: list[dict[str, Any]] = []
    links_out: list[dict[str, str]] = []
    by_id: dict[str, dict[str, Any]] = {}

    def _add_node(nid: str, name: str, ntype: str, *, compose: str = '', compose_name: str = '', is_vuln: bool = False) -> None:
        if not nid or nid in by_id:
            return
        rec = {
            'id': nid,
            'name': name or nid,
            'type': ntype or 'node',
            'compose': compose or '',
            'compose_name': compose_name or '',
            'is_vuln': bool(is_vuln),
            'interfaces': [],
            'services': [],
        }
        by_id[nid] = rec
        nodes_out.append(rec)

    def _add_link(a: str, b: str) -> None:
        a = str(a or '').strip()
        b = str(b or '').strip()
        if not a or not b or a == b:
            return
        if a not in by_id or b not in by_id:
            return
        links_out.append({'node1': a, 'node2': b})

    # Add routers
    for r in (preview.get('routers') or []):
        if not isinstance(r, dict):
            continue
        rid = str(r.get('node_id') or '').strip()
        if not rid:
            continue
        _add_node(rid, str(r.get('name') or f'router-{rid}'), 'router')

    # Add switches
    for s in (preview.get('switches') or []):
        if not isinstance(s, dict):
            continue
        sid = str(s.get('node_id') or '').strip()
        if not sid:
            continue
        _add_node(sid, str(s.get('name') or f'switch-{sid}'), 'switch')

    vuln_ids: set[str] = set()
    try:
        vb = preview.get('vulnerabilities_by_node') if isinstance(preview, dict) else None
        if isinstance(vb, dict):
            for k, v in vb.items():
                if not v:
                    continue
                kk = str(k or '').strip()
                if kk:
                    vuln_ids.add(kk)
    except Exception:
        vuln_ids = set()

    # Add hosts
    for h in (preview.get('hosts') or []):
        if not isinstance(h, dict):
            continue
        hid = str(h.get('node_id') or '').strip()
        if not hid:
            continue
        role = str(h.get('role') or 'Host')
        role_norm = role.strip().lower()
        is_docker_role = role_norm == 'docker'
        vulns = h.get('vulnerabilities') if isinstance(h.get('vulnerabilities'), list) else []
        is_vuln = bool(vulns) or (hid in vuln_ids)
        # IMPORTANT: vulnerabilities should not "take up" Docker node slots.
        # Only Docker-role hosts are treated as docker nodes for Flow eligibility.
        node_type = 'docker' if is_docker_role else 'host'
        compose = ''
        compose_name = ''
        if is_docker_role:
            compose_name = 'standard-ubuntu-docker-core'
            compose = 'scripts/standard-ubuntu-docker-core/docker-compose.yml'
        _add_node(hid, str(h.get('name') or f'host-{hid}'), node_type, compose=compose, compose_name=compose_name, is_vuln=is_vuln)

    # Router-to-router links
    for l in (preview.get('r2r_links_preview') or []):
        if not isinstance(l, dict):
            continue
        routers = l.get('routers')
        if not isinstance(routers, list) or len(routers) < 2:
            continue
        try:
            a = str((routers[0] or {}).get('id') or '').strip()
            b = str((routers[1] or {}).get('id') or '').strip()
        except Exception:
            continue
        _add_link(a, b)

    # Router-switch-host grouping
    try:
        for detail in (preview.get('switches_detail') or []):
            if not isinstance(detail, dict):
                continue
            sid = str(detail.get('switch_id') or '').strip()
            rid = str(detail.get('router_id') or '').strip()
            if sid and rid:
                _add_link(sid, rid)
            for hid in (detail.get('hosts') or []):
                _add_link(sid, str(hid))
    except Exception:
        pass

    # Direct host-router attachment (when no switches)
    try:
        hrm = preview.get('host_router_map') or {}
        if isinstance(hrm, dict):
            for hid, rid in hrm.items():
                _add_link(str(hid), str(rid))
    except Exception:
        pass

    adj: dict[str, set[str]] = {nid: set() for nid in by_id.keys()}
    for l in links_out:
        a = str(l.get('node1') or '').strip()
        b = str(l.get('node2') or '').strip()
        if not a or not b:
            continue
        adj.setdefault(a, set()).add(b)
        adj.setdefault(b, set()).add(a)

    return nodes_out, links_out, adj


@app.route('/api/flag-sequencing/latest_preview_plan')
def api_flow_latest_preview_plan():
    scenario_label = (request.args.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400
    plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm, prefer_flow=False)
    if not plan_path:
        return jsonify({'ok': False, 'error': 'No preview plan found for this scenario. Generate a Full Preview from the Scenarios page first.'}), 404
    try:
        with open(plan_path, 'r', encoding='utf-8') as f:
            payload = json.load(f) or {}
        meta = payload.get('metadata') if isinstance(payload, dict) else None
    except Exception:
        meta = None
    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'preview_plan_path': plan_path,
        'metadata': meta or {},
    })


def _flow_read_flag_value_from_artifacts_dir(artifacts_dir: str) -> str:
    """Read a realized flag value from a generator artifacts directory.

    Only reads from directories under /tmp/vulns to avoid arbitrary file access.
    Returns empty string when not found.
    """
    try:
        d = str(artifacts_dir or '').strip()
        if not d:
            return ''
        base_dir = os.path.abspath(os.path.join('/tmp', 'vulns'))
        dd = os.path.abspath(d)
        if os.path.commonpath([dd, base_dir]) != base_dir:
            return ''
        if not os.path.isdir(dd):
            return ''
        flag_txt = os.path.join(dd, 'flag.txt')
        if os.path.isfile(flag_txt):
            with open(flag_txt, 'r', encoding='utf-8', errors='ignore') as f:
                val = (f.read() or '').strip()
                return val[:4096] if val else ''
        outs_path = os.path.join(dd, 'outputs.json')
        if os.path.isfile(outs_path):
            try:
                with open(outs_path, 'r', encoding='utf-8') as f:
                    m = json.load(f) or {}
                outs = m.get('outputs') if isinstance(m, dict) else None
                if isinstance(outs, dict):
                    v = outs.get('flag')
                    if isinstance(v, str) and v.strip():
                        vv = v.strip()
                        return vv[:4096]
            except Exception:
                return ''
    except Exception:
        return ''
    return ''


@app.route('/api/flag-sequencing/flag_values_for_node')
def api_flow_flag_values_for_node():
    """Return realized flag value(s) for a sequenced node (runtime-only).

    Used by the Scenarios Preview tab graph popup to fetch flag values on-demand.
    """
    scenario_label = (request.args.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    node_id = str((request.args.get('node_id') or '').strip())
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400
    if not node_id:
        return jsonify({'ok': False, 'error': 'No node_id specified.'}), 400

    plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm, prefer_flow=False)
    if not plan_path or not os.path.exists(plan_path):
        return jsonify({'ok': False, 'error': 'No preview plan found for this scenario.'}), 404

    try:
        with open(plan_path, 'r', encoding='utf-8') as f:
            payload = json.load(f) or {}
        meta = payload.get('metadata') if isinstance(payload, dict) else None
        flow = (meta or {}).get('flow') if isinstance(meta, dict) else None
        fas = flow.get('flag_assignments') if isinstance(flow, dict) else None
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to load preview plan: {e}'}), 500

    if not isinstance(fas, list) or not fas:
        return jsonify({'ok': True, 'scenario': scenario_label or scenario_norm, 'node_id': node_id, 'flags': []})

    matches = [a for a in fas if isinstance(a, dict) and str(a.get('node_id') or '').strip() == node_id]
    out_flags: list[dict[str, Any]] = []
    for a in (matches or []):
        try:
            artifacts_dir = str(a.get('artifacts_dir') or a.get('run_dir') or '').strip()
            val = _flow_read_flag_value_from_artifacts_dir(artifacts_dir) if artifacts_dir else ''
            out_flags.append({
                'generator_id': str(a.get('id') or ''),
                'generator_name': str(a.get('name') or ''),
                'flag_value': val,
            })
        except Exception:
            out_flags.append({
                'generator_id': str(a.get('id') or ''),
                'generator_name': str(a.get('name') or ''),
                'flag_value': '',
            })

    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'node_id': node_id,
        'flags': out_flags,
    })


def _flow_compute_flag_assignments(preview: dict, chain_nodes: list[dict[str, Any]], scenario_label: str) -> list[dict[str, Any]]:
    """Return a per-position list of flag assignments aligned to chain_ids.

    Each item includes node_id plus the fields used for metadata.flow_flag.
    Deterministic-ish: shuffles eligible flags based on preview.seed.
    """
    if not isinstance(preview, dict) or not isinstance(chain_nodes, list) or not chain_nodes:
        return []

    chain_ids: list[str] = [str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict) and str(n.get('id') or '').strip()]
    if not chain_ids:
        return []

    vuln_ids: set[str] = set()
    try:
        hosts = preview.get('hosts') if isinstance(preview, dict) else None
        if isinstance(hosts, list):
            for h in hosts:
                if not isinstance(h, dict):
                    continue
                hid = str(h.get('node_id') or '').strip()
                if not hid:
                    continue
                vulns = h.get('vulnerabilities') if isinstance(h.get('vulnerabilities'), list) else []
                if vulns:
                    vuln_ids.add(hid)
    except Exception:
        vuln_ids = set()

    # Use generator-catalog entries:
    # - flag-generators: artifact/flag generation
    # - flag-node-generators: node/docker-compose generation (Docker-role only)
    try:
        generators, _errors = _flag_generators_from_enabled_sources()
    except Exception:
        generators = []

    try:
        node_generators, _errors2 = _flag_node_generators_from_enabled_sources()
    except Exception:
        node_generators = []

    eligible_gens: list[dict[str, Any]] = []
    for g in (generators or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if not gid:
            continue
        g2 = dict(g)
        g2['_flow_kind'] = 'flag-generator'
        g2['_flow_catalog'] = 'flag_generators'
        eligible_gens.append(g2)

    for g in (node_generators or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if not gid:
            continue
        g2 = dict(g)
        g2['_flow_kind'] = 'flag-node-generator'
        g2['_flow_catalog'] = 'flag_node_generators'
        eligible_gens.append(g2)

    if not eligible_gens:
        return []

    # Deterministic randomness for generator selection.
    try:
        import random as _random
        try:
            seed_val = int((preview.get('seed') if isinstance(preview, dict) else None) or 0)
        except Exception:
            seed_val = 0
        rnd = _random.Random(seed_val ^ 0xC0FFEE)
    except Exception:
        rnd = None

    # Map ids -> names for THIS/NEXT substitution.
    id_to_name: dict[str, str] = {}
    for n in chain_nodes:
        try:
            nid = str(n.get('id') or '').strip()
            nm = str(n.get('name') or '').strip()
            if nid:
                id_to_name[nid] = nm or nid
        except Exception:
            pass

    id_to_node: dict[str, dict[str, Any]] = {}
    for n in chain_nodes:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if nid:
            id_to_node[nid] = n

    # Prefer v3 plugin contracts for chaining semantics (requires/produces).
    try:
        plugins_by_id = _flow_enabled_plugin_contracts_by_id()
    except Exception:
        plugins_by_id = {}

    def _render_hint(tpl: str, *, this_id: str, next_id: str) -> str:
        try:
            text = str(tpl or '').strip() or 'Next: {{NEXT_NODE_NAME}}'
            next_id_val = str(next_id or '').strip()
            next_name_val = (id_to_name.get(next_id_val) or '').strip() if next_id_val else ''
            if not next_id_val:
                return "You've completed this sequence of challenges!"
            if not next_name_val:
                next_name_val = next_id_val
            repl = {
                '{{SCENARIO}}': str(scenario_label or ''),
                '{{THIS_NODE_ID}}': this_id,
                '{{THIS_NODE_NAME}}': id_to_name.get(this_id) or this_id,
                '{{NEXT_NODE_ID}}': next_id_val,
                '{{NEXT_NODE_NAME}}': next_name_val,
            }
            for k, v in repl.items():
                text = text.replace(k, str(v))
            return _flow_strip_ids_from_hint(text)
        except Exception:
            return ''

    out: list[dict[str, Any]] = []

    def _artifact_requires_of(gen: dict[str, Any]) -> set[str]:
        required: set[str] = set()
        try:
            pid = str(gen.get('id') or '').strip()
            plugin = plugins_by_id.get(pid)
            if isinstance(plugin, dict) and isinstance(plugin.get('requires'), list):
                for x in (plugin.get('requires') or []):
                    xx = str(x).strip()
                    if xx:
                        required.add(xx)
        except Exception:
            pass
        # Synthesized inputs (e.g., seed/node_name) are *fields*, not chain artifacts.
        # Filter them out even if a plugin contract mistakenly lists them in `requires`.
        try:
            required = {x for x in required if x not in _flow_synthesized_inputs()}
        except Exception:
            pass
        return required

    def _required_input_fields_of(gen: dict[str, Any]) -> set[str]:
        """Return required input field names for a generator.

        Optional inputs (required=False) are intentionally excluded.
        """
        required: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    name = str(inp.get('name') or '').strip()
                    if not name:
                        continue
                    if inp.get('required') is False:
                        continue
                    required.add(name)
        except Exception:
            pass
        return required

    def _all_input_fields_of(gen: dict[str, Any]) -> set[str]:
        fields: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    name = str(inp.get('name') or '').strip()
                    if name:
                        fields.add(name)
        except Exception:
            pass
        return fields

    def _required_inputs_of(gen: dict[str, Any]) -> set[str]:
        # Effective union: artifact dependencies + required runtime input fields.
        # If a plugin-level "requires" token is also present as an *optional* input
        # field (required=False), treat it as optional and exclude it.
        try:
            req_fields = _required_input_fields_of(gen)
            opt_fields = _all_input_fields_of(gen) - set(req_fields)
            req_artifacts = _artifact_requires_of(gen)
            if opt_fields:
                req_artifacts = {r for r in req_artifacts if r not in opt_fields}
            return req_artifacts | set(req_fields)
        except Exception:
            return set()

    def _artifact_produces_of(gen: dict[str, Any]) -> set[str]:
        provides: set[str] = set()
        try:
            pid = str(gen.get('id') or '').strip()
            plugin = plugins_by_id.get(pid)
            if isinstance(plugin, dict) and isinstance(plugin.get('produces'), list):
                for item in (plugin.get('produces') or []):
                    if not isinstance(item, dict):
                        continue
                    a = str(item.get('artifact') or '').strip()
                    if a:
                        provides.add(a)
        except Exception:
            pass
        return provides

    def _output_fields_of(gen: dict[str, Any]) -> set[str]:
        out_fields: set[str] = set()
        try:
            outputs = gen.get('outputs')
            if isinstance(outputs, list):
                for outp in outputs:
                    if not isinstance(outp, dict):
                        continue
                    nm = str(outp.get('name') or '').strip()
                    if nm:
                        out_fields.add(nm)
        except Exception:
            pass
        return out_fields

    def _provides_of(gen: dict[str, Any]) -> set[str]:
        provides: set[str] = set()

        # Plugin-level produces (artifact dependencies).
        try:
            provides |= _artifact_produces_of(gen)
        except Exception:
            pass

        try:
            prov = gen.get('provides')
            if isinstance(prov, list):
                for x in prov:
                    s = str(x).strip()
                    if s:
                        provides.add(s)
        except Exception:
            pass
        try:
            provides |= _output_fields_of(gen)
        except Exception:
            pass
        return provides

    # Inputs we can always synthesize deterministically at preview time.
    # Keep this in sync with _flow_default_generator_config() and common generator conventions.
    available: set[str] = {
        'seed',
        'secret',
        'env_name',
        'challenge',
        'flag_prefix',
        'username_prefix',
        'key_len',
        'node_name',
    }
    prev_outputs: set[str] = set()

    for i, cid in enumerate(chain_ids):
        node = id_to_node.get(str(cid)) or {}
        is_vuln_node = bool(node.get('is_vuln')) or (str(cid) in vuln_ids)
        is_docker_node = _flow_node_is_docker_role(node)
        # Enforce placement policy per node:
        # - flag-generator: allowed on docker-role OR vuln nodes
        # - flag-node-generator: allowed only on non-vuln docker-role nodes
        def _eligible_for_node(g: dict[str, Any]) -> bool:
            k = str(g.get('_flow_kind') or '').strip() or 'flag-generator'
            if k == 'flag-node-generator':
                return bool(is_docker_node and (not is_vuln_node))
            return bool(is_docker_node or is_vuln_node)

        pool = [g for g in eligible_gens if _eligible_for_node(g)]
        if not pool:
            pool = eligible_gens
        # Enforce chainability:
        # - prefer generators whose required inputs are satisfied by the previous step's outputs
        #   (plus globally-available synthesized inputs)
        if i == 0:
            feasible = [g for g in pool if _required_inputs_of(g).issubset(available)]
        else:
            satisfied_now = prev_outputs | available
            feasible = [g for g in pool if _required_inputs_of(g) and _required_inputs_of(g).issubset(satisfied_now)]
            if not feasible:
                feasible = [g for g in pool if _required_inputs_of(g).issubset(satisfied_now)]

        if feasible:
            if rnd is not None:
                try:
                    gen = rnd.choice(feasible)
                except Exception:
                    gen = feasible[0]
            else:
                gen = feasible[0]
        else:
            # If the catalog doesn't contain a seed (or no compatible step exists),
            # fall back to a stable rotation.
            gen = pool[i % len(pool)]

        # Update outputs for the next hop.
        try:
            prev_outputs = _provides_of(gen)
            available |= prev_outputs
        except Exception:
            pass

        next_id = chain_ids[i + 1] if (i + 1) < len(chain_ids) else ''
        hint_templates = _flow_hint_templates_from_generator(gen)
        hint_tpl = hint_templates[0] if hint_templates else 'Next: {{NEXT_NODE_NAME}}'
        rendered_hints = [
            _render_hint(t, this_id=str(cid), next_id=str(next_id))
            for t in (hint_templates or [])
        ]

        requires_artifacts = sorted(list(_artifact_requires_of(gen)))
        produces_artifacts = sorted(list(_artifact_produces_of(gen)))
        input_fields_required = sorted(list(_required_input_fields_of(gen)))
        input_fields_all = sorted(list(_all_input_fields_of(gen)))
        input_fields_optional = sorted([x for x in input_fields_all if x and x not in set(input_fields_required)])
        output_fields = sorted(list(_output_fields_of(gen)))

        # If an artifact "requires" token also appears as an optional input field,
        # treat it as optional (do not block chaining/validation on it).
        try:
            optional_field_set = set(input_fields_optional)
            requires_artifacts = sorted([x for x in requires_artifacts if x and x not in optional_field_set])
        except Exception:
            pass

        out.append({
            'node_id': str(cid),
            'id': str(gen.get('id') or ''),
            'name': str(gen.get('name') or ''),
            'description': str(gen.get('description') or ''),
            'type': str(gen.get('_flow_kind') or 'flag-generator'),
            'flag_generator': str(gen.get('_source_name') or '').strip() or 'unknown',
            'generator_catalog': str(gen.get('_flow_catalog') or 'flag_generators'),
            'language': str(gen.get('language') or ''),
            'description_hints': list(gen.get('description_hints') or []) if isinstance(gen.get('description_hints'), list) else [],
            # Effective union (used for chaining feasibility / ordering validation).
            'inputs': sorted(list(_required_inputs_of(gen))),
            'outputs': sorted(list(_provides_of(gen))),

            # Split-out views for UI transparency.
            'requires': requires_artifacts,
            'produces': produces_artifacts,
            'input_fields': input_fields_all,
            'input_fields_required': input_fields_required,
            'input_fields_optional': input_fields_optional,
            'output_fields': output_fields,
            'hint_template': hint_tpl,
            'hint_templates': hint_templates,
            'hint': rendered_hints[0] if rendered_hints else _render_hint(hint_tpl, this_id=str(cid), next_id=str(next_id)),
            'hints': rendered_hints,
            'next_node_id': str(next_id),
            'next_node_name': str(id_to_name.get(str(next_id)) or ''),
        })
    return out


def _flow_synthesized_inputs() -> set[str]:
    """Inputs we can always synthesize deterministically at preview time.

    Keep this in sync with _flow_compute_flag_assignments() and _flow_default_generator_config().
    """
    return {
        'seed',
        'secret',
        'env_name',
        'challenge',
        'flag_prefix',
        'username_prefix',
        'key_len',
        'node_name',
    }


def _flow_strip_runtime_sensitive_fields(flag_assignments: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """Return a copy of assignments safe to persist in preview-plan metadata.

    NOTE: As of the "flags are required outputs" contract, realized flag strings are part
    of the sequencing contract and are allowed to persist so the UI can display them.
    """
    if not isinstance(flag_assignments, list):
        return flag_assignments
    out: list[dict[str, Any]] = []
    for a in (flag_assignments or []):
        if not isinstance(a, dict):
            continue
        a2 = dict(a)
        a2.pop('runtime_flags', None)
        a2.pop('runtime_outputs', None)
        out.append(a2)
    return out


def _flow_enrich_saved_flag_assignments(
    flag_assignments: list[dict[str, Any]],
    chain_nodes: list[dict[str, Any]],
    *,
    scenario_label: str,
) -> list[dict[str, Any]]:
    """Best-effort enrichment for persisted Flow assignments.

    Older preview plans may persist `flag_assignments` that predate newer UI fields
    (e.g., `description_hints`) or contain hints that still include ids.

    We keep the chosen generator per node, but refresh:
    - `description_hints` from current enabled catalogs
    - `hint_templates`/`hint_template` from current enabled catalogs
    - rendered `hints`/`hint` for the current chain order
    - `next_node_id`/`next_node_name`
    """
    if not isinstance(flag_assignments, list) or not isinstance(chain_nodes, list):
        return flag_assignments

    # Map ids -> names for THIS/NEXT substitution.
    id_to_name: dict[str, str] = {}
    chain_ids: list[str] = []
    for n in (chain_nodes or []):
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if not nid:
            continue
        chain_ids.append(nid)
        nm = str(n.get('name') or '').strip()
        id_to_name[nid] = nm or nid

    # Build a by-id view of currently enabled generators.
    by_id: dict[str, dict[str, Any]] = {}
    try:
        gens, _ = _flag_generators_from_enabled_sources()
        for g in (gens or []):
            if not isinstance(g, dict):
                continue
            gid = str(g.get('id') or '').strip()
            if gid and gid not in by_id:
                by_id[gid] = g
    except Exception:
        pass
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
        for g in (node_gens or []):
            if not isinstance(g, dict):
                continue
            gid = str(g.get('id') or '').strip()
            if gid and gid not in by_id:
                by_id[gid] = g
    except Exception:
        pass

    out: list[dict[str, Any]] = []
    for i, a in enumerate(flag_assignments or []):
        if not isinstance(a, dict):
            continue
        a2 = dict(a)
        this_id = str(a2.get('node_id') or '').strip() or (chain_ids[i] if i < len(chain_ids) else '')
        next_id = chain_ids[i + 1] if (i + 1) < len(chain_ids) else ''
        a2['node_id'] = this_id
        a2['next_node_id'] = str(next_id)
        a2['next_node_name'] = str(id_to_name.get(str(next_id)) or '')

        gen_id = str(a2.get('id') or '').strip()
        gen_def = by_id.get(gen_id) if gen_id else None

        # Ensure generator description exists (if the catalog provides it).
        try:
            existing_desc = str(a2.get('description') or '').strip()
        except Exception:
            existing_desc = ''
        if (not existing_desc) and isinstance(gen_def, dict):
            try:
                a2['description'] = str(gen_def.get('description') or '')
            except Exception:
                pass

        # Ensure description hints exist (if the catalog provides them).
        try:
            existing = a2.get('description_hints')
            if not (isinstance(existing, list) and any(str(x or '').strip() for x in existing)):
                dh = (gen_def or {}).get('description_hints') if isinstance(gen_def, dict) else None
                if isinstance(dh, list):
                    a2['description_hints'] = [str(x or '').strip() for x in dh if str(x or '').strip()]
        except Exception:
            pass

        # Preserve persisted hint text when present (Flow persistence contract), but
        # strip ids if they exist. Only regenerate hints from catalogs when missing.
        has_hints = False
        try:
            if isinstance(a2.get('hints'), list) and any(str(x or '').strip() for x in (a2.get('hints') or [])):
                has_hints = True
            elif str(a2.get('hint') or '').strip():
                has_hints = True
        except Exception:
            has_hints = False

        if has_hints:
            try:
                if isinstance(a2.get('hints'), list) and a2.get('hints'):
                    raw = [str(x or '') for x in (a2.get('hints') or [])]
                    # Only mutate if the text actually contains ids/placeholders.
                    if any(('(id=' in s.lower()) or ('{{NEXT_NODE_ID}}' in s) or ('{{THIS_NODE_ID}}' in s) for s in raw):
                        a2['hints'] = [_flow_strip_ids_from_hint(s) for s in raw]
                        a2['hints'] = [x for x in (a2.get('hints') or []) if str(x).strip()]
                        if a2['hints']:
                            a2['hint'] = a2['hints'][0]
                elif a2.get('hint'):
                    s = str(a2.get('hint') or '')
                    if ('(id=' in s.lower()) or ('{{NEXT_NODE_ID}}' in s) or ('{{THIS_NODE_ID}}' in s):
                        a2['hint'] = _flow_strip_ids_from_hint(s)
            except Exception:
                pass
        elif isinstance(gen_def, dict):
            # No saved hints: fall back to catalog templates and render for current chain order.
            hint_templates: list[str] = []
            try:
                hint_templates = _flow_hint_templates_from_generator(gen_def)
            except Exception:
                hint_templates = []
            hint_tpl = hint_templates[0] if hint_templates else 'Next: {{NEXT_NODE_NAME}}'
            a2['hint_templates'] = hint_templates
            a2['hint_template'] = hint_tpl
            rendered = [
                _flow_render_hint_template(t, scenario_label=scenario_label, id_to_name=id_to_name, this_id=str(this_id), next_id=str(next_id))
                for t in (hint_templates or [hint_tpl])
            ]
            a2['hints'] = rendered
            a2['hint'] = rendered[0] if rendered else _flow_render_hint_template(hint_tpl, scenario_label=scenario_label, id_to_name=id_to_name, this_id=str(this_id), next_id=str(next_id))

        out.append(a2)
    return out


def _flow_parse_bool(value: Any, *, default: bool = False) -> bool:
    """Parse a truthy/falsey value commonly used in query args/JSON."""
    if value is None:
        return default
    try:
        if isinstance(value, bool):
            return bool(value)
        s = str(value).strip().lower()
    except Exception:
        return default
    if s in ('1', 'true', 'yes', 'y', 'on'):
        return True
    if s in ('0', 'false', 'no', 'n', 'off'):
        return False
    return default


def _flow_validate_chain_order_by_requires_produces(
    chain_nodes: list[dict[str, Any]],
    flag_assignments: list[dict[str, Any]],
    *,
    scenario_label: str,
    plugins_by_id_override: dict[str, dict[str, Any]] | None = None,
) -> tuple[bool, list[str]]:
    """Validate that the given chain order is solvable by requires/produces.

    This is a strict, linear check in the *current* order:
    - Each step's effective requires must be satisfied by synthesized inputs or prior produces.
    - Effective requires/produces are derived from v3 plugin contracts, plus assignment-declared
      inputs/outputs (best-effort) to support coarse catalogs.

    Returns: (ok, errors)
    """
    errors: list[str] = []
    if not isinstance(chain_nodes, list) or not chain_nodes:
        return False, ['missing chain nodes']
    if not isinstance(flag_assignments, list) or not flag_assignments:
        return False, ['missing flag assignments']

    chain_ids: list[str] = [
        str(n.get('id') or '').strip()
        for n in chain_nodes
        if isinstance(n, dict) and str(n.get('id') or '').strip()
    ]
    if not chain_ids:
        return False, ['empty chain']

    assign_by_node: dict[str, dict[str, Any]] = {}
    for a in flag_assignments:
        if not isinstance(a, dict):
            continue
        nid = str(a.get('node_id') or '').strip()
        if nid:
            assign_by_node[nid] = a

    if any(cid not in assign_by_node for cid in chain_ids):
        return False, ['missing assignment for at least one chain node']

    if isinstance(plugins_by_id_override, dict) and plugins_by_id_override:
        plugins_by_id = plugins_by_id_override
    else:
        plugins_by_id = _flow_enabled_plugin_contracts_by_id()

    # Best-effort: load enabled generator definitions so we can infer optional
    # input fields even if the assignment payload doesn't include them.
    gen_defs_by_id: dict[str, dict[str, Any]] = {}
    try:
        gens, _ = _flag_generators_from_enabled_sources()
        for g in (gens or []):
            if not isinstance(g, dict):
                continue
            gid = str(g.get('id') or '').strip()
            if gid and gid not in gen_defs_by_id:
                gen_defs_by_id[gid] = g
    except Exception:
        pass
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
        for g in (node_gens or []):
            if not isinstance(g, dict):
                continue
            gid = str(g.get('id') or '').strip()
            if gid and gid not in gen_defs_by_id:
                gen_defs_by_id[gid] = g
    except Exception:
        pass

    available: set[str] = set(_flow_synthesized_inputs())

    for cid in chain_ids:
        a = assign_by_node.get(cid) or {}
        plugin_id = str(a.get('id') or '').strip()
        if not plugin_id:
            errors.append(f"{cid}: missing generator id")
            continue

        plugin = plugins_by_id.get(plugin_id)
        if not isinstance(plugin, dict):
            errors.append(f"{cid}: unknown plugin '{plugin_id}'")
            continue

        inferred_requires = {str(x).strip() for x in (a.get('inputs') or []) if str(x).strip()}
        inferred_produces = {str(x).strip() for x in (a.get('outputs') or []) if str(x).strip()}

        # Optional dependency tokens (do not block ordering). Prefer the assignment
        # field when present; fall back to the generator catalog schema.
        opt_set: set[str] = set()
        try:
            opt_fields = a.get('input_fields_optional') or []
            if isinstance(opt_fields, list):
                opt_set |= {str(x).strip() for x in opt_fields if str(x).strip()}
        except Exception:
            pass
        if not opt_set:
            try:
                gen_def = gen_defs_by_id.get(plugin_id)
                inputs = (gen_def or {}).get('inputs') if isinstance(gen_def, dict) else None
                if isinstance(inputs, list):
                    for inp in inputs:
                        if not isinstance(inp, dict):
                            continue
                        if inp.get('required') is False:
                            nm = str(inp.get('name') or '').strip()
                            if nm:
                                opt_set.add(nm)
            except Exception:
                pass

        base_requires = {str(x).strip() for x in (plugin.get('requires') or []) if str(x).strip()} if isinstance(plugin.get('requires'), list) else set()
        if opt_set:
            base_requires = {r for r in base_requires if r not in opt_set}
            inferred_requires = {r for r in inferred_requires if r not in opt_set}

        requires = base_requires | inferred_requires

        base_prod: set[str] = set()
        try:
            for p in (plugin.get('produces') or []):
                if not isinstance(p, dict):
                    continue
                art = str(p.get('artifact') or '').strip()
                if art:
                    base_prod.add(art)
        except Exception:
            base_prod = set()
        produces = base_prod | inferred_produces

        missing = sorted(list({r for r in requires if r not in available}))
        if missing:
            errors.append(f"{cid}: requires {missing} before they are produced")

        available |= produces

    if errors:
        # Include minimal context; callers may surface this in API error details.
        return False, errors
    return True, []


def _flow_enabled_plugin_contracts_by_id() -> dict[str, dict[str, Any]]:
    """Return enabled v3 plugin contracts indexed by plugin_id.

    This loads only the plugin contracts (not merged generator views) for both
    flag-generators and flag-node-generators.
    """
    plugins_by_id: dict[str, dict[str, Any]] = {}

    def _ingest_state(state: dict) -> None:
        try:
            sources = state.get('sources') or []
        except Exception:
            sources = []
        for s in sources:
            if not isinstance(s, dict):
                continue
            if not s.get('enabled', False):
                continue
            path = s.get('path')
            ok, _note, doc, _skipped = _validate_and_normalize_flag_generator_source_json(path)
            if not ok or not isinstance(doc, dict):
                continue
            for p in (doc.get('plugins') or []):
                if not isinstance(p, dict):
                    continue
                pid = str(p.get('plugin_id') or '').strip()
                if pid and pid not in plugins_by_id:
                    plugins_by_id[pid] = p

    try:
        _ingest_state(_load_flag_generator_sources_state())
    except Exception:
        pass
    try:
        _ingest_state(_load_flag_node_generator_sources_state())
    except Exception:
        pass

    return plugins_by_id


def _flow_reorder_chain_by_generator_dag(
    chain_nodes: list[dict[str, Any]],
    flag_assignments: list[dict[str, Any]],
    *,
    scenario_label: str,
    plugins_by_id_override: dict[str, dict[str, Any]] | None = None,
    return_debug: bool = False,
) -> tuple[list[dict[str, Any]], list[dict[str, Any]], dict[str, Any] | None]:
    """Best-effort: reorder the chain using generator artifact dependencies.

    This does not change generator selection; it only reorders the existing
    chain nodes + assignments so that inputs are satisfied by some producer.

    If the DAG cannot be built, returns inputs unchanged.
    """
    try:
        if not isinstance(chain_nodes, list) or not chain_nodes:
            return chain_nodes, flag_assignments, None
        if not isinstance(flag_assignments, list) or not flag_assignments:
            return chain_nodes, flag_assignments, None

        from core_topo_gen.sequencer.dag import build_dag
        from core_topo_gen.sequencer.chain import validate_chain_doc, validate_linear_chain

        node_by_id: dict[str, dict[str, Any]] = {}
        for n in chain_nodes:
            if not isinstance(n, dict):
                continue
            nid = str(n.get('id') or '').strip()
            if nid:
                node_by_id[nid] = n

        assign_by_node: dict[str, dict[str, Any]] = {}
        for a in flag_assignments:
            if not isinstance(a, dict):
                continue
            nid = str(a.get('node_id') or '').strip()
            if nid:
                assign_by_node[nid] = a

        # Only reorder when we have a 1:1 mapping.
        chain_ids = [str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict) and str(n.get('id') or '').strip()]
        if not chain_ids:
            return chain_nodes, flag_assignments, None
        if any(cid not in assign_by_node for cid in chain_ids):
            return chain_nodes, flag_assignments, None

        plugins_by_id: dict[str, dict[str, Any]] = {}
        if isinstance(plugins_by_id_override, dict) and plugins_by_id_override:
            plugins_by_id = plugins_by_id_override
        else:
            plugins_by_id = _flow_enabled_plugin_contracts_by_id()

        # Build a real sequencer chain instance (YAML-shape) from the current Flow chain.
        #
        # Important: some catalogs may omit requires/produces details or keep them coarse.
        # For Flow ordering we also incorporate the assignment-derived inputs/outputs as
        # additional requires/produces to avoid a no-op DAG.
        chain_doc: dict[str, Any] = {
            'ctf': {
                'id': str(scenario_label or '').strip() or 'flow',
                'version': 'flow',
                'difficulty': 'unknown',
                'description': 'Generated from Flow Sequencing assignments',
            },
            'challenges': [],
        }

        effective_plugins_by_id: dict[str, dict[str, Any]] = dict(plugins_by_id or {})
        challenges: list[dict[str, Any]] = []
        for cid in chain_ids:
            a = assign_by_node.get(cid) or {}
            plugin_id = str(a.get('id') or '').strip()
            if not plugin_id:
                return chain_nodes, flag_assignments, None
            plugin = plugins_by_id.get(plugin_id)
            if not isinstance(plugin, dict):
                # Without the plugin contract we can't build a correct artifact DAG.
                return chain_nodes, flag_assignments, None

            inferred_requires = [str(x).strip() for x in (a.get('inputs') or []) if str(x).strip()]
            inferred_produces = [str(x).strip() for x in (a.get('outputs') or []) if str(x).strip()]

            # Build an "effective" plugin contract for DAG purposes.
            try:
                eff = dict(plugin)
            except Exception:
                eff = plugin

            try:
                base_req = [str(x).strip() for x in (eff.get('requires') or []) if str(x).strip()] if isinstance(eff, dict) else []
            except Exception:
                base_req = []
            eff_requires = sorted(list({*base_req, *inferred_requires}))
            if isinstance(eff, dict):
                eff['requires'] = eff_requires

            base_prod_set: set[str] = set()
            try:
                for p in (eff.get('produces') or []) if isinstance(eff, dict) else []:
                    if not isinstance(p, dict):
                        continue
                    art = str(p.get('artifact') or '').strip()
                    if art:
                        base_prod_set.add(art)
            except Exception:
                base_prod_set = set()
            eff_prod_set = {*(base_prod_set), *(set(inferred_produces))}
            if isinstance(eff, dict):
                eff['produces'] = [{'artifact': x} for x in sorted(list(eff_prod_set))]

            effective_plugins_by_id[plugin_id] = eff

            kind = str(plugin.get('plugin_type') or a.get('type') or 'flag-generator').strip() or 'flag-generator'

            # Populate YAML-chain fields for validation/debug. Note: instance.requires is
            # intentionally empty; plugin.requires drives dependencies.
            produces_list: list[dict[str, str]] = []
            try:
                for p in (eff.get('produces') or []):
                    if not isinstance(p, dict):
                        continue
                    art = str(p.get('artifact') or '').strip()
                    if art:
                        produces_list.append({'name': art, 'artifact': art})
            except Exception:
                produces_list = []

            requires_list: list[dict[str, str]] = []
            try:
                for art in (eff.get('requires') or []):
                    a2 = str(art or '').strip()
                    if a2:
                        requires_list.append({'artifact': a2})
            except Exception:
                requires_list = []

            ch = {
                'challenge_id': cid,
                'kind': kind,
                'generator': {'plugin': plugin_id},
                'requires': requires_list,
                'produces': produces_list,
            }
            chain_doc['challenges'].append(ch)

            # Also build the challenge instances for build_dag (it reads generator.plugin).
            challenges.append(ch)

        chain_ok, chain_errors, chain_norm = validate_chain_doc(chain_doc)

        dag, errors = build_dag(challenges, plugins_by_id=effective_plugins_by_id, initial_artifacts=sorted(list(_flow_synthesized_inputs())))
        debug: dict[str, Any] | None = None
        if return_debug:
            try:
                debug = {
                    'ok': bool(dag is not None),
                    'errors': list(errors or []),
                    'initial_artifacts': sorted(list(_flow_synthesized_inputs())),
                    'chain_ok': bool(chain_ok),
                    'chain_errors': list(chain_errors or []),
                }
                if dag is not None:
                    debug['order'] = list(dag.order)
                    debug['edges'] = [
                        {'src': e.src, 'dst': e.dst, 'artifact': e.artifact}
                        for e in (dag.edges or ())
                    ]
            except Exception:
                debug = None
        if dag is None:
            return chain_nodes, flag_assignments, debug

        order = [str(x) for x in (dag.order or ()) if str(x).strip()]
        if not order:
            return chain_nodes, flag_assignments, debug
        if set(order) != set(chain_ids):
            return chain_nodes, flag_assignments, debug

        # Reorder nodes + assignments.
        new_chain_nodes = [node_by_id[cid] for cid in order if cid in node_by_id]
        new_assignments = [assign_by_node[cid] for cid in order if cid in assign_by_node]
        if len(new_chain_nodes) != len(chain_nodes) or len(new_assignments) != len(flag_assignments):
            return chain_nodes, flag_assignments, debug

        # Optional: validate that the new order is linearly solvable by the chain spec.
        # This uses only instance produces/requires; since we keep instance.requires empty,
        # this is expected to succeed (and is primarily a sanity check).
        if debug is not None:
            try:
                reordered_doc = dict(chain_norm or {})
                reordered_doc['challenges'] = [
                    next((c for c in (chain_norm.get('challenges') or []) if isinstance(c, dict) and str(c.get('challenge_id') or '') == cid), None)
                    for cid in order
                ]
                reordered_doc['challenges'] = [c for c in reordered_doc['challenges'] if isinstance(c, dict)]
                lin_ok, lin_errors = validate_linear_chain(reordered_doc, initial_artifacts=sorted(list(_flow_synthesized_inputs())))
                debug['linear_ok'] = bool(lin_ok)
                debug['linear_errors'] = list(lin_errors or [])
            except Exception:
                pass

        # Update NEXT placeholders in hints for the new order.
        id_to_name: dict[str, str] = {}
        for n in new_chain_nodes:
            try:
                nid = str(n.get('id') or '').strip()
                nm = str(n.get('name') or '').strip()
                if nid:
                    id_to_name[nid] = nm or nid
            except Exception:
                continue

        def _render_hint(tpl: str, *, this_id: str, next_id: str) -> str:
            try:
                text = str(tpl or '').strip() or 'Next: {{NEXT_NODE_NAME}}'
                next_id_val = str(next_id or '').strip()
                next_name_val = (id_to_name.get(next_id_val) or '').strip() if next_id_val else ''
                if not next_id_val:
                    return "You've completed this sequence of challenges!"
                if not next_name_val:
                    next_name_val = next_id_val
                repl = {
                    '{{SCENARIO}}': str(scenario_label or ''),
                    '{{THIS_NODE_ID}}': this_id,
                    '{{THIS_NODE_NAME}}': id_to_name.get(this_id) or this_id,
                    '{{NEXT_NODE_ID}}': next_id_val,
                    '{{NEXT_NODE_NAME}}': next_name_val,
                }
                for k, v in repl.items():
                    text = text.replace(k, str(v))
                return _flow_strip_ids_from_hint(text)
            except Exception:
                return ''

        for i, a in enumerate(new_assignments):
            try:
                this_id = str(a.get('node_id') or '').strip()
                next_id = order[i + 1] if (i + 1) < len(order) else ''
                hint_templates: list[str] = []
                try:
                    ht = a.get('hint_templates')
                    if isinstance(ht, list) and ht:
                        hint_templates = [str(x or '').strip() for x in ht if str(x or '').strip()]
                except Exception:
                    hint_templates = []
                tpl = str(a.get('hint_template') or '').strip() or (hint_templates[0] if hint_templates else 'Next: {{NEXT_NODE_NAME}}')
                a['next_node_id'] = str(next_id)
                a['next_node_name'] = str(id_to_name.get(str(next_id)) or '')
                rendered = [_render_hint(t, this_id=this_id, next_id=str(next_id)) for t in (hint_templates or [tpl])]
                a['hint'] = rendered[0] if rendered else _render_hint(tpl, this_id=this_id, next_id=str(next_id))
                a['hints'] = rendered
            except Exception:
                continue

        return new_chain_nodes, new_assignments, debug
    except Exception:
        return chain_nodes, flag_assignments, None


@app.route('/api/flag-sequencing/attackflow_preview')
def api_flow_attackflow_preview():
    scenario_label = (request.args.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    preset = str(request.args.get('preset') or '').strip()
    length_raw = request.args.get('length')
    try:
        length = int(length_raw) if length_raw is not None else 5
    except Exception:
        length = 5
    preset_steps = _flow_preset_steps(preset)
    if preset_steps:
        length = len(preset_steps)
    length = max(1, min(length, 50))
    requested_length = length

    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    prefer_preview = str(request.args.get('prefer_preview') or '').strip().lower() in ('1', 'true', 'yes', 'y')
    force_preview = str(request.args.get('force_preview') or '').strip().lower() in ('1', 'true', 'yes', 'y')
    best_effort_query = str(request.args.get('best_effort') or '').strip().lower() in ('1', 'true', 'yes', 'y')
    debug_mode = str(request.args.get('debug') or '').strip().lower() in ('1', 'true', 'yes', 'y')

    # When Generate forces preview selection, we still want to use the same *topology*
    # that refresh would choose (to avoid eligible counts flipping), but we should not
    # reuse a previously saved chain/assignments.
    ignore_saved_flow = bool(force_preview)

    selected_by = 'explicit'

    preview_plan_path = (request.args.get('preview_plan') or '').strip() or None
    if preview_plan_path:
        try:
            preview_plan_path = os.path.abspath(preview_plan_path)
            plans_dir = os.path.abspath(os.path.join(_outputs_dir(), 'plans'))
            if os.path.commonpath([preview_plan_path, plans_dir]) != plans_dir:
                preview_plan_path = None
            elif not os.path.exists(preview_plan_path):
                preview_plan_path = None
        except Exception:
            preview_plan_path = None
    if not preview_plan_path:
        if force_preview:
            # Generate button: use the same topology source as refresh (preview plan)
            # so stats/topology do not flip, but ignore any saved chain/assignments.
            selected_by = 'force_preview_best_plan'
            preview_plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm, prefer_flow=False)
        elif prefer_preview:
            selected_by = 'prefer_preview_preview_plan'
            preview_plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm, prefer_flow=False)
        else:
            selected_by = 'default_best_plan'
            preview_plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm, prefer_flow=False)

    if not preview_plan_path:
        return jsonify({'ok': False, 'error': 'No preview plan found for this scenario. Generate a Full Preview first.'}), 404

    try:
        with open(preview_plan_path, 'r', encoding='utf-8') as f:
            payload = json.load(f) or {}
        preview = payload.get('full_preview') if isinstance(payload, dict) else None
        if not isinstance(preview, dict):
            return jsonify({'ok': False, 'error': 'Preview plan is missing full_preview.'}), 422
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to load preview plan: {e}'}), 500

    def _docker_count_from_preview(full_preview: dict) -> int:
        try:
            hosts = full_preview.get('hosts') or []
        except Exception:
            hosts = []
        if not isinstance(hosts, list):
            return 0
        total = 0
        for host in hosts:
            if not isinstance(host, dict):
                continue
            role = str(host.get('role') or '').strip().lower()
            if role == 'docker':
                total += 1
        return total

    def _docker_count_from_editor_snapshot(snapshot: dict, scen_norm: str) -> int:
        try:
            scenarios = snapshot.get('scenarios') or []
        except Exception:
            scenarios = []
        if not isinstance(scenarios, list):
            return 0
        match = None
        for scen in scenarios:
            if not isinstance(scen, dict):
                continue
            nm = _normalize_scenario_label(scen.get('name') or '')
            if nm and nm == scen_norm:
                match = scen
                break
        if not isinstance(match, dict):
            return 0
        sec = (match.get('sections') or {}).get('Node Information')
        if not isinstance(sec, dict):
            return 0
        items = sec.get('items') or []
        if not isinstance(items, list):
            return 0
        total = 0
        for item in items:
            if not isinstance(item, dict):
                continue
            metric = str(item.get('v_metric') or 'Weight').strip()
            if metric != 'Count':
                continue
            sel = str(item.get('selected') or '').strip().lower()
            if sel != 'docker':
                continue
            try:
                total += max(0, int(item.get('v_count') or 0))
            except Exception:
                continue
        return total

    def _plan_epoch_seconds(plan_path: str, plan_payload: dict) -> float:
        try:
            meta = plan_payload.get('metadata') if isinstance(plan_payload, dict) else None
            if isinstance(meta, dict):
                ts = _parse_iso_ts(meta.get('created_at'))
                if ts > 0:
                    return ts
        except Exception:
            pass
        try:
            return float(os.path.getmtime(plan_path))
        except Exception:
            return 0.0

    def _editor_snapshot_epoch_seconds(owner: Optional[dict]) -> float:
        try:
            snap_path = _editor_state_snapshot_path(owner)
            if os.path.exists(snap_path):
                return float(os.path.getmtime(snap_path))
        except Exception:
            pass
        return 0.0

    def _regenerate_preview_plan_from_snapshot(snapshot: dict, *, scenario_name: str, seed: Optional[int] = None) -> Optional[tuple[dict, str]]:
        """Best-effort: render XML from snapshot, compute plan, persist new preview plan."""
        try:
            scenarios = snapshot.get('scenarios')
            if not isinstance(scenarios, list) or not scenarios:
                return None
            core_meta = snapshot.get('core') if isinstance(snapshot.get('core'), dict) else None
            tree = _build_scenarios_xml({'scenarios': scenarios, 'core': _normalize_core_config(core_meta, include_password=True) if core_meta else None})
            ts = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
            out_dir = os.path.join(_outputs_dir(), f'scenarios-{ts}')
            os.makedirs(out_dir, exist_ok=True)
            stem = secure_filename((scenario_name or 'scenarios')).strip('_-.') or 'scenarios'
            xml_path = os.path.join(out_dir, f"{stem}.xml")
            # Pretty print if lxml available else fallback
            try:
                from lxml import etree as LET  # type: ignore
                raw = ET.tostring(tree.getroot(), encoding='utf-8')
                lroot = LET.fromstring(raw)
                pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
                with open(xml_path, 'wb') as handle:
                    handle.write(pretty)
            except Exception:
                tree.write(xml_path, encoding='utf-8', xml_declaration=True)

            from core_topo_gen.planning.orchestrator import compute_full_plan
            from core_topo_gen.planning.plan_cache import hash_xml_file, try_get_cached_plan, save_plan_to_cache

            xml_hash = hash_xml_file(xml_path)
            plan = None
            try:
                plan = try_get_cached_plan(xml_hash, scenario_name, seed)
            except Exception:
                plan = None
            if not plan:
                plan = compute_full_plan(xml_path, scenario=scenario_name, seed=seed, include_breakdowns=True)
                try:
                    save_plan_to_cache(xml_hash, scenario_name, seed, plan)
                except Exception:
                    pass
            if seed is None:
                try:
                    seed = plan.get('seed') or _derive_default_seed(xml_hash)
                except Exception:
                    seed = None
            full_prev = _build_full_preview_from_plan(plan, seed, [], [])
            try:
                _attach_latest_flow_into_full_preview(full_prev, scenario_name)
            except Exception:
                pass
            plans_dir = os.path.join(_outputs_dir(), 'plans')
            os.makedirs(plans_dir, exist_ok=True)
            seed_tag = full_prev.get('seed') or (seed if seed is not None else 'preview')
            unique_tag = f"{seed_tag}_{int(time.time())}_{uuid.uuid4().hex[:6]}"
            new_plan_path = os.path.join(plans_dir, f"plan_from_preview_{unique_tag}.json")
            plan_payload = {
                'full_preview': full_prev,
                'metadata': {
                    'xml_path': xml_path,
                    'scenario': scenario_name,
                    'seed': full_prev.get('seed'),
                    'created_at': datetime.datetime.now(datetime.timezone.utc).isoformat().replace('+00:00', 'Z'),
                },
            }
            with open(new_plan_path, 'w', encoding='utf-8') as handle:
                json.dump(plan_payload, handle, indent=2, sort_keys=True)
            return plan_payload, new_plan_path
        except Exception:
            return None

    # If the editor snapshot requests Docker hosts but the selected plan has zero Docker hosts,
    # regenerate a preview plan from the snapshot so Flag Sequencer reflects current edits.
    try:
        owner = _current_user()
        snapshot = _load_editor_state_snapshot(owner)
        if not snapshot and owner is None:
            # Conservative fallback: if there is exactly one editor snapshot on disk,
            # use it even without an authenticated session. This avoids Generate
            # flipping to stale plans in single-user deployments.
            try:
                snap_dir = _editor_state_snapshot_dir()
                candidates = [
                    os.path.join(snap_dir, fn)
                    for fn in (os.listdir(snap_dir) if os.path.isdir(snap_dir) else [])
                    if fn.endswith('.json') and not fn.startswith('.')
                ]
                if len(candidates) == 1 and os.path.exists(candidates[0]):
                    with open(candidates[0], 'r', encoding='utf-8') as handle:
                        maybe = json.load(handle)
                    if isinstance(maybe, dict) and isinstance(maybe.get('scenarios'), list):
                        snapshot = maybe
            except Exception:
                snapshot = snapshot
        if snapshot:
            desired_docker = _docker_count_from_editor_snapshot(snapshot, scenario_norm)
            have_docker = _docker_count_from_preview(preview)
            if desired_docker > 0 and have_docker == 0:
                seed_hint = None
                try:
                    meta = payload.get('metadata') if isinstance(payload, dict) else None
                    if isinstance(meta, dict) and meta.get('seed') is not None:
                        seed_hint = int(meta.get('seed'))
                except Exception:
                    seed_hint = None
                regenerated = _regenerate_preview_plan_from_snapshot(
                    snapshot,
                    scenario_name=(scenario_label or scenario_norm),
                    seed=seed_hint,
                )
                if regenerated:
                    payload, preview_plan_path = regenerated
                    preview = payload.get('full_preview') if isinstance(payload, dict) else preview
    except Exception:
        pass

    # If we loaded a preview plan (topology) but the user has a saved Flow plan,
    # merge that Flow metadata into this payload so saved chain/assignments still apply.
    try:
        meta0 = payload.get('metadata') if isinstance(payload, dict) else None
        flow0 = (meta0 or {}).get('flow') if isinstance(meta0, dict) else None
        if (not ignore_saved_flow) and (not isinstance(flow0, dict)):
            _attach_latest_flow_into_plan_payload(payload, scenario=(scenario_label or scenario_norm))
    except Exception:
        pass

    nodes, _links, adj = _build_topology_graph_from_preview_plan(preview)
    stats = _flow_compose_docker_stats(nodes)

    # If the latest plan is flow-modified, prefer the saved chain order.
    chain_nodes: list[dict[str, Any]] = []
    used_saved_chain = False
    if (not ignore_saved_flow) and (not preset_steps):
        try:
            meta = payload.get('metadata') if isinstance(payload, dict) else None
            flow_meta = meta.get('flow') if isinstance(meta, dict) else None
            saved_chain = flow_meta.get('chain') if isinstance(flow_meta, dict) else None
            saved_ids: list[str] = []
            if isinstance(saved_chain, list) and saved_chain:
                for entry in saved_chain:
                    if not isinstance(entry, dict):
                        continue
                    cid = str(entry.get('id') or '').strip()
                    if cid:
                        saved_ids.append(cid)
            if saved_ids:
                id_map = {str(n.get('id') or '').strip(): n for n in (nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()}
                # Honor requested length (truncate) but do not try to auto-extend.
                desired = saved_ids[:length]
                chain_nodes = [id_map[cid] for cid in desired if cid in id_map]
                if chain_nodes:
                    # Drop saved chains that contain nodes that are neither docker-role nor vuln nodes.
                    try:
                        for n in chain_nodes:
                            if not isinstance(n, dict):
                                continue
                            t_raw = str(n.get('type') or '')
                            t = t_raw.strip().lower()
                            is_docker = ('docker' in t) or (t_raw.strip().upper() == 'DOCKER')
                            is_vuln = bool(n.get('is_vuln'))
                            if (not is_docker) and (not is_vuln):
                                chain_nodes = []
                                break
                    except Exception:
                        chain_nodes = []
                if chain_nodes:
                    used_saved_chain = True
        except Exception:
            chain_nodes = []

    if not chain_nodes:
        if preset_steps:
            chain_nodes = _pick_flag_chain_nodes_for_preset(nodes, adj, steps=preset_steps)
        else:
            chain_nodes = _pick_flag_chain_nodes(nodes, adj, length=length)

    warning: str | None = None

    # If we loaded a saved chain that is shorter than the requested length (e.g. the UI
    # reset its length input to the default on refresh), treat the saved chain as
    # authoritative and respond with its effective length.
    if used_saved_chain:
        try:
            eff = len(chain_nodes)
            if eff > 0:
                length = eff
        except Exception:
            pass

    # Optional best-effort mode: if the user requests a longer chain than we can
    # build from eligible nodes, clamp to available rather than returning 422.
    if (not used_saved_chain) and (not preset_steps) and best_effort_query:
        try:
            available = len(chain_nodes)
        except Exception:
            available = 0
        if available > 0 and available < length:
            warning = f"Only {available} eligible nodes found; using chain length {available} instead of requested {length}."
            length = available
    # Prefer persisted assignments when the plan comes from a prior Flow save.
    flag_assignments: list[dict[str, Any]] = []
    try:
        meta = payload.get('metadata') if isinstance(payload, dict) else None
        flow_meta = meta.get('flow') if isinstance(meta, dict) else None
        saved_assignments = flow_meta.get('flag_assignments') if isinstance(flow_meta, dict) else None
        if (not ignore_saved_flow) and isinstance(saved_assignments, list) and saved_assignments:
            chain_id_set = {str(n.get('id') or '').strip() for n in (chain_nodes or []) if isinstance(n, dict)}
            filtered: list[dict[str, Any]] = []
            for entry in saved_assignments:
                if not isinstance(entry, dict):
                    continue
                nid = str(entry.get('node_id') or '').strip()
                if nid and nid in chain_id_set:
                    filtered.append(entry)
            # Only accept if we have a complete mapping (one per chain node).
            if filtered and len(filtered) == len(chain_id_set):
                # Preserve chain order.
                by_id = {str(e.get('node_id') or '').strip(): e for e in filtered}
                flag_assignments = [by_id.get(str(n.get('id') or '').strip()) for n in chain_nodes]
                flag_assignments = [e for e in flag_assignments if isinstance(e, dict)]
                # Backfill new UI fields (e.g., description_hints) and refresh hints for current order.
                try:
                    flag_assignments = _flow_enrich_saved_flag_assignments(
                        flag_assignments,
                        chain_nodes,
                        scenario_label=(scenario_label or scenario_norm),
                    )
                except Exception:
                    pass
    except Exception:
        flag_assignments = []

    if not flag_assignments:
        if preset_steps and not used_saved_chain:
            preset_assignments, preset_err = _flow_compute_flag_assignments_for_preset(preview, chain_nodes, scenario_label or scenario_norm, preset)
            if preset_err:
                return jsonify({'ok': False, 'error': f'Error: {preset_err}', 'stats': stats, 'preview_plan_path': preview_plan_path}), 422
            flag_assignments = preset_assignments
        else:
            flag_assignments = _flow_compute_flag_assignments(preview, chain_nodes, scenario_label or scenario_norm)

    # For auto-generated (non-preset) chains only, prefer a dependency-consistent ordering.
    # Presets force an intended generator order and should not be reordered.
    if (not used_saved_chain) and (not preset_steps):
        debug_dag = str(request.args.get('debug_dag') or '').strip().lower() in ('1', 'true', 'yes', 'y')
        chain_nodes, flag_assignments, dag_debug = _flow_reorder_chain_by_generator_dag(
            chain_nodes,
            flag_assignments,
            scenario_label=(scenario_label or scenario_norm),
            return_debug=bool(debug_dag),
        )
    else:
        debug_dag = str(request.args.get('debug_dag') or '').strip().lower() in ('1', 'true', 'yes', 'y')
        dag_debug = None

    # Validate (non-blocking): if invalid, the UI should warn and execution should
    # proceed without flags.
    flow_valid, flow_errors = _flow_validate_chain_order_by_requires_produces(
        chain_nodes,
        flag_assignments,
        scenario_label=(scenario_label or scenario_norm),
    )
    flags_enabled = bool(flow_valid)

    if len(chain_nodes) < 1:
        return jsonify({'ok': False, 'error': 'No eligible nodes found in preview plan (Docker role or vulnerability nodes).', 'stats': stats, 'preview_plan_path': preview_plan_path}), 422
    if (not used_saved_chain) and len(chain_nodes) < length:
        return jsonify({
            'ok': False,
            'error': f'Only {len(chain_nodes)} eligible nodes found for chain length {length}.',
            'available': len(chain_nodes),
            'stats': stats,
            'preview_plan_path': preview_plan_path,
        }), 422

    out = {
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'length': length,
        'requested_length': requested_length,
        'preview_plan_path': preview_plan_path,
        'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or ''), 'is_vuln': bool(n.get('is_vuln'))} for n in chain_nodes],
        'flag_assignments': flag_assignments,
        'stats': stats,
        'flow_valid': bool(flow_valid),
        'flow_errors': list(flow_errors or []),
        'flags_enabled': bool(flags_enabled),
    }
    try:
        meta_out = payload.get('metadata') if isinstance(payload, dict) else None
        if isinstance(meta_out, dict):
            fp = str(meta_out.get('flow_plan_path') or '').strip()
            if fp:
                out['flow_plan_path'] = fp
    except Exception:
        pass
    if warning:
        out['warning'] = warning
    if debug_mode:
        try:
            meta_dbg = payload.get('metadata') if isinstance(payload, dict) else None
        except Exception:
            meta_dbg = None
        out['debug'] = {
            'selected_by': selected_by,
            'prefer_preview': bool(prefer_preview),
            'force_preview': bool(force_preview),
            'ignore_saved_flow': bool(ignore_saved_flow),
            'used_saved_chain': bool(used_saved_chain),
            'preview_plan_path': preview_plan_path,
            'metadata': (meta_dbg if isinstance(meta_dbg, dict) else {}),
        }
    if debug_dag:
        out['sequencer_dag'] = dag_debug or {'ok': False, 'errors': ['not computed (saved chain)']}

    # STIX/AttackFlow bundle export has been removed; keep this endpoint for chain preview only.
    if (request.args.get('download') or '').strip() in {'1', 'true', 'yes'}:
        return jsonify({
            'ok': False,
            'error': 'STIX bundle export has been removed. Use /api/flag-sequencing/afb_from_chain.',
        }), 410

    return jsonify(out)


@app.route('/api/flag-sequencing/prepare_preview_for_execute', methods=['POST'])
def api_flow_prepare_preview_for_execute():
    j = request.get_json(silent=True) or {}
    scenario_label = str(j.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    preset = str(j.get('preset') or '').strip()
    mode = str(j.get('mode') or '').strip().lower()
    best_effort = bool(j.get('best_effort')) or (mode in {'hint', 'hint_only', 'resolve_hints', 'preview'})
    total_timeout_s: int | None = None
    try:
        total_timeout_s = int(j.get('timeout_s') or 0)
    except Exception:
        total_timeout_s = None
    if total_timeout_s is not None and total_timeout_s <= 0:
        total_timeout_s = None
    if best_effort and total_timeout_s is None:
        # UI hint resolution is optional; keep it bounded by default.
        total_timeout_s = 30
    try:
        length = int(j.get('length') or 5)
    except Exception:
        length = 5
    preset_steps = _flow_preset_steps(preset)
    if preset_steps:
        length = len(preset_steps)
    length = max(1, min(length, 50))
    requested_length = length

    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    base_plan_path = str(j.get('preview_plan') or '').strip() or None
    if base_plan_path:
        try:
            base_plan_path = os.path.abspath(base_plan_path)
            plans_dir = os.path.abspath(os.path.join(_outputs_dir(), 'plans'))
            if os.path.commonpath([base_plan_path, plans_dir]) != plans_dir:
                base_plan_path = None
            elif not os.path.exists(base_plan_path):
                base_plan_path = None
        except Exception:
            base_plan_path = None
    if not base_plan_path:
        base_plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm)

    if not base_plan_path:
        return jsonify({'ok': False, 'error': 'No preview plan found for this scenario. Generate a Full Preview first.'}), 404

    started_at = time.monotonic()
    try:
        plan_basename = os.path.basename(base_plan_path)
    except Exception:
        plan_basename = str(base_plan_path or '')
    app.logger.info(
        '[flow.prepare_preview_for_execute] start scenario=%s requested_length=%s preset=%s best_effort=%s timeout_s=%s base_plan=%s',
        scenario_norm,
        requested_length,
        (preset or ''),
        bool(best_effort),
        (total_timeout_s if total_timeout_s is not None else 'none'),
        plan_basename,
    )

    try:
        with open(base_plan_path, 'r', encoding='utf-8') as f:
            payload = json.load(f) or {}
        meta = payload.get('metadata') if isinstance(payload, dict) else {}
        preview = payload.get('full_preview') if isinstance(payload, dict) else None
        if not isinstance(preview, dict):
            return jsonify({'ok': False, 'error': 'Preview plan is missing full_preview.'}), 422
    except FileNotFoundError:
        return jsonify({'ok': False, 'error': 'Preview plan file was not found. Generate a Full Preview again.'}), 404
    except json.JSONDecodeError as e:
        return jsonify({'ok': False, 'error': f'Preview plan file is not valid JSON: {e}'}), 422
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to load preview plan: {e}'}), 500

    # Best-effort guard: the UI expects JSON errors (avoid Flask HTML 500s).
    try:
        nodes, _links, adj = _build_topology_graph_from_preview_plan(preview)
        stats = _flow_compose_docker_stats(nodes)

        # Allow caller to provide an explicit ordered chain.
        chain_ids_in = j.get('chain_ids')
        chain_ids: list[str] = []
        if isinstance(chain_ids_in, list) and chain_ids_in:
            for cid in chain_ids_in:
                c = str(cid or '').strip()
                if c:
                    chain_ids.append(c)
            chain_ids = chain_ids[:length]

        explicit_chain = bool(chain_ids)

        if chain_ids:
            id_map = {str(n.get('id') or '').strip(): n for n in (nodes or []) if isinstance(n, dict)}
            # Preserve positional intent: keep placeholders for missing ids.
            chain_nodes: list[Any] = []
            missing_chain_ids: list[str] = []
            for cid in chain_ids:
                if cid in id_map:
                    chain_nodes.append(id_map[cid])
                else:
                    chain_nodes.append(None)
                    missing_chain_ids.append(cid)

            # Best-effort repair: if some ids don't exist in the selected plan (common when a
            # stale plan path is passed), fill missing positions with unused eligible nodes.
            # If we cannot repair fully, fail loudly (prevents UI from collapsing chain length).
            if missing_chain_ids:
                try:
                    used = {
                        str(n.get('id') or '').strip()
                        for n in chain_nodes
                        if isinstance(n, dict) and str(n.get('id') or '').strip()
                    }

                    def _needs_nonvuln_docker(pos: int) -> bool:
                        if not preset_steps:
                            return False
                        if pos < 0 or pos >= len(preset_steps):
                            return False
                        return (str((preset_steps[pos] or {}).get('kind') or '').strip() == 'flag-node-generator')

                    def _eligible(cand: dict, pos: int) -> bool:
                        try:
                            cid0 = str(cand.get('id') or '').strip()
                            if not cid0 or cid0 in used:
                                return False
                            is_docker = _flow_node_is_docker_role(cand)
                            is_vuln = bool(cand.get('is_vuln'))
                            if _needs_nonvuln_docker(pos):
                                return bool(is_docker) and (not is_vuln)
                            return bool(is_docker) or bool(is_vuln)
                        except Exception:
                            return False

                    for i, node in enumerate(chain_nodes):
                        if isinstance(node, dict):
                            continue
                        replacement = None
                        for cand in (nodes or []):
                            if not isinstance(cand, dict):
                                continue
                            if not _eligible(cand, i):
                                continue
                            replacement = cand
                            break
                        if replacement is not None:
                            rid = str(replacement.get('id') or '').strip()
                            if rid:
                                chain_nodes[i] = replacement
                                chain_ids[i] = rid
                                used.add(rid)

                    # Drop any unrepaired placeholders.
                    chain_nodes = [n for n in chain_nodes if isinstance(n, dict)]
                    if len(chain_nodes) < length:
                        return jsonify({
                            'ok': False,
                            'error': 'Provided chain_ids do not match the selected preview plan (stale preview_plan?) and could not be fully repaired.',
                            'requested_length': requested_length,
                            'matched_length': len(chain_nodes),
                            'missing_chain_ids': missing_chain_ids,
                            'stats': stats,
                            'best_effort': bool(best_effort),
                        }), 422
                except Exception:
                    # Conservative: don't silently shrink.
                    return jsonify({
                        'ok': False,
                        'error': 'Provided chain_ids did not match the selected preview plan and repair failed.',
                        'requested_length': requested_length,
                        'missing_chain_ids': missing_chain_ids,
                        'stats': stats,
                        'best_effort': bool(best_effort),
                    }), 422

            # Presets require certain steps to run on non-vulnerability docker nodes.
            # If the UI provided a chain that violates this (common when many vuln nodes exist),
            # best-effort swap in an eligible docker node.
            if preset_steps and chain_nodes:
                try:
                    used = {str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict)}
                    for i, step in enumerate(preset_steps[:len(chain_nodes)]):
                        if str((step or {}).get('kind') or '').strip() != 'flag-node-generator':
                            continue
                        node = chain_nodes[i] if i < len(chain_nodes) else None
                        if not isinstance(node, dict):
                            continue
                        if not bool(node.get('is_vuln')):
                            continue
                        replacement = None
                        for cand in (nodes or []):
                            if not isinstance(cand, dict):
                                continue
                            cid = str(cand.get('id') or '').strip()
                            if not cid or cid in used:
                                continue
                            t_raw = str(cand.get('type') or '')
                            t = t_raw.strip().lower()
                            is_docker = ('docker' in t) or (t_raw.strip().upper() == 'DOCKER')
                            if is_docker and not bool(cand.get('is_vuln')):
                                replacement = cand
                                break
                        if replacement is not None:
                            rid = str(replacement.get('id') or '').strip()
                            if rid:
                                chain_nodes[i] = replacement
                                chain_ids[i] = rid
                                used.add(rid)
                except Exception:
                    pass

            # Enforce Flow placement rules:
            # - flag-generators may be placed on vulnerability nodes OR docker-role nodes
            # - flag-node-generators must be placed on non-vulnerability docker-role nodes
            # If the caller provided a chain that violates this, best-effort replace nodes
            # with unused eligible nodes; otherwise fail with a clear error.
            if chain_nodes:
                try:
                    used = {str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict) and str(n.get('id') or '').strip()}
                    for i, node in enumerate(chain_nodes):
                        if not isinstance(node, dict):
                            continue
                        t_raw = str(node.get('type') or '')
                        t = t_raw.strip().lower()
                        is_docker = ('docker' in t) or (t_raw.strip().upper() == 'DOCKER')
                        is_vuln = bool(node.get('is_vuln'))

                        need_nonvuln_docker = False
                        if preset_steps and i < len(preset_steps):
                            need_nonvuln_docker = (str((preset_steps[i] or {}).get('kind') or '').strip() == 'flag-node-generator')

                        if need_nonvuln_docker:
                            if is_docker and (not is_vuln):
                                continue
                        else:
                            if is_docker or is_vuln:
                                continue

                        replacement = None
                        for cand in (nodes or []):
                            if not isinstance(cand, dict):
                                continue
                            cid = str(cand.get('id') or '').strip()
                            if not cid or cid in used:
                                continue
                            ct_raw = str(cand.get('type') or '')
                            ct = ct_raw.strip().lower()
                            cand_is_docker = ('docker' in ct) or (ct_raw.strip().upper() == 'DOCKER')
                            cand_is_vuln = bool(cand.get('is_vuln'))
                            if need_nonvuln_docker:
                                if not cand_is_docker:
                                    continue
                                if cand_is_vuln:
                                    continue
                            else:
                                if not (cand_is_docker or cand_is_vuln):
                                    continue
                            replacement = cand
                            break

                        if replacement is None:
                            return jsonify({
                                'ok': False,
                                'error': 'Not enough eligible nodes for the provided chain. Flag-generators require docker-role or vulnerability nodes; flag-node-generators require non-vulnerability docker-role nodes.',
                                'stats': stats,
                            }), 422

                        rid = str(replacement.get('id') or '').strip()
                        if rid:
                            chain_nodes[i] = replacement
                            chain_ids[i] = rid
                            used.add(rid)
                except Exception:
                    pass
            if len(chain_nodes) < 1:
                return jsonify({'ok': False, 'error': 'Provided chain_ids did not match any nodes in the preview plan.', 'stats': stats}), 422
        else:
            if preset_steps:
                chain_nodes = _pick_flag_chain_nodes_for_preset(nodes, adj, steps=preset_steps)
            else:
                chain_nodes = _pick_flag_chain_nodes(nodes, adj, length=length)
            if len(chain_nodes) < length:
                return jsonify({'ok': False, 'error': 'Not enough eligible nodes in preview plan to build the requested chain.', 'available': len(chain_nodes), 'stats': stats}), 422
            chain_ids = [str(n.get('id') or '').strip() for n in chain_nodes if str(n.get('id') or '').strip()]
    except Exception as e:
        app.logger.exception('[flow.prepare_preview_for_execute] internal error: %s', e)
        return jsonify({
            'ok': False,
            'error': f'Internal error preparing preview for execution: {e}',
            'base_preview_plan_path': base_plan_path,
        }), 500

    # Caller may pass fewer ids than requested length; persist effective length.
    try:
        length = len(chain_nodes)
    except Exception:
        pass

    # Inject generator metadata into chain nodes.
    # Flag-generators should NOT create nodes/services; they generate artifacts/flags that can be
    # inserted into existing Docker nodes.
    flag_assignments: list[dict[str, Any]] = []
    # Prefer saved Flow assignments if the caller passed a flow plan (or a plan payload that
    # already includes metadata.flow) and it fully covers this chain.
    try:
        flow_meta = meta.get('flow') if isinstance(meta, dict) else None
        saved_assignments = flow_meta.get('flag_assignments') if isinstance(flow_meta, dict) else None
        if isinstance(saved_assignments, list) and saved_assignments:
            chain_set = {str(n.get('id') or '').strip() for n in (chain_nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()}
            filtered: list[dict[str, Any]] = []
            for entry in saved_assignments:
                if not isinstance(entry, dict):
                    continue
                nid = str(entry.get('node_id') or '').strip()
                if nid and nid in chain_set:
                    filtered.append(entry)
            if filtered and len(filtered) == len(chain_set):
                by_id = {str(e.get('node_id') or '').strip(): e for e in filtered}
                flag_assignments = [by_id.get(str(n.get('id') or '').strip()) for n in chain_nodes]
                flag_assignments = [e for e in flag_assignments if isinstance(e, dict)]
                try:
                    flag_assignments = _flow_enrich_saved_flag_assignments(
                        flag_assignments,
                        chain_nodes,
                        scenario_label=(scenario_label or scenario_norm),
                    )
                except Exception:
                    pass

                # Do not reuse saved assignments if they violate placement rules for
                # the current chain nodes (prevents Preview from showing flags on
                # non-docker/non-vuln nodes or flag-node-generators on vuln nodes).
                try:
                    if flag_assignments and len(flag_assignments) == len(chain_nodes):
                        for i, n in enumerate(chain_nodes):
                            a = flag_assignments[i] if i < len(flag_assignments) else {}
                            if not isinstance(n, dict) or not isinstance(a, dict):
                                raise ValueError('invalid chain/assignment')
                            nid = str(n.get('id') or '').strip()
                            aid = str(a.get('node_id') or '').strip()
                            if nid and aid and nid != aid:
                                raise ValueError('assignment node mismatch')
                            is_docker = _flow_node_is_docker_role(n)
                            is_vuln = bool(n.get('is_vuln'))
                            kind = str(a.get('type') or '').strip() or 'flag-generator'
                            if kind == 'flag-node-generator':
                                if not (is_docker and (not is_vuln)):
                                    raise ValueError('flag-node-generator on ineligible node')
                            else:
                                if not (is_docker or is_vuln):
                                    raise ValueError('flag-generator on ineligible node')
                except Exception:
                    flag_assignments = []
    except Exception:
        flag_assignments = []

    if not flag_assignments:
        if preset_steps:
            preset_assignments, preset_err = _flow_compute_flag_assignments_for_preset(preview, chain_nodes, scenario_label or scenario_norm, preset)
            if preset_err:
                return jsonify({'ok': False, 'error': f'Error: {preset_err}', 'stats': stats}), 422
            flag_assignments = preset_assignments
        else:
            flag_assignments = _flow_compute_flag_assignments(preview, chain_nodes, scenario_label or scenario_norm)

    # For auto-generated (non-preset) chains only, prefer a dependency-consistent ordering.
    # Note: chain_ids is populated for both explicit and auto-picked chains; use explicit_chain.
    if (not explicit_chain) and (not preset_steps):
        debug_dag = bool(j.get('debug_dag'))
        chain_nodes, flag_assignments, dag_debug = _flow_reorder_chain_by_generator_dag(
            chain_nodes,
            flag_assignments,
            scenario_label=(scenario_label or scenario_norm),
            return_debug=bool(debug_dag),
        )
        try:
            chain_ids = [str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict) and str(n.get('id') or '').strip()]
        except Exception:
            pass
    else:
        debug_dag = bool(j.get('debug_dag'))
        dag_debug = None

    # Validate (non-blocking): if invalid, we still allow execution but we do not
    # inject or run any flags.
    flow_valid, flow_errors = _flow_validate_chain_order_by_requires_produces(
        chain_nodes,
        flag_assignments,
        scenario_label=(scenario_label or scenario_norm),
    )
    flags_enabled = bool(flow_valid)

    # Apply Flow modifications and run generators only when the flow is valid.
    if flags_enabled:
        # Promote chain nodes to Docker role (flag payloads attach to Docker nodes).
        try:
            hosts = preview.get('hosts') or []
            if isinstance(hosts, list):
                for h in hosts:
                    if not isinstance(h, dict):
                        continue
                    hid = str(h.get('node_id') or '').strip()
                    if hid and hid in chain_ids:
                        h['role'] = 'Docker'
        except Exception:
            pass

    # Load generator catalogs once so we can prune config to declared inputs.
    # Only needed when flags are enabled.
    if flags_enabled:
        try:
            _gens_for_cfg, _ = _flag_generators_from_enabled_sources()
        except Exception:
            _gens_for_cfg = []
        try:
            _node_gens_for_cfg, _ = _flag_node_generators_from_enabled_sources()
        except Exception:
            _node_gens_for_cfg = []
    else:
        _gens_for_cfg = []
        _node_gens_for_cfg = []

    _gen_by_id: dict[str, dict[str, Any]] = {}
    try:
        for _g in (_gens_for_cfg or []):
            if not isinstance(_g, dict):
                continue
            _gid = str(_g.get('id') or '').strip()
            if _gid:
                _gen_by_id[_gid] = _g
        for _g in (_node_gens_for_cfg or []):
            if not isinstance(_g, dict):
                continue
            _gid = str(_g.get('id') or '').strip()
            if _gid and _gid not in _gen_by_id:
                _gen_by_id[_gid] = _g
    except Exception:
        _gen_by_id = {}

    def _all_input_names_of(gen: dict[str, Any]) -> set[str]:
        names: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    nm = str(inp.get('name') or '').strip()
                    if nm:
                        names.add(nm)
        except Exception:
            pass
        return names

    def _required_input_names_of(gen: dict[str, Any]) -> set[str]:
        names: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    nm = str(inp.get('name') or '').strip()
                    if not nm:
                        continue
                    if inp.get('required') is False:
                        continue
                    names.add(nm)
        except Exception:
            pass
        return names

    def _flow_default_generator_config(assignment: dict[str, Any], *, seed_val: Any) -> dict[str, Any]:
        """Synthesize deterministic default inputs for the seeded generators."""
        node_id = str(assignment.get('node_id') or '').strip()
        gen_id = str(assignment.get('id') or '').strip()
        base_seed = str(seed_val if seed_val not in (None, '') else '0')
        return {
            'seed': f"{base_seed}:{scenario_norm}:{node_id}:{gen_id}",
            'flag_prefix': 'FLAG',
            'secret': f"FLOWSECRET_{base_seed}_{scenario_norm}_{node_id}",
            'env_name': f"env_{scenario_norm}_{node_id}",
            'challenge': f"challenge_{scenario_norm}_{node_id}",
            'username_prefix': 'user',
            'key_len': 16,
        }

    def _flow_try_run_generator(
        generator_id: str,
        *,
        out_dir: str,
        config: dict[str, Any],
        catalog: str = 'flag_generators',
        timeout_s: int = 120,
    ) -> tuple[bool, str, str | None]:
        """Best-effort run of scripts/run_flag_generator.py.

        Returns: (ok, note_or_error, manifest_path)
        """
        try:
            repo_root = _get_repo_root()
            runner_path = os.path.join(repo_root, 'scripts', 'run_flag_generator.py')
            if not os.path.exists(runner_path):
                return False, 'runner script not found', None

            cmd = [
                sys.executable or 'python',
                runner_path,
                '--catalog',
                str(catalog or 'flag_generators'),
                '--generator-id',
                generator_id,
                '--out-dir',
                out_dir,
                '--config',
                json.dumps(config, ensure_ascii=False),
                '--repo-root',
                repo_root,
            ]
            p = subprocess.run(
                cmd,
                cwd=repo_root,
                check=False,
                capture_output=True,
                text=True,
                timeout=max(1, int(timeout_s or 120)),
            )
            manifest_path = os.path.join(out_dir, 'outputs.json')
            if p.returncode != 0:
                err = (p.stderr or p.stdout or '').strip()
                if err:
                    err = err[-800:]
                return False, f'generator failed (rc={p.returncode}): {err}', (manifest_path if os.path.exists(manifest_path) else None)
            if os.path.exists(manifest_path):
                return True, 'ok', manifest_path
            return True, 'ok (no outputs.json)', None
        except subprocess.TimeoutExpired:
            return False, 'generator timed out', None
        except Exception as exc:
            return False, f'generator exception: {exc}', None
    if flags_enabled:
        try:
            host_by_id: dict[str, dict[str, Any]] = {}
            hosts = preview.get('hosts') or []
            if isinstance(hosts, list):
                for h in hosts:
                    if not isinstance(h, dict):
                        continue
                    host_by_id[str(h.get('node_id') or '').strip()] = h

            # Flow has a "god-eye" view of generator outputs across the chain. As we run each
            # generator, we capture outputs.json and feed those values into subsequent generator
            # configs when they declare matching input names (e.g. network.ip, credential.pair).
            flow_context: dict[str, Any] = {}

            def _apply_outputs_to_hint_text(text_in: str, outs: dict[str, Any]) -> str:
                """Replace {{OUTPUT.key}} and {{OUTPUT.key:transform}} placeholders."""
                try:
                    text = str(text_in or '')
                except Exception:
                    return str(text_in or '')
                if not text or not isinstance(outs, dict) or not outs:
                    return text
                try:
                    pattern = re.compile(r"\{\{OUTPUT\.([^}:]+?)(?::([^}]+?))?\}\}")
                except Exception:
                    return text

                def _render_value(val: Any) -> str:
                    if isinstance(val, (dict, list)):
                        return json.dumps(val, ensure_ascii=False)
                    return str(val)

                def _transform(val: Any, tf: str) -> str:
                    t = (tf or '').strip().lower()
                    s = _render_value(val)
                    if not t:
                        return s
                    if t in {'last_octet', 'octet4'}:
                        try:
                            parts = s.strip().split('.')
                            if len(parts) == 4:
                                return parts[3]
                        except Exception:
                            pass
                        return s
                    if t in {'subnet24', 'cidr24'}:
                        try:
                            parts = s.strip().split('.')
                            if len(parts) == 4:
                                return f"{parts[0]}.{parts[1]}.{parts[2]}.0/24"
                        except Exception:
                            pass
                        return s
                    if t in {'redact', 'masked'}:
                        try:
                            parts = s.strip().split('.')
                            if len(parts) == 4:
                                return f"{parts[0]}.{parts[1]}.{parts[2]}.x"
                        except Exception:
                            pass
                        return s
                    return s

                def _repl(match: re.Match) -> str:
                    key = (match.group(1) or '').strip()
                    tf = (match.group(2) or '').strip()
                    if not key:
                        return match.group(0)
                    if key not in outs:
                        return match.group(0)
                    return _transform(outs.get(key), tf)

                try:
                    return pattern.sub(_repl, text)
                except Exception:
                    return text

            generation_failures: list[dict[str, Any]] = []
            generation_skipped: list[dict[str, Any]] = []
            created_run_dirs: list[str] = []
            failed_run_dirs: list[str] = []

            deadline = (started_at + float(total_timeout_s)) if total_timeout_s is not None else None
            for fa in (flag_assignments or []):
                if not isinstance(fa, dict):
                    continue
                cid = str(fa.get('node_id') or '').strip()
                h = host_by_id.get(cid)
                if not h or not isinstance(h, dict):
                    continue

                meta_h = h.get('metadata')
                if not isinstance(meta_h, dict):
                    meta_h = {}
                    h['metadata'] = meta_h

                generator_id = str(fa.get('id') or '').strip()
                assignment_type = str(fa.get('type') or '').strip() or 'flag-generator'
                generator_catalog = str(fa.get('generator_catalog') or '').strip() or 'flag_generators'
                seed_val = preview.get('seed') if isinstance(preview, dict) else None

                cfg_full = _flow_default_generator_config(fa, seed_val=seed_val)

                # Provide basic per-node context for generators that want it.
                try:
                    node_name_val = str(h.get('name') or '').strip()
                    if node_name_val:
                        cfg_full['node_name'] = node_name_val
                except Exception:
                    pass

                cfg = cfg_full
                inputs_mismatch: dict[str, Any] = {}
                try:
                    gen_def = _gen_by_id.get(generator_id)
                    if isinstance(gen_def, dict):
                        allowed = _all_input_names_of(gen_def)

                        declared_required = None
                        try:
                            if isinstance(fa.get('input_fields_required'), list):
                                declared_required = {str(x).strip() for x in (fa.get('input_fields_required') or []) if str(x).strip()}
                        except Exception:
                            declared_required = None
                        if declared_required is None:
                            declared_required = _required_input_names_of(gen_def)

                        # Inject prior outputs into this generator's config, but only for inputs it declares.
                        try:
                            if allowed and flow_context:
                                for k in allowed:
                                    if k in cfg_full:
                                        continue
                                    if k in flow_context:
                                        cfg_full[k] = flow_context[k]
                        except Exception:
                            pass

                        # If the generator declares inputs, only pass those (keeps Flow configs relevant).
                        # HOWEVER: some catalogs/definitions can drift; if the assignment marks an input as
                        # required (e.g., seed, node_name), ensure it is passed even if not in `allowed`.
                        cfg_to_pass = cfg_full
                        if allowed:
                            keep = set(allowed)
                            try:
                                keep |= set(declared_required or set())
                            except Exception:
                                pass
                            cfg_to_pass = {k: v for k, v in cfg_full.items() if k in keep}
                        cfg = cfg_to_pass

                        try:
                            provided_keys = {str(k).strip() for k in (cfg_to_pass or {}).keys() if str(k).strip()}
                        except Exception:
                            provided_keys = set()

                        missing_required = sorted([k for k in (declared_required or set()) if k not in provided_keys])

                        unset_required: list[str] = []
                        try:
                            for k in sorted(list(declared_required or set())):
                                if k not in (cfg_to_pass or {}):
                                    continue
                                v = (cfg_to_pass or {}).get(k)
                                if v is None:
                                    unset_required.append(k)
                                elif isinstance(v, str) and (not v.strip()):
                                    unset_required.append(k)
                        except Exception:
                            unset_required = []

                        dropped_keys: list[str] = []
                        try:
                            if allowed:
                                dropped_keys = sorted([k for k in (cfg_full or {}).keys() if k not in (cfg_to_pass or {})])
                                # Many keys are synthesized by Flow and carried in cfg_full for
                                # convenience (e.g., hint rendering). Generators only receive
                                # inputs they declare, so these show up as "dropped" even though
                                # it's expected. Don't surface them as mismatches.
                                try:
                                    synthesized = set(_flow_synthesized_inputs())
                                except Exception:
                                    synthesized = set()
                                dropped_keys = [k for k in dropped_keys if str(k) not in synthesized]
                        except Exception:
                            dropped_keys = []

                        inputs_mismatch = {
                            'declared_required': sorted(list(declared_required or set())),
                            'provided': sorted(list(provided_keys)),
                            'missing_required': missing_required,
                            'unset_required': unset_required,
                            'dropped': dropped_keys,
                            # Consider dropped keys a mismatch so the UI can surface it.
                            'ok': (not missing_required and not unset_required and not dropped_keys),
                        }
                except Exception:
                    cfg = cfg_full
                    inputs_mismatch = {}

                flow_out_dir = ''
                ok_run = False
                note = ''
                manifest_path = None
                actual_output_keys: list[str] = []
                declared_output_keys: list[str] = []
                try:
                    # IMPORTANT: outputs.json is a runtime manifest of output KEYS.
                    # Use output_fields (runtime) when available; fall back to legacy outputs.
                    declared_src = (fa.get('output_fields') if isinstance(fa.get('output_fields'), list) else None)
                    if declared_src is None:
                        declared_src = (fa.get('outputs') if isinstance(fa.get('outputs'), list) else [])
                    declared_output_keys = sorted([str(x) for x in (declared_src or []) if str(x).strip()])
                except Exception:
                    declared_output_keys = []
                mismatch: dict[str, Any] = {}

                try:
                    if generator_id:
                        if deadline is not None and time.monotonic() >= deadline:
                            generation_skipped.append({
                                'node_id': cid,
                                'node_name': str(h.get('name') or ''),
                                'generator_id': generator_id,
                                'reason': 'time budget exceeded',
                            })
                            break

                        flow_run_id = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:10]
                        # IMPORTANT: stage under /tmp/vulns so the existing sync pipeline can ship
                        # these artifacts to the CORE host (where docker-compose paths resolve).
                        subdir = 'flag_node_generators_runs' if assignment_type == 'flag-node-generator' else 'flag_generators_runs'
                        flow_out_dir = os.path.join('/tmp/vulns', subdir, f"flow-{scenario_norm}-{flow_run_id}")
                        os.makedirs(flow_out_dir, exist_ok=True)
                        try:
                            created_run_dirs.append(str(flow_out_dir))
                        except Exception:
                            pass

                        remaining = None
                        if deadline is not None:
                            try:
                                remaining = int(max(1.0, deadline - time.monotonic()))
                            except Exception:
                                remaining = 1
                        gen_timeout_s = 120
                        if remaining is not None:
                            gen_timeout_s = min(gen_timeout_s, remaining)

                        ok_run, note, manifest_path = _flow_try_run_generator(
                            generator_id,
                            out_dir=flow_out_dir,
                            config=cfg,
                            catalog=generator_catalog,
                            timeout_s=gen_timeout_s,
                        )

                        if not ok_run:
                            generation_failures.append({
                                'node_id': cid,
                                'node_name': str(h.get('name') or ''),
                                'generator_id': generator_id,
                                'error': str(note or 'generator execution failed'),
                                'run_dir': str(flow_out_dir or ''),
                            })
                            try:
                                if flow_out_dir:
                                    failed_run_dirs.append(str(flow_out_dir))
                            except Exception:
                                pass

                        # If the generator produced a manifest, capture the actual output keys.
                        if ok_run and manifest_path and os.path.exists(manifest_path):
                            try:
                                with open(manifest_path, 'r', encoding='utf-8') as f:
                                    m = json.load(f) or {}
                                outs = m.get('outputs') if isinstance(m, dict) else None
                                if isinstance(outs, dict):
                                    actual_output_keys = sorted([str(k) for k in outs.keys() if str(k).strip()])

                                    # Propagate outputs forward so later generators can consume concrete values.
                                    try:
                                        for k, v in outs.items():
                                            kk = str(k)
                                            if not kk:
                                                continue
                                            flow_context[kk] = v
                                    except Exception:
                                        pass

                                    # Substitute output placeholders into hints (best-effort).
                                    try:
                                        if isinstance(fa.get('hints'), list) and fa.get('hints'):
                                            new_hints = [_apply_outputs_to_hint_text(str(t), outs) for t in (fa.get('hints') or [])]
                                            fa['hints'] = new_hints
                                            fa['hint'] = str(new_hints[0] or '') if new_hints else str(fa.get('hint') or '')
                                        else:
                                            hint_final = _apply_outputs_to_hint_text(str(fa.get('hint') or ''), outs)
                                            if hint_final and hint_final != str(fa.get('hint') or ''):
                                                fa['hint'] = hint_final
                                    except Exception:
                                        pass

                                    # Best-effort: if the generator emitted a top-level 'flag' value,
                                    # also write a plain flag.txt for easier participant discovery.
                                    try:
                                        flag_val = outs.get('flag')
                                        if ok_run and flow_out_dir and isinstance(flag_val, str) and flag_val.strip():
                                            with open(os.path.join(flow_out_dir, 'flag.txt'), 'w', encoding='utf-8') as ff:
                                                ff.write(flag_val.strip() + "\n")
                                            # UI convenience: expose the realized flag value (runtime-only).
                                            # IMPORTANT: this must not be persisted into saved plans.
                                            try:
                                                fa['flag_value'] = flag_val.strip()
                                            except Exception:
                                                pass
                                    except Exception:
                                        pass
                            except Exception:
                                actual_output_keys = []

                        # Materialize a human-readable hint file in the artifacts directory.
                        # IMPORTANT: do this after outputs.json has been applied to hints so
                        # the CORE VM sees resolved template values.
                        try:
                            hint_texts: list[str] = []
                            if isinstance(fa.get('hints'), list):
                                hint_texts = [str(x or '').strip() for x in (fa.get('hints') or []) if str(x or '').strip()]
                            if not hint_texts:
                                single = str(fa.get('hint') or '').strip()
                                if single:
                                    hint_texts = [single]
                            if flow_out_dir and hint_texts:
                                with open(os.path.join(flow_out_dir, 'hint.txt'), 'w', encoding='utf-8') as hf:
                                    if len(hint_texts) == 1:
                                        hf.write(hint_texts[0] + "\n")
                                    else:
                                        for idx, ht in enumerate(hint_texts, start=1):
                                            hf.write(f"Hint {idx}/{len(hint_texts)}: {ht}\n")
                        except Exception:
                            pass

                        # Compare declared vs actual output keys (best-effort).
                        try:
                            if ok_run and actual_output_keys:
                                # Some generators include metadata echo outputs (e.g., node_name).
                                # These are not meaningful artifacts for Flow chaining and shouldn't
                                # trigger contract mismatch warnings.
                                ignore_actual = {
                                    'node_name',
                                    'nodename',
                                    'nodeName',
                                }
                                declared_set = set(declared_output_keys or [])
                                actual_set = set([k for k in (actual_output_keys or []) if k not in ignore_actual])
                                missing = sorted(list(declared_set - actual_set))
                                extra = sorted(list(actual_set - declared_set))
                                mismatch = {
                                    'declared': declared_output_keys,
                                    'actual': actual_output_keys,
                                    'missing': missing,
                                    'extra': extra,
                                    'ok': (not missing and not extra),
                                }
                        except Exception:
                            mismatch = {}
                except Exception as exc:
                    ok_run, note, manifest_path = False, f'generator exception: {exc}', None
                    if generator_id:
                        generation_failures.append({
                            'node_id': cid,
                            'node_name': str(h.get('name') or ''),
                            'generator_id': generator_id,
                            'error': str(note or ''),
                            'run_dir': str(flow_out_dir or ''),
                        })

                meta_h['flow_flag'] = {
                    'type': assignment_type,
                    'generator_catalog': generator_catalog,
                    'generator_id': generator_id,
                    'generator_name': str(fa.get('name') or ''),
                    'generator_language': str(fa.get('language') or ''),
                    'generator_source': str(fa.get('flag_generator') or ''),
                    'artifacts_dir': str(flow_out_dir or ''),
                    'inputs': list(fa.get('inputs') or []) if isinstance(fa.get('inputs'), list) else [],
                    'outputs': list(fa.get('outputs') or []) if isinstance(fa.get('outputs'), list) else [],
                    'hint_template': str(fa.get('hint_template') or ''),
                    'hint': str(fa.get('hint') or ''),
                    'next_node_id': str(fa.get('next_node_id') or ''),
                    'next_node_name': str(fa.get('next_node_name') or ''),
                    'generated': bool(ok_run),
                    'generation_note': str(note or ''),
                    'run_dir': str(flow_out_dir or ''),
                    'outputs_manifest': str(manifest_path or ''),
                    'actual_outputs': actual_output_keys,
                    'declared_outputs': declared_output_keys,
                    'outputs_match': bool(mismatch.get('ok')) if isinstance(mismatch, dict) and mismatch else True,
                    'outputs_mismatch': mismatch,
                    'inputs_match': bool(inputs_mismatch.get('ok')) if isinstance(inputs_mismatch, dict) and inputs_mismatch else True,
                    'inputs_mismatch': inputs_mismatch,
                    'config': cfg,
                }

                # Also enrich the assignment itself for the Flow UI response.
                try:
                    fa['generated'] = bool(ok_run)
                    fa['generation_note'] = str(note or '')
                    fa['artifacts_dir'] = str(flow_out_dir or '')
                    fa['outputs_manifest'] = str(manifest_path or '')
                    fa['declared_outputs'] = declared_output_keys
                    fa['actual_outputs'] = actual_output_keys
                    fa['outputs_match'] = bool(mismatch.get('ok')) if isinstance(mismatch, dict) and mismatch else True
                    fa['outputs_mismatch'] = mismatch
                    fa['inputs_match'] = bool(inputs_mismatch.get('ok')) if isinstance(inputs_mismatch, dict) and inputs_mismatch else True
                    fa['inputs_mismatch'] = inputs_mismatch
                except Exception:
                    pass

            if generation_failures:
                # Cleanup partial artifacts so we don't leave confusing residues behind.
                try:
                    base_dir = os.path.abspath(os.path.join('/tmp', 'vulns'))
                    to_rm = (created_run_dirs or [])
                    if best_effort:
                        to_rm = (failed_run_dirs or [])
                    for d in to_rm:
                        try:
                            dd = os.path.abspath(str(d))
                            if os.path.commonpath([dd, base_dir]) != base_dir:
                                continue
                            shutil.rmtree(dd, ignore_errors=True)
                        except Exception:
                            continue
                except Exception:
                    pass
                if not best_effort:
                    return jsonify({
                        'ok': False,
                        'error': f"{len(generation_failures)} generator run(s) failed; cannot prepare preview for execute.",
                        'scenario': scenario_label or scenario_norm,
                        'length': length,
                        'stats': stats,
                        'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or '')} for n in chain_nodes],
                        'flag_assignments': flag_assignments,
                        'generation_failures': generation_failures,
                        'generation_skipped': generation_skipped,
                        'base_preview_plan_path': base_plan_path,
                        'best_effort': bool(best_effort),
                    }), 422
        except Exception:
            pass
    else:
        # Ensure the UI gets a consistent signal when flags are disabled.
        try:
            for fa in (flag_assignments or []):
                if not isinstance(fa, dict):
                    continue
                fa['generated'] = False
                fa['generation_note'] = 'flags disabled (invalid dependency order)'
        except Exception:
            pass

    try:
        persisted_flag_assignments = _flow_strip_runtime_sensitive_fields(flag_assignments)
        flow_meta = {
            'source_preview_plan_path': base_plan_path,
            'scenario': scenario_label or scenario_norm,
            'length': length,
            'requested_length': requested_length,
            'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or '')} for n in chain_nodes],
            # Persist all Flow decisions so returning to Flag Sequencing shows the same
            # chain and generator selections/hints.
            'flag_assignments': persisted_flag_assignments,
            'flags_enabled': bool(flags_enabled),
            'flow_valid': bool(flow_valid),
            'flow_errors': list(flow_errors or []),
            'modified_at': _iso_now(),
        }
        if isinstance(meta, dict):
            meta = dict(meta)
            meta['flow'] = flow_meta
        else:
            meta = {'flow': flow_meta}
    except Exception:
        pass

    # Persist a new plan artifact in outputs/plans so /run_cli_async can safely consume it.
    try:
        plans_dir = Path(_outputs_dir()) / 'plans'
        plans_dir.mkdir(parents=True, exist_ok=True)
        seed = None
        try:
            seed = int((preview.get('seed') if isinstance(preview, dict) else None) or (meta.get('seed') if isinstance(meta, dict) else None) or 0)
        except Exception:
            seed = 0
        suffix = secrets.token_hex(3)
        out_name = f"plan_from_flow_{seed}_{int(time.time())}_{suffix}.json"
        out_path = str(plans_dir / out_name)
        out_payload = {
            'full_preview': preview,
            'metadata': meta,
        }
        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(out_payload, f, indent=2)
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to persist flow-modified preview plan: {e}'}), 500

    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'length': length,
        'requested_length': requested_length,
        'stats': stats,
        'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or ''), 'is_vuln': bool(n.get('is_vuln'))} for n in chain_nodes],
        'flag_assignments': flag_assignments,
        'flags_enabled': bool(flags_enabled),
        'flow_valid': bool(flow_valid),
        'flow_errors': list(flow_errors or []),
        'xml_path': str((meta or {}).get('xml_path') or ''),
        'preview_plan_path': out_path,
        'base_preview_plan_path': base_plan_path,
        'best_effort': bool(best_effort),
        'elapsed_s': round(float(time.monotonic() - started_at), 3),
        **({'sequencer_dag': (dag_debug or {'ok': False, 'errors': ['not computed (explicit chain)']})} if debug_dag else {}),
    })


@app.route('/api/flag-sequencing/save_flow_substitutions', methods=['POST'])
def api_flow_save_flow_substitutions():
    """Persist a user-edited chain + generator assignments (no generator runs).

    This writes a plan_from_flow_*.json that carries metadata.flow.chain and
    metadata.flow.flag_assignments so future preview/prepare/execute honors the
    user's substitutions.
    """
    j = request.get_json(silent=True) or {}
    scenario_label = str(j.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    chain_ids_in = j.get('chain_ids')
    if not isinstance(chain_ids_in, list) or not chain_ids_in:
        return jsonify({'ok': False, 'error': 'Missing chain_ids.'}), 400
    chain_ids: list[str] = [str(x or '').strip() for x in chain_ids_in if str(x or '').strip()]
    if not chain_ids:
        return jsonify({'ok': False, 'error': 'Missing chain_ids.'}), 400

    base_plan_path = str(j.get('preview_plan') or '').strip() or None
    if base_plan_path:
        try:
            base_plan_path = os.path.abspath(base_plan_path)
            plans_dir = os.path.abspath(os.path.join(_outputs_dir(), 'plans'))
            if os.path.commonpath([base_plan_path, plans_dir]) != plans_dir:
                base_plan_path = None
            elif not os.path.exists(base_plan_path):
                base_plan_path = None
        except Exception:
            base_plan_path = None
    if not base_plan_path:
        base_plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm, prefer_flow=False)
    if not base_plan_path or not os.path.exists(base_plan_path):
        return jsonify({'ok': False, 'error': 'No preview plan found for this scenario. Generate a Full Preview first.'}), 404

    try:
        with open(base_plan_path, 'r', encoding='utf-8') as f:
            payload = json.load(f) or {}
        meta = payload.get('metadata') if isinstance(payload, dict) else {}
        preview = payload.get('full_preview') if isinstance(payload, dict) else None
        if not isinstance(preview, dict):
            return jsonify({'ok': False, 'error': 'Preview plan is missing full_preview.'}), 422
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to load preview plan: {e}'}), 500

    # Build chain node dicts with vulnerability flags.
    try:
        nodes, _links, _adj = _build_topology_graph_from_preview_plan(preview)
    except Exception:
        nodes = []
    id_map = {str(n.get('id') or '').strip(): n for n in (nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()}
    chain_nodes: list[dict[str, Any]] = []
    for cid in chain_ids:
        n = id_map.get(str(cid))
        if not isinstance(n, dict):
            return jsonify({'ok': False, 'error': f'Chain node not found in preview plan: {cid}'}), 422
        chain_nodes.append(n)

    # Parse requested assignments (one per chain position).
    fas_in = j.get('flag_assignments')
    if not isinstance(fas_in, list) or len(fas_in) != len(chain_nodes):
        return jsonify({'ok': False, 'error': 'flag_assignments must be a list aligned to chain_ids (same length).'}), 400

    try:
        gens, _ = _flag_generators_from_enabled_sources()
    except Exception:
        gens = []
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
    except Exception:
        node_gens = []
    gen_by_id: dict[str, dict[str, Any]] = {}
    for g in (gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid and gid not in gen_by_id:
            gg = dict(g)
            gg['_flow_kind'] = 'flag-generator'
            gg['_flow_catalog'] = 'flag_generators'
            gen_by_id[gid] = gg
    for g in (node_gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid and gid not in gen_by_id:
            gg = dict(g)
            gg['_flow_kind'] = 'flag-node-generator'
            gg['_flow_catalog'] = 'flag_node_generators'
            gen_by_id[gid] = gg

    try:
        plugins_by_id = _flow_enabled_plugin_contracts_by_id()
    except Exception:
        plugins_by_id = {}

    id_to_name: dict[str, str] = {}
    for n in chain_nodes:
        try:
            nid = str(n.get('id') or '').strip()
            nm = str(n.get('name') or '').strip()
            if nid:
                id_to_name[nid] = nm or nid
        except Exception:
            pass

    def _artifact_requires_of(gen: dict[str, Any]) -> set[str]:
        required: set[str] = set()
        try:
            pid = str(gen.get('id') or '').strip()
            plugin = plugins_by_id.get(pid)
            if isinstance(plugin, dict) and isinstance(plugin.get('requires'), list):
                for x in (plugin.get('requires') or []):
                    xx = str(x).strip()
                    if xx:
                        required.add(xx)
        except Exception:
            pass
        # Synthesized inputs (e.g., seed/node_name) are *fields*, not chain artifacts.
        # Filter them out even if a plugin contract mistakenly lists them in `requires`.
        try:
            required = {x for x in required if x not in _flow_synthesized_inputs()}
        except Exception:
            pass
        return required

    def _artifact_produces_of(gen: dict[str, Any]) -> set[str]:
        provides: set[str] = set()
        try:
            pid = str(gen.get('id') or '').strip()
            plugin = plugins_by_id.get(pid)
            if isinstance(plugin, dict) and isinstance(plugin.get('produces'), list):
                for item in (plugin.get('produces') or []):
                    if not isinstance(item, dict):
                        continue
                    a = str(item.get('artifact') or '').strip()
                    if a:
                        provides.add(a)
        except Exception:
            pass
        return provides

    def _required_input_fields_of(gen: dict[str, Any]) -> set[str]:
        required: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    name = str(inp.get('name') or '').strip()
                    if not name:
                        continue
                    if inp.get('required') is False:
                        continue
                    required.add(name)
        except Exception:
            pass
        return required

    def _all_input_fields_of(gen: dict[str, Any]) -> set[str]:
        fields: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    name = str(inp.get('name') or '').strip()
                    if name:
                        fields.add(name)
        except Exception:
            pass
        return fields

    def _output_fields_of(gen: dict[str, Any]) -> set[str]:
        out_fields: set[str] = set()
        try:
            outputs = gen.get('outputs')
            if isinstance(outputs, list):
                for outp in outputs:
                    if not isinstance(outp, dict):
                        continue
                    nm = str(outp.get('name') or '').strip()
                    if nm:
                        out_fields.add(nm)
        except Exception:
            pass
        return out_fields

    def _provides_of(gen: dict[str, Any]) -> set[str]:
        provides: set[str] = set()
        try:
            provides |= _artifact_produces_of(gen)
        except Exception:
            pass
        try:
            prov = gen.get('provides')
            if isinstance(prov, list):
                for x in prov:
                    s = str(x).strip()
                    if s:
                        provides.add(s)
        except Exception:
            pass
        try:
            provides |= _output_fields_of(gen)
        except Exception:
            pass
        return provides

    out_assignments: list[dict[str, Any]] = []
    for i, cid in enumerate(chain_ids):
        req = fas_in[i] if i < len(fas_in) else {}
        if not isinstance(req, dict):
            return jsonify({'ok': False, 'error': 'flag_assignments entries must be objects.'}), 400
        node_id = str(req.get('node_id') or '').strip()
        if node_id != str(cid):
            return jsonify({'ok': False, 'error': 'flag_assignments must align to chain_ids (node_id mismatch).'}), 400
        gen_id = str(req.get('id') or req.get('generator_id') or '').strip()
        if not gen_id:
            return jsonify({'ok': False, 'error': f'Missing generator id for node {node_id}.'}), 400
        gen = gen_by_id.get(gen_id)
        if not isinstance(gen, dict):
            return jsonify({'ok': False, 'error': f'Generator not found/enabled: {gen_id}'}), 422

        node = chain_nodes[i] if i < len(chain_nodes) else {}
        is_vuln_node = bool(node.get('is_vuln'))
        kind = str(gen.get('_flow_kind') or 'flag-generator')
        if is_vuln_node and kind != 'flag-generator':
            return jsonify({'ok': False, 'error': f'Generator {gen_id} is not compatible with vulnerability node {node_id} (must be flag-generator).'}), 422

        next_id = chain_ids[i + 1] if (i + 1) < len(chain_ids) else ''
        hint_templates = _flow_hint_templates_from_generator(gen)
        hint_tpl = hint_templates[0] if hint_templates else 'Next: {{NEXT_NODE_NAME}}'
        rendered_hints = [
            _flow_render_hint_template(t, scenario_label=(scenario_label or scenario_norm), id_to_name=id_to_name, this_id=str(node_id), next_id=str(next_id))
            for t in (hint_templates or [])
        ]

        requires_artifacts = sorted(list(_artifact_requires_of(gen)))
        produces_artifacts = sorted(list(_artifact_produces_of(gen)))
        input_fields_required = sorted(list(_required_input_fields_of(gen)))
        input_fields_all = sorted(list(_all_input_fields_of(gen)))
        input_fields_optional = sorted([x for x in input_fields_all if x and x not in set(input_fields_required)])
        output_fields = sorted(list(_output_fields_of(gen)))

        # If an artifact "requires" token also appears as an optional input field,
        # treat it as optional (exclude from effective chaining requirements).
        try:
            optional_field_set = set(input_fields_optional)
            requires_effective = [x for x in (requires_artifacts or []) if x and x not in optional_field_set]
        except Exception:
            requires_effective = list(requires_artifacts or [])

        # Effective union for chaining.
        inputs_effective = sorted(list(set(requires_effective) | set(input_fields_required)))
        outputs_effective = sorted(list(_provides_of(gen)))

        out_assignments.append({
            'node_id': str(node_id),
            'id': str(gen.get('id') or ''),
            'name': str(gen.get('name') or ''),
            'type': kind,
            'flag_generator': str(gen.get('_source_name') or '').strip() or 'unknown',
            'generator_catalog': str(gen.get('_flow_catalog') or 'flag_generators'),
            'language': str(gen.get('language') or ''),
            'description_hints': list(gen.get('description_hints') or []) if isinstance(gen.get('description_hints'), list) else [],
            'inputs': inputs_effective,
            'outputs': outputs_effective,
            'requires': requires_artifacts,
            'produces': produces_artifacts,
            'input_fields': input_fields_all,
            'input_fields_required': input_fields_required,
            'input_fields_optional': input_fields_optional,
            'output_fields': output_fields,
            'hint_template': hint_tpl,
            'hint_templates': hint_templates,
            'hint': rendered_hints[0] if rendered_hints else _flow_render_hint_template(hint_tpl, scenario_label=(scenario_label or scenario_norm), id_to_name=id_to_name, this_id=str(node_id), next_id=str(next_id)),
            'hints': rendered_hints,
            'next_node_id': str(next_id),
            'next_node_name': str(id_to_name.get(str(next_id)) or ''),
        })

    # Validate (non-blocking)
    try:
        flow_valid, flow_errors = _flow_validate_chain_order_by_requires_produces(
            chain_nodes,
            out_assignments,
            scenario_label=(scenario_label or scenario_norm),
        )
    except Exception:
        flow_valid, flow_errors = True, []
    flags_enabled = bool(flow_valid)

    # Persist as plan_from_flow_*.json
    try:
        persisted_flag_assignments = _flow_strip_runtime_sensitive_fields(out_assignments)
        flow_meta = {
            'source_preview_plan_path': base_plan_path,
            'scenario': scenario_label or scenario_norm,
            'length': len(chain_nodes),
            'requested_length': len(chain_nodes),
            'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or '')} for n in chain_nodes],
            'flag_assignments': persisted_flag_assignments,
            'flags_enabled': bool(flags_enabled),
            'flow_valid': bool(flow_valid),
            'flow_errors': list(flow_errors or []),
            'modified_at': _iso_now(),
        }
        if isinstance(meta, dict):
            meta2 = dict(meta)
            meta2['flow'] = flow_meta
        else:
            meta2 = {'flow': flow_meta}
    except Exception:
        meta2 = meta

    try:
        plans_dir = Path(_outputs_dir()) / 'plans'
        plans_dir.mkdir(parents=True, exist_ok=True)
        seed = 0
        try:
            seed = int((preview.get('seed') if isinstance(preview, dict) else None) or (meta2.get('seed') if isinstance(meta2, dict) else None) or 0)
        except Exception:
            seed = 0
        suffix = secrets.token_hex(3)
        out_name = f"plan_from_flow_{seed}_{int(time.time())}_{suffix}.json"
        out_path = str(plans_dir / out_name)
        out_payload = {
            'full_preview': preview,
            'metadata': meta2,
        }
        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(out_payload, f, indent=2)
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to persist flow-modified preview plan: {e}'}), 500

    # Stats for UI.
    try:
        stats = _flow_compose_docker_stats(nodes)
    except Exception:
        stats = {}

    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'length': len(chain_nodes),
        'stats': stats,
        'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or ''), 'is_vuln': bool(n.get('is_vuln'))} for n in chain_nodes],
        'flag_assignments': out_assignments,
        'flags_enabled': bool(flags_enabled),
        'flow_valid': bool(flow_valid),
        'flow_errors': list(flow_errors or []),
        'preview_plan_path': base_plan_path,
        'flow_plan_path': out_path,
        'base_preview_plan_path': base_plan_path,
    })


@app.route('/api/flag-sequencing/substitution_candidates', methods=['POST'])
def api_flow_substitution_candidates():
    """Return candidate generators with per-position compatibility info.

    The client uses this to gray out incompatible generators and show why.
    Compatibility here means: for the chain *prefix* (positions < index), the
    candidate's required artifacts/fields are satisfied.
    """
    j = request.get_json(silent=True) or {}
    scenario_label = str(j.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    index_raw = j.get('index')
    try:
        index = int(index_raw)
    except Exception:
        return jsonify({'ok': False, 'error': 'Invalid index.'}), 400
    if index < 0:
        return jsonify({'ok': False, 'error': 'Invalid index.'}), 400

    chain_ids_in = j.get('chain_ids')
    if not isinstance(chain_ids_in, list) or not chain_ids_in:
        return jsonify({'ok': False, 'error': 'Missing chain_ids.'}), 400
    chain_ids: list[str] = [str(x or '').strip() for x in chain_ids_in if str(x or '').strip()]
    if not chain_ids:
        return jsonify({'ok': False, 'error': 'Missing chain_ids.'}), 400
    if index >= len(chain_ids):
        return jsonify({'ok': False, 'error': 'Index out of range.'}), 400

    kind = str(j.get('kind') or 'flag-generator').strip() or 'flag-generator'
    if kind not in {'flag-generator', 'flag-node-generator'}:
        kind = 'flag-generator'

    base_plan_path = str(j.get('preview_plan') or '').strip() or None
    if base_plan_path:
        try:
            base_plan_path = os.path.abspath(base_plan_path)
            plans_dir = os.path.abspath(os.path.join(_outputs_dir(), 'plans'))
            if os.path.commonpath([base_plan_path, plans_dir]) != plans_dir:
                base_plan_path = None
            elif not os.path.exists(base_plan_path):
                base_plan_path = None
        except Exception:
            base_plan_path = None
    if not base_plan_path:
        base_plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm, prefer_flow=False)
    if not base_plan_path or not os.path.exists(base_plan_path):
        return jsonify({'ok': False, 'error': 'No preview plan found for this scenario. Generate a Full Preview first.'}), 404

    try:
        with open(base_plan_path, 'r', encoding='utf-8') as f:
            payload = json.load(f) or {}
        preview = payload.get('full_preview') if isinstance(payload, dict) else None
        if not isinstance(preview, dict):
            return jsonify({'ok': False, 'error': 'Preview plan is missing full_preview.'}), 422
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to load preview plan: {e}'}), 500

    # Build chain nodes and check the target node vulnerability.
    try:
        nodes, _links, _adj = _build_topology_graph_from_preview_plan(preview)
    except Exception:
        nodes = []
    id_map = {str(n.get('id') or '').strip(): n for n in (nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()}
    chain_nodes: list[dict[str, Any]] = []
    for cid in chain_ids:
        n = id_map.get(str(cid))
        if not isinstance(n, dict):
            return jsonify({'ok': False, 'error': f'Chain node not found in preview plan: {cid}'}), 422
        chain_nodes.append(n)
    is_vuln_node = bool((chain_nodes[index] if index < len(chain_nodes) else {}).get('is_vuln'))
    if is_vuln_node:
        kind = 'flag-generator'

    # Parse the current chain assignments (ids only) so we can compute prefix outputs.
    fas_in = j.get('flag_assignments')
    if not isinstance(fas_in, list) or len(fas_in) != len(chain_nodes):
        return jsonify({'ok': False, 'error': 'flag_assignments must be a list aligned to chain_ids (same length).'}), 400
    cur_ids_by_node: dict[str, str] = {}
    for i, cid in enumerate(chain_ids):
        req = fas_in[i] if i < len(fas_in) else {}
        if not isinstance(req, dict):
            continue
        node_id = str(req.get('node_id') or '').strip()
        if node_id != str(cid):
            continue
        gen_id = str(req.get('id') or req.get('generator_id') or '').strip()
        if gen_id:
            cur_ids_by_node[node_id] = gen_id

    # Candidate generator ids (client-filtered).
    cand_ids_in = j.get('candidate_ids')
    candidate_ids: list[str] = []
    if isinstance(cand_ids_in, list) and cand_ids_in:
        for x in cand_ids_in:
            s = str(x or '').strip()
            if s:
                candidate_ids.append(s)
    candidate_ids = list(dict.fromkeys(candidate_ids))

    try:
        gens, _ = _flag_generators_from_enabled_sources()
    except Exception:
        gens = []
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
    except Exception:
        node_gens = []

    # Build generator index by id, annotated with kind.
    gen_by_id: dict[str, dict[str, Any]] = {}
    for g in (gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid and gid not in gen_by_id:
            gg = dict(g)
            gg['_flow_kind'] = 'flag-generator'
            gen_by_id[gid] = gg
    for g in (node_gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid and gid not in gen_by_id:
            gg = dict(g)
            gg['_flow_kind'] = 'flag-node-generator'
            gen_by_id[gid] = gg

    try:
        plugins_by_id = _flow_enabled_plugin_contracts_by_id()
    except Exception:
        plugins_by_id = {}

    def _artifact_requires_of(gen: dict[str, Any]) -> set[str]:
        required: set[str] = set()
        try:
            pid = str(gen.get('id') or '').strip()
            plugin = plugins_by_id.get(pid)
            if isinstance(plugin, dict) and isinstance(plugin.get('requires'), list):
                for x in (plugin.get('requires') or []):
                    xx = str(x).strip()
                    if xx:
                        required.add(xx)
        except Exception:
            pass
        # Synthesized inputs (e.g., seed/node_name) are *fields*, not chain artifacts.
        # Filter them out even if a plugin contract mistakenly lists them in `requires`.
        try:
            required = {x for x in required if x not in _flow_synthesized_inputs()}
        except Exception:
            pass
        return required

    def _artifact_produces_of(gen: dict[str, Any]) -> set[str]:
        provides: set[str] = set()
        try:
            pid = str(gen.get('id') or '').strip()
            plugin = plugins_by_id.get(pid)
            if isinstance(plugin, dict) and isinstance(plugin.get('produces'), list):
                for item in (plugin.get('produces') or []):
                    if not isinstance(item, dict):
                        continue
                    a = str(item.get('artifact') or '').strip()
                    if a:
                        provides.add(a)
        except Exception:
            pass
        return provides

    def _required_input_fields_of(gen: dict[str, Any]) -> set[str]:
        required: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    name = str(inp.get('name') or '').strip()
                    if not name:
                        continue
                    if inp.get('required') is False:
                        continue
                    required.add(name)
        except Exception:
            pass
        return required

    def _all_input_fields_of(gen: dict[str, Any]) -> set[str]:
        fields: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    name = str(inp.get('name') or '').strip()
                    if name:
                        fields.add(name)
        except Exception:
            pass
        return fields

    def _output_fields_of(gen: dict[str, Any]) -> set[str]:
        out_fields: set[str] = set()
        try:
            outputs = gen.get('outputs')
            if isinstance(outputs, list):
                for outp in outputs:
                    if not isinstance(outp, dict):
                        continue
                    nm = str(outp.get('name') or '').strip()
                    if nm:
                        out_fields.add(nm)
        except Exception:
            pass
        return out_fields

    # Compute prefix availability (artifacts + fields) from current assignments.
    # Fields the sequencer provides regardless of earlier chain outputs.
    synthesized_fields = {
        'seed',
        'secret',
        'env_name',
        'challenge',
        'flag_prefix',
        'username_prefix',
        'key_len',
        'node_name',
    }
    have_artifacts: set[str] = set()
    have_fields: set[str] = set(synthesized_fields)

    for pos in range(0, max(0, index)):
        node_id = str(chain_ids[pos])
        gen_id = cur_ids_by_node.get(node_id)
        if not gen_id:
            continue
        g = gen_by_id.get(gen_id)
        if not isinstance(g, dict):
            continue
        try:
            have_artifacts |= _artifact_produces_of(g)
        except Exception:
            pass
        try:
            have_fields |= _output_fields_of(g)
        except Exception:
            pass

    def _blocked_reasons(gen: dict[str, Any]) -> tuple[bool, list[str]]:
        reasons: list[str] = []
        try:
            gkind = str(gen.get('_flow_kind') or '').strip() or 'flag-generator'
            if is_vuln_node and gkind != 'flag-generator':
                reasons.append('Flag-Generator type')
        except Exception:
            pass
        req_a = sorted([x for x in _artifact_requires_of(gen) if x])
        req_f = sorted([x for x in _required_input_fields_of(gen) if x])

        # If an artifact requirement is also present as an *optional* input field,
        # treat it as optional (do not block compatibility on it). This helps
        # avoid flagging things like credential.pair as a hard missing dependency
        # when the generator can operate without it.
        try:
            all_in = _all_input_fields_of(gen)
            req_in = _required_input_fields_of(gen)
            optional_in = set(all_in) - set(req_in)
        except Exception:
            optional_in = set()
        req_a_effective = [x for x in req_a if x not in optional_in]

        missing_a = [x for x in req_a_effective if x not in have_artifacts]
        # Never report sequencer-provided fields as missing.
        missing_f = [x for x in req_f if x not in have_fields and x not in synthesized_fields]
        if missing_a:
            reasons.append('missing inputs (artifacts): ' + ', '.join(missing_a))
        if missing_f:
            reasons.append('missing inputs (fields): ' + ', '.join(missing_f))
        return (len(reasons) == 0), reasons

    out: list[dict[str, Any]] = []
    ids_to_eval = candidate_ids if candidate_ids else list(gen_by_id.keys())
    for gid in ids_to_eval:
        gen = gen_by_id.get(str(gid))
        if not isinstance(gen, dict):
            continue
        gkind = str(gen.get('_flow_kind') or '').strip() or 'flag-generator'
        if gkind != kind:
            continue
        ok, reasons = _blocked_reasons(gen)
        out.append({
            'id': str(gen.get('id') or ''),
            'name': str(gen.get('name') or ''),
            'type': gkind,
            'source': str(gen.get('_source_name') or '').strip() or 'unknown',
            'compatible': bool(ok),
            'blocked_by': reasons,
        })

    out.sort(key=lambda e: (
        0 if bool(e.get('compatible')) else 1,
        str(e.get('name') or '').lower(),
        str(e.get('id') or ''),
    ))

    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'kind': kind,
        'index': index,
        'is_vuln': bool(is_vuln_node),
        'candidates': out,
        'preview_plan_path': base_plan_path,
    })


@app.route('/api/flag-sequencing/bundle_from_chain', methods=['POST'])
def api_flow_bundle_from_chain():
    """Deprecated: STIX bundle export has been removed in favor of .afb."""
    return jsonify({
        'ok': False,
        'error': 'STIX bundle export has been removed. Use /api/flag-sequencing/afb_from_chain.',
    }), 410


@app.route('/api/flag-sequencing/afb_from_chain', methods=['POST'])
def api_flow_afb_from_chain():
    """Build an Attack Flow Builder .afb document from a user-specified ordered chain."""
    j = request.get_json(silent=True) or {}
    scenario_label = str(j.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    chain = j.get('chain')
    if not isinstance(chain, list) or not chain:
        return jsonify({'ok': False, 'error': 'Missing chain.'}), 400

    chain_nodes: list[dict[str, Any]] = []
    for n in chain:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if not nid:
            continue
        chain_nodes.append({
            'id': nid,
            'name': str(n.get('name') or nid),
            'type': str(n.get('type') or ''),
            'compose': str(n.get('compose') or ''),
            'compose_name': str(n.get('compose_name') or ''),
        })
    if not chain_nodes:
        return jsonify({'ok': False, 'error': 'Chain contained no valid nodes.'}), 400

    flag_assignments: list[dict[str, Any]] = []
    preview: dict[str, Any] | None = None
    try:
        plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm)
        if plan_path and os.path.exists(plan_path):
            with open(plan_path, 'r', encoding='utf-8') as f:
                payload = json.load(f) or {}
            preview = payload.get('full_preview') if isinstance(payload, dict) else None
            if isinstance(preview, dict):
                flag_assignments = _flow_compute_flag_assignments(preview, chain_nodes, scenario_label or scenario_norm)
    except Exception:
        flag_assignments = []

    try:
        flow_valid, flow_errors = _flow_validate_chain_order_by_requires_produces(
            chain_nodes,
            flag_assignments,
            scenario_label=(scenario_label or scenario_norm),
        )
    except Exception:
        flow_valid, flow_errors = True, []
    flags_enabled = bool(flow_valid)

    afb = _attack_flow_builder_afb_for_chain(
        chain_nodes=chain_nodes,
        scenario_label=scenario_label or scenario_norm,
        flag_assignments=flag_assignments,
    )
    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'length': len(chain_nodes),
        'chain': chain_nodes,
        'flag_assignments': flag_assignments,
        'afb': afb,
        'flow_valid': bool(flow_valid),
        'flow_errors': list(flow_errors or []),
        'flags_enabled': bool(flags_enabled),
    })


@app.route('/api/flag-sequencing/attackflow')
def api_flow_attackflow():
    return jsonify({
        'ok': False,
        'error': 'STIX/AttackFlow bundle export has been removed. Use /api/flag-sequencing/attackflow_preview for chain preview and /api/flag-sequencing/afb_from_chain for Attack Flow Builder export.',
    }), 410


@app.route('/users', methods=['GET'])
def users_page():
    if not _require_admin():
        return redirect(url_for('index'))
    db = _load_users()
    raw_users = db.get('users', [])
    admin_count = sum(
        1
        for entry in raw_users
        if isinstance(entry, dict) and _normalize_role_value(entry.get('role')) == 'admin'
    )
    scenario_names, _scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(user=_current_user())
    scenario_options: list[dict[str, str]] = []
    display_by_norm: dict[str, str] = {}
    for display_name in scenario_names:
        norm = _normalize_scenario_label(display_name)
        if not norm:
            continue
        display_by_norm[norm] = display_name
        scenario_options.append({'value': norm, 'label': display_name})
    # Ensure stable alphabetical order
    scenario_options.sort(key=lambda o: o['label'].lower())
    users: list[dict] = []
    for entry in raw_users:
        if not isinstance(entry, dict):
            continue
        normalized = dict(entry)
        normalized['role'] = _normalize_role_value(entry.get('role'))
        assigned = _normalize_scenario_assignments(entry.get('scenarios'))
        normalized['assigned_scenarios'] = assigned
        normalized['assigned_scenarios_display'] = [display_by_norm.get(norm, norm) for norm in assigned]
        is_only_admin = normalized['role'] == 'admin' and admin_count <= 1
        normalized['role_locked'] = is_only_admin
        if is_only_admin:
            normalized['role_locked_reason'] = 'At least one admin must remain.'
        else:
            normalized['role_locked_reason'] = ''
        users.append(normalized)
    return render_template(
        'users.html',
        users=users,
        scenario_options=scenario_options,
        scenario_lookup=display_by_norm,
        self_change=False,
    )


@app.route('/users', methods=['POST'])
def users_create():
    if not _require_admin():
        return redirect(url_for('index'))
    username = (request.form.get('username') or '').strip()
    password = (request.form.get('password') or '').strip()
    role = _normalize_role_value(request.form.get('role'))
    scenarios = _normalize_scenario_assignments(request.form.getlist('scenarios'))
    if not username or not password:
        flash('Username and password required')
        return redirect(url_for('users_page'))
    db = _load_users()
    users = db.get('users', [])
    if any(u.get('username') == username for u in users):
        flash('Username already exists')
        return redirect(url_for('users_page'))
    users.append({
        'username': username,
        'password_hash': generate_password_hash(password),
        'role': role,
        'scenarios': scenarios,
    })
    db['users'] = users
    _save_users(db)
    flash('User created')
    return redirect(url_for('users_page'))


@app.route('/users/delete/<username>', methods=['POST'])
def users_delete(username: str):
    if not _require_admin():
        return redirect(url_for('users_page'))
    username = (username or '').strip()
    if not username:
        flash('Invalid username')
        return redirect(url_for('users_page'))
    cur = _current_user()
    db = _load_users()
    users = db.get('users', [])
    remain = [u for u in users if u.get('username') != username]
    if cur and username == cur.get('username'):
        flash('Cannot delete your own account')
        return redirect(url_for('users_page'))
    if not any(u.get('role') == 'admin' for u in remain):
        flash('At least one admin must remain')
        return redirect(url_for('users_page'))
    db['users'] = remain
    _save_users(db)
    flash('User deleted')
    return redirect(url_for('users_page'))


@app.route('/users/password/<username>', methods=['POST'])
def users_password(username: str):
    if not _require_admin():
        return redirect(url_for('users_page'))
    new_pwd = request.form.get('password') or ''
    if not new_pwd:
        flash('New password required')
        return redirect(url_for('users_page'))
    db = _load_users()
    changed = False
    for u in db.get('users', []):
        if u.get('username') == username:
            u['password_hash'] = generate_password_hash(new_pwd)
            changed = True
            break
    if changed:
        _save_users(db)
        flash('Password updated')
    else:
        flash('User not found')
    return redirect(url_for('users_page'))


@app.route('/users/role/<username>', methods=['POST'])
def users_update_role(username: str):
    if not _require_admin():
        return redirect(url_for('users_page'))
    username = (username or '').strip()
    role_value = _normalize_role_value(request.form.get('role'))
    if not username or role_value not in _ALLOWED_USER_ROLES:
        flash('Invalid role update request')
        return redirect(url_for('users_page'))
    db = _load_users()
    users = db.get('users', [])
    target = next((u for u in users if u.get('username') == username), None)
    if not target:
        flash('User not found')
        return redirect(url_for('users_page'))
    current_role = _normalize_role_value(target.get('role'))
    if current_role == role_value:
        flash('Role unchanged')
        return redirect(url_for('users_page'))
    if current_role == 'admin' and role_value != 'admin':
        has_other_admin = any(_normalize_role_value(u.get('role')) == 'admin' and u.get('username') != username for u in users)
        if not has_other_admin:
            flash('At least one admin must remain')
            return redirect(url_for('users_page'))
    target['role'] = role_value
    db['users'] = users
    _save_users(db)
    cur = _current_user()
    if cur and cur.get('username') == username:
        _set_current_user({'username': username, 'role': role_value})
    flash(f"Role updated to {role_value}")
    return redirect(url_for('users_page'))


@app.route('/users/scenarios/<username>', methods=['POST'])
def users_assign_scenarios(username: str):
    if not _require_admin():
        return redirect(url_for('users_page'))
    username = (username or '').strip()
    if not username:
        flash('Invalid username')
        return redirect(url_for('users_page'))
    selections = _normalize_scenario_assignments(request.form.getlist('scenarios'))
    db = _load_users()
    users = db.get('users', [])
    updated = False
    for entry in users:
        if entry.get('username') == username:
            entry['scenarios'] = selections
            updated = True
            break
    if updated:
        _save_users(db)
        flash('Scenario assignments updated')
    else:
        flash('User not found')
    return redirect(url_for('users_page'))


@app.route('/me/password', methods=['GET', 'POST'])
def me_password():
    if _current_user() is None:
        return redirect(url_for('login'))
    if request.method == 'GET':
        return render_template('users.html', self_change=True)
    cur = _current_user()
    cur_pwd = request.form.get('current_password') or ''
    new_pwd = request.form.get('password') or ''
    if not cur_pwd or not new_pwd:
        flash('Current and new passwords required')
        return redirect(url_for('me_password'))
    db = _load_users()
    updated = False
    for u in db.get('users', []):
        if u.get('username') == cur.get('username'):
            if not check_password_hash(u.get('password_hash', ''), cur_pwd):
                flash('Current password incorrect')
                return redirect(url_for('me_password'))
            u['password_hash'] = generate_password_hash(new_pwd)
            updated = True
            break
    if updated:
        _save_users(db)
        flash('Password changed')
    else:
        flash('User not found')
    return redirect(url_for('index'))


@app.route('/healthz')
def healthz():
    return Response('ok', mimetype='text/plain')


# Environment-configurable CORE daemon location (useful inside Docker)
CORE_HOST = os.environ.get('CORE_HOST', 'localhost')
try:
    CORE_PORT = int(os.environ.get('CORE_PORT', '50051'))
except Exception:
    CORE_PORT = 50051

def _default_core_dict():
    return _core_backend_defaults(include_password=False)


def _resolve_cli_venv_bin(
    preferred: Optional[str] = None,
    *,
    allow_fallback: bool = True,
) -> Optional[str]:
    """Return an existing venv/bin directory for local CLI invocations."""

    sanitized_preferred = _sanitize_venv_bin_path(preferred)
    if sanitized_preferred:
        if os.path.isdir(sanitized_preferred):
            return sanitized_preferred
        if not allow_fallback:
            return None

    fallback_candidates = (
        _sanitize_venv_bin_path(os.environ.get('CORE_VENV_BIN')),
        _sanitize_venv_bin_path(DEFAULT_CORE_VENV_BIN),
    )
    for candidate in fallback_candidates:
        if candidate and os.path.isdir(candidate):
            return candidate
    return None


def _prepare_cli_env(
    base_env: Optional[Dict[str, str]] = None,
    preferred_venv_bin: Optional[str] = None,
    *,
    allow_fallback: bool = True,
) -> Dict[str, str]:
    env = dict(base_env or os.environ)
    venv_bin = _resolve_cli_venv_bin(preferred_venv_bin, allow_fallback=allow_fallback)
    if venv_bin:
        original_path = env.get('PATH') or ''
        env['PATH'] = f"{venv_bin}:{original_path}" if original_path else venv_bin
        venv_root = os.path.dirname(venv_bin)
        if venv_root:
            env.setdefault('VIRTUAL_ENV', venv_root)
    return env


def _select_python_interpreter(preferred_venv_bin: Optional[str] = None) -> str:
    """Select the python interpreter to invoke the core_topo_gen CLI.

    Priority order:
    1. Explicit environment override CORE_PY (absolute path wins, otherwise treated as first lookup candidate)
    2. Interpreter binaries that live inside the preferred/core-configured venv bin (falling back to CORE_VENV_BIN env var and /opt/core/venv/bin)
    3. 'core-python', then 'python3', then 'python' discovered via PATH (with the venv bin prepended to PATH)
    4. sys.executable as a final fallback

    Returns the chosen executable string (absolute path or name)."""

    override = os.environ.get('CORE_PY')
    venv_bin = _resolve_cli_venv_bin(preferred_venv_bin)

    name_candidates: list[str] = []

    if override:
        if os.path.isabs(override):
            try:
                if os.access(override, os.X_OK):
                    return override
            except Exception:
                pass
        else:
            name_candidates.append(override)

    if venv_bin:
        preferred_python = _find_python_in_venv_bin(venv_bin)
        if preferred_python:
            return preferred_python

    name_candidates.extend(PYTHON_EXECUTABLE_NAMES)

    search_path = os.environ.get('PATH') or ''
    if venv_bin:
        search_path = f"{venv_bin}:{search_path}" if search_path else venv_bin

    for name in name_candidates:
        try:
            resolved = shutil.which(name, path=search_path)
        except Exception:
            resolved = None
        if resolved:
            return resolved

    return sys.executable or 'python'

def _get_cli_script_path() -> str:
    """Return absolute path to config2scen_core_grpc.py script."""
    return os.path.join(_get_repo_root(), 'config2scen_core_grpc.py')

# Now that helpers can resolve repo root, configure upload folder
UPLOAD_FOLDER = _uploads_dir()
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

UPLOAD_FOLDER = _uploads_dir()
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


# In-memory registry for async runs
RUNS: Dict[str, Dict[str, Any]] = {}

# Run history persistence (simple JSON log)
RUN_HISTORY_PATH = os.path.join(_outputs_dir(), 'run_history.json')
RUN_HISTORY_LOCK = threading.RLock()

def _load_run_history():
    try:
        with RUN_HISTORY_LOCK:
            if os.path.exists(RUN_HISTORY_PATH):
                with open(RUN_HISTORY_PATH, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        return [_normalize_run_history_entry(item) for item in data if isinstance(item, dict)]
                    return []
    except Exception:
        try:
            app.logger.exception('[run_history] failed reading %s', RUN_HISTORY_PATH)
        except Exception:
            pass
    return []

def _append_run_history(entry: dict) -> bool:
    """Append a run history entry to outputs/run_history.json.

    Returns True on success, False on failure.
    """
    # Enforce per-scenario semantics at the write boundary.
    try:
        entry = _normalize_run_history_entry(entry)
    except Exception:
        entry = entry if isinstance(entry, dict) else {}

    run_id = None
    try:
        run_id = (entry.get('run_id') or '').strip() if isinstance(entry.get('run_id'), str) else None
    except Exception:
        run_id = None

    os.makedirs(os.path.dirname(RUN_HISTORY_PATH), exist_ok=True)
    tmp = RUN_HISTORY_PATH + '.tmp'
    with RUN_HISTORY_LOCK:
        history = _load_run_history()
        if run_id:
            try:
                for existing in history:
                    if not isinstance(existing, dict):
                        continue
                    existing_id = existing.get('run_id')
                    if isinstance(existing_id, str) and existing_id.strip() == run_id:
                        return True
            except Exception:
                pass
        history.append(entry)
        try:
            with open(tmp, 'w', encoding='utf-8') as f:
                # Use default=str to avoid silently dropping entries due to an unexpected
                # non-serializable value. Paths/objects become strings.
                json.dump(history, f, indent=2, default=str)
            os.replace(tmp, RUN_HISTORY_PATH)
            return True
        except Exception:
            try:
                app.logger.exception('[run_history] failed writing %s', RUN_HISTORY_PATH)
            except Exception:
                pass
            try:
                if os.path.exists(tmp):
                    os.remove(tmp)
            except Exception:
                pass
            return False


EDITOR_STATE_SNAPSHOT_SUBDIR = 'editor_snapshots'


def _editor_state_snapshot_dir() -> str:
    return _ensure_private_dir(os.path.join(_outputs_dir(), EDITOR_STATE_SNAPSHOT_SUBDIR))


def _editor_state_snapshot_slug(username: Optional[str]) -> str:
    raw = (username or '').strip().lower()
    cleaned = re.sub(r'[^a-z0-9._-]+', '-', raw)
    cleaned = re.sub(r'-{2,}', '-', cleaned).strip('-_.')
    return cleaned or 'anonymous'


def _editor_state_snapshot_path(owner: dict | str | None = None) -> str:
    username: Optional[str]
    if isinstance(owner, dict):
        username = owner.get('username') if owner else None
    elif isinstance(owner, str):
        username = owner
    else:
        username = None
    slug = _editor_state_snapshot_slug(username)
    return os.path.join(_editor_state_snapshot_dir(), f"{slug}.json")


def _sanitize_snapshot_scenario(raw: Any) -> Optional[Dict[str, Any]]:
    if not isinstance(raw, dict):
        return None
    scenario = copy.deepcopy(raw)
    hitl_meta = scenario.get('hitl') if isinstance(scenario.get('hitl'), dict) else None
    if hitl_meta is not None:
        # Only persist HITL fields after verification.
        # Credentials and HITL mappings are saved via explicit validation/apply endpoints.

        prox_meta = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else None
        prox_validated = False
        if prox_meta is not None:
            secret_id = (prox_meta.get('secret_id') or '').strip() if isinstance(prox_meta.get('secret_id'), str) else ''
            prox_validated = bool(prox_meta.get('validated')) and bool(secret_id)
            if prox_validated:
                prox_clean = {
                    k: v
                    for k, v in prox_meta.items()
                    if k not in {'password', 'token_secret', 'api_secret', 'api_token_secret', 'inventory_error'}
                }
                hitl_meta['proxmox'] = prox_clean
            else:
                hitl_meta.pop('proxmox', None)

        core_meta_raw = hitl_meta.get('core') if isinstance(hitl_meta.get('core'), dict) else None
        core_meta = _extract_optional_core_config(core_meta_raw, include_password=False)
        core_validated = False
        if core_meta:
            secret_id = (core_meta.get('core_secret_id') or '').strip() if isinstance(core_meta.get('core_secret_id'), str) else ''
            core_validated = bool(core_meta.get('validated')) and bool(secret_id)
            if core_validated:
                hitl_meta['core'] = core_meta
            else:
                hitl_meta.pop('core', None)
        else:
            hitl_meta.pop('core', None)

        bridge_validated = bool(hitl_meta.get('bridge_validated'))
        if not bridge_validated:
            # Steps 3–5 are not persisted until HITL bridge verification/apply succeeds.
            hitl_meta.pop('interfaces', None)
            hitl_meta.pop('participant_proxmox_url', None)
            hitl_meta.pop('participant_ui_url', None)
            hitl_meta.pop('participant_url', None)
            hitl_meta['enabled'] = False
        else:
            interfaces = hitl_meta.get('interfaces')
            if isinstance(interfaces, list):
                hitl_meta['interfaces'] = [iface for iface in interfaces if isinstance(iface, dict)]
    return scenario


def _build_editor_snapshot_payload(raw_state: Any) -> Optional[Dict[str, Any]]:
    if not isinstance(raw_state, dict):
        return None
    scenarios_raw = raw_state.get('scenarios')
    if not isinstance(scenarios_raw, list):
        return None
    sanitized_scenarios: List[Dict[str, Any]] = []
    for scen in scenarios_raw:
        sanitized = _sanitize_snapshot_scenario(scen)
        if sanitized:
            sanitized_scenarios.append(sanitized)
    if not sanitized_scenarios:
        return None
    snapshot: Dict[str, Any] = {'scenarios': sanitized_scenarios}
    core_meta = _normalize_core_config(raw_state.get('core'), include_password=False)
    if core_meta:
        snapshot['core'] = core_meta
    result_path = raw_state.get('result_path') or raw_state.get('resultPath')
    if isinstance(result_path, str) and result_path.strip():
        snapshot['result_path'] = result_path.strip()
    base_upload = raw_state.get('base_upload')
    if isinstance(base_upload, dict):
        base_copy: Dict[str, Any] = {}
        path_val = base_upload.get('path') or base_upload.get('filepath')
        if isinstance(path_val, str) and path_val:
            base_copy['path'] = path_val
        display_name = base_upload.get('display_name') or base_upload.get('displayName')
        if isinstance(display_name, str) and display_name:
            base_copy['display_name'] = display_name
        if 'valid' in base_upload:
            base_copy['valid'] = bool(base_upload.get('valid'))
        if base_copy:
            snapshot['base_upload'] = base_copy
    host_ifaces = raw_state.get('host_interfaces')
    if isinstance(host_ifaces, list) and host_ifaces:
        snapshot['host_interfaces'] = [iface for iface in host_ifaces if isinstance(iface, dict)]
    for key in ('host_interfaces_source', 'host_interfaces_metadata', 'host_interfaces_fetched_at'):
        value = raw_state.get(key)
        if value is not None:
            snapshot[key] = value
    project_hint = raw_state.get('project_key_hint')
    if isinstance(project_hint, str) and project_hint.strip():
        snapshot['project_key_hint'] = project_hint.strip()
    elif 'result_path' in snapshot:
        snapshot['project_key_hint'] = snapshot['result_path']
    active_index = raw_state.get('active_index')
    if isinstance(active_index, int):
        snapshot['active_index'] = active_index
    elif isinstance(active_index, str):
        try:
            snapshot['active_index'] = int(active_index)
        except Exception:
            pass
    scenario_query = raw_state.get('scenario_query')
    if isinstance(scenario_query, str) and scenario_query.strip():
        snapshot['scenario_query'] = scenario_query.strip()
    return snapshot


def _write_editor_state_snapshot(snapshot: Dict[str, Any], *, user: Optional[dict] = None) -> None:
    if not snapshot:
        return
    path = _editor_state_snapshot_path(user)
    tmp_path = f"{path}.tmp"
    try:
        with open(tmp_path, 'w', encoding='utf-8') as handle:
            json.dump(snapshot, handle, indent=2)
        os.replace(tmp_path, path)
        _ensure_private_file(path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _persist_editor_state_snapshot(raw_state: Any, *, user: Optional[dict] = None) -> None:
    snapshot = _build_editor_snapshot_payload(raw_state)
    if not snapshot:
        return
    _write_editor_state_snapshot(snapshot, user=user)


def _load_editor_state_snapshot(user: Optional[dict] = None) -> Optional[Dict[str, Any]]:
    path = _editor_state_snapshot_path(user)
    if not os.path.exists(path):
        return None
    try:
        with open(path, 'r', encoding='utf-8') as handle:
            data = json.load(handle)
        if isinstance(data, dict):
            snapshot = _scrub_snapshot_transient_errors(data)
            return _sanitize_snapshot_for_user(snapshot, user)
    except Exception:
        pass
    return None


def _scrub_snapshot_transient_errors(snapshot: Dict[str, Any]) -> Dict[str, Any]:
    scenarios = snapshot.get('scenarios')
    if isinstance(scenarios, list):
        for scen in scenarios:
            if not isinstance(scen, dict):
                continue
            hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else None
            if not hitl_meta:
                continue
            prox_meta = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else None
            if prox_meta:
                prox_meta.pop('inventory_error', None)
    return snapshot


def _sanitize_snapshot_for_user(snapshot: Dict[str, Any], user: Optional[dict]) -> Dict[str, Any]:
    allowed_norms = _builder_allowed_norms(user)
    if allowed_norms is None:
        snapshot.pop('builder_restricted_scenarios', None)
        snapshot.pop('builder_assigned_scenarios', None)
        snapshot.pop('builder_no_assignments', None)
        return snapshot
    filtered = _filter_scenarios_by_norms(snapshot.get('scenarios') or [], allowed_norms)
    snapshot['scenarios'] = filtered
    if filtered:
        active_idx = snapshot.get('active_index')
        if not isinstance(active_idx, int) or active_idx < 0 or active_idx >= len(filtered):
            snapshot['active_index'] = 0
    else:
        snapshot.pop('active_index', None)
    snapshot['builder_restricted_scenarios'] = True
    snapshot['builder_assigned_scenarios'] = sorted(allowed_norms)
    snapshot['builder_no_assignments'] = not bool(allowed_norms)
    return snapshot

def _scenario_names_from_xml(xml_path: str) -> list[str]:
    names: list[str] = []
    try:
        if not xml_path or not os.path.exists(xml_path):
            return names
        data = _parse_scenarios_xml(xml_path)
        for scen in data.get('scenarios', []):
            nm = scen.get('name')
            if nm and nm not in names:
                names.append(nm)
    except Exception:
        pass
    return names


def _normalize_scenario_label(value: Any) -> str:
    if value is None:
        return ''
    text = value if isinstance(value, str) else str(value)
    text = text.strip().lower()
    return re.sub(r'\s+', ' ', text)


def _scenario_match_key(value: Any) -> str:
    """Return a forgiving key for scenario-name comparisons.

    We keep _normalize_scenario_label() stable for display and storage,
    but compare assignments using this match key so punctuation differences
    don't hide scenarios (e.g., 'Scenario-1' vs 'Scenario 1').
    """
    norm = _normalize_scenario_label(value)
    if not norm:
        return ''
    return re.sub(r'[^a-z0-9]+', '', norm)


def _scenario_display_sort_key(value: Any) -> tuple:
    """Natural-ish sort for scenario display strings.

    Examples:
      - Scenario 1 < Scenario 1b < Scenario 2
      - Anything without a number sorts after numbered scenarios.
    """
    if value is None:
        return (1, '', float('inf'), '', '')
    text = value if isinstance(value, str) else str(value)
    text = text.strip()
    lowered = text.lower()
    m = re.search(r'(\d+)([a-z]*)', lowered)
    if not m:
        return (1, lowered, float('inf'), '', lowered)
    try:
        num = int(m.group(1))
    except Exception:
        num = float('inf')
    suffix = m.group(2) or ''
    prefix = lowered[:m.start(1)].strip()
    return (0, prefix, num, suffix, lowered)


def _select_latest_core_secret_record(scenario_norm: str | None = None) -> Optional[Dict[str, Any]]:
    """Find the most recent stored CORE credential, optionally filtered by scenario."""

    try:
        secret_dir = _core_secret_dir()
        entries = os.listdir(secret_dir)
    except Exception:
        return None
    if not entries:
        return None
    best_for_scenario: tuple[float, Dict[str, Any]] | None = None
    best_overall: tuple[float, Dict[str, Any]] | None = None
    for name in entries:
        if not name.endswith('.json'):
            continue
        identifier = name[:-5]
        try:
            record = _load_core_credentials(identifier)
        except Exception:
            continue
        if not record:
            continue
        stored_at = record.get('stored_at')
        ts_val = 0.0
        if isinstance(stored_at, str) and stored_at:
            try:
                ts_val = datetime.datetime.fromisoformat(stored_at.replace('Z', '+00:00')).timestamp()
            except Exception:
                ts_val = 0.0
        if not ts_val:
            try:
                ts_val = os.path.getmtime(os.path.join(secret_dir, name))
            except Exception:
                ts_val = 0.0
        record_norm = _normalize_scenario_label(record.get('scenario_name')) if record.get('scenario_name') else ''
        if scenario_norm and record_norm == scenario_norm:
            if not best_for_scenario or ts_val > best_for_scenario[0]:
                best_for_scenario = (ts_val, record)
        else:
            if not best_overall or ts_val > best_overall[0]:
                best_overall = (ts_val, record)
    if best_for_scenario:
        return best_for_scenario[1]
    return best_overall[1] if best_overall else None


def _scenario_catalog_file() -> str:
    return os.path.join(_outputs_dir(), 'scenario_catalog.json')


def _load_scenario_catalog_from_disk() -> tuple[list[str], dict[str, set[str]], dict[str, str]]:
    path = _scenario_catalog_file()
    if not os.path.exists(path):
        return [], {}, {}
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception:
        return [], {}, {}
    names_raw = data.get('names')
    sources_raw = data.get('sources')
    ordered: list[str] = []
    path_map: dict[str, set[str]] = defaultdict(set)
    seen_norms: set[str] = set()
    participant_by_norm: dict[str, str] = {}

    def _record_candidate(norm_key: str, candidate: Any) -> None:
        if not norm_key or candidate in (None, ''):
            return
        if isinstance(candidate, (list, tuple, set)):
            for item in candidate:
                _record_candidate(norm_key, item)
            return
        value = str(candidate).strip()
        if not value:
            return
        try:
            ap = os.path.abspath(value)
        except Exception:
            ap = value
        path_map[norm_key].add(ap)

    if isinstance(names_raw, list):
        for idx, raw_name in enumerate(names_raw):
            name = (str(raw_name).strip() if raw_name is not None else '')
            if not name:
                continue
            norm = _normalize_scenario_label(name)
            if not norm or norm in seen_norms:
                continue
            seen_norms.add(norm)
            ordered.append(name)
            source_candidate: Any = None
            if isinstance(sources_raw, list) and idx < len(sources_raw):
                source_candidate = sources_raw[idx]
            elif isinstance(sources_raw, dict):
                source_candidate = sources_raw.get(name) or sources_raw.get(norm)
            _record_candidate(norm, source_candidate)

    if isinstance(sources_raw, dict):
        for key, candidate in sources_raw.items():
            norm = _normalize_scenario_label(key)
            if not norm:
                continue
            _record_candidate(norm, candidate)

    participant_raw = data.get('participant_urls')
    if isinstance(participant_raw, dict):
        for key, value in participant_raw.items():
            norm = _normalize_scenario_label(key)
            if not norm:
                continue
            # Preserve explicit "cleared" values (empty/invalid) so the UI can override
            # stale scenario XML that might still contain a participant URL.
            url_value = _normalize_participant_proxmox_url(value)
            if url_value:
                participant_by_norm[norm] = url_value
            else:
                participant_by_norm.setdefault(norm, '')

    return ordered, path_map, participant_by_norm


def _persist_scenario_catalog(
    names: Iterable[Any],
    source_path: Optional[str] = None,
    participant_urls: Optional[Dict[str, Any]] = None,
) -> None:
    catalog_path = _scenario_catalog_file()
    tmp_path = catalog_path + '.tmp'
    ordered: list[str] = []
    seen_norms: set[str] = set()
    participant_map_input = participant_urls or {}
    participant_by_norm: dict[str, str] = {}
    for raw in names or []:
        if raw in (None, ''):
            continue
        name = str(raw).strip()
        if not name:
            continue
        norm = _normalize_scenario_label(name)
        if not norm or norm in seen_norms:
            continue
        seen_norms.add(norm)
        ordered.append(name)
        # Capture participant URL hints for each unique scenario
        if norm in participant_map_input and norm not in participant_by_norm:
            normalized_url = _normalize_participant_proxmox_url(participant_map_input[norm])
            if normalized_url:
                participant_by_norm[norm] = normalized_url
    try:
        abs_source = ''
        if source_path:
            candidate = str(source_path).strip()
            if candidate:
                try:
                    abs_source = os.path.abspath(candidate)
                except Exception:
                    abs_source = candidate
        sources_out = [abs_source for _ in ordered]
        payload = {
            'names': ordered,
            'sources': sources_out,
            'updated_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
        }
        if participant_by_norm:
            payload['participant_urls'] = participant_by_norm
        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as f:
            json.dump(payload, f, indent=2)
        os.replace(tmp_path, catalog_path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _remove_scenarios_from_catalog(names_to_remove: Iterable[Any]) -> Dict[str, Any]:
    """Remove scenario names from outputs/scenario_catalog.json.

    This is the server-side source of truth for which scenarios are listed on
    refresh, so UI-only deletion must update this file.
    """
    catalog_path = _scenario_catalog_file()
    try:
        if not os.path.exists(catalog_path):
            return {'removed': 0, 'remaining': 0}
    except Exception:
        return {'removed': 0, 'remaining': 0}

    norms_to_remove: set[str] = set()
    for raw in names_to_remove or []:
        norm = _normalize_scenario_label(raw)
        if norm:
            norms_to_remove.add(norm)
    if not norms_to_remove:
        return {'removed': 0, 'remaining': 0}

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return {'removed': 0, 'remaining': 0}

        existing_names = payload.get('names')
        if not isinstance(existing_names, list):
            existing_names = []
        before = list(existing_names)
        kept_names: list[str] = []
        kept_norms: set[str] = set()
        for name in before:
            norm = _normalize_scenario_label(name)
            if not norm:
                continue
            if norm in norms_to_remove:
                continue
            if norm in kept_norms:
                continue
            kept_norms.add(norm)
            kept_names.append(str(name))
        removed = max(0, len(before) - len(kept_names))

        payload['names'] = kept_names
        # Keep sources aligned to names length (list form).
        existing_sources = payload.get('sources')
        if isinstance(existing_sources, list):
            payload['sources'] = [existing_sources[0] if existing_sources else '' for _ in kept_names]
        elif isinstance(existing_sources, dict):
            # Filter dict-form sources.
            payload['sources'] = {
                key: val
                for key, val in existing_sources.items()
                if _normalize_scenario_label(key) in kept_norms
            }

        # Prune hint maps keyed by scenario norm.
        for hint_key in ('participant_urls', 'hitl_validation', 'hitl_config'):
            hint_map = payload.get(hint_key)
            if isinstance(hint_map, dict):
                payload[hint_key] = {
                    key: val
                    for key, val in hint_map.items()
                    if _normalize_scenario_label(key) in kept_norms
                }

        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
        return {'removed': removed, 'remaining': len(kept_names)}
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass
        raise


def _delete_saved_scenario_xml_artifacts(names_to_remove: Iterable[Any]) -> Dict[str, Any]:
    """Delete saved scenario XML artifacts under outputs/scenarios-*/.

    These artifacts are re-discovered on page load by _collect_scenario_catalog(),
    so UI deletion must remove them to prevent the scenario from reappearing.
    """
    stems: set[str] = set()
    match_keys: set[str] = set()
    for raw in names_to_remove or []:
        try:
            name = str(raw).strip()
        except Exception:
            name = ''
        if not name:
            continue
        mk = _scenario_match_key(name)
        if mk:
            match_keys.add(mk)
        try:
            stem = secure_filename(name).strip('_-.')
        except Exception:
            stem = ''
        if stem:
            stems.add(stem)
    if not stems and not match_keys:
        return {'artifacts_removed': 0}

    outputs_root = os.path.abspath(_outputs_dir())
    removed = 0
    try:
        if not outputs_root or not os.path.isdir(outputs_root):
            return {'artifacts_removed': 0}
    except Exception:
        return {'artifacts_removed': 0}

    # Mirror discovery logic: scan outputs/scenarios-* folders (newest first, bounded).
    dirs: list[tuple[float, str]] = []
    try:
        for entry in os.listdir(outputs_root):
            if not entry.startswith('scenarios-'):
                continue
            folder = os.path.join(outputs_root, entry)
            if not os.path.isdir(folder):
                continue
            try:
                mtime = os.path.getmtime(folder)
            except Exception:
                mtime = 0.0
            dirs.append((mtime, folder))
    except Exception:
        dirs = []
    dirs.sort(key=lambda t: t[0], reverse=True)

    for _mtime, folder in dirs[:500]:
        # Fast path: attempt stem-based delete when possible.
        # NOTE: Some deployments (and some macOS volumes) can be case-sensitive.
        # The UI typically writes files like "Scenario_1.xml" but secure_filename()
        # yields "scenario_1". Use a case-insensitive match within the folder.
        try:
            folder_entries = os.listdir(folder)
        except Exception:
            folder_entries = []

        stem_targets = {f"{stem}.xml".lower() for stem in stems if stem}
        if stem_targets and folder_entries:
            for fname in list(folder_entries):
                try:
                    if fname.lower() not in stem_targets:
                        continue
                    path = os.path.join(folder, fname)
                    if not os.path.isfile(path):
                        continue
                    p_abs = os.path.abspath(path)
                    if not p_abs.startswith(outputs_root + os.sep):
                        continue
                    os.remove(p_abs)
                    removed += 1
                except Exception:
                    continue

        # Robust path: scan XML contents and delete any matching scenario(s).
        if match_keys and folder_entries:
            for fname in list(folder_entries):
                try:
                    if not fname.lower().endswith('.xml'):
                        continue
                    # Skip CORE pre/post captures; only include user-authored scenario XML.
                    if fname.lower().startswith('core-'):
                        continue
                    # Mirror discovery behavior for scenario editor saves (case-insensitive).
                    if not fname.lower().startswith('scenario_'):
                        continue
                    path = os.path.join(folder, fname)
                    if not os.path.isfile(path):
                        continue
                    try:
                        names_in_file = _scenario_names_from_xml(path)
                    except Exception:
                        names_in_file = []
                    if not names_in_file:
                        continue
                    if not any(_scenario_match_key(nm) in match_keys for nm in names_in_file):
                        continue
                    p_abs = os.path.abspath(path)
                    if not p_abs.startswith(outputs_root + os.sep):
                        continue
                    try:
                        os.remove(p_abs)
                        removed += 1
                    except Exception:
                        continue
                except Exception:
                    continue
        # Best-effort cleanup of empty folders
        try:
            if folder.startswith(outputs_root + os.sep) and os.path.isdir(folder) and not os.listdir(folder):
                os.rmdir(folder)
        except Exception:
            pass

    return {'artifacts_removed': removed}


def _remove_scenarios_from_all_editor_snapshots(names_to_remove: Iterable[Any]) -> Dict[str, Any]:
    """Remove scenario entries from all editor snapshots in outputs/editor_snapshots.

    _collect_scenario_catalog() merges names from all snapshots and treats them as
    protected, so we must purge deleted scenarios from these snapshots.
    """
    remove_norms: set[str] = set()
    remove_match: set[str] = set()
    for raw in names_to_remove or []:
        try:
            s = raw if isinstance(raw, str) else str(raw)
        except Exception:
            s = ''
        s = s.strip() if isinstance(s, str) else ''
        if not s:
            continue
        norm = _normalize_scenario_label(s)
        if norm:
            remove_norms.add(norm)
        mk = _scenario_match_key(s)
        if mk:
            remove_match.add(mk)
    if not remove_norms:
        return {'snapshots_updated': 0, 'snapshots_deleted': 0, 'snapshot_scenarios_removed': 0}

    snap_dir = _editor_state_snapshot_dir()
    try:
        entries = [n for n in os.listdir(snap_dir) if n.endswith('.json')]
    except Exception:
        entries = []

    updated = 0
    deleted = 0
    scenarios_removed = 0
    for fname in entries:
        path = os.path.join(snap_dir, fname)
        try:
            with open(path, 'r', encoding='utf-8') as fh:
                snap = json.load(fh)
        except Exception:
            continue
        if not isinstance(snap, dict):
            continue
        scen_list = snap.get('scenarios')
        if not isinstance(scen_list, list) or not scen_list:
            # Still clear scenario_query if it targets a deleted scenario.
            try:
                q = snap.get('scenario_query')
                if q and (_normalize_scenario_label(q) in remove_norms or _scenario_match_key(q) in remove_match):
                    snap['scenario_query'] = ''
                    tmp = path + '.tmp'
                    with open(tmp, 'w', encoding='utf-8') as fh:
                        json.dump(snap, fh, indent=2)
                    os.replace(tmp, path)
                    updated += 1
            except Exception:
                pass
            continue
        kept: list[Any] = []
        removed_here = 0
        for idx, scen in enumerate(scen_list, start=1):
            if not isinstance(scen, dict):
                kept.append(scen)
                continue
            raw_name = scen.get('name')
            display = raw_name.strip() if isinstance(raw_name, str) and raw_name.strip() else f"Scenario {idx}"
            norm = _normalize_scenario_label(display)
            mk = _scenario_match_key(display)
            if (norm and norm in remove_norms) or (mk and mk in remove_match):
                removed_here += 1
                continue
            kept.append(scen)
        if not removed_here:
            # Still clear scenario_query if it targets a deleted scenario.
            try:
                q = snap.get('scenario_query')
                if q and (_normalize_scenario_label(q) in remove_norms or _scenario_match_key(q) in remove_match):
                    snap['scenario_query'] = ''
                    tmp = path + '.tmp'
                    with open(tmp, 'w', encoding='utf-8') as fh:
                        json.dump(snap, fh, indent=2)
                    os.replace(tmp, path)
                    updated += 1
            except Exception:
                pass
            continue
        scenarios_removed += removed_here
        snap['scenarios'] = kept
        # Clear scenario_query if it targets a deleted scenario.
        try:
            q = snap.get('scenario_query')
            if q and (_normalize_scenario_label(q) in remove_norms or _scenario_match_key(q) in remove_match):
                snap['scenario_query'] = ''
        except Exception:
            pass
        try:
            if not kept:
                os.remove(path)
                deleted += 1
            else:
                tmp = path + '.tmp'
                with open(tmp, 'w', encoding='utf-8') as fh:
                    json.dump(snap, fh, indent=2)
                os.replace(tmp, path)
                updated += 1
        except Exception:
            continue

    return {
        'snapshots_updated': updated,
        'snapshots_deleted': deleted,
        'snapshot_scenarios_removed': scenarios_removed,
    }


def _merge_participant_urls_into_scenario_catalog(participant_urls: Dict[str, Any]) -> None:
    """Merge participant URL hints into the scenario catalog without changing names/sources."""
    if not isinstance(participant_urls, dict) or not participant_urls:
        return
    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return
    try:
        if not os.path.exists(catalog_path):
            return
    except Exception:
        return

    normalized_updates: dict[str, str] = {}
    normalized_clears: set[str] = set()
    for key, value in participant_urls.items():
        norm = _normalize_scenario_label(key)
        if not norm:
            continue
        url_value = _normalize_participant_proxmox_url(value)
        if url_value:
            normalized_updates[norm] = url_value
        else:
            # Explicitly cleared values should persist as an override (empty)
            # to prevent falling back to stale scenario XML.
            normalized_clears.add(norm)

    if not normalized_updates and not normalized_clears:
        return

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return
        existing = payload.get('participant_urls')
        if not isinstance(existing, dict):
            existing = {}

        merged = dict(existing)
        for norm_key in normalized_clears:
            merged[norm_key] = ''
        merged.update(normalized_updates)

        payload['participant_urls'] = merged
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _sanitize_hitl_validation_hint(value: Any) -> Optional[Dict[str, Any]]:
    """Return a safe, compact HITL validation hint payload.

    This must never include secret material (passwords, tokens).
    """
    if not isinstance(value, dict):
        return None
    raw = dict(value)
    out: Dict[str, Any] = {}
    for key in (
        'url',
        'port',
        'verify_ssl',
        'secret_id',
        'validated',
        'last_validated_at',
        'stored_at',
        'last_message',
        'vm_key',
        'vm_name',
        'vm_node',
        'grpc_host',
        'grpc_port',
        'ssh_host',
        'ssh_port',
        'core_secret_id',
        'stored_summary',
        'last_tested_at',
        'last_tested_status',
        'last_tested_message',
        'last_tested_host',
        'last_tested_port',
    ):
        if key in raw:
            out[key] = raw.get(key)
    # Normalize identifiers
    if isinstance(out.get('secret_id'), str):
        out['secret_id'] = out['secret_id'].strip() or None
    if isinstance(out.get('core_secret_id'), str):
        out['core_secret_id'] = out['core_secret_id'].strip() or None
    # Drop any known secret material, defensively.
    for secret_field in ('password', 'token_secret', 'api_secret', 'api_token_secret', 'ssh_password'):
        out.pop(secret_field, None)
    if not out:
        return None
    return out


def _load_scenario_hitl_validation_from_disk() -> dict[str, Dict[str, Any]]:
    """Load per-scenario HITL validation hints from scenario_catalog.json."""
    path = _scenario_catalog_file()
    if not os.path.exists(path):
        return {}
    try:
        with open(path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
    except Exception:
        return {}
    if not isinstance(payload, dict):
        return {}
    raw = payload.get('hitl_validation')
    if not isinstance(raw, dict):
        return {}
    out: dict[str, Dict[str, Any]] = {}
    for scen_key, scen_val in raw.items():
        norm = _normalize_scenario_label(scen_key)
        if not norm or not isinstance(scen_val, dict):
            continue
        entry: Dict[str, Any] = {}
        prox = _sanitize_hitl_validation_hint(scen_val.get('proxmox'))
        core = _sanitize_hitl_validation_hint(scen_val.get('core'))
        if prox:
            entry['proxmox'] = prox
        if core:
            entry['core'] = core
        if entry:
            out[norm] = entry
    return out


def _merge_hitl_validation_into_scenario_catalog(
    scenario_name: str,
    *,
    proxmox: Optional[Dict[str, Any]] = None,
    core: Optional[Dict[str, Any]] = None,
) -> None:
    """Merge safe HITL validation hints into the scenario catalog."""
    scenario_norm = _normalize_scenario_label(scenario_name)
    if not scenario_norm:
        return
    prox_clean = _sanitize_hitl_validation_hint(proxmox) if proxmox else None
    core_clean = _sanitize_hitl_validation_hint(core) if core else None
    if not prox_clean and not core_clean:
        return

    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return
    try:
        if not os.path.exists(catalog_path):
            return
    except Exception:
        return

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return
        existing = payload.get('hitl_validation')
        if not isinstance(existing, dict):
            existing = {}
        merged = dict(existing)
        current_entry = merged.get(scenario_norm)
        if not isinstance(current_entry, dict):
            current_entry = {}
        current_entry = dict(current_entry)
        if prox_clean:
            current_entry['proxmox'] = prox_clean
        if core_clean:
            current_entry['core'] = core_clean
        merged[scenario_norm] = current_entry
        payload['hitl_validation'] = merged
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _clear_hitl_validation_in_scenario_catalog(scenario_name: str, *, proxmox: bool = False, core: bool = False) -> None:
    scenario_norm = _normalize_scenario_label(scenario_name)
    if not scenario_norm:
        return
    if not proxmox and not core:
        return
    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return
    try:
        if not os.path.exists(catalog_path):
            return
    except Exception:
        return

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return
        existing = payload.get('hitl_validation')
        if not isinstance(existing, dict):
            return
        merged = dict(existing)
        entry = merged.get(scenario_norm)
        if not isinstance(entry, dict):
            return
        entry = dict(entry)
        if proxmox:
            entry.pop('proxmox', None)
        if core:
            entry.pop('core', None)
        if entry:
            merged[scenario_norm] = entry
        else:
            merged.pop(scenario_norm, None)
        payload['hitl_validation'] = merged
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _clear_hitl_config_in_scenario_catalog(
    scenario_name: str,
    *,
    clear_core_vm: bool = False,
    clear_config: bool = False,
) -> None:
    """Clear builder-safe HITL config hints stored in scenario_catalog.json.

    Notes:
    - This never deletes encrypted secrets; it only clears non-secret hints.
    - clear_core_vm implies clearing all HITL config for that scenario.
    - clear_config clears Steps 3–5 fields, keeping Step 2 selection if present.
    """

    scenario_norm = _normalize_scenario_label(scenario_name)
    if not scenario_norm:
        return
    if not clear_core_vm and not clear_config:
        return

    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return
    try:
        if not os.path.exists(catalog_path):
            return
    except Exception:
        return

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return

        hc = payload.get('hitl_config')
        if not isinstance(hc, dict):
            hc = {}
        hc = dict(hc)

        hv = payload.get('hitl_validation')
        if not isinstance(hv, dict):
            hv = {}
        hv = dict(hv)

        if clear_core_vm:
            # Drop all config hints for this scenario.
            hc.pop(scenario_norm, None)

            # Also clear CORE VM selection fields from hitl_validation so builder doesn't
            # keep showing a previously-selected CORE VM after admin clears it.
            entry = hv.get(scenario_norm)
            if isinstance(entry, dict):
                entry = dict(entry)
                core_hint = entry.get('core')
                if isinstance(core_hint, dict):
                    core_hint = dict(core_hint)
                    for key in ('vm_key', 'vm_name', 'vm_node', 'vmid'):
                        core_hint.pop(key, None)
                    if core_hint:
                        entry['core'] = core_hint
                    else:
                        entry.pop('core', None)
                if entry:
                    hv[scenario_norm] = entry
                else:
                    hv.pop(scenario_norm, None)

        elif clear_config:
            entry = hc.get(scenario_norm)
            if isinstance(entry, dict):
                entry = dict(entry)
                entry['enabled'] = False
                entry.pop('interfaces', None)
                entry.pop('participant_proxmox_url', None)
                core_cfg = entry.get('core')
                if isinstance(core_cfg, dict):
                    core_cfg = dict(core_cfg)
                    core_cfg.pop('internal_bridge', None)
                    core_cfg.pop('internal_bridge_owner', None)
                    if core_cfg:
                        entry['core'] = core_cfg
                    else:
                        entry.pop('core', None)
                hc[scenario_norm] = entry

        payload['hitl_config'] = hc
        payload['hitl_validation'] = hv
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _scrub_hitl_validation_usernames_in_scenario_catalog() -> bool:
    """Remove any stored usernames/secret material from hitl_validation entries.

    This is an idempotent cleanup for older catalogs that may have persisted
    proxmox/core username fields. Returns True if a write occurred.
    """
    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return False
    try:
        if not os.path.exists(catalog_path):
            return False
    except Exception:
        return False

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return False
        hv = payload.get('hitl_validation')
        if not isinstance(hv, dict) or not hv:
            return False

        changed = False
        cleaned: Dict[str, Any] = {}
        for scen_key, scen_val in hv.items():
            if not isinstance(scen_key, str):
                continue
            if not isinstance(scen_val, dict):
                cleaned[scen_key] = scen_val
                continue
            entry = dict(scen_val)
            prox = entry.get('proxmox')
            if isinstance(prox, dict):
                prox2 = dict(prox)
                for k in ('username', 'user', 'password', 'token_secret', 'api_secret', 'api_token_secret'):
                    if k in prox2:
                        prox2.pop(k, None)
                        changed = True
                entry['proxmox'] = prox2
            core = entry.get('core')
            if isinstance(core, dict):
                core2 = dict(core)
                for k in ('ssh_username', 'username', 'user', 'ssh_password', 'password'):
                    if k in core2:
                        core2.pop(k, None)
                        changed = True
                entry['core'] = core2
            cleaned[scen_key] = entry

        if not changed:
            return False
        payload['hitl_validation'] = cleaned
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
        return True
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass
        return False


def _extract_participant_url_hints_from_scenarios(scenarios: Any) -> dict[str, str]:
    hints: dict[str, str] = {}
    if not isinstance(scenarios, list):
        return hints
    for scen in scenarios:
        if not isinstance(scen, dict):
            continue
        norm = _normalize_scenario_label(scen.get('name'))
        if not norm:
            continue
        hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else None
        if not hitl_meta:
            continue
        url_value = ''
        for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
            normalized = _normalize_participant_proxmox_url(hitl_meta.get(key))
            if normalized:
                url_value = normalized
                break
        if url_value:
            hints[norm] = url_value
    return hints


def _sanitize_hitl_config_hint(value: Any) -> Optional[Dict[str, Any]]:
    """Return a safe HITL config payload suitable for sharing with builder views.

    Must not include passwords/tokens and should avoid usernames.
    """
    if not isinstance(value, dict):
        return None
    raw = dict(value)
    out: Dict[str, Any] = {
        'enabled': bool(raw.get('enabled')),
    }

    # Participant UI URL is non-secret and should persist into builder view.
    try:
        for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
            candidate = raw.get(key)
            normalized = _normalize_participant_proxmox_url(candidate)
            if normalized:
                out['participant_proxmox_url'] = normalized
                break
    except Exception:
        pass

    # Core (keep VM selection + connection endpoints, avoid ssh_username/password)
    core_raw = raw.get('core') if isinstance(raw.get('core'), dict) else {}
    core: Dict[str, Any] = {}
    for key in (
        'vm_key',
        'vm_name',
        'vm_node',
        'grpc_host',
        'grpc_port',
        'ssh_host',
        'ssh_port',
        'internal_bridge',
        'internal_bridge_owner',
        'core_secret_id',
        'validated',
        'last_validated_at',
        'stored_at',
        'last_tested_at',
        'last_tested_status',
        'last_tested_message',
        'last_tested_host',
        'last_tested_port',
    ):
        if key in core_raw:
            core[key] = core_raw.get(key)
    if isinstance(core.get('core_secret_id'), str):
        core['core_secret_id'] = core['core_secret_id'].strip() or None
    if core:
        out['core'] = core

    # Proxmox (keep URL/port + secret_id, avoid username/password)
    prox_raw = raw.get('proxmox') if isinstance(raw.get('proxmox'), dict) else {}
    prox: Dict[str, Any] = {}
    for key in ('url', 'port', 'verify_ssl', 'secret_id', 'validated', 'last_validated_at', 'stored_at'):
        if key in prox_raw:
            prox[key] = prox_raw.get(key)
    if isinstance(prox.get('secret_id'), str):
        prox['secret_id'] = prox['secret_id'].strip() or None
    if prox:
        out['proxmox'] = prox

    # Interfaces + mappings (non-secret)
    iface_list = raw.get('interfaces')

    # Best-effort: if the admin snapshot contains a Proxmox inventory, we can
    # resolve the CORE-side bridge by matching interface MACs.
    inv_vms: List[Dict[str, Any]] = []
    try:
        prox_meta = raw.get('proxmox') if isinstance(raw.get('proxmox'), dict) else {}
        inv = prox_meta.get('inventory') if isinstance(prox_meta.get('inventory'), dict) else {}
        inv_vms_raw = inv.get('vms')
        if isinstance(inv_vms_raw, list):
            inv_vms = [v for v in inv_vms_raw if isinstance(v, dict)]
    except Exception:
        inv_vms = []

    def _parse_vm_key(vm_key: Any) -> tuple[str, str]:
        if not isinstance(vm_key, str):
            return ('', '')
        parts = vm_key.split('::', 1)
        if len(parts) != 2:
            return ('', '')
        return (parts[0].strip(), parts[1].strip())

    def _mac_norm(mac: Any) -> str:
        if mac is None:
            return ''
        s = str(mac).strip().lower()
        return s

    core_node, core_vmid = _parse_vm_key((raw.get('core') or {}).get('vm_key') if isinstance(raw.get('core'), dict) else None)
    if not core_node:
        core_node = (core_raw.get('vm_node') or '').strip() if isinstance(core_raw.get('vm_node'), str) else str(core_raw.get('vm_node') or '').strip()
    if not core_vmid:
        core_vmid = str(core_raw.get('vmid') or '').strip()

    def _find_inventory_vm(node: str, vmid: str) -> Optional[Dict[str, Any]]:
        if not node or not vmid:
            return None
        for vm in inv_vms:
            n = str(vm.get('node') or '').strip()
            v = str(vm.get('vmid') or '').strip()
            if n == node and v == vmid:
                return vm
        return None

    core_inv_vm = _find_inventory_vm(core_node, core_vmid)
    core_inv_ifaces = core_inv_vm.get('interfaces') if (core_inv_vm and isinstance(core_inv_vm.get('interfaces'), list)) else []

    def _bridge_for_mac(mac: Any) -> str:
        macn = _mac_norm(mac)
        if not macn:
            return ''
        for it in core_inv_ifaces:
            if not isinstance(it, dict):
                continue
            inv_mac = _mac_norm(it.get('macaddr') or it.get('mac') or it.get('hwaddr'))
            if inv_mac and inv_mac == macn:
                return str(it.get('bridge') or '').strip()
        return ''

    if isinstance(iface_list, list):
        cleaned_ifaces: List[Dict[str, Any]] = []
        for iface in iface_list:
            if not isinstance(iface, dict):
                continue
            name_raw = iface.get('name')
            name = name_raw.strip() if isinstance(name_raw, str) else str(name_raw or '').strip()
            if not name:
                continue
            mac = iface.get('mac')
            entry: Dict[str, Any] = {
                'name': name,
                'attachment': _normalize_hitl_attachment(iface.get('attachment')),
            }

            # Persist the interface MAC and a best-effort CORE bridge hint.
            if mac:
                entry['mac'] = str(mac).strip()
            core_bridge_existing = iface.get('core_bridge')
            core_bridge_hint = str(core_bridge_existing).strip() if core_bridge_existing not in (None, '') else ''
            if not core_bridge_hint:
                core_bridge_hint = _bridge_for_mac(mac)
            if core_bridge_hint:
                entry['core_bridge'] = core_bridge_hint
            prox_target = iface.get('proxmox_target') if isinstance(iface.get('proxmox_target'), dict) else None
            if prox_target:
                pt: Dict[str, Any] = {}
                for key in ('node', 'vmid', 'interface_id', 'vm_name', 'macaddr', 'bridge', 'model'):
                    if key in prox_target:
                        pt[key] = prox_target.get(key)
                entry['proxmox_target'] = pt
            external = iface.get('external_vm') if isinstance(iface.get('external_vm'), dict) else None
            if external:
                ext: Dict[str, Any] = {}
                for key in ('vm_key', 'vmid', 'interface_id', 'interface_bridge', 'vm_node', 'vm_name'):
                    if key in external:
                        ext[key] = external.get(key)
                entry['external_vm'] = ext
            cleaned_ifaces.append(entry)
        if cleaned_ifaces:
            out['interfaces'] = cleaned_ifaces

    # Defensive drop of any known secret/user fields.
    def _strip(obj: Any) -> Any:
        if isinstance(obj, dict):
            cleaned: Dict[str, Any] = {}
            for k, v in obj.items():
                if k in ('password', 'ssh_password', 'token_secret', 'api_secret', 'api_token_secret'):
                    continue
                if k in ('username', 'ssh_username', 'user'):
                    continue
                cleaned[k] = _strip(v)
            return cleaned
        if isinstance(obj, list):
            return [_strip(v) for v in obj]
        return obj
    out = _strip(out)

    return out or None


def _extract_hitl_config_hints_from_scenarios(scenarios: Any) -> Dict[str, Dict[str, Any]]:
    out: Dict[str, Dict[str, Any]] = {}
    if not isinstance(scenarios, list):
        return out
    for scen in scenarios:
        if not isinstance(scen, dict):
            continue
        norm = _normalize_scenario_label(scen.get('name'))
        if not norm:
            continue
        hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else None
        if not hitl_meta:
            continue
        # Only backfill verified HITL configs.
        if not bool(hitl_meta.get('bridge_validated')):
            continue
        hint = _sanitize_hitl_config_hint(hitl_meta)
        if hint:
            out[norm] = hint
    return out


def _scrub_unverified_hitl_config_in_scenario_catalog() -> bool:
    """Remove any HITL config hints that are not backed by verified credentials.

    This is a safety cleanup for older catalogs/snapshots from before the
    "persist only after verify" rule.

    Returns True if the catalog was modified.
    """

    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return False
    try:
        if not os.path.exists(catalog_path):
            return False
    except Exception:
        return False

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return False

        hc = payload.get('hitl_config')
        hv = payload.get('hitl_validation')
        if not isinstance(hc, dict) or not hc:
            return False
        if not isinstance(hv, dict):
            hv = {}

        changed = False
        cleaned: Dict[str, Any] = {}
        for scen_key, cfg in hc.items():
            norm = _normalize_scenario_label(scen_key)
            if not norm or not isinstance(cfg, dict):
                continue
            val_entry = hv.get(norm)
            if not isinstance(val_entry, dict):
                # No verified credentials recorded -> drop config.
                changed = True
                continue
            prox_ok = False
            core_ok = False
            prox = val_entry.get('proxmox')
            if isinstance(prox, dict):
                prox_ok = bool(prox.get('validated')) and bool((prox.get('secret_id') or '').strip())
            core = val_entry.get('core')
            if isinstance(core, dict):
                core_ok = bool(core.get('validated')) and bool((core.get('core_secret_id') or '').strip())
            if not (prox_ok and core_ok):
                changed = True
                continue
            cleaned[norm] = cfg

        if not changed:
            return False

        payload['hitl_config'] = cleaned
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'
        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
        return True
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass
        return False


def _load_scenario_hitl_config_from_disk() -> dict[str, Dict[str, Any]]:
    """Load per-scenario HITL config hints from scenario_catalog.json."""
    path = _scenario_catalog_file()
    if not os.path.exists(path):
        return {}
    try:
        with open(path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
    except Exception:
        return {}
    if not isinstance(payload, dict):
        return {}
    raw = payload.get('hitl_config')
    if not isinstance(raw, dict):
        return {}
    out: dict[str, Dict[str, Any]] = {}
    for scen_key, scen_val in raw.items():
        norm = _normalize_scenario_label(scen_key)
        if not norm or not isinstance(scen_val, dict):
            continue
        hint = _sanitize_hitl_config_hint(scen_val)
        if hint:
            out[norm] = hint
    return out


def _merge_hitl_config_map_into_scenario_catalog(hitl_configs: Dict[str, Dict[str, Any]]) -> None:
    if not isinstance(hitl_configs, dict) or not hitl_configs:
        return
    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return
    try:
        if not os.path.exists(catalog_path):
            return
    except Exception:
        return

    normalized_updates: Dict[str, Dict[str, Any]] = {}
    for scen_key, hitl in hitl_configs.items():
        norm = _normalize_scenario_label(scen_key)
        if not norm:
            continue
        clean = _sanitize_hitl_config_hint(hitl)
        if clean:
            # Stamp a write time so "latest" can be selected later.
            stored_at = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'
            if isinstance(clean.get('core'), dict) and 'stored_at' not in clean['core']:
                clean['core']['stored_at'] = stored_at
            if isinstance(clean.get('proxmox'), dict) and 'stored_at' not in clean['proxmox']:
                clean['proxmox']['stored_at'] = stored_at
            normalized_updates[norm] = clean
    if not normalized_updates:
        return

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return
        existing = payload.get('hitl_config')
        if not isinstance(existing, dict):
            existing = {}
        merged = dict(existing)
        for norm, clean in normalized_updates.items():
            entry = merged.get(norm)
            if not isinstance(entry, dict):
                entry = {}
            entry = dict(entry)
            entry.update(clean)
            merged[norm] = entry
        payload['hitl_config'] = merged
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _merge_hitl_config_into_scenario_catalog(scenario_name: str, hitl_config: Dict[str, Any]) -> None:
    scenario_norm = _normalize_scenario_label(scenario_name)
    if not scenario_norm:
        return
    _merge_hitl_config_map_into_scenario_catalog({scenario_norm: hitl_config})


def _backfill_hitl_config_from_editor_snapshots() -> bool:
    """Seed scenario_catalog.json hitl_config from any existing editor snapshots.

    This is a startup convenience so builder view reflects admin-configured HITL
    settings immediately after deploy/upgrade.
    """
    catalog_path = _scenario_catalog_file()
    try:
        if not catalog_path or not os.path.exists(catalog_path):
            return False
    except Exception:
        return False

    snapshots_dir = os.path.join(_outputs_dir(), 'editor_snapshots')
    try:
        if not os.path.isdir(snapshots_dir):
            return False
    except Exception:
        return False

    any_updates = False
    try:
        for fname in os.listdir(snapshots_dir):
            if not fname.endswith('.json'):
                continue
            path = os.path.join(snapshots_dir, fname)
            try:
                with open(path, 'r', encoding='utf-8') as fh:
                    snap = json.load(fh)
            except Exception:
                continue
            if not isinstance(snap, dict):
                continue
            hints = _extract_hitl_config_hints_from_scenarios(snap.get('scenarios'))
            if hints:
                _merge_hitl_config_map_into_scenario_catalog(hints)
                any_updates = True
    except Exception:
        return any_updates
    return any_updates


def _collect_scenario_catalog(history: Optional[List[dict]] = None) -> tuple[list[str], dict[str, set[str]], dict[str, str]]:
    catalog_names, catalog_paths, catalog_participants = _load_scenario_catalog_from_disk()
    entries = history if history is not None else _load_run_history()
    display_by_norm: dict[str, str] = {}
    ordered: list[str] = []
    path_map: dict[str, set[str]] = defaultdict(set)
    participant_by_norm: dict[str, str] = dict(catalog_participants)

    def _discover_saved_scenarios_from_outputs() -> list[tuple[str, str]]:
        """Discover scenario names from saved web UI artifacts.

        The web UI saves scenario XMLs under outputs/scenarios-*/Scenario_*.xml.
        These may exist even when they haven't been executed (so run_history won't contain them).
        Builders/participants still need to see these scenarios once assigned.
        Returns a list of (scenario_display_name, xml_path).
        """
        out: list[tuple[str, str]] = []
        try:
            base = _outputs_dir()
            if not base or not os.path.isdir(base):
                return out
            dirs: list[tuple[float, str]] = []
            for entry in os.listdir(base):
                if not entry.startswith('scenarios-'):
                    continue
                full = os.path.join(base, entry)
                if not os.path.isdir(full):
                    continue
                try:
                    mtime = os.path.getmtime(full)
                except Exception:
                    mtime = 0.0
                dirs.append((mtime, full))
            # Limit scan for performance; newest first.
            dirs.sort(key=lambda t: t[0], reverse=True)
            for _mtime, folder in dirs[:250]:
                try:
                    for fname in os.listdir(folder):
                        if not fname.endswith('.xml'):
                            continue
                        # Skip CORE pre/post captures; only include user-authored scenario XML.
                        if fname.startswith('core-'):
                            continue
                        if not fname.startswith('Scenario_'):
                            continue
                        path = os.path.join(folder, fname)
                        if not os.path.isfile(path):
                            continue
                        for name in _scenario_names_from_xml(path):
                            if name:
                                out.append((name, path))
                except Exception:
                    continue
        except Exception:
            return out
        return out

    def _record_path(norm_key: str, path_value: Any) -> None:
        if not path_value or not norm_key:
            return
        if isinstance(path_value, (list, tuple, set)):
            for item in path_value:
                _record_path(norm_key, item)
            return
        try:
            ap = os.path.abspath(str(path_value))
        except Exception:
            ap = str(path_value)
        path_map[norm_key].add(ap)

    for entry in entries:
        # Per-scenario: prefer explicit scenario_name, otherwise scenario_names[0].
        primary_display = ''
        try:
            raw_primary = entry.get('scenario_name')
            if isinstance(raw_primary, str) and raw_primary.strip():
                primary_display = raw_primary.strip()
        except Exception:
            primary_display = ''
        if not primary_display:
            names = entry.get('scenario_names') or []
            if isinstance(names, list) and names:
                raw0 = names[0]
                primary_display = raw0.strip() if isinstance(raw0, str) else str(raw0).strip()
        normalized = _normalize_scenario_label(primary_display)
        if not normalized:
            continue
        if normalized not in display_by_norm:
            display_by_norm[normalized] = primary_display
            ordered.append(primary_display)

        # Prefer a single-scenario XML for path matching.
        single_xml = entry.get('single_scenario_xml_path')
        if single_xml:
            _record_path(normalized, single_xml)
        else:
            # Avoid multi-scenario Scenario Editor outputs like outputs/scenarios-*/scenarios.xml.
            for key in ('scenario_xml_path', 'xml_path'):
                candidate = entry.get(key)
                if not candidate:
                    continue
                try:
                    base = os.path.basename(str(candidate))
                except Exception:
                    base = ''
                if base.lower() == 'scenarios.xml':
                    continue
                _record_path(normalized, candidate)

        # Session captures are useful for participant URL inference and mapping.
        for key in ('session_xml_path', 'post_xml_path'):
            candidate = entry.get(key)
            if candidate:
                _record_path(normalized, candidate)

    # Merge in saved scenario XMLs from outputs/ (covers saved-but-not-executed scenarios like "Scenario 1b").
    try:
        for display_name, xml_path in _discover_saved_scenarios_from_outputs():
            norm = _normalize_scenario_label(display_name)
            if not norm:
                continue
            if norm not in display_by_norm:
                display_by_norm[norm] = display_name
                ordered.append(display_name)
            _record_path(norm, xml_path)
    except Exception:
        pass
    if catalog_paths:
        for norm_key, candidates in catalog_paths.items():
            if not candidates:
                continue
            path_map[norm_key].update(candidates)
    if catalog_names:
        merged_order: list[str] = []
        seen_norms: set[str] = set()
        for display in catalog_names:
            norm = _normalize_scenario_label(display)
            if not norm or norm in seen_norms:
                continue
            seen_norms.add(norm)
            display_by_norm[norm] = display
            path_map.setdefault(norm, set())
            merged_order.append(display)
        for display in ordered:
            norm = _normalize_scenario_label(display)
            if not norm or norm in seen_norms:
                continue
            seen_norms.add(norm)
            merged_order.append(display)
        ordered = merged_order

    # Merge scenario names from all editor snapshots so manually created scenarios are assignable
    snapshot_dir = _editor_state_snapshot_dir()
    try:
        snapshot_entries = [name for name in os.listdir(snapshot_dir) if name.endswith('.json')]
    except Exception:
        snapshot_entries = []
    snapshot_protected_norms: set[str] = set()
    for entry in snapshot_entries:
        full_path = os.path.join(snapshot_dir, entry)
        try:
            with open(full_path, 'r', encoding='utf-8') as handle:
                snapshot_data = json.load(handle)
        except Exception:
            continue
        scen_list = snapshot_data.get('scenarios')
        if not isinstance(scen_list, list):
            continue
        for idx, scen in enumerate(scen_list, start=1):
            if not isinstance(scen, dict):
                continue
            raw_name = scen.get('name')
            if isinstance(raw_name, str) and raw_name.strip():
                display = raw_name.strip()
            else:
                display = f"Scenario {idx}"
            norm = _normalize_scenario_label(display)
            if not norm:
                continue
            if norm not in display_by_norm:
                display_by_norm[norm] = display
                path_map.setdefault(norm, set())
                ordered.append(display)
            snapshot_protected_norms.add(norm)
            hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else None
            if hitl_meta:
                participant_url = ''
                for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                    normalized = _normalize_participant_proxmox_url(hitl_meta.get(key))
                    if normalized:
                        participant_url = normalized
                        break
                if participant_url:
                    participant_by_norm[norm] = participant_url

    # Stable, shared ordering across all pages.
    try:
        ordered = sorted(ordered, key=_scenario_display_sort_key)
    except Exception:
        pass
    protected = snapshot_protected_norms or None
    return _prune_stale_scenario_entries(ordered, path_map, participant_by_norm, protected_norms=protected)


def _prune_stale_scenario_entries(
    scenario_names: list[str],
    scenario_paths: dict[str, set[str]],
    scenario_url_hints: dict[str, str],
    *,
    protected_norms: Optional[set[str]] = None,
) -> tuple[list[str], dict[str, set[str]], dict[str, str]]:
    cleaned_names: list[str] = []
    cleaned_paths: dict[str, set[str]] = {}
    cleaned_hints: dict[str, str] = {}
    seen_norms: set[str] = set()
    protected = {n.strip().lower() for n in (protected_norms or set()) if n}

    for display in scenario_names:
        norm = _normalize_scenario_label(display)
        if not norm or norm in seen_norms:
            continue
        seen_norms.add(norm)
        candidates = scenario_paths.get(norm) or set()
        valid_paths: set[str] = set()
        for candidate in candidates:
            if candidate in (None, ''):
                continue
            try:
                abs_candidate = os.path.abspath(str(candidate))
            except Exception:
                abs_candidate = str(candidate)
            try:
                exists = os.path.exists(abs_candidate)
            except Exception:
                exists = False
            if exists:
                valid_paths.add(abs_candidate)
        if not valid_paths and norm not in protected:
            continue
        cleaned_names.append(display)
        cleaned_paths[norm] = valid_paths
        if norm in scenario_url_hints:
            cleaned_hints[norm] = scenario_url_hints[norm]

    return cleaned_names, cleaned_paths, cleaned_hints


def _merge_editor_scenarios_into_catalog(
    scenario_names: list[str],
    scenario_paths: dict[str, set[str]],
    scenario_url_hints: Optional[Dict[str, str]] = None,
    *,
    user: Optional[dict] = None,
) -> tuple[list[str], dict[str, set[str]], dict[str, str]]:
    snapshot = _load_editor_state_snapshot(user)
    scen_list = snapshot.get('scenarios') if isinstance(snapshot, dict) else None
    if not scen_list or not isinstance(scen_list, list):
        return (
            list(scenario_names or []),
            {k: set(v) for k, v in (scenario_paths or {}).items()},
            dict(scenario_url_hints or {}),
        )

    names_out = list(scenario_names or [])
    paths_out: dict[str, set[str]] = {k: set(v) for k, v in (scenario_paths or {}).items()}
    hints_out: dict[str, str] = dict(scenario_url_hints or {})
    seen_norms: set[str] = set()
    for existing in names_out:
        norm = _normalize_scenario_label(existing)
        if norm:
            seen_norms.add(norm)

    raw_result_path = snapshot.get('result_path') if isinstance(snapshot, dict) else None
    abs_result_path = ''
    if isinstance(raw_result_path, str) and raw_result_path.strip():
        candidate = raw_result_path.strip()
        try:
            abs_result_path = os.path.abspath(candidate)
        except Exception:
            abs_result_path = candidate

    snapshot_norms: set[str] = set()
    for idx, scen in enumerate(scen_list, start=1):
        if not isinstance(scen, dict):
            continue
        raw_name_value = scen.get('name')
        if isinstance(raw_name_value, str):
            display_name = raw_name_value.strip()
        elif raw_name_value not in (None, ''):
            try:
                display_name = str(raw_name_value).strip()
            except Exception:
                display_name = ''
        else:
            display_name = ''
        if not display_name:
            display_name = f"Scenario {idx}"
        norm = _normalize_scenario_label(display_name)
        if not norm:
            continue
        if norm not in seen_norms:
            names_out.append(display_name)
            seen_norms.add(norm)
        target_paths = paths_out.setdefault(norm, set())
        if abs_result_path:
            target_paths.add(abs_result_path)
        base_meta = scen.get('base') if isinstance(scen.get('base'), dict) else None
        base_file = base_meta.get('filepath') if base_meta else None
        if isinstance(base_file, str) and base_file.strip():
            try:
                target_paths.add(os.path.abspath(base_file.strip()))
            except Exception:
                target_paths.add(base_file.strip())
        hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else None
        if hitl_meta:
            participant_url = ''
            for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                normalized = _normalize_participant_proxmox_url(hitl_meta.get(key))
                if normalized:
                    participant_url = normalized
                    break
            if participant_url:
                hints_out[norm] = participant_url
        snapshot_norms.add(norm)

    return _prune_stale_scenario_entries(names_out, paths_out, hints_out, protected_norms=snapshot_norms or None)


def _scenario_catalog_for_user(
    history: Optional[List[dict]] = None,
    *,
    user: Optional[dict] = None,
) -> tuple[list[str], dict[str, set[str]], dict[str, str]]:
    names, paths, hints = _collect_scenario_catalog(history)
    return _merge_editor_scenarios_into_catalog(names, paths, hints, user=user)


_SCENARIO_PARTICIPANT_CACHE: dict[str, dict[str, Any]] = {}
_G_USER_RECORD_SENTINEL = object()
_G_PARTICIPANT_STATE_SENTINEL = object()
_UI_VIEW_SESSION_KEY = 'ui_view_mode'
_UI_VIEW_DEFAULT = 'participant'
_UI_VIEW_ALLOWED = {'participant', 'admin', 'builder'}
_ADMIN_VIEW_ROLES = {'admin', 'builder'}
_ALLOWED_USER_ROLES = {'admin', 'builder', 'participant'}
_ROLE_ALIASES = {'user': 'participant'}
_PARTICIPANT_ROLES = {'participant', 'user'}


def _normalize_role_value(role: Any) -> str:
    text = ''
    if role not in (None, ''):
        try:
            text = str(role).strip().lower()
        except Exception:
            text = ''
    if not text:
        text = 'participant'
    text = _ROLE_ALIASES.get(text, text)
    if text not in _ALLOWED_USER_ROLES:
        text = 'participant'
    return text


def _is_participant_role(role: Optional[str]) -> bool:
    if role is None:
        return False
    return role.strip().lower() in _PARTICIPANT_ROLES


def _is_admin_view_role(role: Optional[str]) -> bool:
    if role is None:
        return False
    return role.strip().lower() in _ADMIN_VIEW_ROLES


def _default_ui_view_mode_for_role(role: Any) -> str:
    normalized = _normalize_role_value(role)
    if normalized == 'admin':
        return 'admin'
    if normalized == 'builder':
        return 'builder'
    return _UI_VIEW_DEFAULT


def _current_ui_view_mode() -> str:
    user = _current_user()
    if not user or user.get('role') not in _ADMIN_VIEW_ROLES:
        if session.get(_UI_VIEW_SESSION_KEY) != _UI_VIEW_DEFAULT:
            session[_UI_VIEW_SESSION_KEY] = _UI_VIEW_DEFAULT
        return _UI_VIEW_DEFAULT

    role = _normalize_role_value(user.get('role'))

    # Admin/builder roles: if no preference saved yet, default by role.
    if _UI_VIEW_SESSION_KEY not in session:
        raw = _default_ui_view_mode_for_role(role)
        session[_UI_VIEW_SESSION_KEY] = raw
        return raw

    raw = session.get(_UI_VIEW_SESSION_KEY, _UI_VIEW_DEFAULT)
    if raw not in _UI_VIEW_ALLOWED:
        raw = _default_ui_view_mode_for_role(role)
        session[_UI_VIEW_SESSION_KEY] = raw
        return raw

    # Builders should never be in admin mode.
    if role == 'builder' and raw == 'admin':
        raw = 'builder'
        session[_UI_VIEW_SESSION_KEY] = raw

    return raw


def _normalize_participant_proxmox_url(value: Any) -> str:
    if value in (None, ''):
        return ''
    text = str(value).strip()
    if not text:
        return ''
    candidate = text
    parsed = urlparse(candidate)
    if not parsed.scheme:
        candidate = f'https://{candidate}'
        parsed = urlparse(candidate)
    scheme = (parsed.scheme or '').lower()
    if scheme not in {'http', 'https'}:
        return ''
    if not parsed.netloc:
        return ''
    return parsed.geturl()


def _participant_url_from_editor(editor: Optional[ET.Element]) -> str:
    if editor is None:
        return ''
    hitl_el = editor.find('HardwareInLoop')
    if hitl_el is None:
        return ''
    for attr in (
        'participant_proxmox_url',
        'participant_ui_url',
        'participant_url',
        'participant',
    ):
        raw = hitl_el.get(attr)
        if raw:
            normalized = _normalize_participant_proxmox_url(raw)
            if normalized:
                return normalized
    return ''


def _participant_urls_from_xml(path: str) -> dict[str, str]:
    if not path:
        return {}
    try:
        abs_path = os.path.abspath(path)
    except Exception:
        abs_path = str(path)
    try:
        mtime = os.path.getmtime(abs_path)
    except Exception:
        mtime = None
    cached = _SCENARIO_PARTICIPANT_CACHE.get(abs_path)
    if cached and cached.get('mtime') == mtime:
        return dict(cached.get('data') or {})
    mapping: dict[str, str] = {}
    try:
        tree = ET.parse(abs_path)
        root = tree.getroot()
    except Exception:
        _SCENARIO_PARTICIPANT_CACHE[abs_path] = {'mtime': mtime, 'data': mapping}
        return {}
    if root.tag == 'Scenarios':
        for scen_el in root.findall('Scenario'):
            name = scen_el.get('name') or os.path.splitext(os.path.basename(abs_path))[0]
            norm = _normalize_scenario_label(name)
            if not norm:
                continue
            editor = scen_el.find('ScenarioEditor')
            participant_url = _participant_url_from_editor(editor)
            if participant_url:
                mapping[norm] = participant_url
    elif root.tag == 'ScenarioEditor':
        name = root.get('name') or os.path.splitext(os.path.basename(abs_path))[0]
        norm = _normalize_scenario_label(name)
        if norm:
            participant_url = _participant_url_from_editor(root)
            if participant_url:
                mapping[norm] = participant_url
    _SCENARIO_PARTICIPANT_CACHE[abs_path] = {'mtime': mtime, 'data': mapping}
    return dict(mapping)


def _collect_scenario_participant_urls(
    scenario_paths: dict[str, set[str]],
    catalog_hints: Optional[Dict[str, Any]] = None,
) -> dict[str, str]:
    urls: dict[str, str] = {}
    if catalog_hints:
        for norm_key, raw_value in catalog_hints.items():
            norm = _normalize_scenario_label(norm_key)
            normalized_url = _normalize_participant_proxmox_url(raw_value)
            if not norm:
                continue
            if normalized_url:
                urls[norm] = normalized_url
            else:
                # Explicitly cleared hint overrides stale XML.
                urls.setdefault(norm, '')
    if not scenario_paths:
        return urls
    cache: dict[str, dict[str, str]] = {}
    for norm_key, candidates in scenario_paths.items():
        if not candidates:
            continue
        for candidate in candidates:
            try:
                abs_path = os.path.abspath(str(candidate))
            except Exception:
                abs_path = str(candidate)
            if not abs_path:
                continue
            if abs_path not in cache:
                cache[abs_path] = _participant_urls_from_xml(abs_path)
            url_value = cache[abs_path].get(norm_key)
            if url_value:
                # Prefer catalog hints (e.g., saved from editor snapshots) over XML values.
                # XML can lag behind the editor state when the user hasn't re-saved scenarios.xml.
                if norm_key not in urls:
                    urls[norm_key] = url_value
                break
    return urls


def _normalize_scenario_assignments(values: Iterable[Any] | None) -> list[str]:
    result: list[str] = []
    if not values:
        return result
    for value in values:
        norm = _normalize_scenario_label(value)
        if norm and norm not in result:
            result.append(norm)
    return result


def _nearest_gateway_address_for_scenario(
    scenario_norm: str,
    *,
    scenario_paths: dict[str, set[str]],
) -> str:
    """Best-effort HITL gateway address for a scenario.

    Uses the most recently saved CORE session XML for that scenario (from outputs/core_sessions.json
    + outputs/core-sessions/**/*.xml) and extracts the router-side IP of the HITL attachment.
    Returns an IPv4 string without CIDR (e.g., '10.12.34.1').
    """

    scenario_norm = _normalize_scenario_label(scenario_norm)
    if not scenario_norm:
        return ''

    try:
        store = _load_core_sessions_store()
    except Exception:
        store = {}

    best_path: str = ''
    best_mtime: float = 0.0
    for path, entry in (store or {}).items():
        if not path:
            continue
        stored_norm = _session_store_entry_scenario_norm(entry)
        if stored_norm and stored_norm != scenario_norm:
            continue
        if not stored_norm and not _path_matches_scenario(path, scenario_norm, scenario_paths):
            continue
        try:
            ap = os.path.abspath(str(path))
        except Exception:
            ap = str(path)
        if not ap or not os.path.exists(ap):
            continue
        try:
            mtime = os.path.getmtime(ap)
        except Exception:
            mtime = 0.0
        if not best_path or mtime > best_mtime:
            best_path = ap
            best_mtime = mtime

    if not best_path:
        return ''

    details = _hitl_details_from_path(best_path)
    if not details:
        return ''
    try:
        first = details[0] if details else None
        ips = first.get('ips') if isinstance(first, dict) else None
        if isinstance(ips, list) and ips:
            return str(ips[0]).split('/', 1)[0]
    except Exception:
        return ''
    return ''


def _participant_ui_state() -> Dict[str, Any]:
    if has_request_context():
        cached = getattr(g, '_participant_ui_state', _G_PARTICIPANT_STATE_SENTINEL)
        if cached is not _G_PARTICIPANT_STATE_SENTINEL:
            return cached
    user = _current_user()
    user_role = _normalize_role_value(user.get('role')) if user else ''
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(
        None,
        user=user,
    )
    display_by_norm: dict[str, str] = {}
    ordered_norms: list[str] = []
    for display in scenario_names:
        norm = _normalize_scenario_label(display)
        if not norm or norm in display_by_norm:
            continue
        display_by_norm[norm] = display
        ordered_norms.append(norm)
    mapping = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
    for norm_key in mapping.keys():
        if not norm_key or norm_key in display_by_norm:
            continue
        display_by_norm[norm_key] = norm_key
        ordered_norms.append(norm_key)
    assigned_norms = _current_user_assigned_scenarios()
    assigned_set = set(assigned_norms)
    has_assignments = bool(assigned_norms)
    restrict_to_assigned = user_role in {'participant', 'builder'}
    allowed_norms: Optional[set[str]] = set(assigned_norms) if restrict_to_assigned else None
    scenario_arg_norm = ''
    if has_request_context():
        try:
            scenario_arg_norm = _normalize_scenario_label(request.args.get('scenario'))
        except Exception:
            scenario_arg_norm = ''

    def _humanize_norm_text(value: str) -> str:
        if not value:
            return ''
        text = value.replace('_', ' ')
        text = re.sub(r'\s+', ' ', text).strip()
        return text.title() if text else ''

    def _pick_norm(candidates: Iterable[str]) -> str:
        for norm in candidates:
            if not norm:
                continue
            if allowed_norms is not None and norm not in allowed_norms:
                continue
            url_value = mapping.get(norm)
            if url_value:
                return norm
        return ''

    ordered_mapping_norms = [key for key in mapping.keys() if key]
    selected_norm = (
        _pick_norm([scenario_arg_norm])
        or _pick_norm(assigned_norms)
        or _pick_norm(ordered_norms)
        or _pick_norm(ordered_mapping_norms)
    )
    selected_label = display_by_norm.get(selected_norm, selected_norm)
    selected_url = mapping.get(selected_norm, '')
    selected_nearest_gateway = _nearest_gateway_address_for_scenario(selected_norm, scenario_paths=scenario_paths)

    listing: list[dict[str, Any]] = []
    for norm in ordered_norms:
        if not norm:
            continue
        entry = {
            'norm': norm,
            'display': display_by_norm.get(norm, norm),
            'url': mapping.get(norm, ''),
            'has_url': bool(mapping.get(norm)),
            'assigned': norm in assigned_set,
        }
        listing.append(entry)
    if restrict_to_assigned:
        listing = [row for row in listing if row['assigned']]
        present_norms = {row['norm'] for row in listing}
        missing_norms = [norm for norm in assigned_norms if norm and norm not in present_norms]
        for norm in missing_norms:
            display_value = display_by_norm.get(norm) or _humanize_norm_text(norm) or norm
            listing.append({
                'norm': norm,
                'display': display_value,
                'url': '',
                'has_url': False,
                'assigned': True,
                'placeholder': True,
            })
    for row in listing:
        row['active'] = bool(selected_norm) and row['norm'] == selected_norm
    heading = 'Your Assigned Scenarios' if restrict_to_assigned else 'Available Scenarios'
    if restrict_to_assigned:
        if has_assignments:
            empty_message = 'Your assigned scenarios are not available in the participant console yet. Ask an administrator to publish them.'
        else:
            empty_message = 'No scenarios have been assigned to your account yet. Ask an administrator for access.'
    else:
        empty_message = 'No scenarios with participant links are available yet.'
    hint = 'Load a scenario into the console or open its participant link in a new tab.'
    state = {
        'selected_norm': selected_norm,
        'selected_label': selected_label if selected_norm else '',
        'selected_url': selected_url or '',
        'selected_nearest_gateway': selected_nearest_gateway or '',
        'listing': listing,
        'listing_heading': heading,
        'listing_empty_message': empty_message,
        'listing_hint': hint,
        'restrict_to_assigned': restrict_to_assigned,
        'has_assignments': has_assignments,
    }
    if has_request_context():
        setattr(g, '_participant_ui_state', state)
    return state


def _resolve_participant_ui_target() -> tuple[str, str]:
    state = _participant_ui_state()
    return state.get('selected_url', ''), state.get('selected_label', '')


def _current_nav_participant_url() -> str:
    url_value, _label = _resolve_participant_ui_target()
    return url_value


def _resolve_ui_view_redirect_target(candidate: Optional[str]) -> str:
    fallback = url_for('index')
    if not candidate:
        return fallback
    try:
        parsed = urlparse(candidate)
    except Exception:
        return fallback
    if not parsed.scheme and candidate.startswith('/'):
        return candidate
    try:
        host_url = urlparse(request.host_url)
    except Exception:
        host_url = None
    if parsed.scheme and host_url and parsed.netloc == host_url.netloc:
        return candidate
    return fallback


def _select_existing_path(candidates) -> Optional[str]:
    """Return the newest existing file path from an iterable of candidates."""
    best_path: Optional[str] = None
    best_mtime = float('-inf')
    if not candidates:
        return None
    for candidate in candidates:
        if not candidate:
            continue
        try:
            ap = os.path.abspath(str(candidate))
        except Exception:
            ap = str(candidate)
        if not os.path.exists(ap):
            continue
        try:
            mt = os.path.getmtime(ap)
        except Exception:
            mt = 0.0
        if best_path is None or mt > best_mtime:
            best_path = ap
            best_mtime = mt
    return best_path


def _filter_history_by_scenario(history: List[dict], scenario_norm: str) -> List[dict]:
    if not scenario_norm:
        return history
    filtered: List[dict] = []
    for entry in history:
        names = entry.get('scenario_names') or []
        if not isinstance(names, list):
            continue
        if any(_normalize_scenario_label(name) == scenario_norm for name in names):
            filtered.append(entry)
    return filtered


def _resolve_scenario_display(scenario_norm: str, ordered_names: list[str], fallback: str = '') -> str:
    if not scenario_norm:
        return ''
    for name in ordered_names:
        if _normalize_scenario_label(name) == scenario_norm:
            return name
    return fallback.strip()


def _path_matches_scenario(path_value: Any, scenario_norm: str, scenario_paths: dict[str, set[str]]) -> bool:
    if not scenario_norm:
        return True
    if not path_value:
        return False
    try:
        ap = os.path.abspath(str(path_value))
    except Exception:
        ap = str(path_value)
    if ap in (scenario_paths.get(scenario_norm) or set()):
        return True

    # CORE internal session directories (pycore.<id>) often contain generic filenames
    # like Scenario_1.xml that do not encode the scenario. Even when the webapp runs
    # on the CORE VM and these paths exist locally, do not infer scenario membership
    # from the basename. Use session-meta or our mapping store instead.
    try:
        ap_norm = ap.replace('\\', '/')
        if '/tmp/pycore.' in ap_norm:
            return False
    except Exception:
        pass

    # If the path doesn't exist locally (common for CORE-reported remote paths like
    # /tmp/Scenario_1.xml), do not infer scenario membership from the basename.
    # Rely on explicit scenario_paths or our local mapping store instead.
    try:
        if not os.path.exists(ap):
            return False
    except Exception:
        return False
    base = os.path.splitext(os.path.basename(ap))[0]
    return _normalize_scenario_label(base) == scenario_norm


def _latest_session_owner_by_id(mapping: dict) -> dict[int, dict[str, Any]]:
    """Return session_id -> best mapping entry (newest updated_at).

    This prevents CORE session-id reuse from making a session appear under multiple scenarios.
    """
    owners: dict[int, dict[str, Any]] = {}
    best_ts: dict[int, float] = {}
    for _path, entry in (mapping or {}).items():
        sid = _session_store_entry_session_id(entry)
        if sid is None:
            continue
        ts = _session_store_entry_updated_at_epoch(entry)
        if ts is None:
            ts = 0.0
        prev = best_ts.get(sid)
        if prev is None or ts >= prev:
            best_ts[sid] = ts
            owners[sid] = entry if isinstance(entry, dict) else {'session_id': sid}
    return owners


def _scenario_label_from_path(
    path_value: Any,
    scenario_names: list[str],
    scenario_paths: dict[str, set[str]],
) -> str:
    if not path_value:
        return ''
    try:
        ap = os.path.abspath(str(path_value))
    except Exception:
        ap = str(path_value)

    # Never infer scenario names from CORE internal session paths. These often use
    # generic filenames like Scenario_1.xml that do not correspond to our scenarios.
    try:
        if '/tmp/pycore.' in ap.replace('\\', '/'):
            return ''
    except Exception:
        return ''
    for norm_key, paths in (scenario_paths or {}).items():
        if ap in paths:
            return _resolve_scenario_display(norm_key, scenario_names, norm_key)

    # If the path doesn't exist locally, don't try to infer the scenario by basename.
    # CORE often reports remote paths like /tmp/Scenario_1.xml which would otherwise
    # incorrectly label Scenario 2 sessions as Scenario 1.
    try:
        if not os.path.exists(ap):
            return ''
    except Exception:
        return ''
    base = os.path.splitext(os.path.basename(ap))[0]
    base_norm = _normalize_scenario_label(base)
    if base_norm:
        # Avoid showing CORE internal session directories like /tmp/pycore.* as
        # scenario names in the UI.
        if base_norm.startswith('pycore'):
            return ''
        # Only return a display name if it matches a known scenario.
        display = _resolve_scenario_display(base_norm, scenario_names, '')
        return display or ''
    return ''


def _session_ids_for_scenario(mapping: dict, scenario_norm: str, scenario_paths: dict[str, set[str]]) -> set[int]:
    ids: set[int] = set()
    if not scenario_norm:
        return ids
    # Session IDs can be reused across CORE restarts and across scenarios.
    # When that happens, only the *latest* mapping entry should own the session id.
    owners = _latest_session_owner_by_id(mapping)
    for sid, entry in owners.items():
        stored_norm = _session_store_entry_scenario_norm(entry)
        if stored_norm and stored_norm == scenario_norm:
            ids.add(sid)

    # Legacy fallback: if we have no owner for a session id (missing/invalid updated_at),
    # allow path-based inference based on the mapping key.
    for path, entry in (mapping or {}).items():
        sid = _session_store_entry_session_id(entry)
        if sid is None:
            continue
        if sid in owners:
            continue
        stored_norm = _session_store_entry_scenario_norm(entry)
        if stored_norm and stored_norm == scenario_norm:
            ids.add(sid)
            continue
        if _path_matches_scenario(path, scenario_norm, scenario_paths):
            ids.add(sid)
    return ids


def _filter_sessions_by_scenario(
    sessions: list[dict],
    scenario_norm: str,
    scenario_paths: dict[str, set[str]],
    scenario_session_ids: set[int],
) -> tuple[list[dict], bool]:
    if not scenario_norm:
        return sessions, True
    filtered: list[dict] = []
    matched = False
    for session in sessions:
        sid = session.get('id')
        sid_int: Optional[int]
        try:
            sid_int = int(sid) if sid is not None else None
        except Exception:
            sid_int = None
        label = _normalize_scenario_label(session.get('scenario_name')) if session.get('scenario_name') else ''
        if label and label == scenario_norm:
            filtered.append(session)
            matched = True
            continue
        if _path_matches_scenario(session.get('file'), scenario_norm, scenario_paths):
            filtered.append(session)
            matched = True
            continue
        if _path_matches_scenario(session.get('dir'), scenario_norm, scenario_paths):
            filtered.append(session)
            matched = True
            continue
        # Session IDs can be reused across CORE restarts; only fall back to ID-based
        # matching when we have no path information.
        if (not session.get('file')) and (not session.get('dir')):
            if sid_int is not None and sid_int in scenario_session_ids:
                filtered.append(session)
                matched = True
                continue
    return filtered, matched


def _xml_matches_scenario(
    path_value: Any,
    scenario_norm: str,
    scenario_paths: dict[str, set[str]],
    mapping: dict,
) -> bool:
    if not scenario_norm:
        return True
    if _path_matches_scenario(path_value, scenario_norm, scenario_paths):
        return True
    if not path_value:
        return False
    try:
        abs_path = os.path.abspath(str(path_value))
    except Exception:
        abs_path = str(path_value)
    entry = mapping.get(abs_path)
    if not entry and abs_path:
        entry = mapping.get(abs_path.rstrip('/'))
    if not entry:
        return False
    stored_norm = _session_store_entry_scenario_norm(entry)
    return bool(stored_norm and stored_norm == scenario_norm)


def _filter_xmls_by_scenario(
    xmls: list[dict],
    scenario_norm: str,
    scenario_paths: dict[str, set[str]],
    mapping: dict,
) -> tuple[list[dict], bool]:
    if not scenario_norm:
        return xmls, True
    filtered: list[dict] = []
    matched = False
    for entry in xmls:
        if _xml_matches_scenario(entry.get('path'), scenario_norm, scenario_paths, mapping):
            filtered.append(entry)
            matched = True
    return filtered, matched


def _build_session_scenario_labels(
    mapping: dict,
    scenario_names: list[str],
    scenario_paths: dict[str, set[str]],
) -> dict[int, str]:
    labels: dict[int, str] = {}
    best_ts: dict[int, float] = {}
    for path, entry in (mapping or {}).items():
        sid = _session_store_entry_session_id(entry)
        if sid is None:
            continue
        label = ''
        if isinstance(entry, dict):
            label = str(entry.get('scenario_name') or entry.get('scenario') or '').strip()
        if not label:
            norm = _session_store_entry_scenario_norm(entry)
            if norm:
                label = _resolve_scenario_display(norm, scenario_names, norm)
        if not label:
            label = _scenario_label_from_path(path, scenario_names, scenario_paths)
        if not label:
            continue

        ts = _session_store_entry_updated_at_epoch(entry)
        if ts is None:
            ts = _safe_path_mtime_epoch(path)
        if ts is None:
            ts = 0.0

        prev = best_ts.get(sid)
        if prev is None or ts >= prev:
            best_ts[sid] = ts
            labels[sid] = label
    return labels


def _annotate_sessions_with_scenarios(
    sessions: list[dict],
    session_labels: dict[int, str],
    scenario_norm: str,
    scenario_names: list[str],
    scenario_paths: dict[str, set[str]],
    scenario_query: str = '',
    scenario_session_ids: Optional[set[int]] = None,
) -> None:
    active_display = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query or '') if scenario_norm else ''
    matched_ids = set(scenario_session_ids or [])
    for session in sessions:
        label: Optional[str] = None
        # If the session dict already carries a scenario name, preserve it (when it
        # matches a known scenario) rather than clobbering it with empty inference.
        try:
            existing = (session.get('scenario_name') or '').strip()
        except Exception:
            existing = ''
        if existing:
            existing_norm = _normalize_scenario_label(existing)
            display = _resolve_scenario_display(existing_norm, scenario_names, existing)
            if display:
                label = display
        sid = session.get('id')
        sid_int: Optional[int]
        try:
            sid_int = int(sid) if sid is not None else None
        except Exception:
            sid_int = None
        # Prefer the live session path over any cached session-id mapping.
        candidate_file = session.get('file') or ''
        candidate_dir = session.get('dir') or ''
        if not label and candidate_file:
            label = _scenario_label_from_path(candidate_file, scenario_names, scenario_paths)
        if not label and candidate_dir:
            label = _scenario_label_from_path(candidate_dir, scenario_names, scenario_paths)
        if not label and sid_int is not None:
            label = session_labels.get(sid_int)
        if not label and sid is not None:
            try:
                label = session_labels.get(int(str(sid)))
            except Exception:
                label = None
        if not label and scenario_norm:
            if sid_int is not None and sid_int in matched_ids:
                label = active_display or scenario_query
            elif _path_matches_scenario(session.get('file'), scenario_norm, scenario_paths) or _path_matches_scenario(session.get('dir'), scenario_norm, scenario_paths):
                label = active_display or scenario_query
        session['scenario_name'] = label or ''


def _augment_core_config_from_secret(core_cfg: Dict[str, Any]) -> Dict[str, Any]:
    if not isinstance(core_cfg, dict):
        return core_cfg
    secret_id_raw = core_cfg.get('core_secret_id')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    if not secret_id:
        return core_cfg
    needs_password = not core_cfg.get('ssh_password')
    missing_fields = [
        key for key in (
            'vm_key',
            'vm_name',
            'vm_node',
            'vmid',
            'proxmox_secret_id',
            'proxmox_target',
        ) if not core_cfg.get(key)
    ]
    missing_transport = [
        key for key in ('host', 'port', 'ssh_host', 'ssh_port', 'ssh_username', 'venv_bin')
        if not core_cfg.get(key)
    ]
    if not (needs_password or missing_fields or missing_transport):
        return core_cfg
    try:
        secret_record = _load_core_credentials(secret_id)
    except RuntimeError:
        secret_record = None
    if not secret_record:
        return core_cfg
    augmented = dict(core_cfg)
    if needs_password:
        augmented['ssh_password'] = secret_record.get('ssh_password_plain') or secret_record.get('password_plain') or augmented.get('ssh_password') or ''
    for field in missing_transport:
        if field == 'host':
            value = secret_record.get('host') or secret_record.get('grpc_host')
        elif field == 'port':
            value = secret_record.get('port') or secret_record.get('grpc_port')
        else:
            value = secret_record.get(field)
        if value not in (None, ''):
            augmented[field] = value
    for field in missing_fields:
        value = secret_record.get(field)
        if value not in (None, ''):
            augmented[field] = value
    return augmented


def _ensure_core_vm_metadata(core_cfg: Dict[str, Any]) -> Dict[str, Any]:
    if not isinstance(core_cfg, dict):
        return core_cfg
    vm_key = str(core_cfg.get('vm_key') or '').strip()
    secret_id = str(core_cfg.get('core_secret_id') or '').strip()
    if vm_key or not secret_id:
        return core_cfg
    try:
        secret_record = _load_core_credentials(secret_id)
    except RuntimeError:
        secret_record = None
    if not secret_record:
        return core_cfg
    enriched = dict(core_cfg)
    for field in ('vm_key', 'vm_name', 'vm_node', 'vmid', 'proxmox_secret_id', 'proxmox_target'):
        if enriched.get(field) in (None, '', {}):
            value = secret_record.get(field)
            if value not in (None, ''):
                enriched[field] = value
    return enriched


def _build_core_vm_summary(core_cfg: Dict[str, Any]) -> tuple[bool, Optional[Dict[str, Any]]]:
    cfg = _ensure_core_vm_metadata(core_cfg)
    vm_key = str(cfg.get('vm_key') or '').strip()
    if not vm_key:
        return False, None
    host = str(cfg.get('host') or cfg.get('grpc_host') or CORE_HOST)
    try:
        port = int(cfg.get('port') or cfg.get('grpc_port') or CORE_PORT)
    except Exception:
        port = CORE_PORT
    ssh_host = str(cfg.get('ssh_host') or host)
    try:
        ssh_port = int(cfg.get('ssh_port') or 22)
    except Exception:
        ssh_port = 22
    ssh_username = str(cfg.get('ssh_username') or '').strip()
    vm_summary = {
        'label': cfg.get('vm_name') or vm_key,
        'node': cfg.get('vm_node') or '',
        'host': host,
        'port': port,
        'ssh_host': ssh_host,
        'ssh_port': ssh_port,
        'ssh_username': ssh_username,
    }
    return True, vm_summary


def _select_core_config_for_page(
    scenario_norm: str,
    history: Optional[List[dict]] = None,
    *,
    include_password: bool = True,
) -> Dict[str, Any]:
    """Pick the most relevant CORE config (with SSH creds) for the CORE page/data views."""

    entries = list(history or _load_run_history())

    def _entry_matches(entry: dict) -> bool:
        if not scenario_norm:
            return True
        names = entry.get('scenario_names') or []
        if not isinstance(names, list):
            return False
        return any(_normalize_scenario_label(name) == scenario_norm for name in names)

    def _combine(entry: dict) -> Dict[str, Any]:
        core_raw = entry.get('core') if isinstance(entry.get('core'), dict) else None
        scenario_core_raw = entry.get('scenario_core') if isinstance(entry.get('scenario_core'), dict) else None
        return _merge_core_configs(core_raw, scenario_core_raw, include_password=include_password)

    def _has_ssh(creds: Dict[str, Any]) -> bool:
        username = str(creds.get('ssh_username') or '').strip()
        password_raw = creds.get('ssh_password')
        if password_raw is None:
            password = ''
        elif isinstance(password_raw, str):
            password = password_raw.strip()
        else:
            password = str(password_raw).strip()
        return bool(username and password)

    scenario_fallback: Dict[str, Any] | None = None
    for entry in reversed(entries):
        if not _entry_matches(entry):
            continue
        combined = _combine(entry)
        if _has_ssh(combined):
            return _ensure_core_vm_metadata(_augment_core_config_from_secret(combined))
        if scenario_fallback is None:
            scenario_fallback = combined
    if scenario_fallback:
        return _ensure_core_vm_metadata(_augment_core_config_from_secret(scenario_fallback))

    general_fallback: Dict[str, Any] | None = None
    for entry in reversed(entries):
        combined = _combine(entry)
        if _has_ssh(combined):
            return _ensure_core_vm_metadata(_augment_core_config_from_secret(combined))
        if general_fallback is None:
            general_fallback = combined
    if general_fallback:
        return _ensure_core_vm_metadata(_augment_core_config_from_secret(general_fallback))

    secret_record = _select_latest_core_secret_record(scenario_norm or None)
    secret_password = None
    if secret_record:
        secret_password = secret_record.get('ssh_password_plain') or secret_record.get('password_plain') or ''
    if secret_record and secret_password:
        secret_cfg = {
            'host': secret_record.get('host') or secret_record.get('grpc_host') or CORE_HOST,
            'port': secret_record.get('port') or secret_record.get('grpc_port') or CORE_PORT,
            'ssh_host': secret_record.get('ssh_host') or secret_record.get('host') or secret_record.get('grpc_host'),
            'ssh_port': secret_record.get('ssh_port') or 22,
            'ssh_username': secret_record.get('ssh_username') or '',
            'ssh_password': secret_password,
            'venv_bin': secret_record.get('venv_bin') or DEFAULT_CORE_VENV_BIN,
            'ssh_enabled': True,
            'core_secret_id': secret_record.get('identifier'),
            'vm_key': secret_record.get('vm_key'),
            'vm_name': secret_record.get('vm_name'),
            'vm_node': secret_record.get('vm_node'),
            'vmid': secret_record.get('vmid'),
            'proxmox_secret_id': secret_record.get('proxmox_secret_id'),
            'proxmox_target': secret_record.get('proxmox_target'),
        }
        merged = _merge_core_configs(secret_cfg, include_password=True)
        return _ensure_core_vm_metadata(_augment_core_config_from_secret(merged))

    defaults = _core_backend_defaults(include_password=include_password)
    return _ensure_core_vm_metadata(_augment_core_config_from_secret(defaults))


def _core_config_for_request(*, include_password: bool = True) -> Dict[str, Any]:
    """Return the best CORE config for the current request context (respecting scenario filter)."""

    scenario_raw = ''
    history: Optional[List[dict]] = None
    if has_request_context():
        try:
            scenario_raw = (request.values.get('scenario') or '').strip()
        except Exception:
            scenario_raw = ''
    scenario_norm = _normalize_scenario_label(scenario_raw)
    try:
        history = _load_run_history()
    except Exception:
        history = []
    cfg = _select_core_config_for_page(scenario_norm, history, include_password=include_password)

    def _has_required_creds(c: Dict[str, Any]) -> bool:
        username = str(c.get('ssh_username') or '').strip()
        if not username:
            return False
        if not include_password:
            return True
        password = c.get('ssh_password')
        if isinstance(password, str):
            return bool(password.strip())
        return bool(password)

    if not _has_required_creds(cfg):
        fallback = _core_backend_defaults(include_password=include_password)
        cfg = _merge_core_configs(cfg, fallback, include_password=include_password)
    return cfg

def _extract_report_path_from_text(text: str, *, require_exists: bool = True) -> str | None:
    """Parse CLI output to extract the most recent report path reference."""

    if not text:
        return None
    matches = list(re.finditer(r"Scenario report written to\s+(.+)", text))
    for m in reversed(matches):
        path = m.group(1).strip().rstrip(' .')
        if not os.path.isabs(path):
            repo_root = _get_repo_root()
            path = os.path.abspath(os.path.join(repo_root, path))
        if not require_exists or os.path.exists(path):
            return path
    return None

def _find_latest_report_path(since_ts: float | None = None) -> str | None:
    """Find the most recent scenario_report_*.md under the repo reports directory.

    If since_ts is provided (epoch seconds), prefer files modified after this time.
    """
    try:
        report_dir = _reports_dir()
        if not os.path.isdir(report_dir):
            return None
        cands = []
        for name in os.listdir(report_dir):
            if not name.endswith('.md'):
                continue
            if not name.startswith('scenario_report_'):
                continue
            p = os.path.join(report_dir, name)
            try:
                st = os.stat(p)
                if since_ts is None or st.st_mtime >= max(0.0, float(since_ts) - 5.0):
                    cands.append((st.st_mtime, p))
            except Exception:
                continue
        if not cands:
            return None
        cands.sort(key=lambda x: x[0], reverse=True)
        return cands[0][1]
    except Exception:
        return None


def _derive_summary_from_report(report_path: str | None) -> str | None:
    try:
        if not report_path:
            return None
        candidate = os.path.splitext(report_path)[0] + '.json'
        if os.path.exists(candidate):
            return candidate
    except Exception:
        pass
    return None


def _extract_summary_path_from_text(text: str, *, require_exists: bool = True) -> str | None:
    """Parse CLI output to extract the most recent summary path reference."""

    if not text:
        return None
    try:
        matches = list(re.finditer(r"Scenario summary written to\s+(.+)", text))
        for m in reversed(matches):
            path = m.group(1).strip().rstrip(' .')
            if not os.path.isabs(path):
                repo_root = _get_repo_root()
                path = os.path.abspath(os.path.join(repo_root, path))
            if not require_exists or os.path.exists(path):
                return path
    except Exception:
        pass
    return None


def _extract_docker_conflicts_from_text(text: str) -> dict | None:
    """Parse CLI logs for machine-readable Docker conflict info.

    Expected line emitted by core_topo_gen.cli:
        DOCKER_CONFLICTS_JSON: {"containers": [...], "images": [...]} 
    """

    if not text:
        return None
    try:
        matches = list(re.finditer(r"DOCKER_CONFLICTS_JSON:\s*(\{.*\})\s*$", text, flags=re.MULTILINE))
        if not matches:
            return None
        raw = matches[-1].group(1)
        obj = json.loads(raw)
        if not isinstance(obj, dict):
            return None
        containers = obj.get('containers')
        images = obj.get('images')
        if not isinstance(containers, list):
            containers = []
        if not isinstance(images, list):
            images = []
        containers = [str(x) for x in containers if str(x).strip()]
        images = [str(x) for x in images if str(x).strip()]
        containers = list(dict.fromkeys(containers))
        images = list(dict.fromkeys(images))
        if not containers and not images:
            return None
        return {'containers': containers, 'images': images}
    except Exception:
        return None


def _find_latest_summary_path(since_ts: float | None = None) -> str | None:
    try:
        report_dir = _reports_dir()
        if not os.path.isdir(report_dir):
            return None
        cands = []
        for name in os.listdir(report_dir):
            if not name.endswith('.json'):
                continue
            if not name.startswith('scenario_report_'):
                continue
            p = os.path.join(report_dir, name)
            try:
                st = os.stat(p)
                if since_ts is None or st.st_mtime >= max(0.0, float(since_ts) - 5.0):
                    cands.append((st.st_mtime, p))
            except Exception:
                continue
        if not cands:
            return None
        cands.sort(key=lambda x: x[0], reverse=True)
        return cands[0][1]
    except Exception:
        return None

def _extract_session_id_from_text(text: str) -> str | None:
    """Parse CLI logs for the session id marker emitted by core_topo_gen.cli.

    Expected line:
        CORE_SESSION_ID: <id>
    """
    try:
        if not text:
            return None
        m = re.search(r"CORE_SESSION_ID:\s*(\S+)", text)
        if m:
            return m.group(1)
    except Exception:
        pass
    return None


def _extract_session_id_from_core_path(text: str) -> int | None:
    """Best-effort: extract a CORE session id from a CORE VM path or filename.

    Examples:
      - /tmp/pycore.17/Scenario_1.xml -> 17
      - /tmp/pycore.17 -> 17
      - core-session-17-20251227-153012.xml -> 17
      - session-17.xml -> 17
    """
    if not text:
        return None
    s = str(text)
    try:
        m = re.search(r"(?:^|/|\\)pycore\.(\d+)(?:/|\\|$)", s)
        if m:
            return int(m.group(1))
    except Exception:
        pass
    try:
        m = re.search(r"\bcore-session-(\d+)\b", s)
        if m:
            return int(m.group(1))
    except Exception:
        pass
    try:
        m = re.search(r"\bsession-(\d+)\b", s)
        if m:
            return int(m.group(1))
    except Exception:
        pass
    return None


def _session_store_scenario_for_session_id(store: dict, session_id: int, *, host: str, port: int) -> str | None:
    """Return the newest known scenario_name for a given session id (scoped by CORE host/port)."""
    if not isinstance(store, dict) or not store:
        return None
    best_label: str | None = None
    best_ts: float | None = None
    for _path, entry in store.items():
        try:
            sid = _session_store_entry_session_id(entry)
            if sid is None or int(sid) != int(session_id):
                continue
            if not _session_store_entry_matches_core(entry, host, port):
                continue
            ts = _session_store_entry_updated_at_epoch(entry)
            if ts is None:
                ts = 0.0
            label = (entry.get('scenario_name') or '').strip() if isinstance(entry, dict) else ''
            if not label:
                continue
            if best_ts is None or ts >= best_ts:
                best_ts = ts
                best_label = label
        except Exception:
            continue
    return best_label


def _remote_write_session_scenario_meta_script(
    session_id: int,
    scenario_name: str | None,
    scenario_norm: str | None,
    scenario_xml_basename: str | None,
) -> str:
    sid_lit = json.dumps(int(session_id))
    name_lit = json.dumps((scenario_name or '').strip())
    norm_lit = json.dumps((scenario_norm or '').strip())
    base_lit = json.dumps((scenario_xml_basename or '').strip())
    template = textwrap.dedent(
        """
import json
import os
import time
import traceback


def main():
    payload = {}
    try:
        sid = __SID__
        meta_dir = '/tmp/core-topo-gen/session-meta'
        os.makedirs(meta_dir, exist_ok=True)
        meta_path = os.path.join(meta_dir, f"{sid}.json")
        meta = {
            'session_id': sid,
            'scenario_name': __SCEN_NAME__,
            'scenario_norm': __SCEN_NORM__,
            'scenario_xml_basename': __SCEN_BASE__,
            'written_at_epoch': time.time(),
        }
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(meta, f, indent=2, sort_keys=True)
        payload['ok'] = True
        payload['meta_path'] = meta_path
    except Exception as exc:
        payload['ok'] = False
        payload['error'] = str(exc)
        payload['traceback'] = traceback.format_exc()
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    )
    script = template.replace('__SID__', sid_lit)
    script = script.replace('__SCEN_NAME__', name_lit)
    script = script.replace('__SCEN_NORM__', norm_lit)
    script = script.replace('__SCEN_BASE__', base_lit)
    return script


def _remote_read_session_scenario_meta_script(session_id: int) -> str:
    sid_lit = json.dumps(int(session_id))
    template = textwrap.dedent(
        """
import json
import os
import traceback


def main():
    payload = {}
    try:
        sid = __SID__
        meta_path = os.path.join('/tmp/core-topo-gen/session-meta', f"{sid}.json")
        if not os.path.exists(meta_path):
            raise FileNotFoundError(meta_path)
        with open(meta_path, 'r', encoding='utf-8') as f:
            meta = json.load(f)
        payload['ok'] = True
        payload['meta'] = meta
        payload['meta_path'] = meta_path
    except Exception as exc:
        payload['ok'] = False
        payload['error'] = str(exc)
        payload['traceback'] = traceback.format_exc()
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    )
    return template.replace('__SID__', sid_lit)


def _remote_read_session_scenario_meta_bulk_script(session_ids: list[int]) -> str:
    ids_lit = json.dumps([int(x) for x in (session_ids or [])])
    template = textwrap.dedent(
        """
import json
import os
import traceback


def main():
    payload = {}
    try:
        sids = __SIDS__
        out = {}
        for sid in sids:
            try:
                meta_path = os.path.join('/tmp/core-topo-gen/session-meta', f"{int(sid)}.json")
                if not os.path.exists(meta_path):
                    continue
                with open(meta_path, 'r', encoding='utf-8') as f:
                    meta = json.load(f)
                if isinstance(meta, dict):
                    out[int(sid)] = meta
            except Exception:
                continue
        payload['ok'] = True
        payload['meta_by_sid'] = out
    except Exception as exc:
        payload['ok'] = False
        payload['error'] = str(exc)
        payload['traceback'] = traceback.format_exc()
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    )
    return template.replace('__SIDS__', ids_lit)


def _write_remote_session_scenario_meta(
    core_cfg: Dict[str, Any],
    *,
    session_id: int,
    scenario_name: str | None,
    scenario_xml_basename: str | None = None,
    logger: Optional[logging.Logger] = None,
) -> None:
    """Write a small session->scenario mapping file on the CORE VM.

    This makes it possible to map a *session XML path on the CORE VM* back to the
    scenario by extracting the session id from the path and reading this file.
    """
    log = logger or getattr(app, 'logger', logging.getLogger(__name__))
    scenario_label = (scenario_name or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label) if scenario_label else ''
    script = _remote_write_session_scenario_meta_script(
        int(session_id),
        scenario_label,
        scenario_norm,
        scenario_xml_basename,
    )
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    command_desc = f"remote ssh {ssh_user}@{ssh_host} -> write /tmp/core-topo-gen/session-meta/{int(session_id)}.json"
    try:
        payload = _run_remote_python_json(
            cfg,
            script,
            logger=log,
            label='core.write_session_meta',
            command_desc=command_desc,
            timeout=30.0,
        )
        if payload.get('error'):
            log.debug('[core.session_meta] remote error: %s', payload.get('error'))
    except Exception as exc:
        log.debug('[core.session_meta] write failed: %s', exc)


def _read_remote_session_scenario_meta(
    core_cfg: Dict[str, Any],
    *,
    session_id: int,
    logger: Optional[logging.Logger] = None,
) -> dict[str, Any] | None:
    log = logger or getattr(app, 'logger', logging.getLogger(__name__))
    script = _remote_read_session_scenario_meta_script(int(session_id))
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    command_desc = f"remote ssh {ssh_user}@{ssh_host} -> read /tmp/core-topo-gen/session-meta/{int(session_id)}.json"
    try:
        payload = _run_remote_python_json(
            cfg,
            script,
            logger=log,
            label='core.read_session_meta',
            command_desc=command_desc,
            timeout=30.0,
        )
        if payload.get('ok') and isinstance(payload.get('meta'), dict):
            return payload.get('meta')
    except Exception as exc:
        log.debug('[core.session_meta] read failed: %s', exc)
    return None


def _read_local_session_scenario_meta_bulk(session_ids: list[int]) -> dict[int, dict[str, Any]]:
    """Best-effort local read of session-meta files.

    When the webapp runs on the CORE VM, the session-meta directory is directly
    accessible and SSH may be disabled/unavailable.
    """
    ids = [int(x) for x in (session_ids or []) if x not in (None, '')]
    if not ids:
        return {}
    out: dict[int, dict[str, Any]] = {}
    meta_dir = '/tmp/core-topo-gen/session-meta'
    try:
        if not os.path.isdir(meta_dir):
            return {}
    except Exception:
        return {}
    for sid in ids:
        try:
            meta_path = os.path.join(meta_dir, f"{int(sid)}.json")
            if not os.path.exists(meta_path):
                continue
            with open(meta_path, 'r', encoding='utf-8') as f:
                meta = json.load(f)
            if isinstance(meta, dict):
                out[int(sid)] = meta
        except Exception:
            continue
    return out


def _read_remote_session_scenario_meta_bulk(
    core_cfg: Dict[str, Any],
    *,
    session_ids: list[int],
    logger: Optional[logging.Logger] = None,
) -> dict[int, dict[str, Any]]:
    """Read multiple session meta files from the CORE VM in one remote call."""
    log = logger or getattr(app, 'logger', logging.getLogger(__name__))
    ids = [int(x) for x in (session_ids or []) if x not in (None, '')]
    if not ids:
        return {}

    # Fast-path: if we are running on the CORE VM, read directly.
    local = _read_local_session_scenario_meta_bulk(ids)
    if local and len(local) == len(set(ids)):
        return local
    script = _remote_read_session_scenario_meta_bulk_script(ids)
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    command_desc = f"remote ssh {ssh_user}@{ssh_host} -> read /tmp/core-topo-gen/session-meta/*.json (bulk)"
    try:
        payload = _run_remote_python_json(
            cfg,
            script,
            logger=log,
            label='core.read_session_meta_bulk',
            command_desc=command_desc,
            timeout=30.0,
        )
        if payload.get('ok') and isinstance(payload.get('meta_by_sid'), dict):
            out: dict[int, dict[str, Any]] = {}
            for k, v in payload.get('meta_by_sid', {}).items():
                try:
                    sid = int(k)
                except Exception:
                    continue
                if isinstance(v, dict):
                    out[sid] = v
            if local:
                merged = dict(local)
                merged.update(out)
                return merged
            return out
    except Exception as exc:
        log.debug('[core.session_meta] bulk read failed: %s', exc)
    return local or {}


def _session_store_updated_at_for_session_id(store: dict, session_id: int, *, host: str, port: int) -> float | None:
    """Return the newest known updated_at epoch for a session id (scoped by CORE host/port)."""
    if not isinstance(store, dict) or not store:
        return None
    best_ts: float | None = None
    for _path, entry in store.items():
        try:
            sid = _session_store_entry_session_id(entry)
            if sid is None or int(sid) != int(session_id):
                continue
            if not _session_store_entry_matches_core(entry, host, port):
                continue
            ts = _session_store_entry_updated_at_epoch(entry)
            if ts is None:
                continue
            if best_ts is None or ts >= best_ts:
                best_ts = ts
        except Exception:
            continue
    return best_ts


def _scenario_timestamped_filename(scenario_name: str | None, ts_epoch: float | None) -> str:
    """Build <scenario-name><timestamp>.xml (timestamp includes leading '-') for UI display."""
    try:
        stem = secure_filename((scenario_name or 'scenario')).strip('_-.') or 'scenario'
    except Exception:
        stem = 'scenario'
    try:
        epoch = float(ts_epoch) if ts_epoch is not None else None
    except Exception:
        epoch = None
    if epoch is None or epoch <= 0:
        try:
            epoch = time.time()
        except Exception:
            epoch = 0.0
    try:
        ts = datetime.datetime.fromtimestamp(epoch, tz=datetime.timezone.utc).strftime('-%Y%m%d-%H%M%S')
    except Exception:
        ts = '-unknown'
    return f"{stem}{ts}.xml"

def _safe_add_to_zip(zf: zipfile.ZipFile, abs_path: str, arcname: str) -> None:
    try:
        if abs_path and os.path.exists(abs_path):
            zf.write(abs_path, arcname)
    except Exception:
        pass

def _gather_scripts_into_zip(zf: zipfile.ZipFile, scenario_dir: str | None = None) -> int:
    """Collect generated traffic and segmentation artifacts into the provided zip file.

    This now walks both persistent output directories and the runtime `/tmp`
    locations used by the CLI so that *all* generated scripts and supporting
    files (JSON summaries, helper assets, custom plugin payloads, etc.) are
    included in the bundle. Returns the count of files added.
    """
    added = 0
    seen: set[str] = set()

    def _collect(label: str, dir_candidates: list[str]) -> None:
        nonlocal added
        for base in dir_candidates:
            if not base:
                continue
            try:
                base_abs = os.path.abspath(base)
            except Exception:
                base_abs = base
            if not base_abs or not os.path.isdir(base_abs):
                continue
            for root, dirs, files in os.walk(base_abs):
                # Skip hidden directories to avoid noise like .cache/.DS_Store
                dirs[:] = [d for d in dirs if not d.startswith('.')]
                for fname in files:
                    if fname.startswith('.'):
                        continue
                    try:
                        path = os.path.join(root, fname)
                        if not os.path.isfile(path) or os.path.islink(path):
                            continue
                        rel = os.path.relpath(path, base_abs)
                        # Defensive: ignore paths that navigate upwards
                        if rel.startswith('..'):
                            continue
                        rel = rel.replace('\\', '/')
                        arcname = f"{label}/{rel}".lstrip('/')
                        key = arcname.lower()
                        if key in seen:
                            continue
                        _safe_add_to_zip(zf, path, arcname)
                        seen.add(key)
                        added += 1
                    except Exception:
                        continue

    traffic_dirs = []
    try:
        traffic_dirs.append(_traffic_dir())
    except Exception:
        pass
    # Runtime traffic scripts live under /tmp/traffic by default
    traffic_dirs.extend(filter(None, [
        os.path.join(_outputs_dir(), 'traffic'),
        '/tmp/traffic',
        os.path.join(scenario_dir, 'traffic') if scenario_dir else None,
    ]))
    # Preserve order but drop duplicates
    traffic_dirs = list(dict.fromkeys(traffic_dirs))

    segmentation_dirs = []
    try:
        segmentation_dirs.append(_segmentation_dir())
    except Exception:
        pass
    segmentation_dirs.extend(filter(None, [
        os.path.join(_outputs_dir(), 'segmentation'),
        '/tmp/segmentation',
        os.path.join(scenario_dir, 'segmentation') if scenario_dir else None,
    ]))
    segmentation_dirs = list(dict.fromkeys(segmentation_dirs))

    _collect('traffic', traffic_dirs)
    _collect('segmentation', segmentation_dirs)
    return added

def _normalize_core_device_types(xml_path: str) -> None:
    """Normalize device 'type' attributes in a saved CORE session XML.

    - Docker/podman devices (class='docker'/'podman' or with compose attrs) -> type='docker'
    - Devices with routing services (zebra/BGP/OSPF*/RIP*/Xpimd) -> type='router'
    - Otherwise -> type='PC'
    """
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        devices = root.find('devices')
        if devices is None:
            return
        routing_like = {"zebra", "BGP", "Babel", "OSPFv2", "OSPFv3", "OSPFv3MDR", "RIP", "RIPNG", "Xpimd"}
        changed = False
        for dev in list(devices):
            if not isinstance(dev.tag, str) or dev.tag != 'device':
                continue
            clazz = (dev.get('class') or '').strip().lower()
            compose = (dev.get('compose') or '').strip()
            compose_name = (dev.get('compose_name') or '').strip()
            dtype = dev.get('type') or ''
            # collect services
            svc_names = set()
            try:
                services_el = dev.find('services') or dev.find('configservices')
                if services_el is not None:
                    for s in list(services_el):
                        nm = s.get('name')
                        if nm:
                            svc_names.add(nm)
            except Exception:
                pass
            new_type = None
            if clazz in ('docker', 'podman') or compose or compose_name:
                new_type = 'docker'
            elif any(s in routing_like for s in svc_names):
                new_type = 'router'
            else:
                new_type = 'PC'
            if new_type and new_type != dtype:
                dev.set('type', new_type)
                changed = True
        if changed:
            try:
                raw = ET.tostring(root, encoding='utf-8')
                lroot = LET.fromstring(raw)
                pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
                with open(xml_path, 'wb') as f:
                    f.write(pretty)
            except Exception:
                tree.write(xml_path, encoding='utf-8', xml_declaration=True)
    except Exception:
        pass

def _write_single_scenario_xml(src_xml_path: str, scenario_name: str | None, out_dir: str | None = None) -> str | None:
    """Create a new XML file containing only the selected Scenario from a Scenarios XML.

    - If `scenario_name` is None, selects the first Scenario present.
    - Returns the path to the new XML written under `out_dir` (or next to the source file) or None on failure.
    """
    try:
        if not (src_xml_path and os.path.exists(src_xml_path)):
            return None
        tree = ET.parse(src_xml_path)
        root = tree.getroot()
        # Normalize: if file is a single ScenarioEditor root, just copy it under Scenarios/Scenario
        chosen_se = None
        chosen_name = scenario_name
        if root.tag == 'Scenarios':
            # find Scenario child with matching name, else use first
            scenarios = [c for c in list(root) if isinstance(c.tag, str) and c.tag == 'Scenario']
            target = None
            if chosen_name:
                for s in scenarios:
                    if (s.get('name') or '') == chosen_name:
                        target = s
                        break
            if target is None and scenarios:
                target = scenarios[0]
                chosen_name = target.get('name') or 'Scenario'
            if target is None:
                return None
            se = target.find('ScenarioEditor')
            if se is None:
                # allow copying entire Scenario element if no ScenarioEditor child
                chosen_se = target
            else:
                chosen_se = se
        elif root.tag == 'ScenarioEditor':
            chosen_se = root
            if not chosen_name:
                # attempt to infer from nested metadata (not guaranteed)
                chosen_name = 'Scenario'
        else:
            # if root is Scenario, accept it
            if root.tag == 'Scenario':
                chosen_se = root.find('ScenarioEditor') or root
                chosen_name = chosen_name or (root.get('name') or 'Scenario')
            else:
                return None
        # Build new XML
        new_root = ET.Element('Scenarios')
        scen_el = ET.SubElement(new_root, 'Scenario')
        scen_el.set('name', chosen_name or 'Scenario')
        if chosen_se.tag == 'ScenarioEditor':
            # deep copy ScenarioEditor
            scen_el.append(ET.fromstring(ET.tostring(chosen_se)))
        else:
            # chosen_se was Scenario; append its contents
            scen_el.append(ET.fromstring(ET.tostring(chosen_se.find('ScenarioEditor'))) if chosen_se.find('ScenarioEditor') is not None else ET.Element('ScenarioEditor'))
        new_tree = ET.ElementTree(new_root)
        # Determine output path
        base_dir = out_dir or os.path.dirname(os.path.abspath(src_xml_path))
        os.makedirs(base_dir, exist_ok=True)
        stem = secure_filename((chosen_name or 'scenario')).strip('_-.') or 'scenario'
        out_path = os.path.join(base_dir, f"{stem}.xml")
        try:
            raw = ET.tostring(new_tree.getroot(), encoding='utf-8')
            lroot = LET.fromstring(raw)
            pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
            with open(out_path, 'wb') as f:
                f.write(pretty)
        except Exception:
            new_tree.write(out_path, encoding='utf-8', xml_declaration=True)
        return out_path if os.path.exists(out_path) else None
    except Exception:
        return None

def _build_full_scenario_archive(out_dir: str, scenario_xml_path: str | None, report_path: str | None, pre_xml_path: str | None, post_xml_path: str | None, *, summary_path: str | None = None, run_id: str | None = None) -> str | None:
    """Create a zip bundle that includes the scenario XML, pre/post session XML, report, and any generated scripts.

    Returns the path to the created zip, or None on failure.
    """
    try:
        os.makedirs(out_dir, exist_ok=True)
        stem = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S')
        if run_id:
            stem = f"{stem}-{run_id[:8]}"
        zip_path = os.path.join(out_dir, f"full_scenario_{stem}.zip")
        scenario_dir = os.path.dirname(os.path.abspath(scenario_xml_path)) if scenario_xml_path else None
        with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:
            # Add top-level artifacts if present
            if scenario_xml_path and os.path.exists(scenario_xml_path):
                _safe_add_to_zip(zf, scenario_xml_path, "scenario.xml")
            if report_path and os.path.exists(report_path):
                _safe_add_to_zip(zf, report_path, os.path.join("report", os.path.basename(report_path)))
            if summary_path and os.path.exists(summary_path):
                _safe_add_to_zip(zf, summary_path, os.path.join("report", os.path.basename(summary_path)))
            csv_candidate = f"{report_path}.connectivity.csv" if report_path else None
            if csv_candidate and os.path.exists(csv_candidate):
                _safe_add_to_zip(zf, csv_candidate, os.path.join("report", os.path.basename(csv_candidate)))
            if pre_xml_path and os.path.exists(pre_xml_path):
                _safe_add_to_zip(zf, pre_xml_path, os.path.join("core-session", os.path.basename(pre_xml_path)))
            if post_xml_path and os.path.exists(post_xml_path):
                _safe_add_to_zip(zf, post_xml_path, os.path.join("core-session", os.path.basename(post_xml_path)))
            # Add generated scripts and summaries
            _gather_scripts_into_zip(zf, scenario_dir)
        return zip_path if os.path.exists(zip_path) else None
    except Exception:
        return None

# Data sources state
DATA_SOURCES_DIR = os.path.join(_get_repo_root(), 'data_sources')
DATA_STATE_PATH = os.path.join(DATA_SOURCES_DIR, '_state.json')
os.makedirs(DATA_SOURCES_DIR, exist_ok=True)

# Flag catalog sources state (JSON)
FLAG_SOURCES_DIR = os.path.join(DATA_SOURCES_DIR, 'flags')
FLAG_STATE_PATH = os.path.join(FLAG_SOURCES_DIR, '_state.json')
os.makedirs(FLAG_SOURCES_DIR, exist_ok=True)

# Flag generator sources state (JSON)
FLAG_GENERATORS_SOURCES_DIR = os.path.join(DATA_SOURCES_DIR, 'flag_generators')
FLAG_GENERATORS_STATE_PATH = os.path.join(FLAG_GENERATORS_SOURCES_DIR, '_state.json')
os.makedirs(FLAG_GENERATORS_SOURCES_DIR, exist_ok=True)

# Flag node-generator sources state (JSON)
FLAG_NODE_GENERATORS_SOURCES_DIR = os.path.join(DATA_SOURCES_DIR, 'flag_node_generators')
FLAG_NODE_GENERATORS_STATE_PATH = os.path.join(FLAG_NODE_GENERATORS_SOURCES_DIR, '_state.json')
os.makedirs(FLAG_NODE_GENERATORS_SOURCES_DIR, exist_ok=True)


# ---------------- Flag Generator Catalog Schema (v3) -----------------


_FLAG_GENERATOR_CATALOG_SCHEMA_PATH = os.path.join(_get_repo_root(), 'validation', 'flag_generator_catalog.schema.json')
_JSON_SCHEMA_CACHE: dict[str, dict] = {}


def _load_json_schema_file(schema_path: str) -> dict | None:
    try:
        schema_path = os.path.abspath(schema_path)
        if schema_path in _JSON_SCHEMA_CACHE:
            return _JSON_SCHEMA_CACHE[schema_path]
        if not os.path.exists(schema_path):
            return None
        with open(schema_path, 'r', encoding='utf-8') as fh:
            schema = json.load(fh)
        if isinstance(schema, dict):
            _JSON_SCHEMA_CACHE[schema_path] = schema
            return schema
        return None
    except Exception:
        return None


def _validate_json_schema(instance: object, schema_path: str) -> tuple[bool, str | None]:
    """Best-effort JSON Schema validation.

    Returns (ok, error_message_or_none). If jsonschema isn't installed or schema can't be loaded,
    validation is skipped (ok=True).
    """
    try:
        import jsonschema  # type: ignore
        from jsonschema.exceptions import ValidationError  # type: ignore
    except Exception:
        return True, None

    schema = _load_json_schema_file(schema_path)
    if not schema:
        return True, None
    try:
        jsonschema.validate(instance=instance, schema=schema)
        return True, None
    except ValidationError as e:
        try:
            loc = '/'.join(str(p) for p in list(e.absolute_path))
        except Exception:
            loc = ''
        loc = loc or '(root)'
        return False, f"Schema validation failed at {loc}: {e.message}"
    except Exception as e:
        return False, f"Schema validation failed: {e}"


FLAG_GENERATOR_LANGUAGES = {'python', 'c', 'cpp'}


def _coerce_flaggen_io_type(name: str) -> str:
    n = (name or '').strip().lower()
    if not n:
        return 'text'
    if 'private_key' in n or n.endswith('.key'):
        return 'text'
    if 'password' in n or 'secret' in n:
        return 'text'
    if 'token' in n:
        return 'text'
    if 'flag' in n:
        return 'flag'
    return 'text'


def _coerce_flaggen_io_sensitive(name: str, tp: str) -> bool:
    n = (name or '').strip().lower()
    t = (tp or '').strip().lower()
    if t == 'flag':
        return True
    if any(k in n for k in ('password', 'secret', 'token', 'private_key', 'ssh.private_key')):
        return True
    return False


def _generator_defs_from_flag_catalog_items(items: list[dict]) -> list[dict]:
    """Deprecated: no longer converting flag-catalog templates into generators."""
    return []


def _load_flag_generator_sources_state() -> dict:
    """Load flag generator sources state.

    Mirrors the flag sources registry, but stores generator-catalog sources.
    Auto-seeds a default generator catalog from data_sources/flag_generators_seed.json when present.
    """
    try:
        if not os.path.exists(FLAG_GENERATORS_STATE_PATH):
            seed_path = os.path.join(DATA_SOURCES_DIR, 'flag_generators_seed.json')
            sources: list[dict] = []
            if os.path.exists(seed_path):
                try:
                    unique = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:6]
                    dest = os.path.join(FLAG_GENERATORS_SOURCES_DIR, f"{unique}-flag_generators.json")
                    # Copy seed into managed sources.
                    shutil.copyfile(seed_path, dest)
                    ok, note, _doc, _skipped = _validate_and_normalize_flag_generator_source_json(dest)
                    sources.append({
                        'id': uuid.uuid4().hex[:12],
                        'name': 'flag_generators_seed.json',
                        'path': dest,
                        'enabled': True,
                        'rows': note if ok else f"ERR: {note}",
                        'uploaded': datetime.datetime.utcnow().isoformat() + 'Z',
                    })
                except Exception:
                    pass
            state = {'sources': sources}
            _save_flag_generator_sources_state(state)
            return state
        with open(FLAG_GENERATORS_STATE_PATH, 'r', encoding='utf-8') as fh:
            state = json.load(fh)
        if not isinstance(state, dict):
            return {'sources': []}
        sources = state.get('sources')
        if not isinstance(sources, list):
            state['sources'] = []
        # Keep seeded generator catalog in sync with repo seed.
        try:
            repo_seed = os.path.abspath(os.path.join(DATA_SOURCES_DIR, 'flag_generators_seed.json'))
            if os.path.exists(repo_seed):
                mutated = False
                for s in state.get('sources', []):
                    if not isinstance(s, dict):
                        continue
                    if (s.get('name') or '') != 'flag_generators_seed.json':
                        continue
                    p = s.get('path')
                    if not p or not os.path.exists(p):
                        continue
                    # Always refresh the managed copy from the repo seed.
                    try:
                        shutil.copyfile(repo_seed, p)
                        ok, note, normalized_doc, _skipped = _validate_and_normalize_flag_generator_source_json(p)
                        if ok and normalized_doc:
                            tmp = p + '.tmp'
                            with open(tmp, 'w', encoding='utf-8') as fh:
                                json.dump(normalized_doc, fh, indent=2)
                            os.replace(tmp, p)
                            s['rows'] = note
                            s['uploaded'] = datetime.datetime.utcnow().isoformat() + 'Z'
                        mutated = True
                    except Exception:
                        pass
                if mutated:
                    _save_flag_generator_sources_state(state)
        except Exception:
            pass

        return state
    except Exception:
        return {'sources': []}


def _save_flag_generator_sources_state(state: dict) -> None:
    try:
        os.makedirs(FLAG_GENERATORS_SOURCES_DIR, exist_ok=True)
        tmp = FLAG_GENERATORS_STATE_PATH + '.tmp'
        with open(tmp, 'w', encoding='utf-8') as fh:
            json.dump(state, fh, indent=2)
        os.replace(tmp, FLAG_GENERATORS_STATE_PATH)
    except Exception:
        pass


def _coerce_bool(val, default: bool = False) -> bool:
    try:
        if isinstance(val, bool):
            return val
        if isinstance(val, (int, float)):
            return bool(val)
        if isinstance(val, str):
            v = val.strip().lower()
            if v in {'true', '1', 'yes', 'y', 'on'}:
                return True
            if v in {'false', '0', 'no', 'n', 'off'}:
                return False
    except Exception:
        pass
    return default


def _normalize_generator_id(val: str) -> str:
    raw = (val or '').strip().lower()
    if not raw:
        return ''
    # Keep it simple and stable: allow a-z0-9._-
    out = []
    for ch in raw:
        if ch.isalnum() or ch in {'.', '_', '-'}:
            out.append(ch)
        elif ch.isspace():
            out.append('-')
    return ''.join(out).strip('-')


def _coerce_str(val: object, default: str = '') -> str:
    try:
        if val is None:
            return default
        s = str(val)
        return s
    except Exception:
        return default


def _coerce_stripped(val: object, default: str = '') -> str:
    return _coerce_str(val, default=default).strip()


def _normalize_v3_source(source: object) -> dict | None:
    if not isinstance(source, dict):
        return None
    src_type = _coerce_stripped(source.get('type') if isinstance(source, dict) else None, 'local-path') or 'local-path'
    src_path = _coerce_stripped(source.get('path') if isinstance(source, dict) else None, '')
    if not src_path:
        return None
    return {
        'type': src_type,
        'path': src_path,
        'ref': _coerce_stripped(source.get('ref') if isinstance(source, dict) else None, ''),
        'subpath': _coerce_stripped(source.get('subpath') if isinstance(source, dict) else None, ''),
        'entry': _coerce_stripped(source.get('entry') if isinstance(source, dict) else None, ''),
    }


def _normalize_v3_compose(compose: object) -> dict | None:
    if not isinstance(compose, dict):
        return None
    file_val = _coerce_stripped(compose.get('file'), 'docker-compose.yml') or 'docker-compose.yml'
    service_val = _coerce_stripped(compose.get('service'), 'generator') or 'generator'
    return {
        'file': file_val,
        'service': service_val,
    }


def _normalize_v3_env(env: object) -> dict | None:
    if not isinstance(env, dict):
        return None
    norm_env: dict[str, str] = {}
    for k, v in env.items():
        try:
            kk = str(k).strip()
            if not kk:
                continue
            norm_env[kk] = str(v)
        except Exception:
            continue
    return norm_env or None


def _v3_plugin_inputs_to_io_list(plugin: dict) -> list[dict]:
    """Convert v3 plugin.inputs mapping to the UI/runner IO list format."""
    requires = plugin.get('requires') if isinstance(plugin.get('requires'), list) else []
    requires_set = {str(x).strip() for x in requires if str(x).strip()}

    inputs = plugin.get('inputs') if isinstance(plugin.get('inputs'), dict) else {}
    out: list[dict] = []

    for name, spec in inputs.items():
        nm = _coerce_stripped(name, '')
        if not nm:
            continue
        if not isinstance(spec, dict):
            spec = {}
        tp = _coerce_stripped(spec.get('type'), '') or _coerce_flaggen_io_type(nm)
        required_val = spec.get('required')
        if required_val is None:
            required = nm in requires_set
        else:
            required = _coerce_bool(required_val, default=(nm in requires_set))
        sensitive_in = spec.get('sensitive')
        if sensitive_in is None:
            sensitive_val = _coerce_flaggen_io_sensitive(nm, tp)
        else:
            sensitive_val = _coerce_bool(sensitive_in, default=False)
        default_in = spec.get('default')
        default_ok = isinstance(default_in, (str, int, float, bool)) or default_in is None
        rec = {
            'name': nm,
            'type': tp,
            'description': _coerce_stripped(spec.get('description'), ''),
            'required': bool(required),
            'sensitive': bool(sensitive_val),
        }
        if default_in is not None and default_ok:
            rec['default'] = default_in
        out.append(rec)

    existing_names = {str(x.get('name') or '').strip() for x in out if isinstance(x, dict)}
    for req in sorted(requires_set):
        if req in existing_names:
            continue
        tp = _coerce_flaggen_io_type(req)
        out.append({
            'name': req,
            'type': tp,
            'description': '',
            'required': True,
            'sensitive': bool(_coerce_flaggen_io_sensitive(req, tp)),
        })

    out.sort(key=lambda r: str(r.get('name') or '').lower())
    return out


def _v3_plugin_outputs_to_io_list(plugin: dict) -> list[dict]:
    produces = plugin.get('produces') if isinstance(plugin.get('produces'), list) else []
    out: list[dict] = []
    for item in produces:
        if not isinstance(item, dict):
            continue
        nm = _coerce_stripped(item.get('artifact'), '')
        if not nm:
            continue
        tp = _coerce_flaggen_io_type(nm)
        out.append({
            'name': nm,
            'type': tp,
            'description': _coerce_stripped(item.get('description'), ''),
            'sensitive': bool(_coerce_flaggen_io_sensitive(nm, tp)),
        })
    return out


def _v3_merge_generator_view(plugin: dict, impl: dict) -> dict | None:
    plugin_id = _normalize_generator_id(_coerce_stripped(impl.get('plugin_id'), ''))
    if not plugin_id:
        return None

    language = _coerce_stripped(impl.get('language'), '').lower()
    if language not in FLAG_GENERATOR_LANGUAGES:
        return None

    source = _normalize_v3_source(impl.get('source'))
    if not source:
        return None

    gen: dict = {
        'id': plugin_id,
        'name': _coerce_stripped(impl.get('name'), plugin_id) or plugin_id,
        'language': language,
        'source': source,
    }

    if isinstance(plugin, dict):
        desc = _coerce_stripped(plugin.get('description'), '')
        if desc:
            gen['description'] = desc
        ver = _coerce_stripped(plugin.get('version'), '')
        if ver:
            gen['version'] = ver

    compose = _normalize_v3_compose(impl.get('compose'))
    if compose:
        gen['compose'] = compose

    env = _normalize_v3_env(impl.get('env'))
    if env:
        gen['env'] = env

    if isinstance(impl.get('build'), dict):
        gen['build'] = impl.get('build')
    if isinstance(impl.get('run'), dict):
        gen['run'] = impl.get('run')

    gen['inputs'] = _v3_plugin_inputs_to_io_list(plugin if isinstance(plugin, dict) else {})
    gen['outputs'] = _v3_plugin_outputs_to_io_list(plugin if isinstance(plugin, dict) else {})
    if not gen['outputs']:
        return None

    hint_templates: list[str] = []
    try:
        ht_multi = impl.get('hint_templates')
        if isinstance(ht_multi, list):
            hint_templates = [str(x or '').strip() for x in ht_multi if str(x or '').strip()]
    except Exception:
        hint_templates = []
    if not hint_templates:
        try:
            ht_legacy = impl.get('hint_template')
            if isinstance(ht_legacy, list):
                hint_templates = [str(x or '').strip() for x in ht_legacy if str(x or '').strip()]
            else:
                s = _coerce_stripped(ht_legacy, '')
                if s:
                    hint_templates = [s]
        except Exception:
            hint_templates = []
    if not hint_templates:
        hint_templates = ['Next: {{NEXT_NODE_NAME}}']
    gen['hint_templates'] = hint_templates
    gen['hint_template'] = hint_templates[0]

    # Optional: ordered, high-level description hints about the flag/challenge.
    # These are shown in the Flow chain UI above the NEXT-step hints.
    desc_hints: list[str] = []
    try:
        dh = impl.get('description_hints')
        if isinstance(dh, list):
            desc_hints = [str(x or '').strip() for x in dh if str(x or '').strip()]
        elif isinstance(dh, str):
            s = dh.strip()
            if s:
                desc_hints = [s]
    except Exception:
        desc_hints = []
    if desc_hints:
        gen['description_hints'] = desc_hints

    handoff = impl.get('handoff')
    if isinstance(handoff, dict):
        gen['handoff'] = handoff

    return gen


def _validate_and_normalize_flag_generator_source_json(path: str) -> tuple[bool, str, dict | None, list[dict]]:
    """Validate generator catalog source.

        Expected format (v3):
            {
                "schema_version": 3,
                "plugin_type": "flag-generator" | "flag-node-generator",
                "plugins": [ { pluginContract }, ... ],
                "implementations": [ { impl }, ... ]
            }

    Returns: (ok, note, normalized_doc_or_none, skipped_generators)
    """
    skipped: list[dict] = []
    try:
        if not path or not os.path.exists(path):
            return False, 'File not found', None, skipped
        if os.path.getsize(path) > 5_000_000:
            return False, 'File too large (>5MB)', None, skipped
        with open(path, 'r', encoding='utf-8') as fh:
            doc = json.load(fh)
        if not isinstance(doc, dict):
            return False, 'Top-level JSON must be an object', None, skipped

        # Validate the raw catalog file against the JSON schema (best-effort).
        ok_schema, schema_err = _validate_json_schema(doc, _FLAG_GENERATOR_CATALOG_SCHEMA_PATH)
        if not ok_schema:
            return False, schema_err or 'Schema validation failed', None, skipped

        schema_version = doc.get('schema_version', 3)
        try:
            schema_version = int(schema_version)
        except Exception:
            schema_version = 3
        if schema_version != 3:
            return False, f'Unsupported schema_version: {schema_version}', None, skipped

        plugin_type = _coerce_stripped(doc.get('plugin_type'), '')
        if plugin_type not in {'flag-generator', 'flag-node-generator'}:
            return False, f"Invalid plugin_type: {plugin_type or '(missing)'}", None, skipped

        plugins_raw = doc.get('plugins')
        impls_raw = doc.get('implementations')
        if not isinstance(plugins_raw, list):
            return False, 'Missing plugins[]', None, skipped
        if not isinstance(impls_raw, list):
            return False, 'Missing implementations[]', None, skipped

        plugins_by_id: dict[str, dict] = {}
        normalized_plugins: list[dict] = []
        for p in plugins_raw:
            if not isinstance(p, dict):
                skipped.append({'reason': 'plugin not an object', 'raw': p})
                continue
            pid = _normalize_generator_id(_coerce_stripped(p.get('plugin_id'), ''))
            if not pid:
                skipped.append({'reason': 'plugin missing plugin_id', 'raw': p})
                continue
            p2 = dict(p)
            p2['plugin_id'] = pid
            # Preserve plugin_type but default to top-level.
            p2['plugin_type'] = _coerce_stripped(p2.get('plugin_type'), plugin_type) or plugin_type
            if not isinstance(p2.get('requires'), list):
                p2['requires'] = []
            if not isinstance(p2.get('produces'), list):
                p2['produces'] = []
            if not isinstance(p2.get('inputs'), dict):
                p2['inputs'] = {}

            # STRICT contract validation:
            # - plugin.requires is reserved for *artifact* dependencies (produced/consumed across steps)
            # - runtime input fields belong in plugin.inputs (or generator implementation inputs)
            # - synthesized Flow fields (seed/node_name/etc) must never appear in requires
            try:
                requires_vals = [str(x or '').strip() for x in (p2.get('requires') or []) if str(x or '').strip()]
                p2['requires'] = requires_vals

                synth = set(_flow_synthesized_inputs())
                bad_synth = sorted([x for x in requires_vals if x in synth])
                if bad_synth:
                    return False, f"Strict schema: plugin {pid} has synthesized field(s) in requires: {bad_synth}. Move these into inputs.", None, skipped
            except Exception:
                # If validation itself fails, treat it as a hard error under strict mode.
                return False, f"Strict schema: failed validating requires/inputs for plugin {pid}.", None, skipped

            if pid not in plugins_by_id:
                plugins_by_id[pid] = p2
                normalized_plugins.append(p2)

        normalized_impls: list[dict] = []
        for impl in impls_raw:
            if not isinstance(impl, dict):
                skipped.append({'reason': 'implementation not an object', 'raw': impl})
                continue
            pid = _normalize_generator_id(_coerce_stripped(impl.get('plugin_id'), ''))
            if not pid:
                skipped.append({'reason': 'implementation missing plugin_id', 'raw': impl})
                continue
            if pid not in plugins_by_id:
                skipped.append({'reason': f'implementation references unknown plugin_id: {pid}', 'plugin_id': pid})
                continue
            impl2 = dict(impl)
            impl2['plugin_id'] = pid
            impl2['name'] = _coerce_stripped(impl2.get('name'), pid) or pid
            impl2['language'] = _coerce_stripped(impl2.get('language'), '').lower()

            src = _normalize_v3_source(impl2.get('source'))
            if not src:
                skipped.append({'reason': f'missing/invalid source for {pid}', 'plugin_id': pid})
                continue
            impl2['source'] = src

            # Normalize compose/env for consistency; keep other fields free-form.
            comp = _normalize_v3_compose(impl2.get('compose'))
            if comp:
                impl2['compose'] = comp
            else:
                impl2.pop('compose', None)
            env = _normalize_v3_env(impl2.get('env'))
            if env:
                impl2['env'] = env
            else:
                impl2.pop('env', None)

            hint_templates: list[str] = []
            try:
                ht_multi = impl2.get('hint_templates')
                if isinstance(ht_multi, list):
                    hint_templates = [str(x or '').strip() for x in ht_multi if str(x or '').strip()]
            except Exception:
                hint_templates = []
            if not hint_templates:
                try:
                    ht_legacy = impl2.get('hint_template')
                    if isinstance(ht_legacy, list):
                        hint_templates = [str(x or '').strip() for x in ht_legacy if str(x or '').strip()]
                    else:
                        s = _coerce_stripped(ht_legacy, '')
                        if s:
                            hint_templates = [s]
                except Exception:
                    hint_templates = []
            if not hint_templates:
                hint_templates = ['Next: {{NEXT_NODE_NAME}}']
            impl2['hint_templates'] = hint_templates
            impl2['hint_template'] = hint_templates[0]

            # Optional ordered, high-level description hints.
            try:
                dh = impl2.get('description_hints')
                desc_hints: list[str] = []
                if isinstance(dh, list):
                    desc_hints = [str(x or '').strip() for x in dh if str(x or '').strip()]
                elif isinstance(dh, str):
                    s = dh.strip()
                    if s:
                        desc_hints = [s]
                if desc_hints:
                    impl2['description_hints'] = desc_hints
                else:
                    impl2.pop('description_hints', None)
            except Exception:
                impl2.pop('description_hints', None)

            normalized_impls.append(impl2)

        normalized_plugins.sort(key=lambda p: str(p.get('plugin_id') or ''))
        normalized_impls.sort(key=lambda i: (str(i.get('name') or '').lower(), str(i.get('plugin_id') or '')))

        normalized_doc = {
            'schema_version': 3,
            'plugin_type': plugin_type,
            'plugins': normalized_plugins,
            'implementations': normalized_impls,
        }
        note = f"{len(normalized_impls)} implementation(s)"
        if skipped:
            note = note + f"; skipped {len(skipped)}"
        return True, note, normalized_doc, skipped
    except Exception as e:
        return False, str(e), None, skipped


def _v3_catalog_to_generator_views(doc: dict) -> tuple[list[dict], list[dict]]:
    """Expand a validated v3 catalog doc into merged generator view objects."""
    errors: list[dict] = []
    if not isinstance(doc, dict):
        return [], [{'error': 'catalog doc is not an object'}]
    if int(doc.get('schema_version') or 0) != 3:
        return [], [{'error': f"unsupported schema_version: {doc.get('schema_version')}"}]
    plugins = doc.get('plugins') if isinstance(doc.get('plugins'), list) else []
    impls = doc.get('implementations') if isinstance(doc.get('implementations'), list) else []
    plugins_by_id: dict[str, dict] = {}
    for p in plugins:
        if not isinstance(p, dict):
            continue
        pid = str(p.get('plugin_id') or '').strip()
        if pid and pid not in plugins_by_id:
            plugins_by_id[pid] = p

    out: list[dict] = []
    for impl in impls:
        if not isinstance(impl, dict):
            continue
        pid = str(impl.get('plugin_id') or '').strip()
        plugin = plugins_by_id.get(pid) or {}
        gen = _v3_merge_generator_view(plugin, impl)
        if not gen:
            errors.append({'warning': f'Invalid implementation for plugin_id={pid or "(missing)"}'})
            continue
        out.append(gen)

    out.sort(key=lambda g: (str(g.get('name') or '').lower(), str(g.get('id') or '')))
    return out, errors


def _load_flag_sources_state() -> dict:
    """Load flag sources state.

    Mirrors _load_data_sources_state(), but for JSON-based flag sources.
    Auto-seeds a default source from data_sources/flag_catalog.json when present.
    """
    try:
        if not os.path.exists(FLAG_STATE_PATH):
            # Seed from repo-provided catalog if present.
            seed_path = os.path.abspath(os.path.join(DATA_SOURCES_DIR, 'flag_catalog.json'))
            if os.path.exists(seed_path):
                try:
                    # Copy into the managed sources dir to keep edits isolated.
                    unique = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:6]
                    dest = os.path.join(FLAG_SOURCES_DIR, f"{unique}-flag_catalog.json")
                    shutil.copyfile(seed_path, dest)
                    state = {
                        'sources': [
                            {
                                'id': uuid.uuid4().hex[:12],
                                'name': 'flag_catalog.json',
                                'path': dest,
                                'enabled': True,
                                'rows': 'seeded',
                                'uploaded': datetime.datetime.utcnow().isoformat() + 'Z',
                            }
                        ]
                    }
                    _save_flag_sources_state(state)
                    return state
                except Exception:
                    pass
            return {'sources': []}
        with open(FLAG_STATE_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if not isinstance(data, dict):
            return {'sources': []}
        if 'sources' not in data or not isinstance(data.get('sources'), list):
            data['sources'] = []

        # If we previously auto-seeded from data_sources/flag_catalog.json, keep it in sync
        # when the repo version changes. This avoids requiring a manual re-upload just to
        # pick up new built-in generators.
        try:
            repo_seed = os.path.abspath(os.path.join(DATA_SOURCES_DIR, 'flag_catalog.json'))
            if os.path.exists(repo_seed):
                repo_mtime = os.path.getmtime(repo_seed)
                for s in data.get('sources', []):
                    if not isinstance(s, dict):
                        continue
                    if (s.get('name') or '') != 'flag_catalog.json':
                        continue
                    p = s.get('path')
                    if not p or not os.path.exists(p):
                        continue
                    try:
                        if os.path.getmtime(p) < repo_mtime:
                            shutil.copyfile(repo_seed, p)
                            s['rows'] = 'synced'
                            s['uploaded'] = datetime.datetime.utcnow().isoformat() + 'Z'
                            _save_flag_sources_state(data)
                    except Exception:
                        pass
        except Exception:
            pass

        return data
    except Exception:
        return {'sources': []}


def _save_flag_sources_state(state: dict) -> None:
    tmp = FLAG_STATE_PATH + '.tmp'
    with open(tmp, 'w', encoding='utf-8') as f:
        json.dump(state, f, indent=2)
    os.replace(tmp, FLAG_STATE_PATH)


FLAG_REQUIRED_FIELDS = ['id', 'name', 'type', 'path']
FLAG_OPTIONAL_FIELDS = ['compose_name', 'security_profile', 'enforce_security', 'description', 'inputs', 'outputs', 'hint_template', 'hint_templates']
FLAG_ALL_FIELDS = FLAG_REQUIRED_FIELDS + [f for f in FLAG_OPTIONAL_FIELDS if f not in FLAG_REQUIRED_FIELDS]


def _validate_and_normalize_flag_source_json(file_path: str, max_bytes: int = 5_000_000) -> tuple[bool, str, list[dict], list[str]]:
    """Validate and normalize a flag catalog JSON file.

    Expected format: a JSON list of objects with keys in FLAG_ALL_FIELDS.
    Unknown keys are ignored.
    Returns: (ok, note, items, skipped_reasons)
    """
    skipped: list[str] = []
    try:
        st = os.stat(file_path)
        if st.st_size > max_bytes:
            return False, f"File too large (> {max_bytes} bytes)", [], []
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            raw = json.load(f)
        if not isinstance(raw, list):
            return False, 'JSON must be a list of flag objects', [], []
        out: list[dict] = []
        for idx, item in enumerate(raw):
            if not isinstance(item, dict):
                skipped.append(f"row {idx+1}: not an object")
                continue
            rec = {}
            for k in FLAG_ALL_FIELDS:
                if k in item:
                    rec[k] = item.get(k)
            # Normalize required strings
            for k in ('id', 'name', 'type', 'path', 'compose_name', 'security_profile', 'description', 'hint_template'):
                if k in rec and rec[k] is not None:
                    rec[k] = str(rec[k]).strip()

            # Normalize inputs/outputs to a list of strings.
            for k in ('inputs', 'outputs'):
                if k not in rec:
                    continue
                v = rec.get(k)
                if v is None:
                    rec.pop(k, None)
                    continue
                if isinstance(v, str):
                    raw_items = [p.strip() for p in v.split(',')]
                    rec[k] = [p for p in raw_items if p]
                elif isinstance(v, list):
                    out_list: list[str] = []
                    for x in v:
                        if x is None:
                            continue
                        s = str(x).strip()
                        if s:
                            out_list.append(s)
                    rec[k] = out_list
                else:
                    # Unknown type -> drop rather than fail.
                    rec.pop(k, None)
            if not rec.get('name'):
                skipped.append(f"row {idx+1}: missing name")
                continue
            if not rec.get('id'):
                rec['id'] = _safe_name(rec.get('name') or f"flag-{idx+1}")
            if not rec.get('type'):
                rec['type'] = 'docker-compose'
            if not rec.get('path'):
                skipped.append(f"row {idx+1}: missing path")
                continue
            if not rec.get('compose_name'):
                rec['compose_name'] = 'docker-compose.yml'
            if not rec.get('security_profile'):
                rec['security_profile'] = 'strict'
            if not rec.get('hint_template'):
                # Default hint: Flow will fill NEXT/THIS placeholders when displaying and when preparing execution.
                rec['hint_template'] = 'Next: {{NEXT_NODE_NAME}}'
            # Normalize enforce_security boolean
            if 'enforce_security' in rec:
                v = rec.get('enforce_security')
                if isinstance(v, bool):
                    rec['enforce_security'] = v
                elif isinstance(v, (int, float)):
                    rec['enforce_security'] = bool(v)
                elif isinstance(v, str):
                    rec['enforce_security'] = v.strip().lower() in ('1', 'true', 'yes', 'y', 'on')
                else:
                    rec['enforce_security'] = True
            else:
                rec['enforce_security'] = True
            out.append(rec)
        note = f"{len(out)} items" + (f" (skipped {len(skipped)})" if skipped else '')
        return True, note, out, skipped
    except Exception as e:
        return False, str(e), [], []


def _flag_catalog_items_from_enabled_sources() -> tuple[list[dict], dict]:
    """Aggregate flag catalog items from enabled flag sources."""
    state = _load_flag_sources_state()
    types: set[str] = set()
    profiles: set[str] = set()
    items: list[dict] = []
    for s in state.get('sources', []):
        if not s.get('enabled'):
            continue
        p = s.get('path')
        if not p or not os.path.exists(p):
            continue
        ok, note, norm_items, _skipped = _validate_and_normalize_flag_source_json(p)
        if not ok:
            continue
        for it in norm_items:
            it2 = dict(it) if isinstance(it, dict) else {}
            it2['_source_id'] = str(s.get('id') or '').strip() or None
            it2['_source_name'] = str(s.get('name') or '').strip() or None
            items.append(it2)
            try:
                if it.get('type'):
                    types.add(str(it.get('type')))
                if it.get('security_profile'):
                    profiles.add(str(it.get('security_profile')))
            except Exception:
                pass
    meta = {
        'types': sorted(types),
        'security_profiles': sorted(profiles),
    }
    return items, meta


def _flag_base_dir() -> str:
    """Base directory for downloaded flag compose assets."""
    try:
        return os.path.abspath(os.path.join(_outputs_dir(), 'flags'))
    except Exception:
        return os.path.abspath(os.path.join('outputs', 'flags'))

def _load_data_sources_state():
    try:
        if not os.path.exists(DATA_STATE_PATH):
            return {"sources": []}
        with open(DATA_STATE_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # Legacy format
        if isinstance(data, dict) and 'enabled' in data and 'sources' not in data:
            return {"sources": []}
        if 'sources' not in data:
            data['sources'] = []
        return data
    except Exception:
        return {"sources": []}

def _save_data_sources_state(state):
    tmp = DATA_STATE_PATH + '.tmp'
    with open(tmp, 'w', encoding='utf-8') as f:
        json.dump(state, f, indent=2)
    os.replace(tmp, DATA_STATE_PATH)

def _validate_csv(file_path: str, max_bytes: int = 2_000_000):
    try:
        st = os.stat(file_path)
        if st.st_size > max_bytes:
            return False, f"File too large (> {max_bytes} bytes)"
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            reader = csv.reader(f)
            rows = []
            for i, row in enumerate(reader):
                if i > 10000:
                    break
                rows.append(row)
        if len(rows) < 2:
            return False, "CSV must have header + at least one data row"
        widths = {len(r) for r in rows}
        if len(widths) != 1:
            return False, "Inconsistent column counts"
        return True, f"{len(rows)-1} rows"
    except Exception as e:
        return False, str(e)

# --- Data Source CSV schema enforcement ---
REQUIRED_DS_COLUMNS = ["Name", "Path", "Type", "Startup", "Vector"]
OPTIONAL_DS_DEFAULTS = {
    "CVE": "n/a",
    "Description": "n/a",
    "References": "n/a",
}
ALLOWED_TYPE_VALUES = {"artifact", "docker", "docker-compose", "misconfig", "incompetence"}
ALLOWED_VECTOR_VALUES = {"local", "remote"}

def _validate_and_normalize_data_source_csv(file_path: str, max_bytes: int = 2_000_000, *, skip_invalid: bool = False):
    """Validate uploaded CSV for Data Sources and normalize optional columns.

    Rules:
    - Must be under max size, have header + at least one data row, and consistent row widths (after normalization step below).
    - Must include all REQUIRED_DS_COLUMNS (exact names).
    - Optional columns from OPTIONAL_DS_DEFAULTS will be appended to header if missing, and populated per-row with defaults if empty/missing.
    - Type values must be one of ALLOWED_TYPE_VALUES (case-insensitive), Vector values one of ALLOWED_VECTOR_VALUES (case-insensitive).
    - Name, Path, Startup must be non-empty strings.

        Parameters:
            file_path: path to CSV file
            max_bytes: size cap
            skip_invalid: if True, invalid data rows are skipped instead of failing the whole import.

        Returns: (ok: bool, note_or_error: str, rows: list[list[str]]|None, skipped_rows: list[int])
            ok: overall success
            note_or_error: description / counts; if skip_invalid True may include skip summary
            rows: normalized rows including header (only valid rows if skipping)
            skipped_rows: list of 1-based data row indices (relative to first data line after header) that were skipped
    """
    try:
        st = os.stat(file_path)
        if st.st_size > max_bytes:
            return False, f"File too large (> {max_bytes} bytes)", None
        # Load CSV
        rows: list[list[str]] = []
        with open(file_path, 'r', encoding='utf-8', errors='replace', newline='') as f:
            rdr = csv.reader(f)
            for i, row in enumerate(rdr):
                if i > 10000:
                    break
                rows.append([str(c) if c is not None else '' for c in row])
        if len(rows) < 2:
            return False, "CSV must have header + at least one data row", None, []
        header = rows[0]
        # Strip UTF-8 BOM if present in first cell
        if header and header[0].startswith('\ufeff'):
            header[0] = header[0].lstrip('\ufeff')
        # Ensure required headers exist
        # Case-insensitive match for required headers
        header_lower_map = {h.lower(): h for h in header}
        missing = [h for h in REQUIRED_DS_COLUMNS if h.lower() not in header_lower_map]
        # Normalize header casing to canonical names (only for required columns)
        if not missing:
            for req in REQUIRED_DS_COLUMNS:
                real = header_lower_map.get(req.lower())
                if real != req:
                    # rename in place
                    idx = header.index(real)
                    header[idx] = req
        if missing:
            return False, f"Missing required column(s): {', '.join(missing)}", None, []
        # Append optional headers if missing
        for opt_col, default in OPTIONAL_DS_DEFAULTS.items():
            if opt_col not in header:
                header.append(opt_col)
        # Normalize all rows to header length
        width = len(header)
        norm_rows: list[list[str]] = [header]
        # Build column index map
        col_idx = {name: header.index(name) for name in header}
        # Validate and fill rows
        errs: list[str] = []
        skipped_rows: list[int] = []
        for data_idx, row in enumerate(rows[1:], start=1):  # data_idx: 1-based index of data row (excluding header)
            r = list(row)
            if len(r) < width:
                r = r + [''] * (width - len(r))
            elif len(r) > width:
                r = r[:width]
            # Pull fields
            name = (r[col_idx['Name']]).strip()
            path = (r[col_idx['Path']]).strip()
            typ = (r[col_idx['Type']]).strip()
            startup = (r[col_idx['Startup']]).strip()
            vector = (r[col_idx['Vector']]).strip()
            row_err = False
            if not name:
                errs.append(f"row {data_idx}: Name is required"); row_err = True
            if not path:
                errs.append(f"row {data_idx}: Path is required"); row_err = True
            if not startup:
                errs.append(f"row {data_idx}: Startup is required"); row_err = True
            if typ:
                if typ.lower() not in ALLOWED_TYPE_VALUES:
                    errs.append(f"row {data_idx}: Type '{typ}' not in {sorted(ALLOWED_TYPE_VALUES)}"); row_err = True
                else:
                    # Normalize to lower
                    r[col_idx['Type']] = typ.lower()
            else:
                errs.append(f"row {data_idx}: Type is required"); row_err = True
            if vector:
                if vector.lower() not in ALLOWED_VECTOR_VALUES:
                    errs.append(f"row {data_idx}: Vector '{vector}' not in {sorted(ALLOWED_VECTOR_VALUES)}"); row_err = True
                else:
                    r[col_idx['Vector']] = vector.lower()
            else:
                errs.append(f"row {data_idx}: Vector is required"); row_err = True
            # Fill optionals with defaults if empty
            for opt_col, default in OPTIONAL_DS_DEFAULTS.items():
                if not r[col_idx[opt_col]].strip():
                    r[col_idx[opt_col]] = default
            if row_err and skip_invalid:
                skipped_rows.append(data_idx)
                continue
            norm_rows.append(r)
        if skip_invalid:
            if len(norm_rows) == 1:
                return False, "All data rows invalid", None, skipped_rows
            note_parts = [f"{len(norm_rows)-1} rows"]
            if skipped_rows:
                listed = ','.join(str(i) for i in skipped_rows[:20])
                extra = '' if len(skipped_rows) <= 20 else '...'
                note_parts.append(f"skipped {len(skipped_rows)} invalid row(s): {listed}{extra}")
            return True, ' | '.join(note_parts), norm_rows, skipped_rows
        else:
            if errs:
                return False, "; ".join(errs[:20]) + (" ..." if len(errs)>20 else ''), None, []
            return True, f"{len(norm_rows)-1} rows", norm_rows, []
    except Exception as e:
        return False, str(e), None, []

def _default_scenarios_payload():
    return _default_scenarios_payload_for_names(["Scenario 1"])


def _default_scenario_payload(name: str) -> Dict[str, Any]:
    # Single default scenario with empty sections mirroring PyQt structure
    sections = [
        "Node Information", "Routing", "Services", "Traffic",
        "Events", "Vulnerabilities", "Segmentation", "HITL"
    ]
    display_name = str(name or '').strip() or "Scenario"
    return {
        "name": display_name,
        "base": {"filepath": ""},
        "hitl": {"enabled": False, "interfaces": [], "core": None},
        "sections": {sec_name: {
            "density": 0.5 if sec_name not in ("Node Information", "HITL") else None,
            "total_nodes": 1 if sec_name == "Node Information" else None,
            "items": []
        } for sec_name in sections},
        "notes": ""
    }


def _default_scenarios_payload_for_names(names: Iterable[Any] | None) -> Dict[str, Any]:
    scenario_names: list[str] = []
    for entry in names or []:
        try:
            text = str(entry or '').strip()
        except Exception:
            text = ''
        if text:
            scenario_names.append(text)
    if not scenario_names:
        scenario_names = ["Scenario 1"]

    return {
        "scenarios": [_default_scenario_payload(name) for name in scenario_names],
        "result_path": None,
        "core": _default_core_dict(),
        "host_interfaces": _enumerate_host_interfaces(),
    }


def _filter_scenarios_by_norms(
    scenarios: Iterable[Any],
    allowed_norms: set[str],
) -> List[Dict[str, Any]]:
    filtered: List[Dict[str, Any]] = []
    if not allowed_norms:
        return filtered
    allowed_keys = {key for key in (_scenario_match_key(v) for v in allowed_norms) if key}
    if not allowed_keys:
        return filtered
    for scen in scenarios or []:
        if not isinstance(scen, dict):
            continue
        if _scenario_match_key(scen.get('name')) in allowed_keys:
            filtered.append(scen)
    return filtered


def _collect_scenario_norms(scenarios: Iterable[Any]) -> set[str]:
    norms: set[str] = set()
    for scen in scenarios or []:
        if not isinstance(scen, dict):
            continue
        norm = _normalize_scenario_label(scen.get('name'))
        if norm:
            norms.add(norm)
    return norms


def _prepare_payload_for_index(payload: Optional[Dict[str, Any]], *, user: Optional[dict] = None) -> Dict[str, Any]:
    """Normalize payload data before rendering the index page."""
    if not isinstance(payload, dict):
        payload = {}
    else:
        payload = dict(payload)

    project_hint_raw = payload.get('project_key_hint') if isinstance(payload.get('project_key_hint'), str) else None
    project_hint = project_hint_raw.strip() if isinstance(project_hint_raw, str) else None
    scenario_hint_raw = payload.get('scenario_query') if isinstance(payload.get('scenario_query'), str) else None
    scenario_hint = scenario_hint_raw.strip() if isinstance(scenario_hint_raw, str) else None
    allowed_norms = _builder_allowed_norms(user)
    builder_assignment_order: Optional[list[str]] = None
    if allowed_norms is not None:
        builder_assignment_order = _assigned_scenarios_for_user(user)

    defaults = _default_scenarios_payload()

    # --- Core connection defaults ---
    core_meta = _normalize_core_config(payload.get('core'), include_password=False)
    core_defaults = defaults['core']
    if not core_meta.get('host'):
        core_meta['host'] = core_defaults.get('host')
    if not core_meta.get('port'):
        core_meta['port'] = core_defaults.get('port')
    # Ensure ssh_host defaults to current host to simplify UI when enabling later
    if not core_meta.get('ssh_host'):
        core_meta['ssh_host'] = core_meta.get('host') or core_defaults.get('host')
    if not core_meta.get('ssh_port'):
        core_meta['ssh_port'] = 22
    payload['core'] = core_meta

    # --- Scenarios ---
    scenarios_raw = payload.get('scenarios')
    if not isinstance(scenarios_raw, list) or not scenarios_raw:
        # For actual restricted roles (builder/participant), an empty assignment list
        # should result in an empty scenario list ("all or none" visibility).
        scenarios_raw = defaults['scenarios'] if allowed_norms is None else []
    # For restricted roles, always build scenarios from catalog + assignments so
    # Scenarios/CORE/Reports are consistent (including ordering).
    if allowed_norms is not None and allowed_norms:
        seeded = _builder_catalog_seed_scenarios(allowed_norms, builder_assignment_order, user=user)
        if seeded:
            scenarios_raw = seeded

    required_sections = {
        'Node Information': {'density': None, 'total_nodes': None},
        'Routing': {'density': 0.5},
        'Services': {'density': 0.5},
        'Traffic': {'density': 0.5},
        'Events': {'density': 0.5},
        'Vulnerabilities': {'density': 0.5},
        'Segmentation': {'density': 0.5},
        'HITL': {},
    }

    normalized_scenarios: List[Dict[str, Any]] = []
    for idx, scen in enumerate(scenarios_raw, start=1):
        if not isinstance(scen, dict):
            continue
        scen_norm = dict(scen)
        scen_norm['name'] = scen_norm.get('name') or f"Scenario {idx}"

        base_meta = scen_norm.get('base')
        if not isinstance(base_meta, dict):
            base_meta = {}
        base_meta = dict(base_meta)
        filepath = base_meta.get('filepath')
        if not isinstance(filepath, str):
            filepath = '' if filepath is None else str(filepath)
        base_meta['filepath'] = filepath
        if filepath and not base_meta.get('display_name'):
            base_meta['display_name'] = os.path.basename(filepath)
        scen_norm['base'] = base_meta

        if 'density_count' not in scen_norm:
            scen_norm['density_count'] = 10

        sections_meta = scen_norm.get('sections')
        if not isinstance(sections_meta, dict):
            sections_meta = {}
        sections_out: Dict[str, Any] = {}
        for section_name, defaults_map in required_sections.items():
            sec_val = sections_meta.get(section_name)
            if isinstance(sec_val, dict):
                sec_norm = dict(sec_val)
            else:
                sec_norm = {}
            items = sec_norm.get('items')
            if isinstance(items, list):
                sec_norm['items'] = [item for item in items if isinstance(item, dict)]
            else:
                sec_norm['items'] = []
            for key, val in defaults_map.items():
                sec_norm.setdefault(key, val)
            sections_out[section_name] = sec_norm
        for extra_name, extra_val in sections_meta.items():
            if extra_name not in sections_out:
                sections_out[extra_name] = extra_val
        scen_norm['sections'] = sections_out

        hitl_meta = scen_norm.get('hitl')
        if isinstance(hitl_meta, dict):
            hitl_norm = dict(hitl_meta)
        else:
            hitl_norm = {}
        hitl_norm['enabled'] = bool(hitl_norm.get('enabled'))
        interfaces_raw = hitl_norm.get('interfaces')
        interfaces_norm: List[Dict[str, Any]] = []
        if isinstance(interfaces_raw, list):
            for iface in interfaces_raw:
                if not isinstance(iface, dict):
                    continue
                iface_norm = dict(iface)
                name = iface_norm.get('name')
                if not isinstance(name, str):
                    name = '' if name is None else str(name)
                name = name.strip()
                if not name:
                    continue
                iface_norm['name'] = name
                iface_norm['attachment'] = _normalize_hitl_attachment(iface_norm.get('attachment'))
                for addr_key in ('ipv4', 'ipv6'):
                    vals = iface_norm.get(addr_key)
                    if isinstance(vals, list):
                        iface_norm[addr_key] = [str(v).strip() for v in vals if v is not None and str(v).strip()]
                interfaces_norm.append(iface_norm)
        hitl_norm['interfaces'] = interfaces_norm
        hitl_norm['core'] = _extract_optional_core_config(hitl_norm.get('core'), include_password=False)
        scen_norm['hitl'] = hitl_norm

        normalized_scenarios.append(scen_norm)

    # Only fall back to default scenarios for unrestricted users.
    if not normalized_scenarios and allowed_norms is None:
        normalized_scenarios = defaults['scenarios']

    # If the UI is in builder mode (even for admin users previewing builder view),
    # merge admin-managed HITL validation hints into scenarios so builder cannot
    # appear "unassigned" when admin has already selected a CORE VM.
    try:
        view_mode = getattr(g, 'ui_view_mode', _UI_VIEW_DEFAULT)
    except Exception:
        view_mode = _UI_VIEW_DEFAULT
    if view_mode == 'builder':
        try:
            hitl_validation_hints = _load_scenario_hitl_validation_from_disk()
            builder_hitl_fallback = _select_builder_hitl_fallback(hitl_validation_hints)
            hitl_config_hints = _load_scenario_hitl_config_from_disk()
            builder_hitl_config_fallback = _select_builder_hitl_fallback(hitl_config_hints) if hitl_config_hints else None
            try:
                _, _, participant_urls_by_norm = _load_scenario_catalog_from_disk()
            except Exception:
                participant_urls_by_norm = {}
        except Exception:
            hitl_validation_hints = {}
            builder_hitl_fallback = None
            hitl_config_hints = {}
            builder_hitl_config_fallback = None
            participant_urls_by_norm = {}

        for scen in normalized_scenarios:
            if not isinstance(scen, dict):
                continue
            norm = _normalize_scenario_label(scen.get('name'))
            validation_hint = hitl_validation_hints.get(norm) if (norm and isinstance(hitl_validation_hints, dict)) else None
            effective_hint = validation_hint if isinstance(validation_hint, dict) and validation_hint else builder_hitl_fallback
            config_hint = hitl_config_hints.get(norm) if (norm and isinstance(hitl_config_hints, dict)) else None
            effective_cfg = config_hint if isinstance(config_hint, dict) and config_hint else builder_hitl_config_fallback
            if not (isinstance(effective_hint, dict) and effective_hint) and not (isinstance(effective_cfg, dict) and effective_cfg):
                continue

            effective_hint_dict: Dict[str, Any] = effective_hint if isinstance(effective_hint, dict) else {}

            hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else {}
            hitl_meta = dict(hitl_meta)

            # Merge participant URL hints (non-secret) so builder view shows Step 5 as configured.
            try:
                participant_hint = participant_urls_by_norm.get(norm) if (norm and isinstance(participant_urls_by_norm, dict)) else ''
                if participant_hint:
                    for k in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                        if k not in hitl_meta or hitl_meta.get(k) in (None, ''):
                            hitl_meta[k] = participant_hint
            except Exception:
                pass

            prox_hint = effective_hint_dict.get('proxmox') if isinstance(effective_hint_dict.get('proxmox'), dict) else None
            if isinstance(prox_hint, dict) and prox_hint:
                prox_state = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else {}
                prox_state = dict(prox_state)
                for k, v in prox_hint.items():
                    if k not in prox_state or prox_state.get(k) in (None, '', False):
                        prox_state[k] = v
                hitl_meta['proxmox'] = prox_state

            core_hint = effective_hint_dict.get('core') if isinstance(effective_hint_dict.get('core'), dict) else None
            if isinstance(core_hint, dict) and core_hint:
                core_state = hitl_meta.get('core') if isinstance(hitl_meta.get('core'), dict) else {}
                core_state = dict(core_state)
                for k, v in core_hint.items():
                    if k not in core_state or core_state.get(k) in (None, '', False):
                        core_state[k] = v
                hitl_meta['core'] = core_state

            # Apply HITL configuration hints (enabled/interfaces/mappings)
            if isinstance(effective_cfg, dict) and effective_cfg:
                try:
                    participant_cfg = _normalize_participant_proxmox_url(effective_cfg.get('participant_proxmox_url'))
                    if participant_cfg:
                        for k in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                            if k not in hitl_meta or hitl_meta.get(k) in (None, ''):
                                hitl_meta[k] = participant_cfg
                except Exception:
                    pass
                if 'enabled' in effective_cfg:
                    hitl_meta['enabled'] = bool(effective_cfg.get('enabled'))
                if isinstance(effective_cfg.get('interfaces'), list):
                    hitl_meta['interfaces'] = effective_cfg.get('interfaces')
                cfg_prox = effective_cfg.get('proxmox') if isinstance(effective_cfg.get('proxmox'), dict) else None
                if isinstance(cfg_prox, dict) and cfg_prox:
                    prox_state = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else {}
                    prox_state = dict(prox_state)
                    prox_state.update(cfg_prox)
                    hitl_meta['proxmox'] = prox_state
                cfg_core = effective_cfg.get('core') if isinstance(effective_cfg.get('core'), dict) else None
                if isinstance(cfg_core, dict) and cfg_core:
                    core_state = hitl_meta.get('core') if isinstance(hitl_meta.get('core'), dict) else {}
                    core_state = dict(core_state)
                    core_state.update(cfg_core)
                    hitl_meta['core'] = core_state

            scen['hitl'] = hitl_meta
    if allowed_norms is not None:
        # If there are assignments, filter to them.
        if allowed_norms:
            normalized_scenarios = _filter_scenarios_by_norms(normalized_scenarios, allowed_norms)
        else:
            normalized_scenarios = []
        # Always mark as restricted when allowed_norms is not None.
        payload['builder_restricted_scenarios'] = True
        payload['builder_assigned_scenarios'] = sorted(allowed_norms)
        payload['builder_no_assignments'] = not bool(allowed_norms)
    else:
        payload.pop('builder_restricted_scenarios', None)
        payload.pop('builder_assigned_scenarios', None)
        payload.pop('builder_no_assignments', None)
        payload.pop('builder_assigned_scenarios', None)
        payload.pop('builder_no_assignments', None)
    payload['scenarios'] = normalized_scenarios

    # --- Base upload metadata ---
    base_upload = payload.get('base_upload')
    if isinstance(base_upload, dict):
        base_norm = dict(base_upload)
        path = base_norm.get('path')
        if isinstance(path, str):
            base_norm['path'] = path
            base_norm.setdefault('display_name', os.path.basename(path) if path else '')
        else:
            base_norm['path'] = ''
        if 'valid' in base_norm:
            base_norm['valid'] = bool(base_norm['valid'])
        payload['base_upload'] = base_norm

    # --- Host interfaces ---
    host_ifaces = payload.get('host_interfaces')
    if not isinstance(host_ifaces, list):
        host_ifaces = []
    sanitized_ifaces: List[Dict[str, Any]] = []
    adaptor_names: set[str] = set()
    for iface in host_ifaces:
        if not isinstance(iface, dict):
            continue
        entry = dict(iface)
        name = entry.get('name')
        if isinstance(name, str):
            name = name.strip()
        elif name is not None:
            name = str(name)
        else:
            name = ''
        entry['name'] = name
        if name:
            adaptor_names.add(name)
        for arr_key in ('ipv4', 'ipv6', 'flags'):
            vals = entry.get(arr_key)
            if isinstance(vals, list):
                entry[arr_key] = [v for v in vals if v not in (None, '')]
        sanitized_ifaces.append(entry)
    payload['host_interfaces'] = sanitized_ifaces
    payload['hitl_adaptors'] = sorted(adaptor_names)

    payload.setdefault('result_path', defaults['result_path'])

    try:
        scen_names_for_catalog = [scen.get('name') for scen in normalized_scenarios if isinstance(scen, dict)]
        # Build a per-scenario participant URL hint map. Include explicit empty values
        # so we can clear stale catalog hints when a user removes a URL in the editor.
        participant_hints: Dict[str, Any] = {}
        for scen in normalized_scenarios:
            if not isinstance(scen, dict):
                continue
            name = scen.get('name')
            norm = _normalize_scenario_label(name)
            if not norm:
                continue
            hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else None
            participant_url_value = ''
            if hitl_meta:
                for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                    candidate = hitl_meta.get(key)
                    normalized = _normalize_participant_proxmox_url(candidate)
                    if normalized:
                        participant_url_value = normalized
                        break
            participant_hints[norm] = participant_url_value or ''

        if allowed_norms is None:
            result_path = payload.get('result_path') if isinstance(payload.get('result_path'), str) else None
            should_persist = bool(result_path)
            if not should_persist:
                try:
                    should_persist = not os.path.exists(_scenario_catalog_file())
                except Exception:
                    should_persist = False
            if should_persist:
                _persist_scenario_catalog(scen_names_for_catalog, result_path, participant_urls=participant_hints)
            else:
                if participant_hints:
                    _merge_participant_urls_into_scenario_catalog(participant_hints)
        else:
            if participant_hints:
                _merge_participant_urls_into_scenario_catalog(participant_hints)
    except Exception:
        pass

    if project_hint:
        payload['project_key_hint'] = project_hint
    elif payload.get('result_path') and not payload.get('project_key_hint'):
        payload['project_key_hint'] = payload['result_path']
    if scenario_hint:
        payload['scenario_query'] = scenario_hint

    return payload


@app.route('/api/editor_snapshot', methods=['POST'])
def api_editor_snapshot():
    user = _current_user()
    if not user or not user.get('username'):
        return jsonify({'success': False, 'error': 'Authentication required'}), 401
    payload = request.get_json(silent=True)
    if not isinstance(payload, dict):
        return jsonify({'success': False, 'error': 'Invalid snapshot payload'}), 400
    snapshot = _build_editor_snapshot_payload(payload)
    if not snapshot:
        return jsonify({'success': False, 'error': 'Snapshot rejected'}), 400
    try:
        _write_editor_state_snapshot(snapshot, user=user)
    except Exception as exc:
        try:
            app.logger.exception('[editor_snapshot] persist failed: %s', exc)
        except Exception:
            pass
        return jsonify({'success': False, 'error': 'Unable to persist snapshot'}), 500
    return jsonify({'success': True})


# Hardware in the Loop utilities
@app.route('/api/host_interfaces', methods=['GET', 'POST'])
def api_host_interfaces():
    if request.method == 'POST':
        payload = request.get_json(silent=True) or {}
        secret_id_raw = payload.get('core_secret_id') or payload.get('secret_id')
        secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
        include_down = _coerce_bool(payload.get('include_down'))
        core_vm_payload = payload.get('core_vm') if isinstance(payload.get('core_vm'), dict) else {}
        prox_interfaces_raw = core_vm_payload.get('interfaces') if isinstance(core_vm_payload, dict) else None
        prox_interfaces = [entry for entry in prox_interfaces_raw if isinstance(entry, dict)] if isinstance(prox_interfaces_raw, list) else None
        vm_context = {
            'vm_key': core_vm_payload.get('vm_key'),
            'vm_name': core_vm_payload.get('vm_name'),
            'vm_node': core_vm_payload.get('vm_node'),
            'vmid': core_vm_payload.get('vmid'),
        }
        if not secret_id:
            return jsonify({'success': False, 'error': 'CORE credentials are required to enumerate interfaces from the CORE VM'}), 400
        def _fallback_from_proxmox() -> Optional[Dict[str, Any]]:
            if not prox_interfaces:
                return None
            synthesized: List[Dict[str, Any]] = []
            for idx, vmif in enumerate(prox_interfaces):
                if not isinstance(vmif, dict):
                    continue
                name = str(vmif.get('name') or vmif.get('id') or vmif.get('label') or f'eth{idx}')
                mac = vmif.get('macaddr') or vmif.get('mac') or vmif.get('hwaddr') or ''
                entry: Dict[str, Any] = {
                    'name': name,
                    'display': name,
                    'mac': mac,
                    'ipv4': [],
                    'ipv6': [],
                    'mtu': None,
                    'speed': None,
                    'is_up': None,
                    'flags': [],
                    'proxmox': {
                        'id': vmif.get('id') or vmif.get('name') or vmif.get('label') or name,
                        'macaddr': mac,
                        'bridge': vmif.get('bridge'),
                        'model': vmif.get('model'),
                        'raw': vmif,
                        'vm_key': vm_context.get('vm_key'),
                        'vm_name': vm_context.get('vm_name'),
                        'vm_node': vm_context.get('vm_node'),
                        'vmid': vm_context.get('vmid'),
                    },
                }
                if vmif.get('bridge'):
                    entry['bridge'] = vmif.get('bridge')
                synthesized.append(entry)
            meta = {k: v for k, v in vm_context.items() if v not in (None, '')}
            return {
                'success': True,
                'source': 'proxmox_inventory',
                'interfaces': synthesized,
                'metadata': meta,
                'fetched_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
                'note': 'Using Proxmox inventory as a fallback; CORE VM SSH enumeration unavailable.'
            }
        try:
            interfaces = _enumerate_core_vm_interfaces_from_secret(
                secret_id,
                prox_interfaces=prox_interfaces,
                include_down=include_down,
                vm_context=vm_context,
            )
            if (not interfaces) and prox_interfaces:
                # No interfaces returned from CORE VM; fall back to Proxmox inventory snapshot
                fb = _fallback_from_proxmox()
                if fb is not None:
                    return jsonify(fb)
        except ValueError as exc:
            return jsonify({'success': False, 'error': str(exc)}), 400
        except _SSHTunnelError as exc:
            fb = _fallback_from_proxmox()
            if fb is not None:
                return jsonify(fb)
            return jsonify({'success': False, 'error': str(exc)}), 502
        except RuntimeError as exc:
            fb = _fallback_from_proxmox()
            if fb is not None:
                return jsonify(fb)
            return jsonify({'success': False, 'error': str(exc)}), 500
        except Exception as exc:  # pragma: no cover - defensive logging
            app.logger.exception('[hitl] unexpected failure retrieving CORE VM interfaces: %s', exc)
            return jsonify({'success': False, 'error': 'Unexpected error retrieving CORE VM interfaces'}), 500
        response_data = {
            'success': True,
            'source': 'core_vm',
            'interfaces': interfaces,
            'metadata': {k: v for k, v in vm_context.items() if v not in (None, '')},
            'fetched_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
        }
        return jsonify(response_data)
    try:
        interfaces = _enumerate_host_interfaces()
        return jsonify({'success': True, 'interfaces': interfaces})
    except Exception as exc:  # pragma: no cover - defensive logging for unexpected psutil errors
        app.logger.exception('[hitl] failed to enumerate host interfaces via GET: %s', exc)
        return jsonify({'success': False, 'error': 'Failed to enumerate host interfaces'}), 500


@app.route('/api/proxmox/validate', methods=['POST'])
def api_proxmox_validate():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    if ProxmoxAPI is None:
        return jsonify({'success': False, 'error': 'Proxmox integration unavailable: install proxmoxer package'}), 500
    payload = request.get_json(silent=True) or {}
    url_raw = str(payload.get('url') or '').strip()
    if not url_raw:
        return jsonify({'success': False, 'error': 'URL is required'}), 400
    parsed = urlparse(url_raw)
    if parsed.scheme not in {'http', 'https'}:
        return jsonify({'success': False, 'error': 'URL must start with http:// or https://'}), 400
    host = parsed.hostname
    if not host:
        return jsonify({'success': False, 'error': 'Unable to determine host from URL'}), 400
    try:
        port = int(payload.get('port') or (parsed.port or 8006))
    except Exception:
        port = parsed.port or 8006
    if port < 1 or port > 65535:
        return jsonify({'success': False, 'error': 'Port must be between 1 and 65535'}), 400
    username = str(payload.get('username') or '').strip()
    if not username:
        return jsonify({'success': False, 'error': 'Username is required'}), 400
    password = payload.get('password')
    if password is None:
        password = ''
    if not isinstance(password, str):
        password = str(password)
    verify_ssl_raw = payload.get('verify_ssl')
    reuse_secret_raw = payload.get('reuse_secret_id')
    reuse_secret_id = reuse_secret_raw.strip() if isinstance(reuse_secret_raw, str) else ''
    stored_record: Optional[Dict[str, Any]] = None
    if not password and reuse_secret_id:
        stored_record = _load_proxmox_credentials(reuse_secret_id)
        if not stored_record:
            return jsonify({'success': False, 'error': 'Stored credentials unavailable. Re-enter the password.'}), 400
        stored_password = stored_record.get('password_plain') or ''
        if not stored_password:
            return jsonify({'success': False, 'error': 'Stored credentials are missing password material. Re-enter the password.'}), 400
        stored_url = (stored_record.get('url') or '').strip()
        stored_username = (stored_record.get('username') or '').strip()
        if stored_url and stored_url != url_raw:
            return jsonify({'success': False, 'error': 'URL changed since the last validation. Re-enter the password.'}), 400
        if stored_username and stored_username != username:
            return jsonify({'success': False, 'error': 'Username changed since the last validation. Re-enter the password.'}), 400
        password = stored_password
        if stored_record.get('verify_ssl') is not None and verify_ssl_raw is None:
            verify_ssl_raw = bool(stored_record.get('verify_ssl'))
    if not isinstance(password, str):
        password = str(password)
    if not password:
        return jsonify({'success': False, 'error': 'Password is required'}), 400
    if verify_ssl_raw is None:
        verify_ssl = (parsed.scheme == 'https')
    else:
        verify_ssl = bool(verify_ssl_raw)
    timeout_val = payload.get('timeout', 5.0)
    try:
        timeout = float(timeout_val)
    except Exception:
        timeout = 5.0
    timeout = max(1.0, min(timeout, 30.0))
    # Persist credentials only if explicitly requested.
    remember_credentials = bool(payload.get('remember_credentials', True))
    prox_kwargs = {
        'host': host,
        'user': username,
        'password': password,
        'port': port,
        'verify_ssl': verify_ssl,
        'timeout': timeout,
        'backend': 'https' if parsed.scheme == 'https' else 'http',
    }
    try:
        app.logger.debug('[proxmox] attempting auth for %s@%s:%s (verify_ssl=%s)', username, host, port, verify_ssl)
    except Exception:
        pass
    try:
        prox = ProxmoxAPI(**prox_kwargs)  # type: ignore[arg-type]
        prox.version.get()
    except Exception as exc:
        try:
            app.logger.warning('[proxmox] authentication failed: %s', exc)
        except Exception:
            pass
        return jsonify({'success': False, 'error': f'Authentication failed: {exc}'}), 401
    try:
        app.logger.info('[proxmox] authentication succeeded for %s@%s:%s', username, host, port)
    except Exception:
        pass
    scenario_index = payload.get('scenario_index')
    scenario_name = str(payload.get('scenario_name') or '').strip()
    secret_payload = {
        'scenario_name': scenario_name,
        'scenario_index': scenario_index,
        'url': url_raw,
        'port': port,
        'username': username,
        'password': password,
        'verify_ssl': verify_ssl,
    }
    summary: Dict[str, Any]
    secret_identifier: Optional[str] = None
    stored_at_val: Optional[str] = None
    if remember_credentials:
        try:
            stored_meta = _save_proxmox_credentials(secret_payload)
        except RuntimeError as exc:
            return jsonify({'success': False, 'error': str(exc)}), 500
        except Exception as exc:
            app.logger.exception('[proxmox] failed to persist credentials: %s', exc)
            return jsonify({'success': False, 'error': 'Credentials validated but could not be stored'}), 500
        stored_at_val = stored_meta.get('stored_at')
        summary = {
            'url': stored_meta['url'],
            'port': stored_meta['port'],
            'username': stored_meta['username'],
            'verify_ssl': stored_meta['verify_ssl'],
            'stored_at': stored_at_val,
        }
        secret_identifier = stored_meta['identifier']
    else:
        summary = {
            'url': url_raw,
            'port': port,
            'username': username,
            'verify_ssl': verify_ssl,
            'stored_at': None,
        }
    message = f"Validated Proxmox access for {username} at {host}:{port}"
    # Persist a safe shared hint so builders/participants can see the validated state.
    # This is the non-secret subset (no password/token material).
    try:
        if scenario_name:
            _merge_hitl_validation_into_scenario_catalog(
                scenario_name,
                proxmox={
                    'url': summary.get('url'),
                    'port': summary.get('port'),
                    'verify_ssl': summary.get('verify_ssl'),
                    'secret_id': secret_identifier if remember_credentials else None,
                    'validated': bool(secret_identifier) if remember_credentials else False,
                    'last_validated_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
                    'stored_at': summary.get('stored_at'),
                    'last_message': message,
                },
            )
    except Exception:
        pass
    return jsonify({
        'success': True,
        'message': message,
        'summary': summary,
        'secret_id': secret_identifier,
        'scenario_index': scenario_index,
        'scenario_name': scenario_name,
    })


@app.route('/api/proxmox/clear', methods=['POST'])
def api_proxmox_clear():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    secret_id_raw = payload.get('secret_id')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    scenario_index = payload.get('scenario_index')
    scenario_name = str(payload.get('scenario_name') or '').strip()
    removed = False
    try:
        if secret_id:
            removed = _delete_proxmox_credentials(secret_id)
    except Exception:
        app.logger.exception('[proxmox] failed to clear credentials for %s (scenario %s)', secret_id or 'unknown', scenario_name or scenario_index)
        return jsonify({'success': False, 'error': 'Failed to clear stored Proxmox credentials'}), 500
    try:
        app.logger.info('[proxmox] cleared credentials request for %s (scenario_index=%s, removed=%s)', scenario_name or 'unnamed', scenario_index, removed)
    except Exception:
        pass
    try:
        if scenario_name:
            _clear_hitl_validation_in_scenario_catalog(scenario_name, proxmox=True)
    except Exception:
        pass
    return jsonify({
        'success': True,
        'secret_removed': removed,
        'scenario_index': scenario_index,
        'scenario_name': scenario_name,
    })


@app.route('/api/proxmox/credentials/get', methods=['POST'])
def api_proxmox_credentials_get():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    secret_id_raw = payload.get('secret_id')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    if not secret_id:
        return jsonify({'success': False, 'error': 'secret_id is required'}), 400
    try:
        record = _load_proxmox_credentials(secret_id)
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 500
    if not record:
        return jsonify({'success': False, 'error': 'Stored credentials not found'}), 404
    credentials = {
        'identifier': record.get('identifier') or secret_id,
        'scenario_name': record.get('scenario_name') or '',
        'scenario_index': record.get('scenario_index'),
        'url': record.get('url') or '',
        'port': int(record.get('port') or 8006),
        'username': record.get('username') or '',
        'password': record.get('password_plain') or '',
        'verify_ssl': bool(record.get('verify_ssl', True)),
        'stored_at': record.get('stored_at'),
    }
    return jsonify({'success': True, 'credentials': credentials})


@app.route('/api/core/credentials/clear', methods=['POST'])
def api_core_credentials_clear():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    secret_id_raw = payload.get('core_secret_id') or payload.get('secret_id')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    scenario_index = payload.get('scenario_index')
    scenario_name = str(payload.get('scenario_name') or '').strip()
    removed = False
    try:
        if secret_id:
            removed = _delete_core_credentials(secret_id)
    except Exception:
        app.logger.exception('[core] failed to clear credentials for %s (scenario %s)', secret_id or 'unknown', scenario_name or scenario_index)
        return jsonify({'success': False, 'error': 'Failed to clear stored CORE credentials'}), 500
    try:
        app.logger.info('[core] cleared credentials request for %s (scenario_index=%s, removed=%s)', scenario_name or 'unnamed', scenario_index, removed)
    except Exception:
        pass
    try:
        if scenario_name:
            _clear_hitl_validation_in_scenario_catalog(scenario_name, core=True)
    except Exception:
        pass
    return jsonify({
        'success': True,
        'secret_removed': removed,
        'scenario_index': scenario_index,
        'scenario_name': scenario_name,
    })


@app.route('/api/core/credentials/get', methods=['POST'])
def api_core_credentials_get():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    secret_id_raw = payload.get('core_secret_id') or payload.get('secret_id')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    if not secret_id:
        return jsonify({'success': False, 'error': 'core_secret_id is required'}), 400
    try:
        record = _load_core_credentials(secret_id)
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 500
    if not record:
        return jsonify({'success': False, 'error': 'Stored credentials not found'}), 404
    credentials = {
        'identifier': record.get('identifier') or secret_id,
        'scenario_name': record.get('scenario_name') or '',
        'scenario_index': record.get('scenario_index'),
        'host': record.get('host') or record.get('grpc_host') or '',
        'port': int(record.get('port') or record.get('grpc_port') or 50051),
        'grpc_host': record.get('grpc_host') or record.get('host') or '',
        'grpc_port': int(record.get('grpc_port') or record.get('port') or 50051),
        'ssh_host': record.get('ssh_host') or '',
        'ssh_port': int(record.get('ssh_port') or 22),
        'ssh_username': record.get('ssh_username') or '',
        'ssh_password': record.get('ssh_password_plain') or '',
        'ssh_enabled': bool(record.get('ssh_enabled', True)),
        'venv_bin': record.get('venv_bin') or DEFAULT_CORE_VENV_BIN,
        'vm_key': record.get('vm_key') or '',
        'vm_name': record.get('vm_name') or '',
        'vm_node': record.get('vm_node') or '',
        'vmid': record.get('vmid'),
        'proxmox_secret_id': record.get('proxmox_secret_id'),
        'proxmox_target': record.get('proxmox_target'),
        'stored_at': record.get('stored_at'),
    }
    return jsonify({'success': True, 'credentials': credentials})


@app.route('/api/hitl/core_vm/clear', methods=['POST'])
def api_hitl_core_vm_clear():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    scenario_name = str(payload.get('scenario_name') or payload.get('scenario') or '').strip()
    scenario_index = payload.get('scenario_index')
    if not scenario_name:
        return jsonify({'success': False, 'error': 'scenario_name is required'}), 400
    try:
        _clear_hitl_config_in_scenario_catalog(scenario_name, clear_core_vm=True)
    except Exception:
        app.logger.exception('[hitl] failed clearing CORE VM selection for %s', scenario_name)
        return jsonify({'success': False, 'error': 'Failed to clear CORE VM selection'}), 500
    return jsonify({'success': True, 'scenario_name': scenario_name, 'scenario_index': scenario_index})


@app.route('/api/hitl/config/clear', methods=['POST'])
def api_hitl_config_clear():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    scenario_name = str(payload.get('scenario_name') or payload.get('scenario') or '').strip()
    scenario_index = payload.get('scenario_index')
    if not scenario_name:
        return jsonify({'success': False, 'error': 'scenario_name is required'}), 400
    try:
        _clear_hitl_config_in_scenario_catalog(scenario_name, clear_config=True)
    except Exception:
        app.logger.exception('[hitl] failed clearing HITL config for %s', scenario_name)
        return jsonify({'success': False, 'error': 'Failed to clear HITL config'}), 500
    return jsonify({'success': True, 'scenario_name': scenario_name, 'scenario_index': scenario_index})


@app.route('/api/proxmox/vms', methods=['POST'])
def api_proxmox_vms():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    secret_id_raw = payload.get('secret_id')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    if not secret_id:
        return jsonify({'success': False, 'error': 'secret_id is required'}), 400
    try:
        inventory = _enumerate_proxmox_vms(secret_id)
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 502
    except Exception as exc:  # pragma: no cover - unexpected failure path
        app.logger.exception('[proxmox] unexpected error fetching VM inventory: %s', exc)
        return jsonify({'success': False, 'error': 'Failed to fetch Proxmox VM inventory'}), 500
    return jsonify({'success': True, 'inventory': inventory})


@app.route('/api/hitl/apply_bridge', methods=['POST'])
def api_hitl_apply_bridge():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403

    payload = request.get_json(silent=True) or {}
    bridge_raw = payload.get('bridge_name') or payload.get('internal_bridge') or payload.get('bridge')
    if bridge_raw in (None, ''):
        return jsonify({'success': False, 'error': 'Bridge name is required'}), 400
    try:
        bridge_name = _normalize_internal_bridge_name(bridge_raw)
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400

    scenario_name = str(payload.get('scenario_name') or payload.get('scenario') or '').strip()
    scenario_index_raw = payload.get('scenario_index')
    try:
        scenario_index = int(scenario_index_raw)
    except Exception:
        scenario_index = None

    hitl_payload = payload.get('hitl') or payload.get('scenario_hitl') or payload.get('hitl_config')
    if not isinstance(hitl_payload, dict):
        return jsonify({'success': False, 'error': 'HITL configuration is required to apply bridge changes'}), 400

    prox_state = hitl_payload.get('proxmox') or {}
    secret_id_raw = prox_state.get('secret_id') or prox_state.get('secretId') or prox_state.get('identifier')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    if not secret_id:
        return jsonify({'success': False, 'error': 'Validate and store Proxmox credentials before applying bridge changes'}), 400

    core_state = hitl_payload.get('core') or {}
    vm_key_raw = core_state.get('vm_key') or core_state.get('vmKey')
    vm_key = str(vm_key_raw or '').strip()
    if not vm_key:
        return jsonify({'success': False, 'error': 'Select a CORE VM before applying bridge changes'}), 400
    try:
        core_node, core_vmid = _parse_proxmox_vm_key(vm_key)
    except ValueError as exc:
        return jsonify({'success': False, 'error': f'CORE VM selection invalid: {exc}'}), 400
    core_vm_name = str(core_state.get('vm_name') or core_state.get('vmName') or '').strip()

    interfaces_payload = hitl_payload.get('interfaces')
    if not isinstance(interfaces_payload, list) or not interfaces_payload:
        return jsonify({'success': False, 'error': 'Add at least one HITL interface before applying bridge changes'}), 400

    validation_errors: List[str] = []
    assignments: List[Dict[str, Any]] = []
    for idx, iface in enumerate(interfaces_payload):
        if not isinstance(iface, dict):
            continue
        iface_name = str(iface.get('name') or f'Interface {idx + 1}').strip() or f'Interface {idx + 1}'
        prox_target = iface.get('proxmox_target')
        if not isinstance(prox_target, dict):
            validation_errors.append(f'{iface_name}: Map the interface to a CORE VM adapter in Step 3.')
            continue
        external = iface.get('external_vm')
        attachment = _normalize_hitl_attachment(iface.get('attachment'))
        if attachment != 'proxmox_vm':
            if isinstance(external, dict):
                attachment = 'proxmox_vm'
            else:
                continue
        core_iface_id = str(prox_target.get('interface_id') or '').strip()
        if not core_iface_id:
            validation_errors.append(f'{iface_name}: Select the CORE VM interface to use for HITL connectivity.')
            continue
        target_node = str(prox_target.get('node') or '').strip() or core_node
        try:
            target_vmid = int(prox_target.get('vmid') or core_vmid)
        except Exception:
            validation_errors.append(f'{iface_name}: CORE VM identifier is invalid.')
            continue
        if target_node != core_node or target_vmid != core_vmid:
            validation_errors.append(f'{iface_name}: CORE interface must belong to the selected CORE VM on node {core_node}.')
            continue
        if not isinstance(external, dict):
            validation_errors.append(f'{iface_name}: Select an external Proxmox VM in Step 4.')
            continue
        external_vm_key = str(external.get('vm_key') or external.get('vmKey') or '').strip()
        if not external_vm_key:
            validation_errors.append(f'{iface_name}: Select an external Proxmox VM in Step 4.')
            continue
        try:
            external_node, external_vmid = _parse_proxmox_vm_key(external_vm_key)
        except ValueError as exc:
            validation_errors.append(f'{iface_name}: External VM invalid: {exc}')
            continue
        if external_node != core_node:
            validation_errors.append(f'{iface_name}: External VM must be hosted on node {core_node}.')
            continue
        external_iface_id = str(external.get('interface_id') or '').strip()
        if not external_iface_id:
            validation_errors.append(f'{iface_name}: Select the external VM interface to connect through the bridge.')
            continue
        assignments.append({
            'name': iface_name,
            'core': {
                'node': core_node,
                'vmid': core_vmid,
                'vm_name': core_vm_name,
                'interface_id': core_iface_id,
            },
            'external': {
                'node': external_node,
                'vmid': external_vmid,
                'vm_name': str(external.get('vm_name') or '').strip(),
                'interface_id': external_iface_id,
            },
        })

    if validation_errors:
        message = ' ; '.join(validation_errors[:3])
        if len(validation_errors) > 3:
            message += f' (and {len(validation_errors) - 3} more issue(s))'
        return jsonify({'success': False, 'error': message, 'details': validation_errors}), 400
    if not assignments:
        return jsonify({'success': False, 'error': 'No eligible HITL interface mappings found. Map at least one interface to a CORE VM and external VM before applying.'}), 400

    try:
        client, _record = _connect_proxmox_from_secret(secret_id)
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 502
    except Exception as exc:  # pragma: no cover - defensive logging
        app.logger.exception('[hitl] unexpected failure connecting to Proxmox: %s', exc)
        return jsonify({'success': False, 'error': 'Unexpected error connecting to Proxmox'}), 500

    owner_raw = payload.get('bridge_owner') or core_state.get('internal_bridge_owner') or payload.get('username')
    bridge_owner = str(owner_raw or '').strip()
    comment_bits = ['core-topo-gen HITL bridge']
    if scenario_name:
        comment_bits.append(f'scenario={scenario_name}')
    if bridge_owner:
        comment_bits.append(f'owner={bridge_owner}')
    bridge_comment = ' '.join(comment_bits)
    try:
        bridge_meta = _ensure_proxmox_bridge(client, core_node, bridge_name, comment=bridge_comment)
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 502

    vm_config_cache: Dict[tuple[str, int], Dict[str, Any]] = {}

    def _get_vm_config(node: str, vmid: int) -> Dict[str, Any]:
        key = (node, vmid)
        if key not in vm_config_cache:
            try:
                vm_config_cache[key] = client.nodes(node).qemu(vmid).config.get()
            except Exception as exc:
                raise RuntimeError(f'Failed to fetch configuration for VM {vmid} on node {node}: {exc}') from exc
        return vm_config_cache[key]

    vm_updates: Dict[tuple[str, int], Dict[str, str]] = {}
    change_details: List[Dict[str, Any]] = []
    try:
        for assignment in assignments:
            for role in ('core', 'external'):
                vm_info = assignment[role]
                node = vm_info['node']
                vmid = vm_info['vmid']
                interface_id = vm_info['interface_id']
                vm_name = vm_info.get('vm_name') or ''
                config = _get_vm_config(node, vmid)
                net_config = config.get(interface_id)
                if not isinstance(net_config, str) or not net_config.strip():
                    raise ValueError(f'{role.title()} VM {vm_name or vmid} is missing Proxmox interface {interface_id}.')
                new_config, changed, previous_bridge = _rewrite_bridge_in_net_config(net_config, bridge_name)
                change_details.append({
                    'role': role,
                    'scenario_interface': assignment['name'],
                    'node': node,
                    'vmid': vmid,
                    'vm_name': vm_name,
                    'interface_id': interface_id,
                    'previous_bridge': previous_bridge,
                    'new_bridge': bridge_name,
                    'changed': changed,
                })
                if changed:
                    vm_updates.setdefault((node, vmid), {})[interface_id] = new_config
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 502

    updated_vms: List[Dict[str, Any]] = []
    for (node, vmid), updates in vm_updates.items():
        if not updates:
            continue
        try:
            client.nodes(node).qemu(vmid).config.post(**updates)
        except Exception as exc:  # pragma: no cover - defensive logging
            app.logger.exception('[hitl] failed updating Proxmox VM config: %s', exc)
            return jsonify({'success': False, 'error': f'Failed to update Proxmox VM {vmid} on node {node}: {exc}'}), 502
        updated_vms.append({
            'node': node,
            'vmid': vmid,
            'interfaces': list(updates.keys()),
        })

    changed_interfaces = sum(1 for change in change_details if change.get('changed'))
    unchanged_interfaces = len(change_details) - changed_interfaces
    assignment_count = len(assignments)

    parts = [f'Bridge {bridge_name} applied to {assignment_count} HITL link{"s" if assignment_count != 1 else ""}.']
    if changed_interfaces:
        parts.append(f'{changed_interfaces} Proxmox interface{"s" if changed_interfaces != 1 else ""} updated.')
    if unchanged_interfaces:
        parts.append(f'{unchanged_interfaces} already on the requested bridge.')
    message = ' '.join(parts)

    warnings: List[str] = []
    if bridge_meta.get('created') and not bridge_meta.get('reload_ok'):
        warnings.append('Bridge created but Proxmox did not confirm network reload; apply pending changes manually if required.')

    response: Dict[str, Any] = {
        'success': True,
        'message': message,
        'bridge_name': bridge_name,
        'bridge_created': bool(bridge_meta.get('created')),
        'bridge_already_exists': bool(bridge_meta.get('already_exists')),
        'bridge_reload_ok': bool(bridge_meta.get('reload_ok')),
        'bridge_reload_error': bridge_meta.get('reload_error'),
        'scenario_index': scenario_index,
        'scenario_name': scenario_name,
        'assignments': assignment_count,
        'changed_interfaces': changed_interfaces,
        'unchanged_interfaces': unchanged_interfaces,
        'changes': change_details,
        'updated_vms': updated_vms,
        'proxmox_node': core_node,
    }
    if warnings:
        response['warnings'] = warnings
    if bridge_owner:
        response['bridge_owner'] = bridge_owner

    # Persist the applied HITL config for builder view (non-secret fields only).
    try:
        if scenario_name:
            hitl_to_store = dict(hitl_payload)
            core_store = hitl_to_store.get('core') if isinstance(hitl_to_store.get('core'), dict) else {}
            core_store = dict(core_store)
            core_store['internal_bridge'] = bridge_name
            if bridge_owner:
                core_store['internal_bridge_owner'] = bridge_owner
            hitl_to_store['core'] = core_store
            _merge_hitl_config_into_scenario_catalog(scenario_name, hitl_to_store)
    except Exception:
        pass

    app.logger.info(
        '[hitl] applied internal bridge %s on node %s (%d assignment(s), %d interface change(s))',
        bridge_name,
        core_node,
        assignment_count,
        changed_interfaces,
    )
    return jsonify(response)


@app.route('/api/hitl/validate_bridge', methods=['POST'])
def api_hitl_validate_bridge():
    """Validate HITL bridge configuration without applying any changes."""

    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403

    payload = request.get_json(silent=True) or {}
    bridge_raw = payload.get('bridge_name') or payload.get('internal_bridge') or payload.get('bridge')
    if bridge_raw in (None, ''):
        return jsonify({'success': False, 'error': 'Bridge name is required'}), 400
    try:
        bridge_name = _normalize_internal_bridge_name(bridge_raw)
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400

    scenario_name = str(payload.get('scenario_name') or payload.get('scenario') or '').strip()
    scenario_index_raw = payload.get('scenario_index')
    try:
        scenario_index = int(scenario_index_raw)
    except Exception:
        scenario_index = None

    hitl_payload = payload.get('hitl') or payload.get('scenario_hitl') or payload.get('hitl_config')
    if not isinstance(hitl_payload, dict):
        return jsonify({'success': False, 'error': 'HITL configuration is required to validate bridge settings'}), 400

    prox_state = hitl_payload.get('proxmox') or {}
    secret_id_raw = prox_state.get('secret_id') or prox_state.get('secretId') or prox_state.get('identifier')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    if not secret_id:
        return jsonify({'success': False, 'error': 'Validate and store Proxmox credentials before verifying HITL bridge settings'}), 400

    core_state = hitl_payload.get('core') or {}
    vm_key_raw = core_state.get('vm_key') or core_state.get('vmKey')
    vm_key = str(vm_key_raw or '').strip()
    if not vm_key:
        return jsonify({'success': False, 'error': 'Select a CORE VM before verifying HITL bridge settings'}), 400
    try:
        core_node, core_vmid = _parse_proxmox_vm_key(vm_key)
    except ValueError as exc:
        return jsonify({'success': False, 'error': f'CORE VM selection invalid: {exc}'}), 400
    core_vm_name = str(core_state.get('vm_name') or core_state.get('vmName') or '').strip()

    interfaces_payload = hitl_payload.get('interfaces')
    if not isinstance(interfaces_payload, list) or not interfaces_payload:
        return jsonify({'success': False, 'error': 'Add at least one HITL interface before verifying bridge settings'}), 400

    validation_errors: List[str] = []
    assignments: List[Dict[str, Any]] = []
    for idx, iface in enumerate(interfaces_payload):
        if not isinstance(iface, dict):
            continue
        iface_name = str(iface.get('name') or f'Interface {idx + 1}').strip() or f'Interface {idx + 1}'
        prox_target = iface.get('proxmox_target')
        if not isinstance(prox_target, dict):
            validation_errors.append(f'{iface_name}: Map the interface to a CORE VM adapter in Step 3.')
            continue
        external = iface.get('external_vm')
        attachment = _normalize_hitl_attachment(iface.get('attachment'))
        if attachment != 'proxmox_vm':
            if isinstance(external, dict):
                attachment = 'proxmox_vm'
            else:
                continue
        core_iface_id = str(prox_target.get('interface_id') or '').strip()
        if not core_iface_id:
            validation_errors.append(f'{iface_name}: Select the CORE VM interface to use for HITL connectivity.')
            continue
        target_node = str(prox_target.get('node') or '').strip() or core_node
        try:
            target_vmid = int(prox_target.get('vmid') or core_vmid)
        except Exception:
            validation_errors.append(f'{iface_name}: CORE VM identifier is invalid.')
            continue
        if target_node != core_node or target_vmid != core_vmid:
            validation_errors.append(f'{iface_name}: CORE interface must belong to the selected CORE VM on node {core_node}.')
            continue
        if not isinstance(external, dict):
            validation_errors.append(f'{iface_name}: Select an external Proxmox VM in Step 4.')
            continue
        external_vm_key = str(external.get('vm_key') or external.get('vmKey') or '').strip()
        if not external_vm_key:
            validation_errors.append(f'{iface_name}: Select an external Proxmox VM in Step 4.')
            continue
        try:
            external_node, external_vmid = _parse_proxmox_vm_key(external_vm_key)
        except ValueError as exc:
            validation_errors.append(f'{iface_name}: External VM invalid: {exc}')
            continue
        if external_node != core_node:
            validation_errors.append(f'{iface_name}: External VM must be hosted on node {core_node}.')
            continue
        external_iface_id = str(external.get('interface_id') or '').strip()
        if not external_iface_id:
            validation_errors.append(f'{iface_name}: Select the external VM interface to connect through the bridge.')
            continue
        assignments.append({
            'name': iface_name,
            'core': {
                'node': core_node,
                'vmid': core_vmid,
                'vm_name': core_vm_name,
                'interface_id': core_iface_id,
            },
            'external': {
                'node': external_node,
                'vmid': external_vmid,
                'vm_name': str(external.get('vm_name') or '').strip(),
                'interface_id': external_iface_id,
            },
        })

    if validation_errors:
        message = ' ; '.join(validation_errors[:3])
        if len(validation_errors) > 3:
            message += f' (and {len(validation_errors) - 3} more issue(s))'
        return jsonify({'success': False, 'error': message, 'details': validation_errors}), 400
    if not assignments:
        return jsonify({'success': False, 'error': 'No eligible HITL interface mappings found. Map at least one interface to a CORE VM and external VM before verifying.'}), 400

    try:
        client, _record = _connect_proxmox_from_secret(secret_id)
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 502
    except Exception as exc:  # pragma: no cover
        app.logger.exception('[hitl] unexpected failure connecting to Proxmox: %s', exc)
        return jsonify({'success': False, 'error': 'Unexpected error connecting to Proxmox'}), 500

    try:
        bridge_meta = _ensure_proxmox_bridge(client, core_node, bridge_name)
    except RuntimeError as exc:
        msg = str(exc)
        status = 400 if 'not found on node' in msg.lower() else 502
        return jsonify({'success': False, 'error': msg}), status

    vm_config_cache: Dict[tuple[str, int], Dict[str, Any]] = {}

    def _get_vm_config(node: str, vmid: int) -> Dict[str, Any]:
        key = (node, vmid)
        if key not in vm_config_cache:
            try:
                vm_config_cache[key] = client.nodes(node).qemu(vmid).config.get()
            except Exception as exc:
                raise RuntimeError(f'Failed to fetch configuration for VM {vmid} on node {node}: {exc}') from exc
        return vm_config_cache[key]

    change_details: List[Dict[str, Any]] = []
    try:
        for assignment in assignments:
            for role in ('core', 'external'):
                vm_info = assignment[role]
                node = vm_info['node']
                vmid = vm_info['vmid']
                interface_id = vm_info['interface_id']
                vm_name = vm_info.get('vm_name') or ''
                config = _get_vm_config(node, vmid)
                net_config = config.get(interface_id)
                if not isinstance(net_config, str) or not net_config.strip():
                    raise ValueError(f'{role.title()} VM {vm_name or vmid} is missing Proxmox interface {interface_id}.')
                _new_config, changed, previous_bridge = _rewrite_bridge_in_net_config(net_config, bridge_name)
                change_details.append({
                    'role': role,
                    'scenario_interface': assignment['name'],
                    'node': node,
                    'vmid': vmid,
                    'vm_name': vm_name,
                    'interface_id': interface_id,
                    'previous_bridge': previous_bridge,
                    'new_bridge': bridge_name,
                    'changed': changed,
                })
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 502

    changed_interfaces = sum(1 for change in change_details if change.get('changed'))
    unchanged_interfaces = len(change_details) - changed_interfaces
    assignment_count = len(assignments)

    parts = [f'Bridge {bridge_name} validation succeeded for {assignment_count} HITL link{"s" if assignment_count != 1 else ""}.']
    if changed_interfaces:
        parts.append(f'{changed_interfaces} Proxmox interface{"s" if changed_interfaces != 1 else ""} would be updated.')
    if unchanged_interfaces:
        parts.append(f'{unchanged_interfaces} already on the requested bridge.')
    message = ' '.join(parts)

    response: Dict[str, Any] = {
        'success': True,
        'message': message,
        'bridge_name': bridge_name,
        'bridge_meta': bridge_meta,
        'scenario_index': scenario_index,
        'scenario_name': scenario_name,
        'assignments': assignment_count,
        'changed_interfaces': changed_interfaces,
        'unchanged_interfaces': unchanged_interfaces,
        'changes': change_details,
        'proxmox_node': core_node,
    }

    # Persist verified HITL config for builder view (non-secret fields only).
    try:
        if scenario_name:
            hitl_to_store = dict(hitl_payload)
            core_store = hitl_to_store.get('core') if isinstance(hitl_to_store.get('core'), dict) else {}
            core_store = dict(core_store)
            core_store['internal_bridge'] = bridge_name
            hitl_to_store['core'] = core_store
            _merge_hitl_config_into_scenario_catalog(scenario_name, hitl_to_store)
    except Exception:
        pass
    return jsonify(response)

# ---------------- Docker (per-node) status & cleanup ----------------
def _compose_assignments_path() -> str:
    return os.path.join(_vuln_base_dir() or "/tmp/vulns", "compose_assignments.json")


def _load_compose_assignments() -> dict:
    p = _compose_assignments_path()
    try:
        if os.path.exists(p):
            with open(p, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        app.logger.debug("compose assignments read failed: %s", e)
    return {}


def _compose_file_for_node(node_name: str) -> str:
    base = _vuln_base_dir() or "/tmp/vulns"
    return os.path.join(base, f"docker-compose-{node_name}.yml")


def _docker_container_exists(name: str) -> tuple[bool, bool]:
    try:
        proc = subprocess.run(["docker", "ps", "-a", "--format", "{{.Names}}"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        if proc.returncode != 0:
            return (False, False)
        names = set(ln.strip() for ln in (proc.stdout or '').splitlines() if ln.strip())
        if name not in names:
            return (False, False)
        proc2 = subprocess.run(["docker", "inspect", "-f", "{{.State.Running}}", name], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        running = (proc2.returncode == 0 and (proc2.stdout or '').strip().lower() == 'true')
        return (True, running)
    except Exception:
        return (False, False)


def _images_pulled_for_compose_safe(yml_path: str) -> bool:
    try:
        from core_topo_gen.utils.vuln_process import _images_pulled_for_compose as _pulled  # type: ignore
        return bool(_pulled(yml_path))
    except Exception as e:
        try: app.logger.debug("pull check failed for %s: %s", yml_path, e)
        except Exception: pass
        return False


def _remote_docker_status_script(sudo_password: str | None = None) -> str:
    """Remote script to read compose assignments + docker state on the CORE VM.

    This intentionally runs on the remote CORE host (via SSH) so the Docker Compose
    card reflects remote state, not the local Flask machine.
    """

    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    return (
        """
import json, os, time, subprocess

SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__


def _run_docker(cmd, timeout=20, capture=True):
    # Run docker on the CORE VM via sudo.
    # - Try non-interactive sudo first (fast fail if password is required).
    # - If a sudo password is available, retry with sudo -S.
    stdout = subprocess.PIPE if capture else subprocess.DEVNULL
    stderr = subprocess.STDOUT if capture else subprocess.DEVNULL
    try:
        p = subprocess.run(['sudo', '-n', 'docker'] + list(cmd), stdout=stdout, stderr=stderr, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            # -k forces sudo to prompt for password (so -S matters even if a cached ticket exists).
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', '', 'docker'] + list(cmd),
                input=str(SUDO_PASSWORD) + "\\n",
                stdout=stdout,
                stderr=stderr,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        class _P:
            returncode = 1
            stdout = str(e)
        return _P()


def _first_existing(path_candidates):
    for p in path_candidates:
        if not p:
            continue
        try:
            if os.path.exists(p):
                return p
        except Exception:
            continue
    return None


def _read_json(path):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def _docker_names():
    try:
        p = _run_docker(['ps', '-a', '--format', '{{.Names}}'], timeout=15, capture=True)
        if p.returncode != 0:
            out = (p.stdout or '').strip()
            if out and 'permission denied' in out.lower():
                out = out + "\\nHint: add the SSH user to the 'docker' group or allow passwordless sudo for docker."
            return set(), out
        return set(ln.strip() for ln in (p.stdout or '').splitlines() if ln.strip()), ''
    except Exception as e:
        return set(), str(e)


def _container_running(name):
    try:
        p = _run_docker(['inspect', '-f', '{{.State.Running}}', name], timeout=10, capture=True)
        return (p.returncode == 0 and (p.stdout or '').strip().lower() == 'true')
    except Exception:
        return False


def _images_pulled_for_compose(yml_path):
    # Best-effort: list images referenced by the compose file and confirm they exist locally.
    try:
        p = _run_docker(['compose', '-f', yml_path, 'config', '--images'], timeout=20, capture=True)
        if p.returncode != 0:
            return False
        images = [ln.strip() for ln in (p.stdout or '').splitlines() if ln.strip()]
        if not images:
            return False
        for img in images:
            chk = _run_docker(['image', 'inspect', img], timeout=20, capture=False)
            if chk.returncode != 0:
                return False
        return True
    except Exception:
        return False


def main():
    base = os.environ.get('CORE_REMOTE_BASE_DIR', '/tmp/core-topo-gen')
    candidates = [
        os.path.join(base, 'vulns', 'compose_assignments.json'),
        os.path.join(base, 'outputs', 'vulns', 'compose_assignments.json'),
        os.path.join(base, 'compose_assignments.json'),
        '/tmp/vulns/compose_assignments.json',
    ]
    assignments_path = _first_existing(candidates)
    if not assignments_path:
        print(json.dumps({'items': [], 'timestamp': int(time.time()), 'error': 'compose_assignments.json not found on CORE VM'}))
        return

    try:
        data = _read_json(assignments_path)
    except Exception as e:
        print(json.dumps({'items': [], 'timestamp': int(time.time()), 'error': f'failed reading assignments: {e}'}))
        return

    assignments = data.get('assignments', {}) if isinstance(data, dict) else {}
    if not isinstance(assignments, dict):
        assignments = {}

    assign_dir = os.path.dirname(assignments_path)
    names, docker_err = _docker_names()
    items = []
    for node_name in sorted(assignments.keys()):
        yml = os.path.join(assign_dir, f'docker-compose-{node_name}.yml')
        exists = False
        try:
            exists = os.path.exists(yml)
        except Exception:
            exists = False
        pulled = _images_pulled_for_compose(yml) if exists else False
        c_exists = node_name in names
        running = _container_running(node_name) if c_exists else False
        items.append({
            'name': node_name,
            'compose': yml,
            'exists': bool(exists),
            'pulled': bool(pulled),
            'container_exists': bool(c_exists),
            'running': bool(running),
        })

    payload = {'items': items, 'timestamp': int(time.time())}
    if docker_err:
        payload['error'] = docker_err
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    ).replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal)


def _remote_copy_flow_artifacts_into_containers_script(sudo_password: str | None = None) -> str:
    """Remote script to copy Flow generator artifacts into vuln containers.

    This supports CORETG_FLOW_ARTIFACTS_MODE=copy where compose files contain
    labels describing src/dest paths instead of bind mounts.
    """
    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    return (
        r"""
import json, os, re, subprocess, time

SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__


def _run_docker(cmd, timeout=30, capture=True):
    stdout = subprocess.PIPE if capture else subprocess.DEVNULL
    stderr = subprocess.STDOUT if capture else subprocess.DEVNULL
    try:
        p = subprocess.run(['sudo', '-n', 'docker'] + list(cmd), stdout=stdout, stderr=stderr, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', '', 'docker'] + list(cmd),
                input=str(SUDO_PASSWORD) + "\n",
                stdout=stdout,
                stderr=stderr,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        class _P:
            returncode = 1
            stdout = str(e)
        return _P()


def _first_existing(path_candidates):
    for p in path_candidates:
        if not p:
            continue
        try:
            if os.path.exists(p):
                return p
        except Exception:
            continue
    return None


def _read_json(path):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def _docker_names():
    p = _run_docker(['ps', '-a', '--format', '{{.Names}}'], timeout=20, capture=True)
    if getattr(p, 'returncode', 1) != 0:
        return set(), (getattr(p, 'stdout', '') or '').strip()
    return set(ln.strip() for ln in (p.stdout or '').splitlines() if ln.strip()), ''


def _parse_flow_labels(txt: str):
    # YAML-ish key:value, possibly quoted.
    src = None
    dest = None
    m1 = re.search(r"coretg\.flow_artifacts\.src\s*:\s*(['\"]?)([^\n'\"]+)\1", txt)
    if m1:
        src = (m1.group(2) or '').strip()
    m2 = re.search(r"coretg\.flow_artifacts\.dest\s*:\s*(['\"]?)([^\n'\"]+)\1", txt)
    if m2:
        dest = (m2.group(2) or '').strip()
    return src, dest


def _parse_flow_bind_mount(txt: str):
    # Fallback: parse a volumes entry like /tmp/vulns/flag_generators_runs/...:/flow_artifacts:ro
    m = re.search(r"(/tmp/vulns/(?:flag_generators_runs|flag_node_generators_runs)/[^:\s\"\']+)\s*:\s*([^:\s\"\']+)", txt)
    if not m:
        return None, None
    return (m.group(1) or '').strip(), (m.group(2) or '').strip()


def _compose_container_ids(project: str, yml: str):
    p = _run_docker(['compose', '-p', project, '-f', yml, 'ps', '-q'], timeout=25, capture=True)
    if getattr(p, 'returncode', 1) != 0:
        return []
    return [ln.strip() for ln in (p.stdout or '').splitlines() if ln.strip()]


def main():
    base = os.environ.get('CORE_REMOTE_BASE_DIR', '/tmp/core-topo-gen')
    candidates = [
        os.path.join(base, 'vulns', 'compose_assignments.json'),
        os.path.join(base, 'outputs', 'vulns', 'compose_assignments.json'),
        os.path.join(base, 'compose_assignments.json'),
        '/tmp/vulns/compose_assignments.json',
    ]
    assignments_path = _first_existing(candidates)
    if not assignments_path:
        print(json.dumps({'ok': False, 'items': [], 'error': 'compose_assignments.json not found on CORE VM'}))
        return

    try:
        data = _read_json(assignments_path)
    except Exception as e:
        print(json.dumps({'ok': False, 'items': [], 'error': f'failed reading assignments: {e}'}))
        return

    assignments = data.get('assignments', {}) if isinstance(data, dict) else {}
    if not isinstance(assignments, dict):
        assignments = {}

    assign_dir = os.path.dirname(assignments_path)
    items = []
    for node_name in sorted(assignments.keys()):
        yml = os.path.join(assign_dir, f'docker-compose-{node_name}.yml')
        if not os.path.exists(yml):
            continue
        try:
            txt = open(yml, 'r', encoding='utf-8', errors='ignore').read()
        except Exception:
            txt = ''

        src, dest = _parse_flow_labels(txt)
        if not src or not dest:
            src2, dest2 = _parse_flow_bind_mount(txt)
            src = src or src2
            dest = dest or dest2

        if not src or not dest:
            continue

        if not os.path.isdir(src):
            items.append({'node': node_name, 'compose': yml, 'src': src, 'dest': dest, 'ok': False, 'error': 'src dir missing'})
            continue

        # Find container targets: prefer container named node_name; else use compose project containers.
        targets = []
        last_err = ''
        for _ in range(6):
            names, err = _docker_names()
            last_err = err or last_err
            if node_name in names:
                targets = [node_name]
                break
            project = f"{node_name}conf" if node_name else 'coretg'
            ids = _compose_container_ids(project, yml)
            if ids:
                targets = ids
                break
            time.sleep(2)

        if not targets:
            items.append({'node': node_name, 'compose': yml, 'src': src, 'dest': dest, 'ok': False, 'error': 'container not found', 'docker_error': last_err})
            continue

        copied_any = False
        errs = []
        for t in targets:
            # Copy directory contents into dest.
            p = _run_docker(['cp', src.rstrip('/') + '/.', f"{t}:{dest}"], timeout=60, capture=True)
            if getattr(p, 'returncode', 1) == 0:
                copied_any = True
            else:
                out = (getattr(p, 'stdout', '') or '').strip()
                errs.append(out)
        items.append({'node': node_name, 'compose': yml, 'src': src, 'dest': dest, 'targets': targets, 'ok': bool(copied_any), 'errors': errs})

    print(json.dumps({'ok': True, 'items': items, 'timestamp': int(time.time())}))


if __name__ == '__main__':
    main()
"""
    ).replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal)


def _sync_local_vulns_to_remote(
    core_cfg: Dict[str, Any],
    *,
    local_dir: str = "/tmp/vulns",
    remote_dir: str = "/tmp/vulns",
    logger: Optional[logging.Logger] = None,
) -> bool:
    """Copy vulnerability compose artifacts created locally to the remote CORE host.

    The webapp runs the CLI locally (with a tunneled gRPC connection), so the CLI
    writes compose artifacts under local /tmp. The Docker Compose card and CORE
    DockerComposeService run on the CORE VM and expect the same artifacts on the
    remote filesystem.

    Returns True if any files were uploaded.
    """

    log = logger or getattr(app, 'logger', logging.getLogger(__name__))

    assignments_path = os.path.join(local_dir, "compose_assignments.json")
    if not os.path.exists(assignments_path):
        return False

    node_names: List[str] = []
    try:
        with open(assignments_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        assignments = data.get('assignments', {}) if isinstance(data, dict) else {}
        if isinstance(assignments, dict):
            node_names = [str(k) for k in assignments.keys()]
    except Exception:
        node_names = []

    local_files: List[str] = [assignments_path]
    for nm in sorted(set(node_names)):
        yml = os.path.join(local_dir, f"docker-compose-{nm}.yml")
        if os.path.exists(yml):
            local_files.append(yml)

    # Flow flag artifacts: bind-mounted into docker-compose services (e.g., to /flow_artifacts).
    # These directories live under /tmp/vulns/* on the webapp host and must be present on the
    # CORE VM too; otherwise the container sees an empty mount.
    local_dirs: List[str] = []
    try:
        import re

        for yml in [p for p in local_files if p.endswith('.yml') or p.endswith('.yaml')]:
            try:
                txt = ''
                with open(yml, 'r', encoding='utf-8', errors='ignore') as f:
                    txt = f.read()
                # Match the source side of a bind mount that points into /tmp/vulns.
                # Example: /tmp/vulns/flag_generators_runs/flow-...:/flow_artifacts:ro
                for m in re.findall(r'(/tmp/vulns/(?:flag_generators_runs|flag_node_generators_runs)/[^:\s\"\']+)', txt):
                    pth = str(m).strip().strip('"').strip("'")
                    if pth and os.path.isdir(pth):
                        local_dirs.append(pth)
            except Exception:
                continue
    except Exception:
        local_dirs = []

    if not local_files:
        return False

    client = None
    sftp = None
    uploaded = 0
    try:
        client = _open_ssh_client(core_cfg)
        sftp = client.open_sftp()
        remote_dir_resolved = _remote_expand_path(sftp, remote_dir)
        _remote_mkdirs(client, remote_dir_resolved)
        made_dirs: set[str] = set()
        for lp in local_files:
            rp = _remote_path_join(remote_dir_resolved, os.path.basename(lp))
            try:
                sftp.put(lp, rp)
                uploaded += 1
            except Exception:
                log.exception("[sync] Failed uploading %s -> %s", lp, rp)

        # Upload any referenced Flow generator run directories, preserving relative paths.
        # Example local:  /tmp/vulns/flag_generators_runs/flow-.../outputs.json
        # Example remote: /tmp/vulns/flag_generators_runs/flow-.../outputs.json
        for d in sorted(set(local_dirs)):
            try:
                rel = os.path.relpath(d, local_dir)
            except Exception:
                rel = None
            if not rel or rel.startswith('..'):
                continue
            remote_d = _remote_path_join(remote_dir_resolved, rel)
            try:
                if remote_d not in made_dirs:
                    _remote_mkdirs(client, remote_d)
                    made_dirs.add(remote_d)
            except Exception:
                pass
            for root, dirs, files in os.walk(d):
                # Create directories first.
                for dn in dirs:
                    lp_dir = os.path.join(root, dn)
                    try:
                        rel_dir = os.path.relpath(lp_dir, local_dir)
                    except Exception:
                        continue
                    if not rel_dir or rel_dir.startswith('..'):
                        continue
                    rp_dir = _remote_path_join(remote_dir_resolved, rel_dir)
                    if rp_dir in made_dirs:
                        continue
                    try:
                        _remote_mkdirs(client, rp_dir)
                        made_dirs.add(rp_dir)
                    except Exception:
                        pass
                for fn in files:
                    lp_file = os.path.join(root, fn)
                    try:
                        rel_file = os.path.relpath(lp_file, local_dir)
                    except Exception:
                        continue
                    if not rel_file or rel_file.startswith('..'):
                        continue
                    rp_file = _remote_path_join(remote_dir_resolved, rel_file)
                    rp_parent = os.path.dirname(rp_file)
                    try:
                        if rp_parent and rp_parent not in made_dirs:
                            _remote_mkdirs(client, rp_parent)
                            made_dirs.add(rp_parent)
                    except Exception:
                        pass
                    try:
                        sftp.put(lp_file, rp_file)
                        uploaded += 1
                    except Exception:
                        log.exception("[sync] Failed uploading %s -> %s", lp_file, rp_file)
        if uploaded:
            log.info("[sync] Uploaded %d vuln artifact(s) to CORE host dir=%s", uploaded, remote_dir_resolved)
        return uploaded > 0
    except Exception:
        log.exception("[sync] Vulnerability artifact upload skipped/failed")
        return False
    finally:
        try:
            if sftp:
                sftp.close()
        except Exception:
            pass
        try:
            if client:
                client.close()
        except Exception:
            pass


def _remote_docker_cleanup_script(names: list[str], sudo_password: str | None = None) -> str:
    names_literal = json.dumps([str(x) for x in (names or [])])
    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    return (
        """
import json, subprocess

NAMES = __NAMES_LITERAL__

SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__


def _run_docker(cmd, timeout=20):
    # Run docker on the CORE VM via sudo.
    # - Try non-interactive sudo first (fast fail if password is required).
    # - If a sudo password is available, retry with sudo -S.
    try:
        p = subprocess.run(['sudo', '-n', 'docker'] + list(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', '', 'docker'] + list(cmd),
                input=str(SUDO_PASSWORD) + "\\n",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        class _P:
            returncode = 1
            stdout = str(e)
        return _P()


def main():
    results = []
    for nm in (NAMES or []):
        stopped = removed = False
        try:
            p1 = _run_docker(['stop', nm], timeout=20)
            stopped = (p1.returncode == 0)
        except Exception:
            stopped = False
        try:
            p2 = _run_docker(['rm', nm], timeout=20)
            removed = (p2.returncode == 0)
        except Exception:
            removed = False
        results.append({'name': nm, 'stopped': bool(stopped), 'removed': bool(removed)})
    # Prune unused networks to avoid exhausting Docker's default address pools.
    # CORE docker nodes frequently create per-node compose networks; if sessions are
    # started/stopped repeatedly without network cleanup, docker can hit:
    # "all predefined address pools have been fully subnetted".
    net_prune_ok = False
    net_prune_out = ''
    try:
        p3 = _run_docker(['network', 'prune', '-f'], timeout=30)
        net_prune_ok = (getattr(p3, 'returncode', 1) == 0)
        net_prune_out = (getattr(p3, 'stdout', '') or '').strip()
    except Exception as e:
        net_prune_ok = False
        net_prune_out = str(e)
    print(json.dumps({'ok': True, 'results': results, 'network_prune': {'ok': bool(net_prune_ok), 'output': net_prune_out}}))


if __name__ == '__main__':
    main()
"""
    ).replace('__NAMES_LITERAL__', names_literal).replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal)


def _remote_docker_remove_wrapper_images_script(sudo_password: str | None = None) -> str:
    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    return (
        """
import json, subprocess

SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__


def _run_docker(cmd, timeout=30):
    try:
        p = subprocess.run(['sudo', '-n', 'docker'] + list(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', '', 'docker'] + list(cmd),
                input=str(SUDO_PASSWORD) + "\\n",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        class _P:
            returncode = 1
            stdout = str(e)
        return _P()


def main():
    # List all tool-generated wrapper images. We scope these under the "coretg" repo prefix.
    # Only remove images whose tag begins with "iproute2" (our wrapper tag).
    listed = []
    removed = []
    errors = []
    p = _run_docker(['images', '--format', '{{.Repository}}:{{.Tag}}', '--filter', 'reference=coretg/*'], timeout=30)
    out = (getattr(p, 'stdout', '') or '').strip()
    if out:
        for line in out.splitlines():
            ref = (line or '').strip()
            if not ref or ':' not in ref:
                continue
            listed.append(ref)
    targets = []
    for ref in listed:
        try:
            repo, tag = ref.rsplit(':', 1)
        except Exception:
            continue
        if tag.startswith('iproute2'):
            targets.append(ref)
    for ref in targets:
        try:
            pr = _run_docker(['image', 'rm', '-f', ref], timeout=60)
            if getattr(pr, 'returncode', 1) == 0:
                removed.append(ref)
            else:
                errors.append({'ref': ref, 'output': (getattr(pr, 'stdout', '') or '').strip()})
        except Exception as e:
            errors.append({'ref': ref, 'output': str(e)})
    print(json.dumps({'ok': True, 'listed': listed, 'targets': targets, 'removed': removed, 'errors': errors}))


if __name__ == '__main__':
    main()
"""
    ).replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal)


def _remote_docker_remove_all_containers_script(sudo_password: str | None = None) -> str:
    """Remote script to delete ALL docker containers on the CORE VM (does not remove images)."""

    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    return (
        """
import json, subprocess

SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__


def _run_docker(cmd, timeout=60):
    try:
        p = subprocess.run(['sudo', '-n', 'docker'] + list(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', '', 'docker'] + list(cmd),
                input=str(SUDO_PASSWORD) + "\n",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        class _P:
            returncode = 1
            stdout = str(e)
        return _P()


def _list_ids(args, timeout=30):
    p = _run_docker(args, timeout=timeout)
    if getattr(p, 'returncode', 1) != 0:
        return [], (getattr(p, 'stdout', '') or '').strip()
    out = (getattr(p, 'stdout', '') or '').strip()
    ids = [ln.strip() for ln in out.splitlines() if ln.strip()]
    return ids, ''


def _chunks(seq, n=40):
    for i in range(0, len(seq), n):
        yield seq[i:i + n]


def main():
    errors = []

    container_ids, cerr = _list_ids(['ps', '-aq'], timeout=30)
    if cerr:
        errors.append({'stage': 'list_containers', 'output': cerr})
    stopped_attempted = 0
    removed_attempted = 0
    if container_ids:
        for chunk in _chunks(container_ids, 25):
            p = _run_docker(['stop'] + list(chunk), timeout=120)
            if getattr(p, 'returncode', 1) != 0:
                out = (getattr(p, 'stdout', '') or '').strip()
                if out:
                    errors.append({'stage': 'stop_containers', 'output': out[:4000]})
            else:
                stopped_attempted += len(chunk)
        for chunk in _chunks(container_ids, 25):
            p = _run_docker(['rm', '-f'] + list(chunk), timeout=120)
            if getattr(p, 'returncode', 1) != 0:
                out = (getattr(p, 'stdout', '') or '').strip()
                if out:
                    errors.append({'stage': 'rm_containers', 'output': out[:4000]})
            else:
                removed_attempted += len(chunk)

    print(json.dumps({
        'ok': True,
        'containers': {
            'found': len(container_ids),
            'stopped_attempted': stopped_attempted,
            'removed_attempted': removed_attempted,
        },
        'images': {
            'removed_attempted': 0,
            'skipped': True,
        },
        'errors': errors,
    }))


if __name__ == '__main__':
    main()
"""
    ).replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal)


@app.route('/docker/status', methods=['GET'])
def docker_status():
    # Use the scenario-selected CORE VM (remote) to populate docker status.
    history = _load_run_history()
    current_user = _current_user()
    scenario_names, _scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
    scenario_query = request.args.get('scenario', '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    if scenario_names:
        if not scenario_norm or not any(_normalize_scenario_label(n) == scenario_norm for n in scenario_names):
            scenario_norm = _normalize_scenario_label(scenario_names[0])
    core_cfg = _select_core_config_for_page(scenario_norm, history, include_password=True)
    core_cfg = _ensure_core_vm_metadata(core_cfg)
    try:
        payload = _run_remote_python_json(
            core_cfg,
            _remote_docker_status_script(core_cfg.get('ssh_password')),
            logger=app.logger,
            label='docker.status',
            timeout=60.0,
        )
        if not isinstance(payload, dict):
            payload = {'items': [], 'timestamp': int(time.time()), 'error': 'invalid remote payload'}
        payload.setdefault('timestamp', int(time.time()))
        return jsonify(payload)
    except Exception as exc:
        return jsonify({'items': [], 'timestamp': int(time.time()), 'error': str(exc)}), 200


@app.route('/docker/cleanup', methods=['POST'])
def docker_cleanup():
    names: list[str] = []
    try:
        if request.is_json:
            body = request.get_json(silent=True) or {}
            if isinstance(body.get('names'), list):
                names = [str(x) for x in body.get('names')]
        else:
            raw = request.form.get('names')
            if raw:
                try:
                    arr = json.loads(raw)
                    if isinstance(arr, list):
                        names = [str(x) for x in arr]
                    else:
                        names = [str(raw)]
                except Exception:
                    names = [str(raw)]
        # Use the scenario-selected CORE VM (remote) to cleanup docker containers.
        history = _load_run_history()
        current_user = _current_user()
        scenario_names, _scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
        scenario_query = request.args.get('scenario', '').strip()
        scenario_norm = _normalize_scenario_label(scenario_query)
        if scenario_names:
            if not scenario_norm or not any(_normalize_scenario_label(n) == scenario_norm for n in scenario_names):
                scenario_norm = _normalize_scenario_label(scenario_names[0])
        core_cfg = _select_core_config_for_page(scenario_norm, history, include_password=True)
        core_cfg = _ensure_core_vm_metadata(core_cfg)

        if not names:
            # If no names supplied, reuse the remote status listing to derive all node names.
            try:
                status_payload = _run_remote_python_json(
                    core_cfg,
                    _remote_docker_status_script(core_cfg.get('ssh_password')),
                    logger=app.logger,
                    label='docker.status(for cleanup)',
                    timeout=60.0,
                )
                if isinstance(status_payload, dict) and isinstance(status_payload.get('items'), list):
                    names = [str(it.get('name')) for it in status_payload['items'] if isinstance(it, dict) and it.get('name')]
            except Exception:
                names = []

        payload = _run_remote_python_json(
            core_cfg,
            _remote_docker_cleanup_script(names, core_cfg.get('ssh_password')),
            logger=app.logger,
            label='docker.cleanup',
            timeout=120.0,
        )
        if not isinstance(payload, dict):
            payload = {'ok': False, 'error': 'invalid remote payload'}
        return jsonify(payload)
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)}), 500


def _find_latest_session_xml(base_dir: str, stem: str | None = None) -> str | None:
    """Best-effort helper to locate the newest session XML written previously.

    If `stem` is provided, prefer files beginning with that stem; otherwise
    return the most recent .xml file under the directory.
    """
    try:
        if not base_dir or not os.path.isdir(base_dir):
            return None
        candidates: list[tuple[float, str]] = []
        for name in os.listdir(base_dir):
            if not name.lower().endswith('.xml'):
                continue
            if stem:
                prefix = f"{stem}-"
                if not name.startswith(prefix):
                    continue
            path = os.path.join(base_dir, name)
            try:
                st = os.stat(path)
                candidates.append((st.st_mtime, path))
            except Exception:
                continue
        if not candidates and stem:
            # If no stem-specific match, allow fallback to any latest XML
            return _find_latest_session_xml(base_dir, stem=None)
        if not candidates:
            return None
        candidates.sort(key=lambda item: item[0], reverse=True)
        return candidates[0][1]
    except Exception:
        return None


def _grpc_save_current_session_xml_with_config(core_cfg: Dict[str, Any], out_dir: str, session_id: str | None = None) -> str | None:
    """Attempt to connect to CORE daemon via gRPC and save the active session XML.

    The save_xml call now executes on the remote CORE host via SSH. The resulting
    XML is downloaded locally into out_dir (and mirrored to outputs/core-sessions).
    """
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    address = f"{cfg.get('host') or CORE_HOST}:{cfg.get('port') or CORE_PORT}"

    # Desired local filename: <scenario-name><timestamp>.xml
    sid_int: int | None = None
    try:
        sid_int = int(session_id) if session_id not in (None, '') else None
    except Exception:
        sid_int = None
    scenario_label: str | None = None
    ts_epoch: float | None = None
    if sid_int is not None:
        try:
            meta_by_sid = _read_remote_session_scenario_meta_bulk(cfg, session_ids=[sid_int], logger=app.logger)
            meta = meta_by_sid.get(sid_int) if isinstance(meta_by_sid, dict) else None
            if isinstance(meta, dict):
                scenario_label = (meta.get('scenario_name') or '').strip() or None
                if meta.get('written_at_epoch') not in (None, ''):
                    try:
                        ts_epoch = float(meta.get('written_at_epoch'))
                    except Exception:
                        ts_epoch = None
        except Exception:
            pass
    if ts_epoch is None and sid_int is not None:
        try:
            store_map = _load_core_sessions_store()
            host = cfg.get('host') or CORE_HOST
            port = int(cfg.get('port') or CORE_PORT)
            ts_epoch = _session_store_updated_at_for_session_id(store_map, sid_int, host=host, port=port)
        except Exception:
            ts_epoch = None
    desired_name = _scenario_timestamped_filename(scenario_label or 'scenario', ts_epoch)
    # Use the scenario stem (not full desired_name) for remote uniqueness prefix.
    try:
        desired_stem = os.path.splitext(secure_filename(desired_name))[0]
    except Exception:
        desired_stem = 'scenario'
    stem = desired_stem.split('-')[0] or desired_stem or 'scenario'
    base_sessions_dir = os.path.join(_outputs_dir(), 'core-sessions')
    for directory in (out_dir, base_sessions_dir):
        try:
            os.makedirs(directory, exist_ok=True)
        except Exception:
            pass
    remote_dir = posixpath.join('/tmp', 'core-topo-gen', 'session_exports')
    remote_name = f"{stem}-{uuid.uuid4().hex}.xml"
    remote_path = posixpath.join(remote_dir, remote_name)
    script = _remote_core_save_xml_script(address, session_id, remote_path)
    command_desc = (
        f"remote ssh {ssh_user}@{ssh_host} -> CoreGrpcClient.save_xml {address} (session={session_id or 'first'})"
    )
    try:
        payload = _run_remote_python_json(
            cfg,
            script,
            logger=app.logger,
            label='core.save_xml',
            command_desc=command_desc,
            timeout=180.0,
        )
    except Exception as exc:
        app.logger.warning('Remote CORE save_xml failed: %s', exc)
        return _find_latest_session_xml(out_dir, stem)
    error_text = payload.get('error')
    if error_text:
        app.logger.warning('Remote CORE save_xml reported error: %s', error_text)
        tb = payload.get('traceback')
        if tb:
            app.logger.debug('save_xml traceback: %s', tb)
        return _find_latest_session_xml(out_dir, stem)
    remote_output = payload.get('output_path') or remote_path
    session_id_remote = payload.get('session_id') or session_id or 'core'
    local_path = os.path.join(out_dir, secure_filename(desired_name))
    try:
        _download_remote_file(cfg, remote_output, local_path)
    except Exception as exc:
        app.logger.warning('Failed downloading remote CORE XML %s: %s', remote_output, exc)
        return _find_latest_session_xml(out_dir, stem)
    finally:
        try:
            _remove_remote_file(cfg, remote_output)
        except Exception:
            pass
    try:
        ok, errs = _validate_core_xml(local_path)
    except Exception as exc:
        app.logger.warning('Validation raised exception for %s: %s', local_path, exc)
        ok = False
        errs = str(exc)
    if not ok:
        app.logger.warning('Downloaded CORE XML failed validation (%s); removing file %s', errs, local_path)
        try:
            os.remove(local_path)
        except Exception:
            pass
        return _find_latest_session_xml(out_dir, stem)
    try:
        _normalize_core_device_types(local_path)
    except Exception as exc:
        app.logger.debug('Device type normalization skipped for %s: %s', local_path, exc)
    try:
        size = os.path.getsize(local_path)
    except Exception:
        size = -1
    app.logger.info('Saved CORE session XML (%s bytes) to %s', size if size >= 0 else '?', local_path)
    try:
        dest = os.path.join(base_sessions_dir, os.path.basename(local_path))
        if os.path.abspath(dest) != os.path.abspath(local_path):
            if os.path.exists(dest):
                root, ext = os.path.splitext(dest)
                counter = 2
                while os.path.exists(f"{root}-{counter}{ext}"):
                    counter += 1
                dest = f"{root}-{counter}{ext}"
            shutil.copy2(local_path, dest)
    except Exception:
        pass
    return local_path

def _grpc_save_current_session_xml(core_cfg_or_host: Any, maybe_port: Any, out_dir: str | None = None, session_id: str | None = None) -> str | None:
    if isinstance(core_cfg_or_host, dict):
        core_cfg = core_cfg_or_host
        target_out_dir = maybe_port
    else:
        core_cfg = {'host': core_cfg_or_host, 'port': maybe_port}
        target_out_dir = out_dir
    if not isinstance(target_out_dir, str) or not target_out_dir:
        raise ValueError('out_dir is required for _grpc_save_current_session_xml')
    return _grpc_save_current_session_xml_with_config(core_cfg, target_out_dir, session_id=session_id)

def _attach_base_upload(payload: Dict[str, Any]):
    """Ensure payload['base_upload'] is present if first scenario has a base filepath referencing an existing file.
    Performs validation to set valid flag. Does nothing if already present.
    """
    try:
        if payload.get('base_upload'):
            return
        scen_list = payload.get('scenarios') or []
        if not scen_list:
            return
        base_path = scen_list[0].get('base', {}).get('filepath') or ''
        if not base_path or not os.path.exists(base_path):
            return
        ok, _errs = _validate_core_xml(base_path)
        display_name = os.path.basename(base_path)
        payload['base_upload'] = {
            'path': base_path,
            'valid': bool(ok),
            'display_name': display_name,
        }
        try:
            scen_list[0].setdefault('base', {})['display_name'] = display_name
        except Exception:
            pass
    except Exception:
        pass


def _parse_scenarios_xml(path):
    data = {"scenarios": []}
    tree = ET.parse(path)
    root = tree.getroot()
    if root.tag != "Scenarios":
        # Fallback: if file is a single ScenarioEditor, wrap
        if root.tag == "ScenarioEditor":
            scen = _parse_scenario_editor(root)
            scen["name"] = os.path.splitext(os.path.basename(path))[0]
            data["scenarios"].append(scen)
            return data
        raise ValueError("Root element must be <Scenarios> or <ScenarioEditor>")
    core_el = root.find('CoreConnection')
    if core_el is not None:
        core_meta = {
            'host': core_el.get('host'),
            'port': core_el.get('port'),
            'ssh_enabled': core_el.get('ssh_enabled'),
            'ssh_host': core_el.get('ssh_host'),
            'ssh_port': core_el.get('ssh_port'),
            'ssh_username': core_el.get('ssh_username'),
        }
        data['core'] = _normalize_core_config(core_meta, include_password=False)
    else:
        data['core'] = _default_core_dict()

    for scen_el in root.findall("Scenario"):
        scen = {"name": scen_el.get("name", "Scenario")}
        # Capture scenario-level density_count attribute if present
        dc_attr = scen_el.get('density_count')
        if dc_attr is not None and dc_attr != '':
            try:
                scen['density_count'] = int(dc_attr)
            except Exception:
                pass
        total_nodes_attr = scen_el.get('scenario_total_nodes')
        if total_nodes_attr not in (None, ''):
            try:
                scen['scenario_total_nodes'] = int(total_nodes_attr)
            except Exception:
                scen['scenario_total_nodes'] = total_nodes_attr
        base_nodes_attr = scen_el.get('base_nodes')
        if base_nodes_attr not in (None, ''):
            try:
                scen['base_nodes'] = int(base_nodes_attr)
            except Exception:
                scen['base_nodes'] = base_nodes_attr
        se = scen_el.find("ScenarioEditor")
        if se is None:
            continue
        scen.update(_parse_scenario_editor(se))
        # If scenario-level density_count was absent but Node Information section provided one, keep existing.
        data["scenarios"].append(scen)
    return data


def _parse_scenario_editor(se):
    scen = {"base": {"filepath": ""}, "sections": {}, "notes": ""}
    # If parent <Scenario> carries scenario-level density_count attribute, capture it.
    try:
        parent = se.getparent()  # lxml style (if ever switched) - fallback below
    except Exception:
        parent = None
    # ElementTree doesn't support getparent; instead inspect tail by traversing immediate children of root in caller.
    # Simplest: look for density_count on any ancestor via attrib access on 'se' .attrib is only local, so rely on caller to have set scen_el attrib earlier.
    # We can recover by walking up using a cheap hack: se._parent if present (cpython impl detail) else ignore.
    try:
        scen_el = getattr(se, 'attrib', None)
    except Exception:
        scen_el = None
    # Instead of fragile parent access, during parsing of root we can read attribute directly from the sibling Scenario element (handled in outer loop); emulate by checking se.get('density_count') first.
    # For backward compatibility, allow density_count on Scenario or Node Information section.
    # We'll set scen['density_count'] here only if Scenario element attribute is available; Node Information section handled later.
    # Outer loop already hands us 'se'; its parent was processed to create scen dict. We'll modify outer function to inject attribute before calling this if needed.
    # Simpler: just check if 'density_count' exists on any ancestor by scanning se.iterfind('..') unsupported; fallback: pass.
    pass
    base = se.find("BaseScenario")
    if base is not None:
        scen["base"]["filepath"] = base.get("filepath", "")
    hitl_el = se.find("HardwareInLoop")
    hitl_info: Dict[str, Any] = {"enabled": False, "interfaces": [], "core": None}
    if hitl_el is not None:
        enabled_raw = (hitl_el.get("enabled") or "").strip().lower()
        hitl_info["enabled"] = enabled_raw in ("1", "true", "yes", "on")
        participant_attr = (
            hitl_el.get('participant_proxmox_url')
            or hitl_el.get('participant_url')
            or hitl_el.get('participant')
        )
        if participant_attr not in (None, ''):
            participant_clean = str(participant_attr).strip()
            if participant_clean:
                hitl_info['participant_proxmox_url'] = participant_clean
        interfaces: List[Dict[str, Any]] = []
        for iface_el in hitl_el.findall("Interface"):
            name = (iface_el.get("name") or "").strip()
            if not name:
                continue
            entry: Dict[str, Any] = {"name": name}
            alias = (iface_el.get("alias") or iface_el.get("display") or iface_el.get("description") or "").strip()
            if alias:
                entry["alias"] = alias
            mac_attr = iface_el.get("mac")
            if mac_attr:
                entry["mac"] = mac_attr
            attachment_attr = iface_el.get("attachment") or iface_el.get("attach")
            entry["attachment"] = _normalize_hitl_attachment(attachment_attr)
            ipv4_attr = iface_el.get("ipv4") or iface_el.get("ipv4_addresses")
            if ipv4_attr:
                entry["ipv4"] = [p.strip() for p in ipv4_attr.split(',') if p.strip()]
            ipv6_attr = iface_el.get("ipv6") or iface_el.get("ipv6_addresses")
            if ipv6_attr:
                entry["ipv6"] = [p.strip() for p in ipv6_attr.split(',') if p.strip()]
            interfaces.append(entry)
        hitl_info["interfaces"] = interfaces
        core_el = hitl_el.find("CoreConnection")
        if core_el is not None:
            core_cfg = _extract_optional_core_config(dict(core_el.attrib), include_password=False)
            if core_cfg:
                hitl_info["core"] = core_cfg
    scen["hitl"] = hitl_info
    # Sections
    for sec in se.findall("section"):
        name = sec.get("name", "")
        if not name:
            continue
        entry = {"density": None, "total_nodes": None, "items": []}
        if name == "Node Information":
            tn = sec.get("total_nodes")
            if tn is not None:
                try:
                    entry["total_nodes"] = int(tn)
                except Exception:
                    entry["total_nodes"] = 1
        else:
            dens = sec.get("density")
            entry["density"] = float(dens) if dens is not None else 0.5
        for item in sec.findall("item"):
            d = {
                "selected": item.get("selected", "Random"),
                "factor": float(item.get("factor", "1.0")),
            }
            if name == "Routing":
                em = item.get('r2r_mode')
                if em is not None:
                    # Store under new key r2r_mode; keep legacy key for UI components still referencing it
                    d['r2r_mode'] = em
                ev = item.get('r2r_edges') or item.get('edges')
                if ev is not None and ev.strip() != '':
                    try:
                        d['r2r_edges'] = int(ev)
                    except Exception:
                        pass
                r2s_m = item.get('r2s_mode')
                if r2s_m is not None:
                    d['r2s_mode'] = r2s_m
                r2s_ev = item.get('r2s_edges')
                if r2s_ev is not None and r2s_ev.strip() != '':
                    try:
                        d['r2s_edges'] = int(r2s_ev)
                    except Exception:
                        pass
                # New per-item hosts-per-switch bounds
                hmin_attr = item.get('r2s_hosts_min')
                hmax_attr = item.get('r2s_hosts_max')
                try:
                    if hmin_attr is not None and hmin_attr.strip() != '':
                        d['r2s_hosts_min'] = int(hmin_attr)
                except Exception:
                    pass
                try:
                    if hmax_attr is not None and hmax_attr.strip() != '':
                        d['r2s_hosts_max'] = int(hmax_attr)
                except Exception:
                    pass
            if name == "Events":
                d["script_path"] = item.get("script_path", "")
            if name == "Traffic":
                d.update({
                    "pattern": item.get("pattern", "continuous"),
                    "rate_kbps": float(item.get("rate_kbps", "64.0")),
                    "period_s": float(item.get("period_s", "1.0")),
                    "jitter_pct": float(item.get("jitter_pct", "10.0")),
                    "content_type": (item.get("content_type") or item.get("content") or "Random"),
                })
            if name == "Vulnerabilities":
                # Extra attributes for Vulnerabilities section
                sel = (d.get("selected") or "").strip()
                # UI historically uses "Category" while XML schema uses "Type/Vector".
                # Normalize to UI label "Category" so round-trips are stable.
                if sel in ("Type/Vector", "Category"):
                    d["selected"] = "Category"
                    d["v_type"] = item.get("v_type", "Random")
                    d["v_vector"] = item.get("v_vector", "Random")
                elif sel == "Specific":
                    d["v_name"] = item.get("v_name", "")
                    d["v_path"] = item.get("v_path", "")
                    # Default count to 1 if missing/invalid
                    try:
                        d["v_count"] = int(item.get("v_count", "1"))
                    except Exception:
                        d["v_count"] = 1
                # Persist metric choice if present (Weight or Count)
                vm = item.get("v_metric")
                if vm:
                    d["v_metric"] = vm
            # Generic metric/count for all sections (including Vulnerabilities)
            try:
                vm_generic = item.get("v_metric")
                if vm_generic and vm_generic in ("Weight", "Count"):
                    d["v_metric"] = vm_generic
                vc_generic = item.get("v_count")
                if vc_generic is not None:
                    try:
                        d["v_count"] = int(vc_generic)
                    except Exception:
                        d["v_count"] = 1
            except Exception:
                pass
            entry["items"].append(d)
        scen["sections"][name] = entry
        # Capture scenario-level density_count if present on Scenario element once
        if 'density_count' not in scen and se is not None:
            # Attempt to access parent <Scenario> by scanning for attribute on sec's ancestors is not directly supported.
            # Instead, rely on convention: during writing we place density_count on <Scenario>. So parse root manually here.
            try:
                # Walk up by brute force: find the nearest ancestor named 'Scenario'
                # We don't have parent links; reconstruct by searching from current element root.
                root = sec
                while getattr(root, 'tag', None) and root.tag != 'Scenario':
                    # ElementTree lacks parent pointer; break to avoid infinite loop
                    break
            except Exception:
                root = None
        # Fallback: if Node Information section carries density_count/base_nodes and scenario-level missing, propagate to scen.
        if name == 'Node Information' and 'density_count' not in scen:
            dc_attr = sec.get('density_count') or sec.get('base_nodes') or sec.get('total_nodes')
            if dc_attr:
                try:
                    scen['density_count'] = int(dc_attr)
                except Exception:
                    pass
    # Notes
    notes_sec = None
    for sec in se.findall("section"):
        if sec.get("name") == "Notes":
            notes_sec = sec; break
    if notes_sec is not None:
        notes_el = notes_sec.find("notes")
        if notes_el is not None and notes_el.text:
            scen["notes"] = notes_el.text
    return scen


def _build_scenarios_xml(data_dict: dict) -> ET.ElementTree:
    root = ET.Element("Scenarios")
    core_cfg = _normalize_core_config(data_dict.get('core'), include_password=False)
    core_el = ET.SubElement(root, 'CoreConnection')
    core_el.set('host', str(core_cfg.get('host') or ''))
    core_el.set('port', str(core_cfg.get('port') or ''))
    core_el.set('ssh_enabled', 'true' if core_cfg.get('ssh_enabled') else 'false')
    core_el.set('ssh_host', str(core_cfg.get('ssh_host') or core_cfg.get('host') or ''))
    core_el.set('ssh_port', str(core_cfg.get('ssh_port') or ''))
    core_el.set('ssh_username', str(core_cfg.get('ssh_username') or ''))
    for scen in data_dict.get("scenarios", []):
        scen_el = ET.SubElement(root, "Scenario")
        scen_el.set("name", scen.get("name", "Scenario"))
        # Persist scenario-level density_count (Count for Density) so parser priority can pick it up on reload.
        try:
            if 'density_count' in scen and scen.get('density_count') is not None:
                scen_el.set('density_count', str(int(scen.get('density_count'))))
        except Exception:
            pass
        se = ET.SubElement(scen_el, "ScenarioEditor")
        base = ET.SubElement(se, "BaseScenario")
        base.set("filepath", scen.get("base", {}).get("filepath", ""))

        hitl = scen.get("hitl") or {}
        raw_ifaces = hitl.get("interfaces") if isinstance(hitl, dict) else None
        normalized_ifaces: List[Dict[str, Any]] = []
        if isinstance(raw_ifaces, list):
            for entry in raw_ifaces:
                if not entry:
                    continue
                if isinstance(entry, str):
                    normalized_ifaces.append({"name": entry})
                    continue
                if not isinstance(entry, dict):
                    continue
                name = entry.get("name") or entry.get("interface") or entry.get("iface")
                if not name:
                    continue
                clean: Dict[str, Any] = {"name": str(name)}
                for key in ("alias", "display", "description"):
                    val = entry.get(key)
                    if isinstance(val, str) and val.strip():
                        clean["alias"] = val.strip()
                        break
                mac_val = entry.get("mac")
                if isinstance(mac_val, str) and mac_val.strip():
                    clean["mac"] = mac_val.strip()
                clean["attachment"] = _normalize_hitl_attachment(entry.get("attachment"))
                for attr_key in ("ipv4", "ipv6"):
                    val = entry.get(attr_key)
                    if isinstance(val, str):
                        items = [p.strip() for p in val.split(',') if p.strip()]
                        if items:
                            clean[attr_key] = items
                    elif isinstance(val, (list, tuple, set)):
                        items = [str(p).strip() for p in val if str(p).strip()]
                        if items:
                            clean[attr_key] = items
                normalized_ifaces.append(clean)
        for iface in normalized_ifaces:
            if "attachment" not in iface:
                iface["attachment"] = _DEFAULT_HITL_ATTACHMENT

        participant_url_raw = hitl.get("participant_proxmox_url") if isinstance(hitl, dict) else None
        if participant_url_raw not in (None, ''):
            try:
                participant_url = str(participant_url_raw).strip()
            except Exception:
                participant_url = ''
        else:
            participant_url = ''

        if hitl and (hitl.get("enabled") or normalized_ifaces or hitl.get("core") or participant_url):
            hitl_el = ET.SubElement(se, "HardwareInLoop")
            hitl_el.set("enabled", "true" if hitl.get("enabled") else "false")
            if participant_url:
                hitl_el.set('participant_proxmox_url', participant_url)
            hitl_core_cfg = _extract_optional_core_config(hitl.get("core"), include_password=False)
            if hitl_core_cfg:
                hitl_core_el = ET.SubElement(hitl_el, "CoreConnection")
                hitl_core_el.set("host", str(hitl_core_cfg.get("host") or ""))
                hitl_core_el.set("port", str(hitl_core_cfg.get("port") or ""))
                hitl_core_el.set("ssh_enabled", "true" if hitl_core_cfg.get("ssh_enabled") else "false")
                hitl_core_el.set("ssh_host", str(hitl_core_cfg.get("ssh_host") or hitl_core_cfg.get("host") or ""))
                hitl_core_el.set("ssh_port", str(hitl_core_cfg.get("ssh_port") or ""))
                hitl_core_el.set("ssh_username", str(hitl_core_cfg.get("ssh_username") or ""))
                for extra_key, extra_val in hitl_core_cfg.items():
                    if extra_key in {"host", "port", "ssh_enabled", "ssh_host", "ssh_port", "ssh_username", "ssh_password"}:
                        continue
                    try:
                        hitl_core_el.set(str(extra_key), str(extra_val))
                    except Exception:
                        continue
            for iface in normalized_ifaces:
                name = iface.get("name")
                if not name:
                    continue
                iface_el = ET.SubElement(hitl_el, "Interface")
                iface_el.set("name", str(name))
                alias = iface.get("alias")
                if alias:
                    iface_el.set("alias", str(alias))
                if iface.get("mac"):
                    iface_el.set("mac", str(iface["mac"]))
                attachment = _normalize_hitl_attachment(iface.get("attachment"))
                if attachment:
                    iface_el.set("attachment", attachment)
                ipv4_vals = iface.get("ipv4")
                if isinstance(ipv4_vals, (list, tuple, set)):
                    joined = ",".join(str(p) for p in ipv4_vals if p)
                    if joined:
                        iface_el.set("ipv4", joined)
                ipv6_vals = iface.get("ipv6")
                if isinstance(ipv6_vals, (list, tuple, set)):
                    joined6 = ",".join(str(p) for p in ipv6_vals if p)
                    if joined6:
                        iface_el.set("ipv6", joined6)

        order = [
            "Node Information", "Routing", "Services", "Traffic",
            "Events", "Vulnerabilities", "Segmentation", "Notes"
        ]
        combined_host_pool: int | None = None
        scenario_host_additive = 0
        scenario_routing_total = 0
        scenario_vuln_total = 0

        for name in order:
            if name == "Notes":
                sec_el = ET.SubElement(se, "section", name="Notes")
                ne = ET.SubElement(sec_el, "notes")
                ne.text = scen.get("notes", "") or ""
                continue
            sec = scen.get("sections", {}).get(name)
            if not sec:
                continue
            sec_el = ET.SubElement(se, "section", name=name)
            items_list = sec.get("items", []) or []
            weight_rows = [it for it in items_list if (it.get('v_metric') or (it.get('selected')=='Specific' and name=='Vulnerabilities') or 'Weight') == 'Weight']
            count_rows = [it for it in items_list if (it.get('v_metric') == 'Count') or (name == 'Vulnerabilities' and it.get('selected') == 'Specific')]
            weight_sum = sum(float(it.get('factor', 0) or 0) for it in weight_rows) if weight_rows else 0.0

            if name == "Node Information":
                # Determine if an explicit density_count was provided (scenario-level or legacy section field).
                explicit_density_raw = None
                scen_level_dc = scen.get('density_count')
                if scen_level_dc is not None:
                    explicit_density_raw = scen_level_dc
                else:
                    for legacy_key in ('density_count', 'total_nodes', 'base_nodes'):
                        if sec.get(legacy_key) not in (None, ""):
                            explicit_density_raw = sec.get(legacy_key)
                            break
                density_count: int | None = None
                if explicit_density_raw is not None:
                    try:
                        density_count = max(0, int(explicit_density_raw))
                    except Exception:
                        density_count = 0
                # additive Count rows always additive even if base omitted
                additive_nodes = sum(int(it.get('v_count') or 0) for it in count_rows)
                # For derived counts we only have a combined host pool if explicit density_count provided
                combined_nodes = (density_count or 0) + additive_nodes
                norm_sum = 0.0
                if weight_rows:
                    raw_sum = sum(float(it.get('factor') or 0) for it in weight_rows)
                    if raw_sum > 0:
                        for it in weight_rows:
                            try:
                                it['factor'] = float(it.get('factor') or 0) / raw_sum
                            except Exception:
                                it['factor'] = 0.0
                        norm_sum = 1.0
                    else:
                        weight_rows[0]['factor'] = 1.0
                        for it in weight_rows[1:]:
                            it['factor'] = 0.0
                        norm_sum = 1.0
                # Only persist base-related fields if an explicit density_count was supplied. Otherwise omit so parser can apply default.
                if density_count is not None:
                    sec_el.set("density_count", str(density_count))
                    sec_el.set("base_nodes", str(density_count))
                sec_el.set("additive_nodes", str(additive_nodes))
                if density_count is not None:
                    sec_el.set("combined_nodes", str(combined_nodes))
                sec_el.set("weight_rows", str(len(weight_rows)))
                sec_el.set("count_rows", str(len(count_rows)))
                sec_el.set("weight_sum", f"{weight_sum:.3f}")
                sec_el.set("normalized_weight_sum", f"{norm_sum:.3f}")
                combined_host_pool = combined_nodes if density_count is not None else None
                scenario_host_additive += combined_nodes if density_count is not None else additive_nodes
            else:
                dens = sec.get("density")
                if dens is not None:
                    try:
                        sec_el.set("density", f"{float(dens):.3f}")
                    except Exception:
                        sec_el.set("density", str(dens))
                if name in ("Routing", "Vulnerabilities"):
                    base_pool = combined_host_pool if isinstance(combined_host_pool, int) else None
                    explicit = sum(int(it.get('v_count') or 0) for it in count_rows)
                    derived = 0
                    try:
                        dens_val = float(dens or 0)
                    except Exception:
                        dens_val = 0.0
                    if weight_rows and base_pool and base_pool > 0:
                        if name == 'Routing':
                            if dens_val >= 1:
                                derived = int(round(dens_val))
                            elif dens_val > 0:
                                derived = int(round(base_pool * dens_val))
                        else:  # Vulnerabilities
                            if dens_val > 0:
                                dens_clip = min(1.0, dens_val)
                                derived = int(round(base_pool * dens_clip))
                    total_planned = explicit + derived
                    sec_el.set("explicit_count", str(explicit))
                    sec_el.set("derived_count", str(derived))
                    sec_el.set("total_planned", str(total_planned))
                    sec_el.set("weight_rows", str(len(weight_rows)))
                    sec_el.set("count_rows", str(len(count_rows)))
                    sec_el.set("weight_sum", f"{weight_sum:.3f}")
                    if name == 'Routing':
                        scenario_routing_total += total_planned
                    else:
                        scenario_vuln_total += total_planned
                elif name in ("Services", "Traffic", "Segmentation"):
                    explicit = sum(int(it.get('v_count') or 0) for it in count_rows)
                    sec_el.set("explicit_count", str(explicit))
                    sec_el.set("weight_rows", str(len(weight_rows)))
                    sec_el.set("count_rows", str(len(count_rows)))
                    sec_el.set("weight_sum", f"{weight_sum:.3f}")

            for item in items_list:
                it = ET.SubElement(sec_el, "item")
                sel_for_xml = str(item.get('selected', 'Random'))
                # UI label "Category" maps to schema label "Type/Vector".
                if name == 'Vulnerabilities' and sel_for_xml == 'Category':
                    sel_for_xml = 'Type/Vector'
                it.set("selected", sel_for_xml)
                try:
                    it.set("factor", f"{float(item.get('factor', 1.0)):.3f}")
                except Exception:
                    it.set("factor", "0.000")
                if name == 'Routing':
                    em = (item.get('r2r_mode') or '').strip()
                    r2s_mode = (item.get('r2s_mode') or '').strip()
                    if em:
                        it.set('r2r_mode', em)
                    if r2s_mode:
                        it.set('r2s_mode', r2s_mode)
                    # Persist edge budget hints when provided (including Uniform/NonUniform / aggregate modes)
                    try:
                        ev_raw = item.get('r2r_edges') or item.get('edges')
                        if em == 'Exact' and ev_raw is not None and str(ev_raw).strip() != '':
                            ev = int(ev_raw)
                            if ev > 0:  # only meaningful positive degrees
                                it.set('r2r_edges', str(ev))
                    except Exception:
                        pass
                    try:
                        r2s_raw = item.get('r2s_edges')
                        if r2s_raw is not None and str(r2s_raw).strip() != '':
                            ev2 = int(r2s_raw)
                            if ev2 >= 0:
                                it.set('r2s_edges', str(ev2))
                    except Exception:
                        pass
                    # Persist per-item host grouping bounds if provided (non-empty and >=0)
                    try:
                        hmin_raw = item.get('r2s_hosts_min')
                        if hmin_raw not in (None, ''):
                            hmin_val = int(hmin_raw)
                            if hmin_val >= 0:
                                it.set('r2s_hosts_min', str(hmin_val))
                    except Exception:
                        pass
                    try:
                        hmax_raw = item.get('r2s_hosts_max')
                        if hmax_raw not in (None, ''):
                            hmax_val = int(hmax_raw)
                            if hmax_val >= 0:
                                it.set('r2s_hosts_max', str(hmax_val))
                    except Exception:
                        pass
                    # If still absent, write explicit defaults (UI defaults 1 and 4) for deterministic round-trip
                    if 'r2s_hosts_min' not in it.attrib:
                        it.set('r2s_hosts_min', '1')
                    if 'r2s_hosts_max' not in it.attrib:
                        it.set('r2s_hosts_max', '4')
                if name == 'Events':
                    sp = item.get('script_path') or ''
                    if sp:
                        it.set('script_path', sp)
                if name == 'Traffic':
                    it.set('pattern', str(item.get('pattern', 'continuous')))
                    it.set('rate_kbps', f"{float(item.get('rate_kbps', 64.0)):.1f}")
                    it.set('period_s', f"{float(item.get('period_s', 1.0)):.1f}")
                    it.set('jitter_pct', f"{float(item.get('jitter_pct', 10.0)):.1f}")
                    ct = (item.get('content_type') or item.get('content') or '').strip()
                    if ct:
                        it.set('content_type', ct)
                if name == 'Vulnerabilities':
                    sel = sel_for_xml
                    if sel in ('Type/Vector', 'Category'):
                        vt = item.get('v_type')
                        vv = item.get('v_vector')
                        if vt:
                            it.set('v_type', str(vt))
                        if vv:
                            it.set('v_vector', str(vv))
                    elif sel == 'Specific':
                        vn = item.get('v_name')
                        vp = item.get('v_path')
                        if vn:
                            it.set('v_name', str(vn))
                        if vp:
                            it.set('v_path', str(vp))
                vm_any = item.get('v_metric')
                if vm_any:
                    it.set('v_metric', str(vm_any))
                if (item.get('v_metric') == 'Count') or (name == 'Vulnerabilities' and str(item.get('selected', '')) == 'Specific'):
                    vc_any = item.get('v_count')
                    try:
                        if vc_any is not None:
                            it.set('v_count', str(int(vc_any)))
                    except Exception:
                        pass

        # Final scenario-level aggregate
        try:
            total_nodes = scenario_host_additive + scenario_routing_total + scenario_vuln_total
            scen_el.set('scenario_total_nodes', str(total_nodes))
            scen_el.set('base_nodes', '0')
        except Exception:
            pass

    return ET.ElementTree(root)


def _validate_core_xml(xml_path: str):
    """Validate the scenario XML against the CORE XML XSD. Returns (ok, errors_text)."""
    try:
        # Derive project root relative to this file (../) then the validation directory
        here = os.path.abspath(os.path.dirname(__file__))
        repo_root = os.path.abspath(os.path.join(here, '..'))
        xsd_path = os.path.join(repo_root, 'validation', 'core-xml-syntax', 'corexml_codebased.xsd')
        # Fallback: if not found, try relative to current working directory (for unusual run contexts)
        if not os.path.exists(xsd_path):
            alt = os.path.abspath(os.path.join(os.getcwd(), 'validation', 'core-xml-syntax', 'corexml_codebased.xsd'))
            if os.path.exists(alt):
                xsd_path = alt
        if not os.path.exists(xsd_path):
            return False, f"Schema not found: {xsd_path}"
        with open(xsd_path, 'rb') as f:
            schema_doc = LET.parse(f)
        schema = LET.XMLSchema(schema_doc)
        # Read original XML; if it contains any <container> elements (session export artifacts),
        # strip them prior to validation so that user-provided or auto-exported session XML can
        # still be validated against the scenario schema. This addresses UI errors like:
        #   Element 'container': This element is not expected.
        # We purposefully do NOT mutate the source file on disk; sanitization is in-memory.
        try:
            raw_tree = LET.parse(xml_path)
            root = raw_tree.getroot()
            # Collect and remove any elements whose local-name is 'container'
            containers = root.xpath('.//*[local-name()="container"]')
            if containers:
                for el in containers:
                    parent = el.getparent()
                    if parent is not None:
                        parent.remove(el)
                # Validate sanitized tree
                try:
                    schema.assertValid(root)
                    return True, ''
                except LET.DocumentInvalid as e:
                    # Fall through to structured error collection below
                    err_log = e.error_log
                    lines = [f"{er.level_name} L{er.line}:C{er.column} - {er.message}" for er in err_log]
                    return False, "\n".join(lines) or str(e)
            else:
                # No <container>; validate normally using parser bound to schema for speed
                parser = LET.XMLParser(schema=schema)
                LET.parse(xml_path, parser)
                return True, ''
        except LET.XMLSyntaxError as e:  # low-level parse error before schema phase
            lines = []
            for err in e.error_log:
                lines.append(f"{err.level_name} L{err.line}:C{err.column} - {err.message}")
            return False, "\n".join(lines) or str(e)
    except LET.XMLSyntaxError as e:
        lines = []
        for err in e.error_log:
            lines.append(f"{err.level_name} L{err.line}:C{err.column} - {err.message}")
        return False, "\n".join(lines) or str(e)
    except Exception as e:
        return False, str(e)


def _analyze_core_xml(xml_path: str) -> Dict[str, Any]:
    """Extract a topology summary from a CORE session/scenario XML."""
    info: Dict[str, Any] = {}
    try:
        tree = LET.parse(xml_path)
        root = tree.getroot()

        def attrs(el, *names):
            return {n: el.get(n) for n in names if el.get(n) is not None}

        def local(tag: str) -> str:
            if not tag:
                return ''
            if '}' in tag:
                return tag.split('}', 1)[1]
            return tag

        def iter_by_local(el, lname: str):
            lname = lname.lower()
            for e in el.iter():
                if local(getattr(e, 'tag', '')).lower() == lname:
                    yield e

        # Combine device/node representations (session exports sometimes use <node>)
        candidates = list(iter_by_local(root, 'device')) + list(iter_by_local(root, 'node'))
        devices: list[Any] = []
        seen_ids: set[str] = set()
        for cand in candidates:
            ident = cand.get('id') or cand.get('name')
            key = str(ident).strip() if ident is not None else ''
            if key and key in seen_ids:
                continue
            if key:
                seen_ids.add(key)
            devices.append(cand)

        networks = list(iter_by_local(root, 'network'))
        links = list(iter_by_local(root, 'link'))
        services = list(iter_by_local(root, 'service'))

        routing_edge_policies: list[dict] = []
        try:
            for sec in root.findall('.//section'):
                if (sec.get('name') or '').strip() != 'Routing':
                    continue
                for item in sec.findall('./item'):
                    r2r_edges = item.get('r2r_edges') or item.get('edges')
                    r2s_edges = item.get('r2s_edges')
                    if any([item.get('r2r_mode'), r2r_edges, item.get('r2s_mode'), r2s_edges]):
                        routing_edge_policies.append({
                            'r2r_mode': item.get('r2r_mode') or '',
                            'r2r_edges': int(r2r_edges) if (r2r_edges and r2r_edges.isdigit()) else None,
                            'r2s_mode': item.get('r2s_mode') or '',
                            'r2s_edges': int(r2s_edges) if (r2s_edges and r2s_edges.isdigit()) else None,
                            'protocol': item.get('selected') or '',
                        })
        except Exception:
            routing_edge_policies = []

        interface_store: Dict[str, Dict[str, Dict[str, Any]]] = defaultdict(dict)

        def record_interface(node_ref: Any, iface_el: Any) -> None:
            node_key = str(node_ref or '').strip()
            if not node_key or iface_el is None:
                return
            try:
                attrs_iface = dict(getattr(iface_el, 'attrib', {}) or {})
            except Exception:
                attrs_iface = {}
            name_raw = (attrs_iface.get('name') or attrs_iface.get('id') or '').strip()
            if not name_raw:
                name_el = iface_el.find('./name') if hasattr(iface_el, 'find') else None
                if name_el is not None and getattr(name_el, 'text', None):
                    name_raw = name_el.text.strip()
            mac = (attrs_iface.get('mac') or '').strip()
            if not mac and hasattr(iface_el, 'find'):
                mac_el = iface_el.find('./mac')
                if mac_el is not None and getattr(mac_el, 'text', None):
                    mac = mac_el.text.strip()
            ip4 = (attrs_iface.get('ip4') or attrs_iface.get('ipv4') or '').strip()
            ip4_mask = (attrs_iface.get('ip4_mask') or attrs_iface.get('ipv4_mask') or '').strip()
            ip6 = (attrs_iface.get('ip6') or attrs_iface.get('ipv6') or '').strip()
            ip6_mask = (attrs_iface.get('ip6_mask') or attrs_iface.get('ipv6_mask') or '').strip()
            try:
                for addr in iface_el.findall('.//addr'):
                    addr_type = (addr.get('type') or addr.get('family') or '').lower()
                    addr_val = (addr.get('address') or addr.get('ip') or (addr.text or '')).strip()
                    mask_val = (addr.get('mask') or addr.get('prefix') or addr.get('netmask') or '').strip()
                    if not addr_val:
                        continue
                    if '6' in addr_type:
                        if not ip6:
                            ip6 = addr_val
                        if not ip6_mask:
                            ip6_mask = mask_val
                    else:
                        if not ip4:
                            ip4 = addr_val
                        if not ip4_mask:
                            ip4_mask = mask_val
                for addr in iface_el.findall('.//address'):
                    addr_val = (addr.get('value') or addr.get('address') or (addr.text or '')).strip()
                    mask_val = (addr.get('mask') or addr.get('prefix') or '').strip()
                    addr_type = (addr.get('type') or '').lower()
                    if not addr_val:
                        continue
                    if '6' in addr_type:
                        if not ip6:
                            ip6 = addr_val
                        if not ip6_mask:
                            ip6_mask = mask_val
                    else:
                        if not ip4:
                            ip4 = addr_val
                        if not ip4_mask:
                            ip4_mask = mask_val
            except Exception:
                pass
            if not any([name_raw, mac, ip4, ip6]):
                return
            stable_key = '|'.join([
                name_raw.lower(),
                ip4,
                ip6,
            ])
            existing = interface_store[node_key].get(stable_key)
            if existing:
                if mac and not existing.get('mac'):
                    existing['mac'] = mac
                if ip4_mask and not existing.get('ipv4_mask'):
                    existing['ipv4_mask'] = ip4_mask
                if ip6_mask and not existing.get('ipv6_mask'):
                    existing['ipv6_mask'] = ip6_mask
                if name_raw and not existing.get('name'):
                    existing['name'] = name_raw
                return
            interface_store[node_key][stable_key] = {
                'name': name_raw or None,
                'mac': mac or None,
                'ipv4': ip4 or None,
                'ipv4_mask': ip4_mask or None,
                'ipv6': ip6 or None,
                'ipv6_mask': ip6_mask or None,
            }

        id_to_name: Dict[str, str] = {}
        id_to_type: Dict[str, str] = {}
        id_to_services: Dict[str, list] = {}

        def coerce_device_id(raw_id: Any, fallback_index: int) -> str:
            value = str(raw_id).strip() if raw_id is not None else ''
            if value:
                return value
            return f"device_{fallback_index}"

        for idx, dev in enumerate(devices, start=1):
            did = coerce_device_id(dev.get('id') or dev.get('name'), idx)
            name_val = (dev.get('name') or '').strip()
            if not name_val and hasattr(dev, 'find'):
                name_el = dev.find('./name')
                if name_el is not None and getattr(name_el, 'text', None):
                    name_val = name_el.text.strip()
            type_val = (dev.get('type') or '').strip()
            if not type_val and hasattr(dev, 'find'):
                type_el = dev.find('./type') or dev.find('./model') or dev.find('./icon')
                if type_el is not None and getattr(type_el, 'text', None):
                    type_val = type_el.text.strip()
            id_to_name[did] = name_val or did
            id_to_type[did] = type_val or ''

            services_found: set[str] = set()
            try:
                for svc in dev.findall('./services/service'):
                    nm = (svc.get('name') or (svc.text or '')).strip()
                    if nm:
                        services_found.add(nm)
                for svc in dev.findall('./service'):
                    nm = (svc.get('name') or (svc.text or '')).strip()
                    if nm:
                        services_found.add(nm)
            except Exception:
                pass
            id_to_services[did] = sorted(services_found)

            try:
                for iface in dev.findall('.//interface'):
                    record_interface(did, iface)
                for iface in dev.findall('.//iface'):
                    record_interface(did, iface)
            except Exception:
                pass

        adj: Dict[str, set[str]] = defaultdict(set)

        def normalize_ref(value: Any) -> str:
            return str(value).strip() if value is not None else ''

        for link in links:
            n1 = normalize_ref(link.get('node1') or link.get('node1_id'))
            n2 = normalize_ref(link.get('node2') or link.get('node2_id'))
            if not n1 or not n2:
                try:
                    if not n1 and hasattr(link, 'find'):
                        iface1 = link.find('.//iface1')
                        if iface1 is not None:
                            n1 = normalize_ref(iface1.get('node') or iface1.get('device') or iface1.get('node_id'))
                    if not n2 and hasattr(link, 'find'):
                        iface2 = link.find('.//iface2')
                        if iface2 is not None:
                            n2 = normalize_ref(iface2.get('node') or iface2.get('device') or iface2.get('node_id'))
                except Exception:
                    pass
            if n1 and n2:
                adj[n1].add(n2)
                adj[n2].add(n1)
            try:
                for child in list(link):
                    tag = local(getattr(child, 'tag', '')).lower()
                    target = None
                    if tag in ('iface1', 'interface1'):
                        target = n1
                    elif tag in ('iface2', 'interface2'):
                        target = n2
                    elif tag in ('iface', 'interface'):
                        target = normalize_ref(child.get('node') or child.get('node_id') or child.get('device')) or n2
                    if target:
                        record_interface(target, child)
            except Exception:
                continue

        nodes: list[dict] = []
        for idx, dev in enumerate(devices, start=1):
            did = coerce_device_id(dev.get('id') or dev.get('name'), idx)
            raw_ifaces = interface_store.get(did, {})
            iface_entries = []
            for entry in raw_ifaces.values():
                cleaned = {k: v for k, v in entry.items() if v not in (None, '')}
                if cleaned:
                    iface_entries.append(cleaned)
            iface_entries.sort(key=lambda e: ((e.get('name') or '').lower(), e.get('ipv4') or '', e.get('ipv6') or ''))
            nodes.append({
                'id': did,
                'name': id_to_name.get(did, did),
                'type': id_to_type.get(did, ''),
                'services': id_to_services.get(did, []),
                'linked_nodes': [],
                'interfaces': iface_entries,
            })

        switches = [n for n in nodes if (n.get('type') or '').lower() == 'switch']
        extra_switch_nodes: list[dict] = []
        extra_hitl_network_nodes: list[dict] = []
        hitl_network_markers = ('rj45', 'rj-45', 'hitl', 'tap', 'bridge', 'ethernet', 'physical')
        try:
            for net in networks:
                ntype = (net.get('type') or '').lower()
                is_switch_candidate = 'switch' in ntype
                is_hitl_candidate = any(marker in ntype for marker in hitl_network_markers)
                if not is_switch_candidate and not is_hitl_candidate:
                    continue
                sw_id = normalize_ref(net.get('id') or net.get('name'))
                if not sw_id:
                    continue
                sw_name = (net.get('name') or sw_id).strip() or sw_id
                if is_switch_candidate and any(sw_id == sw.get('id') or sw_name == sw.get('name') for sw in switches):
                    continue
                if is_hitl_candidate and any(sw_id == n.get('id') or sw_name == n.get('name') for n in nodes):
                    continue
                raw_ifaces = interface_store.get(sw_id, {})
                iface_entries = []
                for entry in raw_ifaces.values():
                    cleaned = {k: v for k, v in entry.items() if v not in (None, '')}
                    if cleaned:
                        iface_entries.append(cleaned)
                iface_entries.sort(key=lambda e: ((e.get('name') or '').lower(), e.get('ipv4') or '', e.get('ipv6') or ''))
                if is_switch_candidate:
                    extra_switch = {
                        'id': sw_id,
                        'name': sw_name,
                        'type': 'switch',
                        'services': [],
                        'linked_nodes': [],
                        'interfaces': iface_entries,
                    }
                    switches.append(extra_switch)
                    extra_switch_nodes.append(extra_switch)
                    id_to_name.setdefault(sw_id, sw_name)
                    id_to_type.setdefault(sw_id, 'switch')
                elif is_hitl_candidate:
                    extra_hitl = {
                        'id': sw_id,
                        'name': sw_name,
                        'type': net.get('type') or 'rj45',
                        'services': [],
                        'linked_nodes': [],
                        'interfaces': iface_entries,
                    }
                    extra_hitl_network_nodes.append(extra_hitl)
                    id_to_name.setdefault(sw_id, sw_name)
                    id_to_type.setdefault(sw_id, extra_hitl['type'].lower())
        except Exception:
            pass

        valid_ids: set[str] = {n['id'] for n in nodes}
        valid_ids.update(sw['id'] for sw in extra_switch_nodes)
        valid_ids.update(hitl['id'] for hitl in extra_hitl_network_nodes)
        adj_clean: Dict[str, set[str]] = {}
        for nid, neighbors in adj.items():
            if nid not in valid_ids:
                continue
            adj_clean[nid] = {nbr for nbr in neighbors if nbr in valid_ids}
        for sw in extra_switch_nodes:
            adj_clean.setdefault(sw['id'], set())
        for hitl in extra_hitl_network_nodes:
            adj_clean.setdefault(hitl['id'], set())

        def _prune_neighbors(node_id: str, node_type: str) -> None:
            if node_type in ('router', 'switch'):
                return
            current_neighbors = sorted(adj_clean.get(node_id, set()), key=lambda vid: id_to_name.get(vid, vid).lower())
            routers = [vid for vid in current_neighbors if (id_to_type.get(vid, '').lower() == 'router')]
            switches_local = [vid for vid in current_neighbors if (id_to_type.get(vid, '').lower() == 'switch')]

            def _trim_group(group: list[str]) -> None:
                if len(group) <= 1:
                    return
                keep = group[0]
                for extra in group[1:]:
                    adj_clean.get(node_id, set()).discard(extra)
                    if extra in adj_clean:
                        adj_clean[extra].discard(node_id)

            _trim_group(routers)
            _trim_group(switches_local)

        for node in nodes:
            _prune_neighbors(node['id'], (node.get('type') or '').lower())
        for sw in extra_switch_nodes:
            _prune_neighbors(sw['id'], (sw.get('type') or '').lower())
        for hitl in extra_hitl_network_nodes:
            _prune_neighbors(hitl['id'], (hitl.get('type') or '').lower())

        def _neighbor_names(node_id: str) -> list[str]:
            neighbors = sorted(adj_clean.get(node_id, set()), key=lambda vid: id_to_name.get(vid, vid).lower())
            return [id_to_name.get(vid, vid) for vid in neighbors]

        filtered_nodes: list[dict] = []
        important_types = {'router', 'switch', 'rj45', 'rj-45', 'tap', 'bridge'}
        for node in nodes:
            nid = node['id']
            linked = adj_clean.get(nid, set())
            node_type = (node.get('type') or '').lower()
            if linked or (node.get('interfaces') and len(node.get('interfaces')) > 0) or node_type in important_types:
                node['linked_nodes'] = _neighbor_names(nid)
                filtered_nodes.append(node)
        nodes = filtered_nodes

        filtered_extra_switch_nodes: list[dict] = []
        for sw in extra_switch_nodes:
            nid = sw['id']
            linked = adj_clean.get(nid, set())
            if linked:
                sw['linked_nodes'] = _neighbor_names(nid)
                filtered_extra_switch_nodes.append(sw)
        extra_switch_nodes = filtered_extra_switch_nodes

        filtered_extra_hitl_nodes: list[dict] = []
        for hitl in extra_hitl_network_nodes:
            nid = hitl['id']
            linked = adj_clean.get(nid, set())
            if linked:
                hitl['linked_nodes'] = _neighbor_names(nid)
                filtered_extra_hitl_nodes.append(hitl)
        extra_hitl_network_nodes = filtered_extra_hitl_nodes

        valid_ids = {n['id'] for n in nodes}
        valid_ids.update(sw['id'] for sw in extra_switch_nodes)
        valid_ids.update(hitl['id'] for hitl in extra_hitl_network_nodes)
        adj_clean = {nid: {nbr for nbr in neighbors if nbr in valid_ids} for nid, neighbors in adj_clean.items() if nid in valid_ids}

        switches = [n for n in nodes if (n.get('type') or '').lower() == 'switch']
        hitl_markers = ('rj45', 'hitl', 'tap', 'bridge', 'ethernet', 'rj-45')

        def _is_hitl_node(node: dict) -> bool:
            node_type = (node.get('type') or '').lower()
            if node_type and any(marker in node_type for marker in hitl_markers):
                return True
            for svc in node.get('services') or []:
                svc_name = (svc or '').lower()
                if any(marker in svc_name for marker in hitl_markers):
                    return True
            node_name = (node.get('name') or '').lower()
            if node_name and any(marker in node_name for marker in ('rj45', 'hitl')):
                return True
            return False

        hitl_nodes: list[dict] = []
        for node in nodes:
            if _is_hitl_node(node):
                node['is_hitl'] = True
                hitl_nodes.append(node)
        for hitl in extra_hitl_network_nodes:
            hitl['is_hitl'] = True
            hitl_nodes.append(hitl)

        link_details: list[dict] = []
        seen_pairs: set[tuple[str, str]] = set()
        for src, neighbors in adj_clean.items():
            for dst in neighbors:
                if src == dst or dst not in valid_ids:
                    continue
                ordered = tuple(sorted((src, dst)))
                if ordered in seen_pairs:
                    continue
                seen_pairs.add(ordered)
                link_details.append({
                    'node1': ordered[0],
                    'node2': ordered[1],
                    'node1_name': id_to_name.get(ordered[0], ordered[0]),
                    'node2_name': id_to_name.get(ordered[1], ordered[1]),
                })

        info.update({
            'nodes_count': len(nodes),
            'networks_count': len(networks),
            'links_count': len(link_details),
            'services_count': len(services),
            'nodes': nodes,
            'switches_count': len(switches) + len(extra_switch_nodes),
            'switches_device_count': len(switches),
            'switches': [sw['name'] for sw in switches] + [sw['name'] for sw in extra_switch_nodes],
            'switch_nodes': extra_switch_nodes,
            'hitl_network_nodes': extra_hitl_network_nodes,
            'hitl_nodes': hitl_nodes,
            'hitl_nodes_count': len(hitl_nodes),
            'links_detail': link_details,
            'routing_edges_policies': routing_edge_policies,
        })
        info['devices'] = [attrs(d, 'id', 'name', 'type', 'class', 'image') for d in devices[:50]]
        info['networks'] = [attrs(n, 'id', 'name', 'type', 'model', 'mobility') for n in networks[:50]]
        info['links_sample'] = len(link_details)

        try:
            st = os.stat(xml_path)
            info['file_size_bytes'] = st.st_size
        except Exception:
            pass

        try:
            router_ids = [normalize_ref(dev.get('id')) for dev in devices if (dev.get('type') or '').lower().find('router') >= 0]
            degs = {rid: len(adj_clean.get(rid, set())) for rid in router_ids if rid}
            if degs:
                vals = list(degs.values())
                info['router_degree_stats'] = {
                    'min': min(vals),
                    'max': max(vals),
                    'avg': round(sum(vals) / len(vals), 2),
                    'per_router': degs,
                }
        except Exception:
            pass

        return info
    except Exception as e:
        return {'error': str(e)}


def _summarize_planner_scenarios(xml_path: str) -> Dict[str, Any]:
    """Create a lightweight summary for Scenario Editor bundles (root <Scenarios>)."""
    summary: Dict[str, Any] = {'__planner_bundle': True, 'scenarios': [], 'scenarios_count': 0}
    try:
        payload = _parse_scenarios_xml(xml_path)
    except Exception as exc:
        summary['error'] = str(exc)
        return summary
    scen_list = payload.get('scenarios') or []
    for scen in scen_list:
        if not isinstance(scen, dict):
            continue
        sections_meta = scen.get('sections') if isinstance(scen.get('sections'), dict) else {}
        sections_summary: List[Dict[str, Any]] = []
        for sec_name, sec_val in sections_meta.items():
            if not isinstance(sec_val, dict):
                continue
            sections_summary.append({
                'name': sec_name,
                'item_count': len(sec_val.get('items') or []),
                'density': sec_val.get('density'),
                'total_nodes': sec_val.get('total_nodes'),
            })
        summary['scenarios'].append({
            'name': scen.get('name') or 'Scenario',
            'density_count': scen.get('density_count'),
            'scenario_total_nodes': scen.get('scenario_total_nodes'),
            'base_nodes': scen.get('base_nodes'),
            'hitl_enabled': bool(((scen.get('hitl') or {}).get('enabled'))),
            'base_file': ((scen.get('base') or {}).get('filepath') or ''),
            'sections': sections_summary,
        })
    summary['scenarios_count'] = len(summary['scenarios'])
    core_meta = payload.get('core') if isinstance(payload.get('core'), dict) else None
    if core_meta:
        summary['core'] = core_meta
    return summary


@app.route('/', methods=['GET'])
def index():
    current = _current_user()
    scenario_query = ''
    try:
        scenario_query = (request.args.get('scenario') or '').strip()
    except Exception:
        scenario_query = ''
    if current and _is_participant_role(current.get('role')):
        target_args = {'scenario': scenario_query} if scenario_query else {}
        return redirect(url_for('participant_ui_page', **target_args))
    # Admin Scenarios editor should list the same scenarios as CORE/Reports.
    # Start from catalog names so the left-hand scenario list includes saved-but-not-executed
    # scenarios like "Scenario 1b".
    payload = _default_scenarios_payload()
    try:
        role = _normalize_role_value(current.get('role')) if current else ''
        if role == 'admin':
            scenario_names, _scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(None, user=current)
            if scenario_names:
                payload = _default_scenarios_payload_for_names(scenario_names)
    except Exception:
        payload = _default_scenarios_payload()
    # Reconstruct base_upload if base filepath already present
    _attach_base_upload(payload)
    _hydrate_base_upload_from_disk(payload)
    payload['host_interfaces'] = _enumerate_host_interfaces()
    if payload.get('base_upload'):
        _save_base_upload_state(payload['base_upload'])
    payload = _prepare_payload_for_index(payload, user=current)
    if scenario_query:
        payload['scenario_query'] = scenario_query
    if payload.get('result_path') and not payload.get('project_key_hint'):
        payload['project_key_hint'] = payload['result_path']
    snapshot = _load_editor_state_snapshot(current)
    if snapshot:
        payload['editor_snapshot'] = snapshot
        if snapshot.get('project_key_hint') and not payload.get('project_key_hint'):
            payload['project_key_hint'] = snapshot.get('project_key_hint')
        if snapshot.get('scenario_query') and not payload.get('scenario_query'):
            payload['scenario_query'] = snapshot.get('scenario_query')
    return render_template('index.html', payload=payload, logs="", xml_preview="")


@app.route('/load_xml', methods=['POST'])
def load_xml():
    user = _current_user()
    file = request.files.get('scenarios_xml')
    if not file or file.filename == '':
        flash('No file selected.')
        return redirect(url_for('index'))
    if not allowed_file(file.filename):
        flash('Invalid file type. Only XML allowed.')
        return redirect(url_for('index'))
    filename = secure_filename(file.filename)
    filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
    file.save(filepath)
    try:
        payload = _parse_scenarios_xml(filepath)
        # add default CORE connection parameters
        if "core" not in payload:
            payload["core"] = _default_core_dict()
        payload["result_path"] = filepath
        _attach_base_upload(payload)
        _hydrate_base_upload_from_disk(payload)
        payload['host_interfaces'] = _enumerate_host_interfaces()
        if payload.get('base_upload'):
            _save_base_upload_state(payload['base_upload'])
        xml_text = ""
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                xml_text = f.read()
        except Exception:
            xml_text = ""
        payload = _prepare_payload_for_index(payload, user=user)
        snapshot_source = dict(payload)
        snapshot_source['active_index'] = 0
        snapshot_source['project_key_hint'] = payload.get('result_path')
        _persist_editor_state_snapshot(snapshot_source, user=user)
        snapshot = _load_editor_state_snapshot(user)
        if snapshot:
            payload['editor_snapshot'] = snapshot
        return render_template('index.html', payload=payload, logs="", xml_preview=xml_text)
    except Exception as e:
        flash(f'Failed to parse XML: {e}')
        return redirect(url_for('index'))


@app.route('/save_xml', methods=['POST'])
def save_xml():
    data_str = request.form.get('scenarios_json')
    if not data_str:
        flash('No data received.')
        return redirect(url_for('index'))
    user = _current_user()
    try:
        data = json.loads(data_str)
    except Exception as e:
        flash(f'Invalid JSON: {e}')
        return redirect(url_for('index'))
    try:
        active_index = None
        try:
            active_index = int(data.get('active_index')) if 'active_index' in data else None
        except Exception:
            active_index = None
        core_meta = None
        try:
            core_str = request.form.get('core_json')
            if core_str:
                core_meta = json.loads(core_str)
        except Exception:
            core_meta = None
        client_project_hint = (request.form.get('project_key_hint') or '').strip()
        client_scenario_query = (request.form.get('scenario_query') or '').strip()
        normalized_core = _normalize_core_config(core_meta, include_password=True) if core_meta else None
        # Enforce unique scenario names (case-insensitive, trimmed) to prevent confusing overwrites.
        try:
            scenarios_list = data.get('scenarios') or []
            seen: set[str] = set()
            dupes: list[str] = []
            if isinstance(scenarios_list, list):
                for idx, sc in enumerate(scenarios_list):
                    if not isinstance(sc, dict):
                        continue
                    raw_name = (sc.get('name') or '').strip()
                    name = raw_name or f"Scenario {idx + 1}"
                    key = name.casefold()
                    if key in seen:
                        dupes.append(name)
                    else:
                        seen.add(key)
            if dupes:
                pretty = ', '.join(sorted(set(dupes)))
                flash(f'Duplicate scenario names are not allowed: {pretty}')
                return redirect(url_for('index'))
        except Exception:
            # If validation fails unexpectedly, fall through to existing error handling.
            pass
        scenario_count = len(data.get('scenarios') or []) if isinstance(data.get('scenarios'), list) else 0
        scenario_names_desc = []
        try:
            scenario_names_desc = [str((sc or {}).get('name') or '').strip() for sc in (data.get('scenarios') or []) if isinstance(sc, dict)]
        except Exception:
            scenario_names_desc = []
        username = (user or {}).get('username') if isinstance(user, dict) else None
        try:
            app.logger.info(
                '[save_xml] user=%s scen_count=%s active_index=%s project_hint=%s scenario_query=%s names=%s',
                username or 'anonymous',
                scenario_count,
                active_index if active_index is not None else 'none',
                client_project_hint or '<none>',
                client_scenario_query or '<none>',
                ', '.join(name for name in scenario_names_desc if name) or '<unnamed>'
            )
        except Exception:
            pass
        tree = _build_scenarios_xml({ 'scenarios': data.get('scenarios'), 'core': normalized_core })
        ts = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
        out_dir = os.path.join(_outputs_dir(), f'scenarios-{ts}')
        os.makedirs(out_dir, exist_ok=True)
        # Determine filename: <scenario-name>.xml (no timestamp in filename)
        try:
            scen_names = [s.get('name') for s in (data.get('scenarios') or []) if isinstance(s, dict) and s.get('name')]
        except Exception:
            scen_names = []
        chosen_name = None
        try:
            if active_index is not None and 0 <= active_index < len(scen_names):
                chosen_name = scen_names[active_index]
        except Exception:
            chosen_name = None
        stem_raw = (chosen_name or (scen_names[0] if scen_names else 'scenarios')) or 'scenarios'
        stem = secure_filename(stem_raw).strip('_-.') or 'scenarios'
        out_path = os.path.join(out_dir, f"{stem}.xml")
        try:
            app.logger.info('[save_xml] writing xml out_path=%s stem=%s', out_path, stem)
        except Exception:
            pass
        # Pretty print if lxml available else fallback
        try:
            from lxml import etree as LET  # type: ignore
            raw = ET.tostring(tree.getroot(), encoding='utf-8')
            lroot = LET.fromstring(raw)
            pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
            with open(out_path, 'wb') as f:
                f.write(pretty)
        except Exception:
            tree.write(out_path, encoding='utf-8', xml_declaration=True)
        # Read back XML content for preview
        xml_text = ""
        try:
            with open(out_path, 'r', encoding='utf-8', errors='ignore') as f:
                xml_text = f.read()
        except Exception:
            xml_text = ""
        flash(f'Scenarios saved as {os.path.basename(out_path)}')
        # Re-parse the saved XML to ensure the UI reflects exactly what was persisted
        try:
            payload = _parse_scenarios_xml(out_path)
            if "core" not in payload:
                payload["core"] = _default_core_dict()
            payload["result_path"] = out_path
            if normalized_core:
                payload['core'] = _normalize_core_config(normalized_core, include_password=False)
        except Exception:
            payload = {"scenarios": data.get("scenarios", []), "result_path": out_path, "core": _normalize_core_config(normalized_core or {}, include_password=False) if normalized_core else _default_core_dict()}
        payload['host_interfaces'] = _enumerate_host_interfaces()
        _attach_base_upload(payload)
        _hydrate_base_upload_from_disk(payload)
        if payload.get('base_upload'):
            _save_base_upload_state(payload['base_upload'])
        payload = _prepare_payload_for_index(payload, user=user)
        if client_project_hint:
            payload['project_key_hint'] = client_project_hint
        if client_scenario_query:
            payload['scenario_query'] = client_scenario_query
        snapshot_source = dict(payload)
        try:
            snapshot_source['scenarios'] = copy.deepcopy(data.get('scenarios') or [])
        except Exception:
            snapshot_source['scenarios'] = data.get('scenarios') or []
        snapshot_source['active_index'] = active_index
        if client_project_hint:
            snapshot_source['project_key_hint'] = client_project_hint
        elif payload.get('project_key_hint'):
            snapshot_source['project_key_hint'] = payload.get('project_key_hint')
        else:
            snapshot_source['project_key_hint'] = payload.get('result_path')
        if client_scenario_query:
            snapshot_source['scenario_query'] = client_scenario_query
        elif payload.get('scenario_query'):
            snapshot_source['scenario_query'] = payload.get('scenario_query')
        _persist_editor_state_snapshot(snapshot_source, user=user)
        snapshot = _load_editor_state_snapshot(user)
        if snapshot:
            payload['editor_snapshot'] = snapshot
        try:
            app.logger.info('[save_xml] success user=%s xml=%s scen_count=%s', username or 'anonymous', out_path, scenario_count)
        except Exception:
            pass
        return render_template('index.html', payload=payload, logs="", xml_preview=xml_text)
    except Exception as e:
        flash(f'Failed to save XML: {e}')
        return redirect(url_for('index'))


@app.route('/save_xml_api', methods=['POST'])
def save_xml_api():
    try:
        user = _current_user()
        data = request.get_json(silent=True) or {}
        scenarios = data.get('scenarios')
        core_meta = data.get('core')
        normalized_core = _normalize_core_config(core_meta, include_password=True) if isinstance(core_meta, (dict, list)) or core_meta else None
        raw_project_hint = data.get('project_key_hint') if isinstance(data, dict) else None
        project_key_hint = raw_project_hint.strip() if isinstance(raw_project_hint, str) else ''
        raw_scenario_query = data.get('scenario_query') if isinstance(data, dict) else None
        scenario_query_hint = raw_scenario_query.strip() if isinstance(raw_scenario_query, str) else ''
        active_index = None
        try:
            active_index = int(data.get('active_index')) if 'active_index' in data else None
        except Exception:
            active_index = None
        if not isinstance(scenarios, list):
            return jsonify({ 'ok': False, 'error': 'Invalid payload (scenarios list required)' }), 400
        # Enforce unique scenario names (case-insensitive, trimmed).
        try:
            seen: set[str] = set()
            dupes: list[str] = []
            for idx, sc in enumerate(scenarios):
                if not isinstance(sc, dict):
                    continue
                raw_name = (sc.get('name') or '').strip()
                name = raw_name or f"Scenario {idx + 1}"
                key = name.casefold()
                if key in seen:
                    dupes.append(name)
                else:
                    seen.add(key)
            if dupes:
                pretty = ', '.join(sorted(set(dupes)))
                return jsonify({ 'ok': False, 'error': f'Duplicate scenario names are not allowed: {pretty}' }), 400
        except Exception:
            # Best-effort validation; if it fails, continue with existing behavior.
            pass
        scenario_names: list[str] = []
        try:
            scenario_names = [str((s or {}).get('name') or '').strip() for s in scenarios if isinstance(s, dict)]
        except Exception:
            scenario_names = []
        username = (user or {}).get('username') if isinstance(user, dict) else None
        try:
            app.logger.info(
                '[save_xml_api] user=%s scen_count=%s active_index=%s project_hint=%s scenario_query=%s names=%s',
                username or 'anonymous',
                len(scenarios),
                active_index if active_index is not None else 'none',
                project_key_hint or '<none>',
                scenario_query_hint or '<none>',
                ', '.join(name for name in scenario_names if name) or '<unnamed>'
            )
        except Exception:
            pass
        tree = _build_scenarios_xml({ 'scenarios': scenarios, 'core': normalized_core })
        ts = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
        out_dir = os.path.join(_outputs_dir(), f'scenarios-{ts}')
        os.makedirs(out_dir, exist_ok=True)
        # Determine filename: <scenario-name>.xml
        try:
            scen_names = [s.get('name') for s in scenarios if isinstance(s, dict) and s.get('name')]
        except Exception:
            scen_names = []
        chosen_name = None
        try:
            if active_index is not None and 0 <= active_index < len(scen_names):
                chosen_name = scen_names[active_index]
        except Exception:
            chosen_name = None
        stem_raw = (chosen_name or (scen_names[0] if scen_names else 'scenarios')) or 'scenarios'
        stem = secure_filename(stem_raw).strip('_-.') or 'scenarios'
        out_path = os.path.join(out_dir, f"{stem}.xml")
        try:
            app.logger.info('[save_xml_api] writing xml out_path=%s stem=%s', out_path, stem)
        except Exception:
            pass
        # Pretty print when possible
        try:
            raw = ET.tostring(tree.getroot(), encoding='utf-8')
            lroot = LET.fromstring(raw)
            pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
            with open(out_path, 'wb') as f:
                f.write(pretty)
        except Exception:
            tree.write(out_path, encoding='utf-8', xml_declaration=True)
        resp_core = _normalize_core_config(normalized_core or core_meta or {}, include_password=False) if (normalized_core or core_meta) else _default_core_dict()
        snapshot_source = {
            'scenarios': scenarios,
            'core': resp_core,
            'result_path': out_path,
            'active_index': active_index,
            'project_key_hint': project_key_hint or out_path,
        }
        if scenario_query_hint:
            snapshot_source['scenario_query'] = scenario_query_hint
        _persist_editor_state_snapshot(snapshot_source, user=user)
        try:
            app.logger.info('[save_xml_api] success user=%s xml=%s scen_count=%s', username or 'anonymous', out_path, len(scenarios))
        except Exception:
            pass
        return jsonify({ 'ok': True, 'result_path': out_path, 'core': resp_core })
    except Exception as e:
        try:
            app.logger.exception("[save_xml_api] failed: %s", e)
        except Exception:
            pass
        return jsonify({ 'ok': False, 'error': str(e) }), 500


@app.route('/render_xml_api', methods=['POST'])
def render_xml_api():
    """Render scenario XML for preview without persisting to disk."""
    try:
        data = request.get_json(silent=True) or {}
        scenarios = data.get('scenarios')
        core_meta = data.get('core')
        normalized_core = _normalize_core_config(core_meta, include_password=True) if isinstance(core_meta, (dict, list)) or core_meta else None
        if not isinstance(scenarios, list):
            return jsonify({ 'ok': False, 'error': 'Invalid payload (scenarios list required)' }), 400
        tree = _build_scenarios_xml({ 'scenarios': scenarios, 'core': normalized_core })
        # Pretty print when possible
        try:
            raw = ET.tostring(tree.getroot(), encoding='utf-8')
            lroot = LET.fromstring(raw)
            pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
            return Response(pretty, mimetype='application/xml')
        except Exception:
            out = ET.tostring(tree.getroot(), encoding='utf-8', xml_declaration=True)
            return Response(out, mimetype='application/xml')
    except Exception as e:
        try:
            app.logger.exception('[render_xml_api] failed: %s', e)
        except Exception:
            pass
        return jsonify({ 'ok': False, 'error': str(e) }), 500


@app.route('/run_cli', methods=['POST'])
def run_cli():
    user = _current_user()
    xml_path = request.form.get('xml_path')
    if not xml_path:
        flash('XML path missing. Save XML first.')
        return redirect(url_for('index'))
    # Always resolve to absolute path
    xml_path = os.path.abspath(xml_path)
    # Path fallback: if user supplied /app/outputs but actual saved path lives under /app/webapp/outputs (volume mapping difference)
    if not os.path.exists(xml_path) and '/outputs/' in xml_path:
        try:
            # Replace first occurrence of '/app/outputs' with '/app/webapp/outputs'
            alt = xml_path.replace('/app/outputs', '/app/webapp/outputs')
            if alt != xml_path and os.path.exists(alt):
                app.logger.info("[sync] Remapped XML path %s -> %s", xml_path, alt)
                xml_path = alt
        except Exception:
            pass
    if not os.path.exists(xml_path):
        try:
            recovered = _try_resolve_latest_outputs_xml(xml_path)
            if recovered and os.path.exists(recovered):
                app.logger.warning('[sync] XML path missing; recovered to newest match: %s -> %s', xml_path, recovered)
                xml_path = recovered
        except Exception:
            pass
    if not os.path.exists(xml_path):
        flash(f'XML path not found: {xml_path}')
        return redirect(url_for('index'))
    # Skip schema validation: format differs from CORE XML
    # Determine CORE connection settings (including SSH details) from saved payload and defaults
    core_override = None
    try:
        core_json = request.form.get('core_json')
        if core_json:
            core_override = json.loads(core_json)
    except Exception:
        core_override = None
    scenario_core_override = None
    try:
        hitl_core_json = request.form.get('hitl_core_json')
        if hitl_core_json:
            scenario_core_override = json.loads(hitl_core_json)
    except Exception:
        scenario_core_override = None
    scenario_name_hint = request.form.get('scenario') or request.form.get('scenario_name') or None
    docker_cleanup_before_run = _coerce_bool(request.form.get('docker_cleanup_before_run'))
    docker_remove_all_containers = _coerce_bool(
        request.form.get('docker_remove_all_containers')
    ) or _coerce_bool(request.form.get('docker_nuke_all'))
    scenario_index_hint: Optional[int] = None
    try:
        raw_index = request.form.get('scenario_index')
        if raw_index not in (None, ''):
            scenario_index_hint = int(raw_index)
    except Exception:
        scenario_index_hint = None
    payload_for_core: Dict[str, Any] | None = None
    try:
        payload_for_core = _parse_scenarios_xml(xml_path)
    except Exception:
        payload_for_core = None
    scenario_payload: Dict[str, Any] | None = None
    if payload_for_core:
        scen_list = payload_for_core.get('scenarios') or []
        if isinstance(scen_list, list) and scen_list:
            if scenario_name_hint:
                for scen_entry in scen_list:
                    if not isinstance(scen_entry, dict):
                        continue
                    if str(scen_entry.get('name') or '').strip() == str(scenario_name_hint).strip():
                        scenario_payload = scen_entry
                        break
            if scenario_payload is None and scenario_index_hint is not None:
                if 0 <= scenario_index_hint < len(scen_list):
                    candidate = scen_list[scenario_index_hint]
                    if isinstance(candidate, dict):
                        scenario_payload = candidate
            if scenario_payload is None:
                for scen_entry in scen_list:
                    if isinstance(scen_entry, dict):
                        scenario_payload = scen_entry
                        break
    scenario_core_saved = None
    if scenario_payload and isinstance(scenario_payload.get('hitl'), dict):
        scenario_core_saved = scenario_payload['hitl'].get('core')
    global_core_saved = payload_for_core.get('core') if (payload_for_core and isinstance(payload_for_core.get('core'), dict)) else None
    scenario_core_public: Dict[str, Any] | None = None
    candidate_scenario_core = scenario_core_override if isinstance(scenario_core_override, dict) else None
    if not candidate_scenario_core and isinstance(scenario_core_saved, dict):
        candidate_scenario_core = scenario_core_saved
    if candidate_scenario_core:
        scenario_core_public = _scrub_scenario_core_config(candidate_scenario_core)
    core_cfg = _merge_core_configs(
        global_core_saved,
        scenario_core_saved,
        core_override,
        scenario_core_override,
        include_password=True,
    )
    try:
        core_cfg = _require_core_ssh_credentials(core_cfg)
    except _SSHTunnelError as exc:
        flash(str(exc))
        return redirect(url_for('index'))
    core_host = core_cfg.get('host', '127.0.0.1')
    try:
        core_port = int(core_cfg.get('port', 50051))
    except Exception:
        core_port = 50051
    remote_desc = f"{core_host}:{core_port}"
    app.logger.info("[sync] Preparing CLI run against CORE remote=%s (ssh_enabled=%s), xml=%s", remote_desc, core_cfg.get('ssh_enabled'), xml_path)
    preferred_cli_venv = _sanitize_venv_bin_path(core_cfg.get('venv_bin'))
    venv_is_explicit = _venv_is_explicit(core_cfg, preferred_cli_venv)
    if venv_is_explicit and preferred_cli_venv:
        cli_venv_bin = _resolve_cli_venv_bin(preferred_cli_venv, allow_fallback=False)
        if not cli_venv_bin:
            flash(
                f"Remote CORE venv bin '{preferred_cli_venv}' is not accessible from this host. "
                "Mount that directory or adjust the path before running the CLI.",
            )
            return redirect(url_for('index'))
    else:
        cli_venv_bin = _resolve_cli_venv_bin(preferred_cli_venv, allow_fallback=True)
    # Run gRPC CLI script (config2scen_core_grpc.py) instead of internal module
    try:
        # Pre-save any existing active CORE session XML (best-effort) using derived config
        try:
            pre_dir = os.path.join(os.path.dirname(xml_path) or _outputs_dir(), 'core-pre')
            pre_saved = _grpc_save_current_session_xml_with_config(core_cfg, pre_dir)
            if pre_saved:
                flash(f'Captured current CORE session XML: {os.path.basename(pre_saved)}')
                app.logger.debug("[sync] Pre-run session XML saved to %s", pre_saved)
        except Exception:
            pre_saved = None
        repo_root = _get_repo_root()
        # Invoke package CLI so it can generate reports under repo_root/reports
        py_exec = _select_python_interpreter(cli_venv_bin)
        cli_env = _prepare_cli_env(preferred_venv_bin=cli_venv_bin)
        cli_env.setdefault('PYTHONUNBUFFERED', '1')
        # Deliver Flow generator artifacts into vuln containers by copying files in,
        # rather than bind-mounting directories from the host.
        cli_env.setdefault('CORETG_FLOW_ARTIFACTS_MODE', 'copy')
        app.logger.info("[sync] Using python interpreter: %s", py_exec)
        # Determine active scenario name (prefer explicit hint, fallback to first in XML)
        active_scenario_name = None
        if scenario_name_hint:
            active_scenario_name = scenario_name_hint
        elif scenario_payload and isinstance(scenario_payload.get('name'), str):
            active_scenario_name = scenario_payload.get('name')
        if not active_scenario_name:
            try:
                names_for_cli = _scenario_names_from_xml(xml_path)
                if names_for_cli:
                    active_scenario_name = names_for_cli[0]
            except Exception:
                active_scenario_name = None

        # Scope tool-generated wrapper images by upload/scenario to prevent cross-scenario image reuse.
        try:
            out_dir_for_tag = os.path.dirname(xml_path) if xml_path else ''
            upload_base = os.path.basename(out_dir_for_tag) if out_dir_for_tag else ''
            parts = []
            if upload_base:
                parts.append(upload_base)
            if active_scenario_name:
                parts.append(active_scenario_name)
            scenario_tag = _safe_name('-'.join(parts) if parts else (active_scenario_name or 'scenario'))
            cli_env.setdefault('CORETG_SCENARIO_TAG', scenario_tag)
        except Exception:
            pass
        with _core_connection(core_cfg) as (conn_host, conn_port):
            forwarded_desc = f"{conn_host}:{conn_port}"
            app.logger.info(
                "[sync] Running CLI with CORE remote=%s via=%s, xml=%s",
                remote_desc,
                forwarded_desc,
                xml_path,
            )

            # Optional: best-effort cleanup of tool-managed Docker state before execution.
            # This is conservative: it only targets vuln docker-compose node containers derived
            # from compose_assignments.json plus tool-generated wrapper images under coretg/*.

            # Optional: very destructive cleanup.
            # Stops/removes ALL containers on the remote CORE VM.
            if docker_remove_all_containers:
                try:
                    app.logger.warning('[sync] Pre-run: docker remove-all-containers requested')
                    _run_remote_python_json(
                        core_cfg,
                        _remote_docker_remove_all_containers_script(core_cfg.get('ssh_password')),
                        logger=app.logger,
                        label='docker.remove_all_containers(prerun)',
                        timeout=900.0,
                    )
                except Exception as exc:
                    try:
                        app.logger.warning('[sync] Pre-run docker remove-all-containers skipped/failed: %s', exc)
                    except Exception:
                        pass
            if docker_cleanup_before_run:
                try:
                    app.logger.info('[sync] Pre-run: docker cleanup requested (containers + wrapper images)')
                    status_payload = _run_remote_python_json(
                        core_cfg,
                        _remote_docker_status_script(core_cfg.get('ssh_password')),
                        logger=app.logger,
                        label='docker.status(for prerun cleanup)',
                        timeout=60.0,
                    )
                    names: list[str] = []
                    if isinstance(status_payload, dict) and isinstance(status_payload.get('items'), list):
                        for it in status_payload.get('items') or []:
                            if isinstance(it, dict) and it.get('name'):
                                names.append(str(it.get('name')))
                    if names:
                        _run_remote_python_json(
                            core_cfg,
                            _remote_docker_cleanup_script(names, core_cfg.get('ssh_password')),
                            logger=app.logger,
                            label='docker.cleanup(prerun)',
                            timeout=120.0,
                        )
                    _run_remote_python_json(
                        core_cfg,
                        _remote_docker_remove_wrapper_images_script(core_cfg.get('ssh_password')),
                        logger=app.logger,
                        label='docker.wrapper_images.cleanup(prerun)',
                        timeout=180.0,
                    )
                except Exception as exc:
                    try:
                        app.logger.warning('[sync] Pre-run docker cleanup skipped/failed: %s', exc)
                    except Exception:
                        pass

            cli_args = [
                py_exec,
                '-m',
                'core_topo_gen.cli',
                '--xml',
                xml_path,
                '--host',
                conn_host,
                '--port',
                str(conn_port),
                '--verbose',
            ]
            if active_scenario_name:
                cli_args.extend(['--scenario', active_scenario_name])
            proc = subprocess.run(cli_args, cwd=repo_root, check=False, capture_output=True, text=True, env=cli_env)
        logs = (proc.stdout or '') + ('\n' + proc.stderr if proc.stderr else '')
        app.logger.debug("[sync] CLI return code: %s", proc.returncode)

        # If the CLI generated vulnerability compose artifacts locally, copy them to the CORE VM
        # so the remote Docker Compose card and DockerComposeService can access them.
        uploaded_vuln_artifacts = False
        try:
            uploaded_vuln_artifacts = bool(_sync_local_vulns_to_remote(core_cfg, logger=app.logger))
        except Exception as exc:
            uploaded_vuln_artifacts = False
            try:
                app.logger.warning('[sync] Vuln artifact upload failed: %s', exc)
            except Exception:
                pass
        try:
            app.logger.info('[sync] Vuln artifact upload complete uploaded=%s', bool(uploaded_vuln_artifacts))
        except Exception:
            pass

        # Flow flag artifacts: copy them into running/stopped vuln containers on the CORE VM.
        # This is used when CORETG_FLOW_ARTIFACTS_MODE=copy (no bind mounts).
        try:
            payload = _run_remote_python_json(
                core_cfg,
                _remote_copy_flow_artifacts_into_containers_script(core_cfg.get('ssh_password')),
                logger=app.logger,
                label='docker.copy_flow_artifacts(postrun)',
                timeout=180.0,
            )
            try:
                copied_ok = 0
                copied_total = 0
                if isinstance(payload, dict) and isinstance(payload.get('items'), list):
                    copied_total = len(payload.get('items') or [])
                    copied_ok = sum(1 for it in (payload.get('items') or []) if isinstance(it, dict) and it.get('ok'))
                app.logger.info('[sync] docker cp flow artifacts results ok=%s total=%s', int(copied_ok), int(copied_total))
            except Exception:
                pass
        except Exception as exc:
            try:
                app.logger.warning('[sync] docker cp flow artifacts skipped/failed: %s', exc)
            except Exception:
                pass

        # Report path (if generated by CLI): parse logs or fallback to latest under reports/
        report_md = _extract_report_path_from_text(logs) or _find_latest_report_path()
        if report_md:
            app.logger.info("[sync] Detected report path: %s", report_md)
        summary_json = _extract_summary_path_from_text(logs)
        if not summary_json:
            summary_json = _derive_summary_from_report(report_md)
        if not summary_json and not report_md:
            summary_json = _find_latest_summary_path()
        if summary_json and not os.path.exists(summary_json):
            summary_json = None
        if summary_json:
            app.logger.info("[sync] Detected summary path: %s", summary_json)
        # Try to capture the exact session id from logs for precise post-run save
        session_id = _extract_session_id_from_text(logs)
        if session_id:
            app.logger.info("[sync] Detected CORE session id: %s", session_id)
            _record_session_mapping(
                xml_path,
                session_id,
                scenario_name=active_scenario_name or scenario_name_hint or None,
            )
            try:
                sid_int = int(str(session_id).strip())
                _write_remote_session_scenario_meta(
                    core_cfg,
                    session_id=sid_int,
                    scenario_name=active_scenario_name or scenario_name_hint or None,
                    scenario_xml_basename=os.path.basename(xml_path),
                    logger=app.logger,
                )
            except Exception:
                pass
        # Read XML for preview
        xml_text = ""
        try:
            with open(xml_path, 'r', encoding='utf-8', errors='ignore') as f:
                xml_text = f.read()
        except Exception:
            xml_text = ""
        run_success = (proc.returncode == 0)
        post_saved = None
        # Inform user
        if run_success:
            if report_md and os.path.exists(report_md):
                flash('CLI completed. Report ready to download.')
            else:
                flash('CLI completed. No report found.')
        else:
            flash('CLI finished with errors. See logs.')
        # Best-effort: save the active CORE session XML after run (try even on failures)
        try:
            post_dir = os.path.join(os.path.dirname(xml_path), 'core-post')
            post_saved = _grpc_save_current_session_xml_with_config(core_cfg, post_dir, session_id=session_id)
            if post_saved:
                flash(f'Captured post-run CORE session XML: {os.path.basename(post_saved)}')
                app.logger.debug("[sync] Post-run session XML saved to %s", post_saved)
        except Exception:
            post_saved = None
        payload = payload_for_core or {}
        if not payload:
            try:
                payload = _parse_scenarios_xml(xml_path)
            except Exception:
                payload = {}
        if "core" not in payload:
            payload["core"] = _default_core_dict()
        try:
            payload['core'] = _normalize_core_config(core_cfg, include_password=False)
        except Exception:
            pass
        _attach_base_upload(payload)
        # Always use absolute xml_path for result_path fallback
        payload["result_path"] = report_md if (report_md and os.path.exists(report_md)) else xml_path
        # Append run history entry regardless of intermediate failures; log details
        scen_names = []
        try:
            scen_names = _scenario_names_from_xml(xml_path)
        except Exception as e_names:
            app.logger.exception("[sync] failed extracting scenario names from %s: %s", xml_path, e_names)
        full_bundle_path = None
        single_scen_xml = None
        try:
            # Build a single-scenario XML containing only the active scenario to satisfy bundling constraint
            try:
                single_scen_xml = _write_single_scenario_xml(xml_path, (active_scenario_name or (scen_names[0] if scen_names else None)), out_dir=os.path.dirname(xml_path))
            except Exception:
                single_scen_xml = None
            bundle_xml = single_scen_xml or xml_path
            app.logger.info("[sync] Building full scenario archive (xml=%s, report=%s, pre=%s, post=%s)", bundle_xml, report_md, (pre_saved if 'pre_saved' in locals() else None), post_saved)
            full_bundle_path = _build_full_scenario_archive(
                os.path.dirname(bundle_xml),
                bundle_xml,
                (report_md if (report_md and os.path.exists(report_md)) else None),
                (pre_saved if 'pre_saved' in locals() else None),
                post_saved,
                summary_path=summary_json,
                run_id=None,
            )
        except Exception as e_bundle:
            app.logger.exception("[sync] failed building full scenario bundle: %s", e_bundle)
        try:
            session_xml_path = post_saved if (post_saved and os.path.exists(post_saved)) else None
            core_public = dict(core_cfg)
            core_public.pop('ssh_password', None)
            _append_run_history({
                'timestamp': datetime.datetime.utcnow().isoformat() + 'Z',
                'mode': 'sync',
                'xml_path': xml_path,
                'post_xml_path': session_xml_path,
                'session_xml_path': session_xml_path,
                'scenario_xml_path': xml_path,
                'report_path': report_md if (report_md and os.path.exists(report_md)) else None,
                'summary_path': summary_json if (summary_json and os.path.exists(summary_json)) else None,
                'pre_xml_path': pre_saved if 'pre_saved' in locals() else None,
                'full_scenario_path': full_bundle_path,
                'single_scenario_xml_path': single_scen_xml,
                'returncode': proc.returncode,
                'scenario_names': scen_names,
                'scenario_name': active_scenario_name,
                'core': _normalize_core_config(core_cfg, include_password=False),
                'core_cfg_public': core_public,
                'scenario_core': scenario_core_public,
            })
        except Exception as e_hist:
            app.logger.exception("[sync] failed appending run history: %s", e_hist)
        payload = _prepare_payload_for_index(payload, user=user)
        return render_template('index.html', payload=payload, logs=logs, xml_preview=xml_text, run_success=run_success)
    except Exception as e:
        flash(f'Error running core-topo-gen: {e}')
        return redirect(url_for('index'))


# ----------------------- Planning (Preview / Run) -----------------------



@app.route('/api/plan/preview_full', methods=['POST'])
def api_plan_preview_full():
    """Compute a full dry-run plan (no CORE session) including routers, hosts, IPs, services,
    vulnerabilities, segmentation slot preview and connectivity policies.

    Request JSON: { xml_path: "/abs/scenarios.xml", scenario: optionalName }
    Response: { ok, full_preview: {...} }
    """
    try:
        payload = request.get_json(silent=True) or {}
        xml_path = payload.get('xml_path')
        scenarios_inline = payload.get('scenarios')
        core_inline = payload.get('core')
        scenario = payload.get('scenario') or None
        seed = payload.get('seed')
        r2s_hosts_min_list = payload.get('r2s_hosts_min_list') or []
        r2s_hosts_max_list = payload.get('r2s_hosts_max_list') or []
        try:
            if seed is not None:
                seed = int(seed)
        except Exception:
            seed = None
        if not xml_path:
            # Builder/participant roles may preview without saving by posting scenarios/core.
            if isinstance(scenarios_inline, list):
                try:
                    normalized_core = _normalize_core_config(core_inline, include_password=True) if isinstance(core_inline, dict) else None
                    tree = _build_scenarios_xml({ 'scenarios': scenarios_inline, 'core': normalized_core })
                    ts = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
                    tag = str(uuid.uuid4())[:8]
                    out_dir = os.path.join(_outputs_dir(), f'tmp-preview-{ts}-{tag}')
                    os.makedirs(out_dir, exist_ok=True)
                    stem_raw = scenario or None
                    if not stem_raw:
                        try:
                            first_name = None
                            for sc in scenarios_inline:
                                if isinstance(sc, dict) and sc.get('name'):
                                    first_name = sc.get('name')
                                    break
                            stem_raw = first_name or 'scenarios'
                        except Exception:
                            stem_raw = 'scenarios'
                    stem = secure_filename(str(stem_raw)).strip('_-.') or 'scenarios'
                    xml_path = os.path.join(out_dir, f"{stem}.xml")
                    try:
                        from lxml import etree as LET  # type: ignore
                        raw = ET.tostring(tree.getroot(), encoding='utf-8')
                        lroot = LET.fromstring(raw)
                        pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
                        with open(xml_path, 'wb') as f:
                            f.write(pretty)
                    except Exception:
                        tree.write(xml_path, encoding='utf-8', xml_declaration=True)
                except Exception as exc:
                    return jsonify({'ok': False, 'error': f'Failed to render XML for preview: {exc}'}), 400
            else:
                return jsonify({'ok': False, 'error': 'xml_path missing'}), 400
        xml_path = os.path.abspath(xml_path)
        if not os.path.exists(xml_path):
            return jsonify({'ok': False, 'error': f'XML not found: {xml_path}'}), 404
        from core_topo_gen.planning.orchestrator import compute_full_plan
        from core_topo_gen.planning.plan_cache import hash_xml_file, try_get_cached_plan, save_plan_to_cache
        xml_hash = hash_xml_file(xml_path)
        plan = try_get_cached_plan(xml_hash, scenario, seed)
        if plan:
            app.logger.debug('[plan.preview_full] using cached plan (%s, scenario=%s, seed=%s)', xml_hash[:12], scenario, seed)
        else:
            plan = compute_full_plan(xml_path, scenario=scenario, seed=seed, include_breakdowns=True)
            try:
                save_plan_to_cache(xml_hash, scenario, seed, plan)
            except Exception as ce:
                app.logger.debug('[plan.preview_full] cache save failed: %s', ce)
        if seed is None:
            seed = plan.get('seed') or _derive_default_seed(xml_hash)
        full_prev = _build_full_preview_from_plan(plan, seed, r2s_hosts_min_list, r2s_hosts_max_list)
        xml_basename = os.path.splitext(os.path.basename(xml_path))[0]
        try:
            raw_hitl_config = parse_hitl_info(xml_path, scenario)
        except Exception as hitl_exc:
            try:
                app.logger.debug('[plan.preview_full] hitl parse failed: %s', hitl_exc)
            except Exception:
                pass
            raw_hitl_config = {"enabled": False, "interfaces": []}
        hitl_config = _sanitize_hitl_config(raw_hitl_config, scenario, xml_basename)
        try:
            full_prev['hitl_interfaces'] = hitl_config.get('interfaces', [])
            full_prev['hitl_enabled'] = bool(hitl_config.get('enabled'))
            full_prev['hitl_scenario_key'] = hitl_config.get('scenario_key')
            if hitl_config.get('core'):
                full_prev['hitl_core'] = hitl_config.get('core')
        except Exception:
            pass
        try:
            _merge_hitl_preview_with_full_preview(full_prev, hitl_config)
        except Exception:
            pass
        try:
            _attach_latest_flow_into_full_preview(full_prev, scenario)
        except Exception:
            pass
        return jsonify({'ok': True, 'full_preview': full_prev, 'plan': plan, 'breakdowns': plan.get('breakdowns')})
    except Exception as e:
        app.logger.exception('[plan.preview_full] error: %s', e)
        return jsonify({'ok': False, 'error': str(e) }), 500


@app.route('/api/plan/persist_preview_plan', methods=['POST'])
def api_plan_persist_preview_plan():
    """Compute a full preview and persist it as a plan artifact under outputs/plans.

    Request JSON: { xml_path: "/abs/path.xml", scenario: "name" (optional), seed: int (optional) }
    Response JSON: { ok, xml_path, scenario, seed, preview_plan_path }
    """
    try:
        payload = request.get_json(silent=True) or {}
        xml_path = (payload.get('xml_path') or '').strip()
        scenario = (payload.get('scenario') or '').strip() or None
        seed = payload.get('seed')
        try:
            if seed is not None:
                seed = int(seed)
        except Exception:
            seed = None

        if not xml_path:
            return jsonify({'ok': False, 'error': 'xml_path missing'}), 400
        xml_path = os.path.abspath(xml_path)
        if not os.path.exists(xml_path):
            return jsonify({'ok': False, 'error': f'XML not found: {xml_path}'}), 404

        from core_topo_gen.planning.orchestrator import compute_full_plan
        from core_topo_gen.planning.plan_cache import hash_xml_file, try_get_cached_plan, save_plan_to_cache

        xml_hash = hash_xml_file(xml_path)
        plan = None
        try:
            plan = try_get_cached_plan(xml_hash, scenario, seed)
        except Exception:
            plan = None
        if not plan:
            plan = compute_full_plan(xml_path, scenario=scenario, seed=seed, include_breakdowns=True)
            try:
                save_plan_to_cache(xml_hash, scenario, seed, plan)
            except Exception:
                pass

        if seed is None:
            try:
                seed = plan.get('seed') or _derive_default_seed(xml_hash)
            except Exception:
                seed = None

        full_prev = _build_full_preview_from_plan(plan, seed, [], [])
        # Keep Flow UI consistent when it later loads the plan.
        try:
            _attach_latest_flow_into_full_preview(full_prev, scenario)
        except Exception:
            pass

        plans_dir = os.path.join(_outputs_dir(), 'plans')
        os.makedirs(plans_dir, exist_ok=True)
        seed_tag = full_prev.get('seed') or (seed if seed is not None else 'preview')
        unique_tag = f"{seed_tag}_{int(time.time())}_{uuid.uuid4().hex[:6]}"
        preview_plan_path = os.path.join(plans_dir, f"plan_from_preview_{unique_tag}.json")
        plan_payload = {
            'full_preview': full_prev,
            'metadata': {
                'xml_path': xml_path,
                'scenario': scenario,
                'seed': full_prev.get('seed'),
                'created_at': datetime.datetime.now(datetime.timezone.utc).isoformat().replace('+00:00', 'Z'),
            },
        }
        with open(preview_plan_path, 'w', encoding='utf-8') as pf:
            json.dump(plan_payload, pf, indent=2, sort_keys=True)

        return jsonify({
            'ok': True,
            'xml_path': xml_path,
            'scenario': scenario,
            'seed': full_prev.get('seed'),
            'preview_plan_path': preview_plan_path,
        })
    except Exception as e:
        try:
            app.logger.exception('[plan.persist_preview_plan] error: %s', e)
        except Exception:
            pass
        return jsonify({'ok': False, 'error': str(e)}), 500

@app.route('/plan/full_preview_page', methods=['POST'])
def plan_full_preview_page():
    """Generate a full preview and render a dedicated page similar to core_details but without CORE.

    Form fields: xml_path, optional scenario, seed
    """
    try:
        embed_raw = request.args.get('embed') or request.form.get('embed') or ''
        embed = str(embed_raw).strip().lower() in ['1', 'true', 'yes', 'y', 'on']
        xml_path = request.form.get('xml_path')
        scenario = request.form.get('scenario') or None
        seed_raw = request.form.get('seed') or ''
        seed = None
        try:
            if seed_raw:
                s = int(seed_raw)
                if s>0: seed = s
        except Exception:
            seed = None
        if not xml_path:
            if embed:
                return Response(
                    '<div style="font-family: system-ui; padding: 16px; color: #6c757d;">'
                    'Save XML first to preview (missing xml_path).'
                    '</div>',
                    mimetype='text/html',
                )
            flash('xml_path missing (full preview page)')
            return redirect(url_for('index'))
        xml_path = os.path.abspath(xml_path)
        xml_basename = os.path.splitext(os.path.basename(xml_path))[0] if xml_path else ''
        # Path fallback: handle container/volume mapping differences (e.g., /app/outputs vs /app/webapp/outputs)
        if not os.path.exists(xml_path) and '/outputs/' in xml_path:
            try:
                alt = xml_path.replace('/app/outputs', '/app/webapp/outputs')
                if alt != xml_path and os.path.exists(alt):
                    app.logger.info('[full_preview] remapped xml_path %s -> %s', xml_path, alt)
                    xml_path = alt
            except Exception:
                pass
        if not os.path.exists(xml_path) and '/outputs/' in xml_path:
            try:
                alt = xml_path.replace('/app/webapp/outputs', '/app/outputs')
                if alt != xml_path and os.path.exists(alt):
                    app.logger.info('[full_preview] remapped xml_path %s -> %s', xml_path, alt)
                    xml_path = alt
            except Exception:
                pass
        if not os.path.exists(xml_path):
            if embed:
                safe = (xml_path or '').replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
                return Response(
                    '<div style="font-family: system-ui; padding: 16px; color: #6c757d;">'
                    'Save XML first to preview (XML not found):<br><code style="color:#495057;">'
                    + safe +
                    '</code></div>',
                    mimetype='text/html',
                )
            flash(f'XML not found: {xml_path}')
            return redirect(url_for('index'))
        from core_topo_gen.planning.orchestrator import compute_full_plan
        from core_topo_gen.planning.plan_cache import hash_xml_file, try_get_cached_plan, save_plan_to_cache

        plan = None
        xml_hash = None
        try:
            xml_hash = hash_xml_file(xml_path)
            plan = try_get_cached_plan(xml_hash, scenario, seed)
            if plan:
                app.logger.debug('[plan.full_preview_page] using cached plan (%s, scenario=%s, seed=%s)', (xml_hash or '')[:12], scenario, seed)
        except Exception as cache_err:
            try:
                app.logger.debug('[plan.full_preview_page] cache lookup failed: %s', cache_err)
            except Exception:
                pass
            plan = None
        if not plan:
            plan = compute_full_plan(xml_path, scenario=scenario, seed=seed, include_breakdowns=True)
            try:
                if xml_hash is None:
                    xml_hash = hash_xml_file(xml_path)
                save_plan_to_cache(xml_hash, scenario, seed, plan)
            except Exception as cache_save_err:
                try:
                    app.logger.debug('[plan.full_preview_page] cache save failed: %s', cache_save_err)
                except Exception:
                    pass
        if seed is None:
            seed = plan.get('seed') or _derive_default_seed(xml_hash or hash_xml_file(xml_path))
        full_prev = _build_full_preview_from_plan(plan, seed, [], [])
        display_artifacts = full_prev.get('display_artifacts')
        if not display_artifacts:
            try:
                display_artifacts = _attach_display_artifacts(full_prev)
            except Exception:
                display_artifacts = {
                    'segmentation': {
                        'rows': [],
                        'table_rows': [],
                        'tableRows': [],
                        'json': {'rules_count': 0, 'types_summary': {}, 'rules': [], 'metadata': None},
                    },
                    '__version': FULL_PREVIEW_ARTIFACT_VERSION,
                }
        segmentation_artifacts = (display_artifacts or {}).get('segmentation')
        # Annotate & enforce enumerated host roles (Server, Workstation, PC) in preview
        # Full preview already receives normalized roles from planning layer
        # Attempt scenario name
        scenario_name = scenario or None
        if not scenario_name:
            try:
                names_for_cli = _scenario_names_from_xml(xml_path)
                if names_for_cli: scenario_name = names_for_cli[0]
            except Exception:
                pass
        try:
            raw_hitl_config = parse_hitl_info(xml_path, scenario_name)
        except Exception as hitl_exc:
            try:
                app.logger.debug('[plan.full_preview_page] hitl parse failed: %s', hitl_exc)
            except Exception:
                pass
            raw_hitl_config = {"enabled": False, "interfaces": []}
        hitl_config = _sanitize_hitl_config(raw_hitl_config, scenario_name, xml_basename)
        try:
            full_prev['hitl_interfaces'] = hitl_config.get('interfaces', [])
            full_prev['hitl_enabled'] = bool(hitl_config.get('enabled'))
            full_prev['hitl_scenario_key'] = hitl_config.get('scenario_key')
            if hitl_config.get('core'):
                full_prev['hitl_core'] = hitl_config.get('core')
        except Exception:
            pass
        try:
            _merge_hitl_preview_with_full_preview(full_prev, hitl_config)
        except Exception:
            pass
        try:
            _attach_latest_flow_into_full_preview(full_prev, scenario_name)
        except Exception:
            pass
        # Persist preview payload for downstream execution wiring
        preview_plan_path = None
        try:
            import json as _json
            plans_dir = os.path.join(_outputs_dir(), 'plans')
            os.makedirs(plans_dir, exist_ok=True)
            seed_tag = full_prev.get('seed') or 'preview'
            unique_tag = f"{seed_tag}_{int(time.time())}_{uuid.uuid4().hex[:6]}"
            preview_plan_path = os.path.join(plans_dir, f"plan_from_preview_{unique_tag}.json")
            plan_payload = {
                'full_preview': full_prev,
                'metadata': {
                    'xml_path': xml_path,
                    'scenario': scenario_name,
                    'seed': full_prev.get('seed'),
                    'created_at': datetime.datetime.now(datetime.timezone.utc).isoformat().replace('+00:00', 'Z'),
                },
            }
            with open(preview_plan_path, 'w', encoding='utf-8') as pf:
                _json.dump(plan_payload, pf, indent=2, sort_keys=True)
        except Exception as plan_err:
            preview_plan_path = None
            try:
                app.logger.warning('[plan.full_preview_page] failed to persist preview plan: %s', plan_err)
            except Exception:
                pass
        # Provide JSON string for embedding (stringify smaller subset for safety)
        import json as _json
        preview_json_str = _json.dumps(full_prev, indent=2, default=str)
        return render_template(
            'full_preview.html',
            full_preview=full_prev,
            preview_json=preview_json_str,
            xml_path=xml_path,
            scenario=scenario_name,
            seed=full_prev.get('seed'),
            preview_plan_path=preview_plan_path,
            display_artifacts=display_artifacts,
            segmentation_artifacts=segmentation_artifacts,
            hitl_config=hitl_config,
            xml_basename=xml_basename,
            hide_chrome=embed,
        )
    except Exception as e:
        app.logger.exception('[plan.full_preview_page] error: %s', e)
        flash(f'Full preview page error: {e}')
        return redirect(url_for('index'))

def _plan_summary_from_full_preview(full_prev: dict) -> dict:
    try:
        role_counts = full_prev.get('role_counts') or {}
    except Exception:
        role_counts = {}
    hosts_total = len(full_prev.get('hosts') or [])
    routers_planned = len(full_prev.get('routers') or [])
    switches = full_prev.get('switches_detail') or []
    services_plan = full_prev.get('services_plan') or full_prev.get('services_preview') or {}
    vuln_plan = full_prev.get('vulnerabilities_plan') or full_prev.get('vulnerabilities_preview') or {}
    r2r_policy = full_prev.get('r2r_policy_preview') or {}
    r2s_policy = full_prev.get('r2s_policy_preview') or {}
    summary = {
        'hosts_total': hosts_total,
        'routers_planned': routers_planned,
        'hosts_allocated': 0,
        'routers_allocated': 0,
        'role_counts': role_counts,
        'services_plan': services_plan,
        'services_assigned': {},
        'vulnerabilities_plan': vuln_plan,
        'vulnerabilities_assigned': 0,
        'r2r_policy': r2r_policy,
        'r2s_policy': r2s_policy,
        'switches_allocated': len(switches),
        'notes': ['generated_from_full_preview'],
        'full_preview_seed': full_prev.get('seed'),
    }
    return summary

# --- Unified Preview Helpers (ensure modal JSON preview == full page preview) ---
def _derive_routing_policies(routing_items):
    """Derive R2R and R2S policies from routing items (first item wins)."""
    r2r_policy_plan = None
    r2s_policy_plan = None
    try:
        first_r2r = next((ri for ri in (routing_items or []) if getattr(ri,'r2r_mode',None)), None)  # type: ignore
        if first_r2r:
            m = getattr(first_r2r, 'r2r_mode', '')
            if m == 'Exact' and getattr(first_r2r, 'r2r_edges', 0) > 0:
                r2r_policy_plan = { 'mode': 'Exact', 'target_degree': int(getattr(first_r2r,'r2r_edges',0)) }
            elif m:
                r2r_policy_plan = { 'mode': m }
        first_r2s = next((ri for ri in (routing_items or []) if getattr(ri,'r2s_mode',None)), None)  # type: ignore
        if first_r2s:
            m2 = getattr(first_r2s, 'r2s_mode', '')
            if m2 == 'Exact' and getattr(first_r2s, 'r2s_edges', 0) > 0:
                r2s_policy_plan = { 'mode': 'Exact', 'target_per_router': int(getattr(first_r2s,'r2s_edges',0)) }
            elif m2:
                r2s_policy_plan = { 'mode': m2 }
    except Exception:
        pass
    return r2r_policy_plan, r2s_policy_plan

def _json_ready(value):
    """Convert nested objects into JSON-friendly primitives (recursively)."""
    if isinstance(value, dict):
        return {k: _json_ready(v) for k, v in value.items()}
    if isinstance(value, (list, tuple, set)):
        return [_json_ready(v) for v in value]
    if isinstance(value, (str, int, float, bool)) or value is None:
        return value
    if hasattr(value, '__dict__'):
        try:
            return {k: _json_ready(v) for k, v in vars(value).items() if not k.startswith('_')}
        except Exception:
            pass
    try:
        return str(value)
    except Exception:
        return repr(value)


def _summarize_seg_rule(rule_dict: dict) -> str:
    if not isinstance(rule_dict, dict):
        return ''
    type_raw = rule_dict.get('type') or rule_dict.get('action') or ''
    type_str = str(type_raw).strip()
    if not type_str:
        return ''
    lower = type_str.lower()
    if lower == 'nat':
        mode = str(rule_dict.get('mode')).strip() if rule_dict.get('mode') not in (None, '') else ''
        internal = rule_dict.get('internal') or rule_dict.get('internal_subnet') or ''
        external = rule_dict.get('external') or rule_dict.get('external_subnet') or ''
        parts = []
        if mode:
            parts.append(mode)
        if internal:
            parts.append(str(internal))
        summary = ' '.join(parts)
        if internal and external:
            summary = f"{summary} -> {external}" if summary else f"{internal} -> {external}"
        elif external:
            summary = f"{summary} {external}".strip()
        return summary.strip()
    if lower == 'host_block':
        src = rule_dict.get('src') or rule_dict.get('source') or ''
        dst = rule_dict.get('dst') or rule_dict.get('destination') or ''
        return f"{src} X {dst}".strip()
    if lower == 'custom':
        desc = rule_dict.get('description') or rule_dict.get('summary') or ''
        desc_str = str(desc).strip()
        return desc_str or 'custom'
    src = rule_dict.get('src') or rule_dict.get('source')
    dst = rule_dict.get('dst') or rule_dict.get('destination')
    if src or dst:
        return f"{type_str}: {src or '*'} -> {dst or '*'}"
    return type_str


def _build_segmentation_display_artifacts(full_preview: dict) -> dict:
    seg_preview = {}
    try:
        seg_preview = _json_ready((full_preview or {}).get('segmentation_preview') or {})
    except Exception:
        seg_preview = {}
    raw_rules = []
    if isinstance(seg_preview, dict):
        raw_rules = seg_preview.get('rules') or []
    if not isinstance(raw_rules, list):
        raw_rules = []
    entries = []
    type_counts = {}
    for raw_entry in raw_rules:
        entry = _json_ready(raw_entry)
        if not isinstance(entry, dict):
            continue
        rule_dict = entry.get('rule')
        if rule_dict is None:
            rule_dict = entry
        rule_dict = _json_ready(rule_dict)
        if not isinstance(rule_dict, dict):
            continue
        node_id = entry.get('node_id', rule_dict.get('node_id'))
        rule_type = rule_dict.get('type') or rule_dict.get('action')
        rule_type_str = str(rule_type) if rule_type not in (None, '') else None
        summary = _summarize_seg_rule(rule_dict)
        script_path = entry.get('script') or rule_dict.get('script')
        if not isinstance(script_path, str):
            script_path = None
        script_name = os.path.basename(script_path) if script_path else None
        table_row = {
            'node_id': node_id,
            'type': rule_type_str,
            'summary': summary,
            'src': rule_dict.get('src') or rule_dict.get('source'),
            'dst': rule_dict.get('dst') or rule_dict.get('destination'),
            'subnet': rule_dict.get('subnet'),
            'internal': rule_dict.get('internal') or rule_dict.get('internal_subnet'),
            'external': rule_dict.get('external') or rule_dict.get('external_subnet'),
            'proto': rule_dict.get('proto') or rule_dict.get('protocol'),
            'port': rule_dict.get('port'),
            'script_path': script_path,
            'script_name': script_name,
            'detail': rule_dict,
        }
        entries.append(table_row)
        key = rule_type_str or 'unknown'
        type_counts[key] = type_counts.get(key, 0) + 1
    metadata = None
    if isinstance(seg_preview, dict):
        metadata = {k: v for k, v in seg_preview.items() if k != 'rules'}
        if not metadata:
            metadata = None
    result = {
        'rows': [{'node_id': e['node_id'], 'type': e['type'], 'summary': e['summary']} for e in entries],
        'table_rows': entries,
        'tableRows': entries,
        'json': {
            'rules_count': len(entries),
            'types_summary': type_counts,
            'rules': [
                {
                    'node_id': e['node_id'],
                    'type': e['type'],
                    'summary': e['summary'],
                    'detail': e['detail'],
                }
                for e in entries
            ],
            'metadata': metadata,
        },
    }
    return result


def _attach_display_artifacts(full_preview: dict) -> dict:
    artifacts = {
        'segmentation': _build_segmentation_display_artifacts(full_preview),
        '__version': FULL_PREVIEW_ARTIFACT_VERSION,
    }
    if isinstance(full_preview, dict):
        full_preview['display_artifacts'] = artifacts
        full_preview['display_artifacts_version'] = FULL_PREVIEW_ARTIFACT_VERSION
    return artifacts


def _build_full_preview_from_plan(plan: dict, seed, r2s_hosts_min_list=None, r2s_hosts_max_list=None):
    """Single source of truth to invoke build_full_preview using a compute_full_plan result."""
    try:
        from core_topo_gen.planning.full_preview import build_full_preview  # lazy import
    except ModuleNotFoundError:
        if _ensure_full_preview_module():
            from core_topo_gen.planning.full_preview import build_full_preview  # type: ignore
        else:
            raise
    role_counts = plan['role_counts']
    prelim_router_count = plan['routers_planned']
    routing_items = plan.get('routing_items') or []
    service_plan = plan.get('service_plan') or {}
    vplan = plan.get('vulnerability_plan') or {}
    seg_items_serial = plan.get('breakdowns', {}).get('segmentation', {}).get('raw_items_serialized') or []
    seg_density = plan.get('breakdowns', {}).get('segmentation', {}).get('density')
    r2r_policy_plan, r2s_policy_plan = _derive_routing_policies(routing_items)
    fp = build_full_preview(
        role_counts=role_counts,
        routers_planned=prelim_router_count,
        services_plan=service_plan,
        vulnerabilities_plan=vplan,
        r2r_policy=r2r_policy_plan,
        r2s_policy=r2s_policy_plan,
        routing_items=routing_items,
        routing_plan=plan.get('breakdowns', {}).get('router', {}).get('simple_plan', {}),
        segmentation_density=seg_density,
        segmentation_items=seg_items_serial,
        traffic_plan=plan.get('traffic_plan'),
        seed=seed,
        ip4_prefix='10.0.0.0/24',
        r2s_hosts_min_list=r2s_hosts_min_list,
        r2s_hosts_max_list=r2s_hosts_max_list,
        base_scenario=plan.get('base_scenario'),
    )
    fp['router_plan'] = plan.get('breakdowns', {}).get('router', {})
    try:
        _attach_display_artifacts(fp)
    except Exception:
        pass
    return fp


@app.route('/api/open_scripts', methods=['GET'])
def api_open_scripts():
    """Return a listing of traffic or segmentation script directory contents.

    Query params: kind=traffic|segmentation
    """
    kind = request.args.get('kind','traffic').lower()
    scope = request.args.get('scope','runtime').lower()  # runtime|preview
    if kind not in ('traffic','segmentation'):
        return jsonify({'ok': False, 'error': 'invalid kind'}), 400
    if scope == 'preview':
        # Look for latest preview dir (deterministic naming core-topo-preview-*)
        import tempfile, glob
        base = tempfile.gettempdir()
        pattern = 'core-topo-preview-traffic-*' if kind=='traffic' else 'core-topo-preview-seg-*'
        candidates = sorted(glob.glob(os.path.join(base, pattern)), key=lambda p: os.path.getmtime(p), reverse=True)
        path = candidates[0] if candidates else None
        if not path:
            return jsonify({'ok': False, 'error': 'no preview dir found for kind', 'pattern': pattern}), 404
    else:
        path = '/tmp/traffic' if kind == 'traffic' else '/tmp/segmentation'
    if not os.path.isdir(path):
        return jsonify({'ok': False, 'error': 'directory does not exist', 'path': path}), 404
    files = []
    try:
        for name in sorted(os.listdir(path)):
            fp = os.path.join(path, name)
            if not os.path.isfile(fp):
                continue
            try:
                sz = os.path.getsize(fp)
            except Exception:
                sz = 0
            files.append({'file': name, 'size': sz})
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)}), 500
    return jsonify({'ok': True, 'kind': kind, 'path': path, 'files': files})

@app.route('/api/open_script_file', methods=['GET'])
def api_open_script_file():
    """Return (truncated) contents of a requested script file.

    Query params: kind=traffic|segmentation, scope=runtime|preview, file=<filename>
    """
    kind = request.args.get('kind','traffic').lower()
    scope = request.args.get('scope','runtime').lower()
    fname = request.args.get('file') or ''
    if kind not in ('traffic','segmentation'):
        return jsonify({'ok': False, 'error': 'invalid kind'}), 400
    if not fname or '/' in fname or '..' in fname:
        return jsonify({'ok': False, 'error': 'invalid filename'}), 400
    if scope == 'preview':
        import tempfile, glob
        base = tempfile.gettempdir()
        pattern = 'core-topo-preview-traffic-*' if kind=='traffic' else 'core-topo-preview-seg-*'
        candidates = sorted(glob.glob(os.path.join(base, pattern)), key=lambda p: os.path.getmtime(p), reverse=True)
        path = candidates[0] if candidates else None
    else:
        path = '/tmp/traffic' if kind == 'traffic' else '/tmp/segmentation'
    if not path or not os.path.isdir(path):
        return jsonify({'ok': False, 'error': 'dir not found', 'path': path}), 404
    fp = os.path.join(path, fname)
    if not os.path.isfile(fp):
        return jsonify({'ok': False, 'error': 'file not found', 'file': fname}), 404
    try:
        with open(fp, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(8000)
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)}), 500
    return jsonify({'ok': True, 'file': fname, 'path': path, 'content': content, 'truncated': len(content)==8000})

@app.route('/api/download_scripts', methods=['GET'])
def api_download_scripts():
    """Download a zip of segmentation or traffic scripts (preview or runtime).

    Query: kind=traffic|segmentation scope=runtime|preview
    """
    kind = request.args.get('kind','traffic').lower()
    scope = request.args.get('scope','runtime').lower()
    if kind not in ('traffic','segmentation'):
        return jsonify({'ok': False, 'error': 'invalid kind'}), 400
    if scope not in ('runtime','preview'):
        return jsonify({'ok': False, 'error': 'invalid scope'}), 400
    # Resolve directory
    if scope == 'runtime':
        base_dir = '/tmp/traffic' if kind=='traffic' else '/tmp/segmentation'
    else:
        import tempfile, glob
        pattern = 'core-topo-preview-traffic-*' if kind=='traffic' else 'core-topo-preview-seg-*'
        cands = sorted(glob.glob(os.path.join(tempfile.gettempdir(), pattern)), key=lambda p: os.path.getmtime(p), reverse=True)
        base_dir = cands[0] if cands else None
    if not base_dir or not os.path.isdir(base_dir):
        return jsonify({'ok': False, 'error': 'directory not found'}), 404
    import io, zipfile
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, 'w', zipfile.ZIP_DEFLATED) as zf:
        for root, _dirs, files in os.walk(base_dir):
            for f in files:
                fp = os.path.join(root, f)
                # avoid huge non-script artifacts except summary json
                if not (f.endswith('.py') or f.endswith('.json')):
                    continue
                arc = os.path.relpath(fp, base_dir)
                try:
                    zf.write(fp, arc)
                except Exception:
                    continue
    buf.seek(0)
    from flask import send_file as _send_file
    filename = f"{kind}_{scope}_scripts.zip"
    return _send_file(buf, mimetype='application/zip', as_attachment=True, download_name=filename)

@app.route('/download_report')
def download_report():
    result_path = request.args.get('path')
    # Normalize incoming value: strip quotes, decode percent-encoding, handle file://, expand ~
    try:
        if result_path:
            # strip surrounding quotes if present
            if (result_path.startswith('"') and result_path.endswith('"')) or (result_path.startswith("'") and result_path.endswith("'")):
                result_path = result_path[1:-1]
            # convert file:// URIs
            if result_path.startswith('file://'):
                result_path = result_path[len('file://'):]
            # percent-decode
            try:
                from urllib.parse import unquote
                result_path = unquote(result_path)
            except Exception:
                pass
            # expand ~ and normalize slashes
            result_path = os.path.expanduser(result_path)
            result_path = os.path.normpath(result_path)
    except Exception:
        pass
    # Attempt to resolve common path variants to absolute existing file
    candidates = []
    if result_path:
        candidates.append(result_path)
        # Absolute from repo root if provided as repo-relative
        try:
            repo_root = _get_repo_root()
            if not os.path.isabs(result_path):
                candidates.append(os.path.abspath(os.path.join(repo_root, result_path)))
            # Also try if client included an extra 'webapp/' segment
            if result_path.startswith('webapp' + os.sep):
                candidates.append(os.path.abspath(os.path.join(repo_root, result_path)))
                # Strip 'webapp/' and try from repo root
                candidates.append(os.path.abspath(os.path.join(repo_root, result_path.split(os.sep, 1)[-1])))
            # If path looks like outputs/<...>, join with configured outputs dir
            if result_path.startswith('outputs' + os.sep):
                candidates.append(os.path.abspath(os.path.join(_outputs_dir(), result_path.split(os.sep, 1)[-1])))
            # If absolute path contains '/webapp/outputs/...', remap to configured outputs dir
            rp_norm = os.path.normpath(result_path)
            parts = rp_norm.strip(os.sep).split(os.sep)
            if os.path.isabs(result_path) and 'outputs' in parts:
                try:
                    idx = parts.index('outputs')
                    tail = os.path.join(*parts[idx+1:]) if idx+1 < len(parts) else ''
                    candidates.append(os.path.join(_outputs_dir(), tail))
                except Exception:
                    pass
            if os.path.isabs(result_path) and 'webapp' in parts:
                # Remove the 'webapp' segment entirely
                parts_wo = [p for p in parts if p != 'webapp']
                candidates.append(os.path.sep + os.path.join(*parts_wo))
            # If the path already lives under our configured outputs dir but with different root, try direct mapping
            try:
                outputs_dir = os.path.abspath(_outputs_dir())
                if os.path.isabs(result_path) and 'core-sessions' in parts and not result_path.startswith(outputs_dir):
                    # replace everything up to 'core-sessions' with outputs_dir/core-sessions
                    idx = parts.index('core-sessions')
                    tail = os.path.join(*parts[idx+1:]) if idx+1 < len(parts) else ''
                    candidates.append(os.path.join(outputs_dir, 'core-sessions', tail))
            except Exception:
                pass
        except Exception:
            pass
    # Pick the first existing path
    chosen = None
    for p in candidates:
        if p and os.path.exists(p):
            chosen = p
            break
    if chosen:
        try:
            app.logger.info("[download] serving file: %s", os.path.abspath(chosen))
        except Exception:
            pass
        return send_file(chosen, as_attachment=True)
    # Fallback: try to match by basename within outputs/core-sessions and outputs/scenarios-*
    try:
        # Log diagnostics about missing primary candidates
        app.logger.warning("[download] file not found via direct candidates; requested=%s; candidates=%s", result_path, candidates)
    except Exception:
        pass
    try:
        base_name = None
        try:
            base_name = os.path.basename(result_path) if result_path else None
        except Exception:
            base_name = None
        if base_name and base_name.lower().endswith('.xml'):
            candidates_found = []
            # Search core-sessions
            root_dir = os.path.join(_outputs_dir(), 'core-sessions')
            if os.path.exists(root_dir):
                for dp, _dn, files in os.walk(root_dir):
                    for fn in files:
                        if fn == base_name:
                            alt = os.path.join(dp, fn)
                            if os.path.exists(alt):
                                candidates_found.append(alt)
            # Search scenarios-* (Scenario Editor saves)
            out_dir = _outputs_dir()
            if os.path.exists(out_dir):
                try:
                    for name in os.listdir(out_dir):
                        if not name.startswith('scenarios-'):
                            continue
                        p = os.path.join(out_dir, name)
                        if not os.path.isdir(p):
                            continue
                        for dp, _dn, files in os.walk(p):
                            for fn in files:
                                if fn == base_name:
                                    alt = os.path.join(dp, fn)
                                    if os.path.exists(alt):
                                        candidates_found.append(alt)
                except Exception:
                    pass
            if candidates_found:
                # Prefer the newest by mtime
                try:
                    candidates_found.sort(key=lambda p: os.stat(p).st_mtime, reverse=True)
                except Exception:
                    pass
                chosen_alt = candidates_found[0]
                app.logger.info("[download] basename match: %s -> %s", base_name, chosen_alt)
                return send_file(chosen_alt, as_attachment=True)
    except Exception:
        pass
    app.logger.warning("[download] file not found: %s (candidates=%s)", result_path, candidates)
    return "File not found", 404

@app.route('/reports')
def reports_page():
    raw = _load_run_history()
    enriched = []
    for entry in raw:
        e = dict(entry)
        # Per-scenario: always show exactly one scenario name.
        if not (isinstance(e.get('scenario_names'), list) and e.get('scenario_names')):
            scen = (e.get('scenario_name') or '').strip() if isinstance(e.get('scenario_name'), str) else ''
            if scen:
                e['scenario_names'] = [scen]
            else:
                src_xml = e.get('single_scenario_xml_path') or e.get('scenario_xml_path') or e.get('xml_path')
                names = _scenario_names_from_xml(src_xml) if src_xml else []
                e['scenario_names'] = [names[0]] if isinstance(names, list) and names else []
        # Normalize session xml pointer for UI compatibility
        session_xml = e.get('session_xml_path') or e.get('post_xml_path')
        if session_xml:
            e['session_xml_path'] = session_xml
        if not e.get('summary_path'):
            derived_summary = _derive_summary_from_report(e.get('report_path'))
            if derived_summary:
                e['summary_path'] = derived_summary
        # Hardening: ensure scenario_names is always a list (and truncate to 1)
        sn = e.get('scenario_names')
        if not isinstance(sn, list):
            if sn is None:
                e['scenario_names'] = []
            elif isinstance(sn, str):
                # Split comma or pipe delimited legacy forms
                if '||' in sn:
                    e['scenario_names'] = [s for s in sn.split('||') if s]
                else:
                    e['scenario_names'] = [s.strip() for s in sn.split(',') if s.strip()]
            else:
                e['scenario_names'] = []
        if isinstance(e.get('scenario_names'), list) and len(e['scenario_names']) > 1:
            e['scenario_names'] = [e['scenario_names'][0]]
        enriched.append(e)
    enriched = sorted(enriched, key=lambda x: x.get('timestamp',''), reverse=True)
    user = _current_user()
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(
        enriched,
        user=user,
    )
    scenario_participant_urls = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
    participant_url_flags = {
        norm: bool(url)
        for norm, url in scenario_participant_urls.items()
        if isinstance(norm, str) and norm
    }
    scenario_query = request.args.get('scenario', '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    scenario_names, scenario_norm, _allowed_norms = _builder_filter_report_scenarios(
        scenario_names,
        scenario_norm,
        user=user,
    )
    # Enforce per-scenario view: default to the first available scenario when unset.
    if scenario_names and not scenario_norm:
        scenario_norm = _normalize_scenario_label(scenario_names[0])
    if scenario_norm:
        enriched = _filter_history_by_scenario(enriched, scenario_norm)
    scenario_display = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query)
    return render_template(
        'reports.html',
        history=enriched,
        scenarios=scenario_names,
        active_scenario=scenario_display,
        participant_url_flags=participant_url_flags,
    )

@app.route('/reports_data')
def reports_data():
    raw = _load_run_history()
    enriched = []
    for entry in raw:
        e = dict(entry)
        # Per-scenario: always show exactly one scenario name.
        if not (isinstance(e.get('scenario_names'), list) and e.get('scenario_names')):
            scen = (e.get('scenario_name') or '').strip() if isinstance(e.get('scenario_name'), str) else ''
            if scen:
                e['scenario_names'] = [scen]
            else:
                src_xml = e.get('single_scenario_xml_path') or e.get('scenario_xml_path') or e.get('xml_path')
                names = _scenario_names_from_xml(src_xml) if src_xml else []
                e['scenario_names'] = [names[0]] if isinstance(names, list) and names else []
        session_xml = e.get('session_xml_path') or e.get('post_xml_path')
        if session_xml:
            e['session_xml_path'] = session_xml
        if not e.get('summary_path'):
            derived_summary = _derive_summary_from_report(e.get('report_path'))
            if derived_summary:
                e['summary_path'] = derived_summary
        # Hardening: normalize scenario_names to list (and truncate to 1)
        sn = e.get('scenario_names')
        if not isinstance(sn, list):
            if sn is None:
                e['scenario_names'] = []
            elif isinstance(sn, str):
                if '||' in sn:
                    e['scenario_names'] = [s for s in sn.split('||') if s]
                else:
                    e['scenario_names'] = [s.strip() for s in sn.split(',') if s.strip()]
            else:
                e['scenario_names'] = []
        if isinstance(e.get('scenario_names'), list) and len(e['scenario_names']) > 1:
            e['scenario_names'] = [e['scenario_names'][0]]
        enriched.append(e)
    enriched = sorted(enriched, key=lambda x: x.get('timestamp',''), reverse=True)
    user = _current_user()
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(
        enriched,
        user=user,
    )
    scenario_participant_urls = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
    participant_url_flags = {
        norm: bool(url)
        for norm, url in scenario_participant_urls.items()
        if isinstance(norm, str) and norm
    }
    scenario_query = request.args.get('scenario', '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    scenario_names, scenario_norm, _allowed_norms = _builder_filter_report_scenarios(
        scenario_names,
        scenario_norm,
        user=user,
    )
    # Enforce per-scenario view: default to the first available scenario when unset.
    if scenario_names and not scenario_norm:
        scenario_norm = _normalize_scenario_label(scenario_names[0])
    if scenario_norm:
        enriched = _filter_history_by_scenario(enriched, scenario_norm)
    scenario_display = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query)
    return jsonify({
        'history': enriched,
        'scenarios': scenario_names,
        'active_scenario': scenario_display,
        'participant_url_flags': participant_url_flags,
    })

@app.route('/reports/delete', methods=['POST'])
def reports_delete():
    """Delete run history entries by run_id and remove associated artifacts under outputs/.
    Does not delete files under ./reports (reports are preserved by policy).
    Body: { "run_ids": ["...", ...] }
    """
    try:
        payload = request.get_json(force=True, silent=True) or {}
        run_ids = payload.get('run_ids') or []
        if not isinstance(run_ids, list):
            return jsonify({ 'error': 'run_ids must be a list' }), 400
        run_ids_set = set([str(x) for x in run_ids if x])
        if not run_ids_set:
            return jsonify({ 'deleted': 0 })
        history = _load_run_history()
        kept = []
        deleted_count = 0
        outputs_dir = _outputs_dir()
        for entry in history:
            rid = str(entry.get('run_id') or '')
            # fallback composite id to support entries without run_id
            rid_fallback = "|".join([
                str(entry.get('timestamp') or ''),
                str(entry.get('scenario_xml_path') or entry.get('xml_path') or ''),
                str(entry.get('report_path') or ''),
                str(entry.get('full_scenario_path') or ''),
            ])
            if (rid and rid in run_ids_set) or (rid_fallback and rid_fallback in run_ids_set):
                # Delete artifacts scoped to outputs/ only
                for key in ('full_scenario_path','scenario_xml_path','pre_xml_path','post_xml_path','xml_path','single_scenario_xml_path'):
                    p = entry.get(key)
                    if not p: continue
                    try:
                        ap = os.path.abspath(p)
                        if ap.startswith(os.path.abspath(outputs_dir)) and os.path.exists(ap):
                            try:
                                os.remove(ap)
                                app.logger.info("[reports.delete] removed %s", ap)
                            except IsADirectoryError:
                                # just in case, do not remove directories recursively here
                                app.logger.warning("[reports.delete] skipping directory %s", ap)
                    except Exception as e:
                        app.logger.warning("[reports.delete] error removing %s: %s", p, e)
                deleted_count += 1
            else:
                kept.append(entry)
        # Persist pruned history
        os.makedirs(os.path.dirname(RUN_HISTORY_PATH), exist_ok=True)
        tmp = RUN_HISTORY_PATH + '.tmp'
        with open(tmp, 'w', encoding='utf-8') as f:
            json.dump(kept, f, indent=2)
        os.replace(tmp, RUN_HISTORY_PATH)
        return jsonify({ 'deleted': deleted_count })
    except Exception as e:
        app.logger.exception("[reports.delete] failed: %s", e)
        return jsonify({ 'error': 'internal error' }), 500


def _close_async_run_tunnel(meta: Dict[str, Any]) -> None:
    tunnel = meta.pop('ssh_tunnel', None)
    if tunnel:
        try:
            tunnel.close()
        except Exception:
            pass


def _append_async_run_log_line(meta: Dict[str, Any] | None, line: str) -> None:
    if not meta:
        return
    lp = meta.get('log_path')
    if not lp:
        return
    try:
        with open(lp, 'a', encoding='utf-8', buffering=1) as _f:
            _f.write(line.rstrip('\n') + "\n")
    except Exception:
        pass


def _maybe_copy_flow_artifacts_into_containers(meta: Dict[str, Any] | None, *, stage: str, log_prefix: str = '[remote] ') -> None:
    if not meta:
        return
    if not meta.get('remote'):
        return
    if meta.get('flow_artifacts_copied'):
        return
    cfg = meta.get('core_cfg')
    if not isinstance(cfg, dict):
        return

    try:
        _log_remote_vulns_inventory(meta, stage=f'before_copy.{stage}', log_prefix=log_prefix)
    except Exception:
        pass

    _append_async_run_log_line(meta, f"{log_prefix}=== docker.copy_flow_artifacts({stage}) ===")
    try:
        payload = _run_remote_python_json(
            cfg,
            _remote_copy_flow_artifacts_into_containers_script(cfg.get('ssh_password')),
            logger=app.logger,
            label=f'docker.copy_flow_artifacts({stage})',
            timeout=180.0,
        )
        items = payload.get('items') if isinstance(payload, dict) else None
        copied_total = len(items or []) if isinstance(items, list) else 0
        copied_ok = (
            sum(1 for it in (items or []) if isinstance(it, dict) and it.get('ok'))
            if isinstance(items, list)
            else 0
        )
        _append_async_run_log_line(
            meta,
            f"{log_prefix}docker.copy_flow_artifacts({stage}) complete ok={int(copied_ok)} total={int(copied_total)}",
        )
        if isinstance(items, list) and items:
            for it in items[:25]:
                if not isinstance(it, dict):
                    continue
                node = it.get('node')
                ok = bool(it.get('ok'))
                src = it.get('src')
                dest = it.get('dest')
                targets = it.get('targets')
                err = it.get('error') or ''
                errs = it.get('errors') or []
                err_tail = ''
                try:
                    if err:
                        err_tail = str(err)
                    elif isinstance(errs, list) and errs:
                        err_tail = _summarize_for_log(str(errs[0]))
                except Exception:
                    err_tail = ''
                target_desc = None
                try:
                    if isinstance(targets, list) and targets:
                        target_desc = ','.join(str(x) for x in targets[:3])
                except Exception:
                    target_desc = None
                detail = f"node={node} ok={ok} src={src} dest={dest}"
                if target_desc:
                    detail += f" targets={target_desc}"
                if err_tail:
                    detail += f" error={err_tail}"
                _append_async_run_log_line(meta, f"{log_prefix}{detail}")

        # Verify: list /flow_artifacts inside target container(s) so logs prove the copy worked.
        try:
            verify_targets: List[str] = []
            if isinstance(items, list):
                for it in items:
                    if not isinstance(it, dict) or not it.get('ok'):
                        continue
                    targets = it.get('targets')
                    if isinstance(targets, list):
                        for t in targets:
                            name = str(t or '').strip()
                            if name:
                                verify_targets.append(name)
            seen: set[str] = set()
            uniq: List[str] = []
            for t in verify_targets:
                if t in seen:
                    continue
                seen.add(t)
                uniq.append(t)
                if len(uniq) >= 5:
                    break

            if uniq:
                _append_async_run_log_line(meta, f"{log_prefix}=== docker.exec.verify_flow_artifacts({stage}) ===")
                verify_payload = _run_remote_python_json(
                    cfg,
                    _remote_docker_exec_flow_artifacts_listing_script(
                        containers=uniq,
                        sudo_password=cfg.get('ssh_password'),
                        max_find=200,
                    ),
                    logger=app.logger,
                    label=f'docker.exec.verify_flow_artifacts({stage})',
                    timeout=90.0,
                )
                vitems = verify_payload.get('items') if isinstance(verify_payload, dict) else None
                if isinstance(vitems, list):
                    for vit in vitems[:5]:
                        if not isinstance(vit, dict):
                            continue
                        container = vit.get('container')
                        ok2 = bool(vit.get('ok'))
                        rc2 = vit.get('rc')
                        out2 = vit.get('output') or ''
                        _append_async_run_log_line(meta, f"{log_prefix}docker.exec.verify container={container} ok={ok2} rc={rc2}")
                        for line in str(out2).splitlines()[:60]:
                            _append_async_run_log_line(meta, f"{log_prefix}{line}")
        except Exception as exc_verify:
            _append_async_run_log_line(meta, f"{log_prefix}docker.exec.verify_flow_artifacts({stage}) failed: {exc_verify}")
        meta['flow_artifacts_copied'] = True
    except Exception as exc:
        _append_async_run_log_line(meta, f"{log_prefix}docker.copy_flow_artifacts({stage}) failed: {exc}")


def _remote_vulns_inventory_script(base_dir: str | None) -> str:
    base_dir_literal = json.dumps(str(base_dir) if base_dir else '')
    return (
        r"""
import glob, json, os

BASE_DIR = __BASE_DIR_LITERAL__


def _safe_exists(p: str) -> bool:
    try:
        return bool(p) and os.path.exists(p)
    except Exception:
        return False


def _first_existing(paths):
    for p in paths:
        if not p:
            continue
        if _safe_exists(p):
            return p
    return None


def _list_dirs(pattern: str, limit: int = 10):
    out = []
    try:
        for p in sorted(glob.glob(pattern)):
            try:
                if os.path.isdir(p):
                    out.append(p)
            except Exception:
                continue
            if len(out) >= limit:
                break
    except Exception:
        return []
    return out


def _count_files_under(path: str, limit: int = 2000) -> int:
    count = 0
    try:
        for _, _, files in os.walk(path):
            count += len(files or [])
            if count >= limit:
                return limit
    except Exception:
        return -1
    return count


def main():
    base = BASE_DIR or os.environ.get('CORE_REMOTE_BASE_DIR') or '/tmp/core-topo-gen'
    base = os.path.abspath(base)
    candidates = {
        'base_dir': base,
        'tmp_vulns': '/tmp/vulns',
        'base_vulns': os.path.join(base, 'vulns'),
        'base_outputs_vulns': os.path.join(base, 'outputs', 'vulns'),
    }

    assignments_candidates = [
        os.path.join(base, 'vulns', 'compose_assignments.json'),
        os.path.join(base, 'outputs', 'vulns', 'compose_assignments.json'),
        os.path.join(base, 'compose_assignments.json'),
        '/tmp/vulns/compose_assignments.json',
    ]
    assignments_path = _first_existing(assignments_candidates)

    run_dirs = []
    for root in (candidates.get('tmp_vulns'), candidates.get('base_vulns'), candidates.get('base_outputs_vulns')):
        if not root:
            continue
        run_dirs.extend(_list_dirs(os.path.join(root, 'flag_generators_runs', '*'), limit=10))
        run_dirs.extend(_list_dirs(os.path.join(root, 'flag_node_generators_runs', '*'), limit=10))

    run_dir_summaries = []
    for p in run_dirs[:10]:
        run_dir_summaries.append({
            'path': p,
            'files': _count_files_under(p, limit=2000),
        })

    payload = {
        'ok': True,
        'base_dir': base,
        'paths': {k: {'path': v, 'exists': _safe_exists(v)} for k, v in candidates.items()},
        'compose_assignments': {
            'candidates': assignments_candidates,
            'found': assignments_path,
            'found_exists': bool(assignments_path and _safe_exists(assignments_path)),
        },
        'run_dirs': run_dir_summaries,
    }
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    ).replace('__BASE_DIR_LITERAL__', base_dir_literal)


def _log_remote_vulns_inventory(meta: Dict[str, Any] | None, *, stage: str, log_prefix: str = '[remote] ') -> None:
    if not meta or not meta.get('remote'):
        return
    cfg = meta.get('core_cfg')
    if not isinstance(cfg, dict):
        return
    base_dir = meta.get('remote_base_dir') or (meta.get('remote_context') or {}).get('base_dir')

    _append_async_run_log_line(meta, f"{log_prefix}=== remote.vulns_inventory({stage}) ===")
    try:
        payload = _run_remote_python_json(
            cfg,
            _remote_vulns_inventory_script(str(base_dir) if base_dir else None),
            logger=app.logger,
            label=f'remote.vulns_inventory({stage})',
            timeout=60.0,
        )
    except Exception as exc:
        _append_async_run_log_line(meta, f"{log_prefix}remote.vulns_inventory({stage}) failed: {exc}")
        return

    try:
        base = payload.get('base_dir') if isinstance(payload, dict) else None
        if base:
            _append_async_run_log_line(meta, f"{log_prefix}CORE_REMOTE_BASE_DIR={base}")
    except Exception:
        pass
    try:
        paths = payload.get('paths') if isinstance(payload, dict) else None
        if isinstance(paths, dict):
            for key in ('tmp_vulns', 'base_vulns', 'base_outputs_vulns'):
                entry = paths.get(key)
                if isinstance(entry, dict):
                    _append_async_run_log_line(
                        meta,
                        f"{log_prefix}{key} path={entry.get('path')} exists={bool(entry.get('exists'))}",
                    )
    except Exception:
        pass
    try:
        ca = payload.get('compose_assignments') if isinstance(payload, dict) else None
        if isinstance(ca, dict):
            _append_async_run_log_line(
                meta,
                f"{log_prefix}compose_assignments found={ca.get('found')} exists={bool(ca.get('found_exists'))}",
            )
    except Exception:
        pass
    try:
        runs = payload.get('run_dirs') if isinstance(payload, dict) else None
        if isinstance(runs, list) and runs:
            _append_async_run_log_line(meta, f"{log_prefix}run_dirs found={len(runs)} (showing up to 10)")
            for it in runs[:10]:
                if not isinstance(it, dict):
                    continue
                _append_async_run_log_line(meta, f"{log_prefix}run_dir path={it.get('path')} files={it.get('files')}")
        else:
            _append_async_run_log_line(meta, f"{log_prefix}run_dirs found=0")
    except Exception:
        pass


def _log_remote_vulns_inventory_to_handle(
    *,
    core_cfg: Dict[str, Any],
    log_handle: Any,
    stage: str,
    base_dir: str | None = None,
    log_prefix: str = '[remote] ',
) -> None:
    """Write the same inventory info as _log_remote_vulns_inventory, but without requiring RUNS meta.

    This is useful for early-run diagnostics before RUNS[run_id] is populated, or
    in failure paths where we never reach the postrun copy stage.
    """
    try:
        log_handle.write(f"{log_prefix}=== remote.vulns_inventory({stage}) ===\n")
    except Exception:
        return

    try:
        payload = _run_remote_python_json(
            core_cfg,
            _remote_vulns_inventory_script(str(base_dir) if base_dir else None),
            logger=app.logger,
            label=f'remote.vulns_inventory({stage})',
            timeout=60.0,
        )
    except Exception as exc:
        try:
            log_handle.write(f"{log_prefix}remote.vulns_inventory({stage}) failed: {exc}\n")
        except Exception:
            pass
        return

    def _w(line: str) -> None:
        try:
            log_handle.write(line.rstrip('\n') + "\n")
        except Exception:
            pass

    try:
        base = payload.get('base_dir') if isinstance(payload, dict) else None
        if base:
            _w(f"{log_prefix}CORE_REMOTE_BASE_DIR={base}")
    except Exception:
        pass
    try:
        paths = payload.get('paths') if isinstance(payload, dict) else None
        if isinstance(paths, dict):
            for key in ('tmp_vulns', 'base_vulns', 'base_outputs_vulns'):
                entry = paths.get(key)
                if isinstance(entry, dict):
                    _w(f"{log_prefix}{key} path={entry.get('path')} exists={bool(entry.get('exists'))}")
    except Exception:
        pass
    try:
        ca = payload.get('compose_assignments') if isinstance(payload, dict) else None
        if isinstance(ca, dict):
            _w(f"{log_prefix}compose_assignments found={ca.get('found')} exists={bool(ca.get('found_exists'))}")
    except Exception:
        pass
    try:
        runs = payload.get('run_dirs') if isinstance(payload, dict) else None
        if isinstance(runs, list) and runs:
            _w(f"{log_prefix}run_dirs found={len(runs)} (showing up to 10)")
            for it in runs[:10]:
                if not isinstance(it, dict):
                    continue
                _w(f"{log_prefix}run_dir path={it.get('path')} files={it.get('files')}")
        else:
            _w(f"{log_prefix}run_dirs found=0")
    except Exception:
        pass


def _remote_docker_exec_flow_artifacts_listing_script(
    *,
    containers: List[str],
    sudo_password: str | None = None,
    max_find: int = 200,
) -> str:
    containers_literal = json.dumps([str(x) for x in (containers or [])])
    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    max_find_literal = json.dumps(int(max_find))
    return (
        r"""
import json, subprocess

CONTAINERS = __CONTAINERS_LITERAL__
SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__
MAX_FIND = __MAX_FIND_LITERAL__


def _run(cmd, timeout=25):
    try:
        p = subprocess.run(['sudo', '-n'] + list(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', ''] + list(cmd),
                input=str(SUDO_PASSWORD) + "\n",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        return subprocess.CompletedProcess(cmd, 127, stdout=str(e))


def main():
    items = []
    for c in CONTAINERS:
        c = str(c or '').strip()
        if not c:
            continue
        shell = (
            'set -e; '
            'echo "--- ls -la /flow_artifacts ---"; '
            'ls -la /flow_artifacts 2>&1 || true; '
            'echo "--- find /flow_artifacts (files) ---"; '
            f"find /flow_artifacts -maxdepth 3 -type f -print 2>/dev/null | head -n {MAX_FIND} || true; "
        )
        p = _run(['docker', 'exec', c, 'sh', '-lc', shell], timeout=25)
        items.append({'container': c, 'ok': p.returncode == 0, 'rc': int(p.returncode), 'output': (p.stdout or '')})
    print(json.dumps({'ok': True, 'items': items}))


if __name__ == '__main__':
    main()
"""
    ).replace('__CONTAINERS_LITERAL__', containers_literal) \
     .replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal) \
     .replace('__MAX_FIND_LITERAL__', max_find_literal)


@app.route('/run_cli_async', methods=['POST'])
def run_cli_async():
    seed = None
    xml_path = None
    preview_plan_path = None
    core_override = None
    scenario_core_override = None
    scenario_name_hint = None
    scenario_index_hint: Optional[int] = None
    update_remote_repo = False
    adv_fix_docker_daemon = False
    adv_run_core_cleanup = False
    adv_check_core_version = False
    adv_restart_core_daemon = False
    adv_start_core_daemon = False
    adv_auto_kill_sessions = False
    docker_remove_conflicts = False
    docker_cleanup_before_run = False
    docker_remove_all_containers = False
    overwrite_existing_images = False
    scenarios_inline = None
    # Prefer form fields (existing UI) but fall back to JSON
    if request.form:
        xml_path = request.form.get('xml_path')
        raw_seed = request.form.get('seed')
        if raw_seed:
            try: seed = int(raw_seed)
            except Exception: seed = None
        preview_plan_path = request.form.get('preview_plan') or preview_plan_path
        scenario_name_hint = request.form.get('scenario') or request.form.get('scenario_name') or scenario_name_hint
        try:
            raw_index = request.form.get('scenario_index')
            if raw_index not in (None, ''):
                scenario_index_hint = int(raw_index)
        except Exception:
            scenario_index_hint = scenario_index_hint
        try:
            core_json = request.form.get('core_json')
            if core_json:
                core_override = json.loads(core_json)
        except Exception:
            core_override = None
        try:
            hitl_core_json = request.form.get('hitl_core_json')
            if hitl_core_json:
                scenario_core_override = json.loads(hitl_core_json)
        except Exception:
            scenario_core_override = None
        if 'update_remote_repo' in request.form:
            update_remote_repo = _coerce_bool(request.form.get('update_remote_repo'))
        adv_fix_docker_daemon = _coerce_bool(request.form.get('adv_fix_docker_daemon'))
        adv_run_core_cleanup = _coerce_bool(request.form.get('adv_run_core_cleanup'))
        adv_check_core_version = _coerce_bool(request.form.get('adv_check_core_version'))
        adv_restart_core_daemon = _coerce_bool(request.form.get('adv_restart_core_daemon'))
        adv_start_core_daemon = _coerce_bool(request.form.get('adv_start_core_daemon'))
        adv_auto_kill_sessions = _coerce_bool(request.form.get('adv_auto_kill_sessions'))
        docker_remove_conflicts = _coerce_bool(request.form.get('docker_remove_conflicts'))
        docker_cleanup_before_run = _coerce_bool(request.form.get('docker_cleanup_before_run'))
        docker_remove_all_containers = _coerce_bool(
            request.form.get('docker_remove_all_containers')
        ) or _coerce_bool(request.form.get('docker_nuke_all'))
        overwrite_existing_images = _coerce_bool(request.form.get('overwrite_existing_images'))
    if not xml_path:
        try:
            j = request.get_json(silent=True) or {}
            xml_path = j.get('xml_path')
            scenarios_inline = j.get('scenarios')
            if 'seed' in j:
                try: seed = int(j.get('seed'))
                except Exception: seed = None
            if 'preview_plan' in j and not preview_plan_path:
                preview_plan_path = j.get('preview_plan')
            if 'core' in j and j.get('core') is not None:
                core_override = j.get('core')
            if 'hitl_core' in j and isinstance(j.get('hitl_core'), dict):
                scenario_core_override = j.get('hitl_core')
            if 'scenario' in j and j.get('scenario') not in (None, ''):
                scenario_name_hint = j.get('scenario')
            if 'scenario_index' in j:
                try:
                    scenario_index_hint = int(j.get('scenario_index'))
                except Exception:
                    scenario_index_hint = None
            if 'update_remote_repo' in j:
                update_remote_repo = _coerce_bool(j.get('update_remote_repo'))
            adv_fix_docker_daemon = _coerce_bool(j.get('adv_fix_docker_daemon'))
            adv_run_core_cleanup = _coerce_bool(j.get('adv_run_core_cleanup'))
            adv_check_core_version = _coerce_bool(j.get('adv_check_core_version'))
            adv_restart_core_daemon = _coerce_bool(j.get('adv_restart_core_daemon'))
            adv_start_core_daemon = _coerce_bool(j.get('adv_start_core_daemon'))
            adv_auto_kill_sessions = _coerce_bool(j.get('adv_auto_kill_sessions'))
            docker_remove_conflicts = _coerce_bool(j.get('docker_remove_conflicts'))
            docker_cleanup_before_run = _coerce_bool(j.get('docker_cleanup_before_run'))
            docker_remove_all_containers = _coerce_bool(
                j.get('docker_remove_all_containers')
            ) or _coerce_bool(j.get('docker_nuke_all'))
            overwrite_existing_images = _coerce_bool(j.get('overwrite_existing_images'))
        except Exception:
            pass
    if not xml_path:
        # Builder/participant roles may execute without saving by posting scenarios/core.
        if isinstance(scenarios_inline, list):
            try:
                # Build a temporary XML for this run.
                core_meta = core_override if isinstance(core_override, dict) else None
                normalized_core = _normalize_core_config(core_meta, include_password=True) if core_meta else None
                tree = _build_scenarios_xml({ 'scenarios': scenarios_inline, 'core': normalized_core })
                ts = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
                run_tag = str(uuid.uuid4())[:8]
                out_dir = os.path.join(_outputs_dir(), f'tmp-exec-{ts}-{run_tag}')
                os.makedirs(out_dir, exist_ok=True)
                # Filename hint: active scenario name if available, else generic.
                stem_raw = None
                try:
                    if scenario_name_hint:
                        stem_raw = str(scenario_name_hint)
                except Exception:
                    stem_raw = None
                if not stem_raw:
                    try:
                        first_name = None
                        for sc in scenarios_inline:
                            if isinstance(sc, dict) and sc.get('name'):
                                first_name = sc.get('name')
                                break
                        stem_raw = first_name or 'scenarios'
                    except Exception:
                        stem_raw = 'scenarios'
                stem = secure_filename(str(stem_raw)).strip('_-.') or 'scenarios'
                xml_path = os.path.join(out_dir, f"{stem}.xml")
                # Pretty print when possible
                try:
                    from lxml import etree as LET  # type: ignore
                    raw = ET.tostring(tree.getroot(), encoding='utf-8')
                    lroot = LET.fromstring(raw)
                    pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
                    with open(xml_path, 'wb') as f:
                        f.write(pretty)
                except Exception:
                    tree.write(xml_path, encoding='utf-8', xml_declaration=True)
            except Exception as exc:
                return jsonify({"error": f"Failed to render XML for execution: {exc}"}), 400
        else:
            return jsonify({"error": "XML path missing. Save XML first."}), 400
    xml_path = os.path.abspath(xml_path)
    if not os.path.exists(xml_path) and '/outputs/' in xml_path:
        try:
            alt = xml_path.replace('/app/outputs', '/app/webapp/outputs')
            if alt != xml_path and os.path.exists(alt):
                app.logger.info("[async] Remapped XML path %s -> %s", xml_path, alt)
                xml_path = alt
        except Exception:
            pass
    if not os.path.exists(xml_path):
        try:
            recovered = _try_resolve_latest_outputs_xml(xml_path)
            if recovered and os.path.exists(recovered):
                app.logger.warning('[async] XML path missing; recovered to newest match: %s -> %s', xml_path, recovered)
                xml_path = recovered
        except Exception:
            pass
    if not os.path.exists(xml_path):
        return jsonify({"error": f"XML path not found: {xml_path}"}), 400
    preview_plan_path = (preview_plan_path or '').strip() or None
    if preview_plan_path:
        try:
            preview_plan_path = os.path.abspath(preview_plan_path)
            plans_dir = os.path.abspath(os.path.join(_outputs_dir(), 'plans'))
            if os.path.commonpath([preview_plan_path, plans_dir]) != plans_dir:
                app.logger.warning('[async] preview plan outside allowed directory: %s', preview_plan_path)
                try:
                    log_f.write(f"[async] preview plan rejected (outside outputs/plans): {preview_plan_path}\n")
                except Exception:
                    pass
                preview_plan_path = None
            elif not os.path.exists(preview_plan_path):
                app.logger.warning('[async] preview plan path missing: %s', preview_plan_path)
                try:
                    log_f.write(f"[async] preview plan rejected (missing): {preview_plan_path}\n")
                except Exception:
                    pass
                preview_plan_path = None
        except Exception:
            preview_plan_path = None

    # If no preview plan path was provided (or it was rejected), but the user has
    # saved a Flag Sequencing (flow) plan for this scenario, use it automatically.
    # Without a plan, the CLI will execute with 0 flows (no flags/generators).
    if not preview_plan_path:
        try:
            scenario_norm = None
            if scenario_name_hint:
                scenario_norm = _normalize_scenario_label(str(scenario_name_hint))
            if not scenario_norm and isinstance(scenario_payload, dict) and isinstance(scenario_payload.get('name'), str):
                scenario_norm = _normalize_scenario_label(str(scenario_payload.get('name') or ''))
            if scenario_norm:
                flow_plan = _latest_flow_plan_for_scenario_norm(scenario_norm)
                if flow_plan and os.path.exists(flow_plan):
                    preview_plan_path = os.path.abspath(flow_plan)
                    try:
                        log_f.write(f"[async] Auto-selected flow plan for scenario={scenario_norm}: {preview_plan_path}\n")
                    except Exception:
                        pass
        except Exception:
            pass
    # Skip schema validation: format differs from CORE XML
    run_id = str(uuid.uuid4())
    out_dir = os.path.dirname(xml_path)
    log_path = os.path.join(out_dir, f'cli-{run_id}.log')
    # Redirect output directly to log file for easy tailing
    # Open log file in line-buffered mode so subprocess logging (stdout+stderr) flushes promptly for UI streaming
    try:
        log_f = open(log_path, 'w', encoding='utf-8', buffering=1)
    except Exception:
        # Fallback to default buffering if line buffering not available
        log_f = open(log_path, 'w', encoding='utf-8')
    try:
        app.logger.debug("[async] Opened CLI log (line-buffered) at %s", log_path)
    except Exception:
        pass
    app.logger.info("[async] Starting CLI; log: %s", log_path)
    payload_for_core: Dict[str, Any] | None = None
    try:
        payload_for_core = _parse_scenarios_xml(xml_path)
    except Exception:
        payload_for_core = None
    scenario_payload: Dict[str, Any] | None = None
    if payload_for_core:
        scen_list = payload_for_core.get('scenarios') or []
        if isinstance(scen_list, list) and scen_list:
            if scenario_name_hint:
                for scen_entry in scen_list:
                    if not isinstance(scen_entry, dict):
                        continue
                    if str(scen_entry.get('name') or '').strip() == str(scenario_name_hint).strip():
                        scenario_payload = scen_entry
                        break
            if scenario_payload is None and scenario_index_hint is not None:
                if 0 <= scenario_index_hint < len(scen_list):
                    candidate = scen_list[scenario_index_hint]
                    if isinstance(candidate, dict):
                        scenario_payload = candidate
            if scenario_payload is None:
                for scen_entry in scen_list:
                    if isinstance(scen_entry, dict):
                        scenario_payload = scen_entry
                        break
    scenario_core_saved = None
    if scenario_payload and isinstance(scenario_payload.get('hitl'), dict):
        scenario_core_saved = scenario_payload['hitl'].get('core')
    global_core_saved = payload_for_core.get('core') if (payload_for_core and isinstance(payload_for_core.get('core'), dict)) else None
    scenario_core_public: Dict[str, Any] | None = None
    candidate_scenario_core = scenario_core_override if isinstance(scenario_core_override, dict) else None
    if not candidate_scenario_core and isinstance(scenario_core_saved, dict):
        candidate_scenario_core = scenario_core_saved
    if candidate_scenario_core:
        scenario_core_public = _scrub_scenario_core_config(candidate_scenario_core)
    core_cfg = _merge_core_configs(
        global_core_saved,
        scenario_core_saved,
        core_override if isinstance(core_override, dict) else None,
        scenario_core_override if isinstance(scenario_core_override, dict) else None,
        include_password=True,
    )
    # Flag Sequencing (flow.html) invokes /run_cli_async via JSON without sending core config.
    # In that case, we must honor the "Select CORE VM" dialog (including ssh_port) rather than
    # defaulting to backend/env values.
    try:
        request_provided_core = bool(
            (isinstance(core_override, dict) and core_override)
            or (isinstance(scenario_core_override, dict) and scenario_core_override)
        )
        history = _load_run_history()
        scenario_for_secret = None
        try:
            scenario_for_secret = _normalize_scenario_label(str(scenario_name_hint or '').strip())
        except Exception:
            scenario_for_secret = None
        if not scenario_for_secret:
            try:
                if isinstance(scenario_payload, dict) and isinstance(scenario_payload.get('name'), str):
                    scenario_for_secret = _normalize_scenario_label(str(scenario_payload.get('name') or '').strip())
            except Exception:
                scenario_for_secret = None

        selected_cfg = None
        if scenario_for_secret:
            selected_cfg = _select_core_config_for_page(scenario_for_secret, history, include_password=True)

        pw_raw = core_cfg.get('ssh_password') if isinstance(core_cfg, dict) else None
        pw_ok = bool(str(pw_raw).strip()) if pw_raw not in (None, '') else False

        if request_provided_core:
            # Only fill missing secrets; do not override request-provided host/ports.
            if selected_cfg and not pw_ok:
                core_cfg = _merge_core_configs(selected_cfg, core_cfg, include_password=True)
        else:
            # Only auto-select a saved CORE config when we have a scenario label.
            if selected_cfg:
                core_cfg = _merge_core_configs(core_cfg, selected_cfg, include_password=True)
    except Exception:
        pass
    try:
        core_cfg = _require_core_ssh_credentials(core_cfg)
    except _SSHTunnelError as exc:
        try:
            log_f.close()
        except Exception:
            pass
        return jsonify({"error": str(exc)}), 400
    preferred_cli_venv = _sanitize_venv_bin_path(core_cfg.get('venv_bin'))
    venv_is_explicit = _venv_is_explicit(core_cfg, preferred_cli_venv)
    if preferred_cli_venv and venv_is_explicit:
        venv_error: Optional[str] = None
        remote_venv_allowed = bool(core_cfg.get('ssh_enabled'))
        if not os.path.isabs(preferred_cli_venv):
            venv_error = f"Preferred CORE venv bin must be an absolute path: {preferred_cli_venv}"
        elif os.path.isdir(preferred_cli_venv):
            if not _find_python_in_venv_bin(preferred_cli_venv):
                expected = ', '.join(PYTHON_EXECUTABLE_NAMES)
                venv_error = (
                    f"Preferred CORE venv bin {preferred_cli_venv} does not contain a python interpreter "
                    f"(expected one of {expected})."
                )
        elif not remote_venv_allowed:
            venv_error = f"Preferred CORE venv bin not found: {preferred_cli_venv}"
        else:
            try:
                app.logger.info(
                    "[async] Preferred CORE venv bin %s not present locally; assuming remote path",
                    preferred_cli_venv,
                )
            except Exception:
                pass
            try:
                log_f.write(
                    f"[remote] Preferred CORE venv bin {preferred_cli_venv} not present locally; assuming remote path\n"
                )
            except Exception:
                pass
        if venv_error:
            try:
                app.logger.warning("[async] %s", venv_error)
            except Exception:
                pass
            try:
                log_f.write(venv_error + "\n")
            except Exception:
                pass
            try:
                log_f.close()
            except Exception:
                pass
            try:
                os.remove(log_path)
            except Exception:
                pass
            return jsonify({"error": venv_error}), 400
    if update_remote_repo:
        try:
            log_f.write("[remote] Repo upload starting (requested by execute dialog)\n")
        except Exception:
            pass
        try:
            repo_sync = _push_repo_to_remote(core_cfg, logger=app.logger)
        except Exception as exc:
            try:
                log_f.write(f"[remote] Repo upload failed: {exc}\n")
            except Exception:
                pass
            try:
                log_f.close()
            except Exception:
                pass
            try:
                os.remove(log_path)
            except Exception:
                pass
            return jsonify({"error": f"Repo upload failed: {exc}"}), 500
        else:
            repo_path = repo_sync.get('repo_path') if isinstance(repo_sync, dict) else None
            try:
                detail = f" ({repo_path})" if repo_path else ''
                log_f.write(f"[remote] Repo upload complete{detail}\n")
            except Exception:
                pass
    core_host = core_cfg.get('host', '127.0.0.1')
    try:
        core_port = int(core_cfg.get('port', 50051))
    except Exception:
        core_port = 50051
    remote_desc = f"{core_host}:{core_port}"

    def _collect_blocking_sessions() -> list[dict]:
        blocking: list[dict] = []
        try:
            sessions = _list_active_core_sessions(core_host, core_port, core_cfg, errors=[], meta={})
        except Exception as exc:
            try:
                app.logger.warning('[async] Failed to enumerate existing CORE sessions: %s', exc)
            except Exception:
                pass
            sessions = []
        for entry in sessions:
            state_raw = str(entry.get('state') or '').strip().lower()
            if state_raw in {'shutdown'}:
                continue
            blocking.append(entry)
        return blocking

    # Check for active session conflicts early so we can block quickly without requiring SSH
    # tunnel or remote docker access.
    # NOTE: Active-session checks are performed later, after SSH is confirmed and any
    # selected cleanup actions are executed.
    app.logger.info("[async] CORE remote=%s (ssh_enabled=%s)", remote_desc, core_cfg.get('ssh_enabled'))
    pre_saved = None
    # Establish SSH tunnel so CLI subprocess can reach CORE
    conn_host = core_host
    conn_port = core_port
    ssh_tunnel = None
    try:
        tunnel = _SshTunnel(
            ssh_host=str(core_cfg.get('ssh_host') or core_host),
            ssh_port=int(core_cfg.get('ssh_port') or 22),
            username=str(core_cfg.get('ssh_username') or ''),
            password=(core_cfg.get('ssh_password') or None),
            remote_host=str(core_host),
            remote_port=int(core_port),
        )
        conn_host, conn_port = tunnel.start()
        ssh_tunnel = tunnel
        app.logger.info(
            "[async] SSH tunnel active: %s -> %s",
            f"{conn_host}:{conn_port}",
            remote_desc,
        )
        try:
            log_f.write(f"[remote] SSH tunnel active: {conn_host}:{conn_port} -> {remote_desc}\n")
            _write_sse_marker(
                log_f,
                'phase',
                {
                    'stage': 'ssh.tunnel',
                    'kind': 'tunnel',
                    'local': f"{conn_host}:{conn_port}",
                    'remote': remote_desc,
                },
            )
        except Exception:
            pass
    except _SSHTunnelError as exc:
        try:
            app.logger.warning("[async] SSH tunnel setup failed: %s", exc)
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        if ssh_tunnel:
            try:
                ssh_tunnel.close()
            except Exception:
                pass
        return jsonify({"error": f"SSH tunnel failed: {exc}"}), 500
    except Exception as exc:
        try:
            app.logger.exception("[async] Unexpected SSH tunnel failure: %s", exc)
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        if ssh_tunnel:
            try:
                ssh_tunnel.close()
            except Exception:
                pass
        return jsonify({"error": "Failed to establish SSH tunnel"}), 500
    # Capture scenario names from the editor XML now (CORE post XML will not be parsable by our scenarios parser)
    scen_names = _scenario_names_from_xml(xml_path)
    log_prefix = "[remote] "
    remote_client = None
    remote_ctx: Dict[str, Any] | None = None
    remote_python = None

    def _purge_remote_run_dir() -> None:
        if not remote_ctx:
            return
        run_dir = remote_ctx.get('run_dir') if isinstance(remote_ctx, dict) else None
        if not run_dir or not remote_client:
            return
        try:
            _remote_remove_path(remote_client, run_dir)
        except Exception:
            pass

    try:
        remote_client = _open_ssh_client(core_cfg)
    except Exception as exc:
        try:
            log_f.write(f"{log_prefix}Failed to open SSH session for CLI: {exc}\n")
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
        return jsonify({"error": f"Failed to open SSH session for CLI: {exc}"}), 500
    auto_start_allowed = _coerce_bool(core_cfg.get('auto_start_daemon')) or bool(adv_restart_core_daemon)
    try:
        log_f.write(f"{log_prefix}core-daemon auto-start allowed: {auto_start_allowed}\n")
    except Exception:
        pass

    def _exec_sudo(
        cmd: str,
        *,
        timeout: float = 30.0,
        stage: str = 'sudo',
    ) -> tuple[int, str, str]:
        sudo_password = core_cfg.get('ssh_password')
        # Wrap in `timeout` to avoid hanging indefinitely.
        wrapped = f"sh -c 'timeout {int(max(5, timeout))}s {cmd}'"
        if sudo_password:
            sudo_cmd = f"sudo -S -p '' {wrapped}"
        else:
            sudo_cmd = f"sudo -n {wrapped}"
        stdin = stdout = stderr = None
        try:
            stdin, stdout, stderr = remote_client.exec_command(sudo_cmd, timeout=timeout + 5.0, get_pty=True)
            if sudo_password:
                try:
                    stdin.write(str(sudo_password) + '\n')
                    stdin.flush()
                except Exception:
                    pass
            stdout_data = stdout.read()
            stderr_data = stderr.read()
            try:
                exit_code = stdout.channel.recv_exit_status() if hasattr(stdout, 'channel') else 0
            except Exception:
                exit_code = 0
            out_text = stdout_data.decode('utf-8', 'ignore') if isinstance(stdout_data, bytes) else str(stdout_data or '')
            err_text = stderr_data.decode('utf-8', 'ignore') if isinstance(stderr_data, bytes) else str(stderr_data or '')
            try:
                log_f.write(
                    f"{log_prefix}{stage}: {sudo_cmd} -> exit={exit_code} stdout={_summarize_for_log(out_text.strip())} stderr={_summarize_for_log(err_text.strip())}\n"
                )
            except Exception:
                pass
            return exit_code, out_text, err_text
        finally:
            try:
                if stdin:
                    stdin.close()
            except Exception:
                pass

    # (Scenario tag + Docker existing-image precheck runs later, after cleanup actions.)
    def _check_core_version(required: str = '9.2.1') -> None:
        candidates = [
            "sh -c 'timeout 6s core-daemon --version 2>/dev/null || true'",
            "sh -c 'timeout 6s core-daemon -v 2>/dev/null || true'",
            "sh -c 'timeout 6s dpkg-query -W -f=\"${Version}\" core-daemon 2>/dev/null || true'",
            "sh -c 'timeout 6s rpm -q --qf \"%{VERSION}-%{RELEASE}\" core-daemon 2>/dev/null || true'",
            "sh -c 'timeout 6s core --version 2>/dev/null || true'",
        ]
        raw = ''
        for cmd in candidates:
            try:
                code, out, err = _exec_ssh_command(remote_client, cmd, timeout=10.0)
            except Exception:
                continue
            text = (out or '').strip() or (err or '').strip()
            if not text:
                continue
            raw = text
            break
        found = None
        try:
            m = re.search(r"(\d+\.\d+\.\d+)", raw)
            if m:
                found = m.group(1)
        except Exception:
            found = None
        try:
            log_f.write(f"{log_prefix}CORE version probe: {raw or '(no output)'}\n")
        except Exception:
            pass
        if not found:
            raise RuntimeError('Unable to determine CORE version on remote host')
        if found != required:
            raise RuntimeError(f"CORE version mismatch: expected {required}, found {found}")

    def _maybe_fix_docker_daemon() -> None:
        desired = {'bridge': 'none', 'iptables': False}
        # Best-effort merge with existing daemon.json if readable.
        existing: dict[str, Any] = {}
        try:
            code, out, err = _exec_ssh_command(remote_client, "sh -c 'timeout 5s cat /etc/docker/daemon.json 2>/dev/null || true'", timeout=10.0)
            text = (out or '').strip()
            if text:
                try:
                    existing = json.loads(text)
                except Exception:
                    existing = {}
        except Exception:
            existing = {}
        merged = dict(existing) if isinstance(existing, dict) else {}
        merged.update(desired)
        payload = json.dumps(merged, indent=2, sort_keys=True) + "\n"
        tmp_local = None
        remote_tmp = None
        try:
            import tempfile

            with tempfile.NamedTemporaryFile('w', delete=False, encoding='utf-8') as tf:
                tf.write(payload)
                tmp_local = tf.name
            remote_tmp = _upload_file_to_core_host(core_cfg, tmp_local, remote_dir='/tmp/core-topo-gen/uploads')
            try:
                log_f.write(f"{log_prefix}Uploaded docker daemon.json to {remote_tmp}\n")
            except Exception:
                pass
            # Install into /etc/docker/daemon.json with sudo.
            try:
                log_f.write(f"{log_prefix}Installing docker daemon config to /etc/docker/daemon.json (sudo)…\n")
            except Exception:
                pass
            # Some images don't have /etc/docker pre-created.
            exit_code, _out, err = _exec_sudo("install -d -m 0755 /etc/docker", timeout=15.0, stage='docker.etcdir')
            if exit_code != 0:
                err_lower = (err or '').lower()
                if (not core_cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                    raise RuntimeError('Fix docker daemon failed: sudo requires a password (none provided).')
                raise RuntimeError('Fix docker daemon failed: could not create /etc/docker')
            exit_code, _out, err = _exec_sudo(f"install -m 0644 {shlex.quote(remote_tmp)} /etc/docker/daemon.json", timeout=20.0, stage='docker.daemon.json')
            if exit_code != 0:
                err_lower = (err or '').lower()
                if (not core_cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                    raise RuntimeError('Fix docker daemon failed: sudo requires a password (none provided).')
                raise RuntimeError('Fix docker daemon failed: could not write /etc/docker/daemon.json')
            try:
                log_f.write(f"{log_prefix}Wrote /etc/docker/daemon.json successfully.\n")
            except Exception:
                pass
            # Verify file exists and is readable.
            _exec_sudo("ls -l /etc/docker/daemon.json", timeout=10.0, stage='docker.daemon.json.verify')
            # Restart docker so changes take effect.
            try:
                log_f.write(f"{log_prefix}Restarting docker daemon (sudo systemctl restart docker)…\n")
            except Exception:
                pass
            # Capture systemd service info (best-effort) so we can verify a real restart.
            show_cmd = "systemctl show docker -p MainPID -p ActiveEnterTimestampMonotonic -p SubState --no-pager"
            show_cmd_alt = "systemctl show docker.service -p MainPID -p ActiveEnterTimestampMonotonic -p SubState --no-pager"
            show_before = None
            try:
                rc_show, out_show, _err_show = _exec_sudo(show_cmd, timeout=15.0, stage='docker.show.before')
                if rc_show != 0:
                    rc_show, out_show, _err_show = _exec_sudo(show_cmd_alt, timeout=15.0, stage='docker.show.before')
                show_before = (out_show or '').strip() or None
            except Exception:
                show_before = None
            rc, _o, _e = _exec_sudo("systemctl restart docker", timeout=35.0, stage='docker.restart')
            if rc != 0:
                try:
                    log_f.write(f"{log_prefix}systemctl restart docker failed; trying: sudo service docker restart…\n")
                except Exception:
                    pass
                rc2, _o2, _e2 = _exec_sudo("service docker restart", timeout=35.0, stage='docker.restart')
                if rc2 != 0:
                    raise RuntimeError('Fix docker daemon failed: docker restart did not succeed')
            else:
                # Some systems require the explicit unit name.
                show_after = None
                try:
                    rc_show2, out_show2, _err_show2 = _exec_sudo(show_cmd, timeout=15.0, stage='docker.show.after')
                    if rc_show2 != 0:
                        rc_show2, out_show2, _err_show2 = _exec_sudo(show_cmd_alt, timeout=15.0, stage='docker.show.after')
                    show_after = (out_show2 or '').strip() or None
                except Exception:
                    show_after = None
                try:
                    if show_before and show_after and show_before == show_after:
                        log_f.write(f"{log_prefix}NOTE: docker systemd service metadata unchanged after restart; retrying with docker.service…\n")
                        _exec_sudo("systemctl restart docker.service", timeout=35.0, stage='docker.restart')
                        _exec_sudo(show_cmd_alt, timeout=15.0, stage='docker.show.after')
                except Exception:
                    pass
            rc3, out3, err3 = _exec_sudo("systemctl is-active docker", timeout=15.0, stage='docker.is-active')
            try:
                status = (out3 or '').strip() or (err3 or '').strip() or f"(exit={rc3})"
                log_f.write(f"{log_prefix}docker service status: {status}\n")
            except Exception:
                pass
        finally:
            try:
                if tmp_local and os.path.exists(tmp_local):
                    os.remove(tmp_local)
            except Exception:
                pass
            try:
                if remote_tmp:
                    _remove_remote_file(core_cfg, remote_tmp)
            except Exception:
                pass

    def _maybe_core_cleanup() -> None:
        # Prefer system-provided core-cleanup if available; otherwise do a safe stale /tmp/pycore.* purge.
        try:
            code, out, err = _exec_ssh_command(remote_client, "sh -c 'command -v core-cleanup >/dev/null 2>&1; echo $?'", timeout=10.0)
            has_core_cleanup = (out or '').strip() == '0'
        except Exception:
            has_core_cleanup = False
        if has_core_cleanup:
            exit_code, _out, err = _exec_sudo('core-cleanup', timeout=60.0, stage='core.cleanup')
            if exit_code != 0:
                err_lower = (err or '').lower()
                if (not core_cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                    raise RuntimeError('core-cleanup failed: sudo requires a password (none provided).')
                raise RuntimeError('core-cleanup failed')
            return
        # Fallback: remove stale pycore directories not in active session ids.
        try:
            sessions = _list_active_core_sessions(core_host, core_port, core_cfg, errors=[], meta={})
        except Exception:
            sessions = []
        active_ids: set[int] = set()
        for entry in sessions:
            try:
                sid = entry.get('id')
                if sid is None:
                    continue
                active_ids.add(int(str(sid).strip()))
            except Exception:
                continue
        active_json = json.dumps(sorted(active_ids))
        cleanup_cmd = (
            "python3 - <<'PY'\n"
            "import os, json, glob, shutil, time\n"
            "active=set(json.loads(os.environ.get('ACTIVE_IDS','[]')))\n"
            "removed=[]\nkept=[]\nnow=time.time()\n"
            "for p in glob.glob('/tmp/pycore.*'):\n"
            "  base=os.path.basename(p)\n"
            "  try: sid=int(base.split('.')[-1])\n"
            "  except Exception: kept.append(p); continue\n"
            "  if sid in active: kept.append(p); continue\n"
            "  try: age=now-os.stat(p).st_mtime\n"
            "  except Exception: age=999\n"
            "  if age < 30: kept.append(p); continue\n"
            "  try: shutil.rmtree(p); removed.append(p)\n"
            "  except Exception: kept.append(p)\n"
            "print(json.dumps({'removed':removed,'kept':kept,'active_session_ids':sorted(active)}))\n"
            "PY"
        )
        shell_cmd = f"ACTIVE_IDS={shlex.quote(active_json)} {cleanup_cmd}"
        code, out, err = _exec_ssh_command(
            remote_client,
            f"sh -c {shlex.quote(shell_cmd)}",
            timeout=25.0,
        )
        try:
            log_f.write(f"{log_prefix}pycore cleanup result: {(out or err or '').strip()}\n")
        except Exception:
            pass

    def _maybe_restart_core_daemon() -> None:
        exit_code, _out, err = _exec_sudo('systemctl restart core-daemon', timeout=30.0, stage='core-daemon.restart')
        if exit_code != 0:
            err_lower = (err or '').lower()
            if (not core_cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                raise CoreDaemonMissingError(
                    'Restart core-daemon failed: sudo requires a password (none provided).',
                    can_auto_start=False,
                    start_command='sudo systemctl restart core-daemon',
                )
            raise CoreDaemonMissingError(
                'Restart core-daemon failed.',
                can_auto_start=False,
                start_command='sudo systemctl restart core-daemon',
            )

    def _maybe_kill_active_sessions() -> tuple[list[int], list[str]]:
        deleted: list[int] = []
        errors: list[str] = []
        try:
            sessions = _list_active_core_sessions(core_host, core_port, core_cfg, errors=[], meta={})
        except Exception as exc:
            errors.append(f"Failed listing active CORE sessions: {exc}")
            return deleted, errors
        ids: list[int] = []
        for entry in sessions:
            sid = entry.get('id')
            if sid in (None, ''):
                continue
            try:
                ids.append(int(str(sid).strip()))
            except Exception:
                continue
        seen: set[int] = set()
        ordered: list[int] = []
        for sid in ids:
            if sid in seen:
                continue
            seen.add(sid)
            ordered.append(sid)
        for sid in ordered:
            try:
                _execute_remote_core_session_action(core_cfg, 'delete', sid, logger=app.logger)
                deleted.append(sid)
            except Exception as exc:
                errors.append(f"Failed deleting session {sid}: {exc}")
        return deleted, errors

    # These are populated during preflight; initialize so later code can safely reference them.
    active_scenario_name = None
    scenario_tag = _safe_name('scenario')
    try:
        log_f.write(f"{log_prefix}=== CORE services startup (core-daemon) ===\n")
        _write_sse_marker(
            log_f,
            'phase',
            {
                'stage': 'core-daemon.startup',
                'detail': 'Starting/validating core-daemon before launching CLI',
            },
        )
    except Exception:
        pass
    try:
        # Execute advanced pre-flight actions (remote only)
        try:
            log_f.write(
                f"{log_prefix}Advanced options received: "
                f"fix_docker_daemon={adv_fix_docker_daemon} "
                f"run_core_cleanup={adv_run_core_cleanup} "
                f"check_core_version={adv_check_core_version} "
                f"restart_core_daemon={adv_restart_core_daemon} "
                f"auto_kill_sessions={adv_auto_kill_sessions}\n"
            )
        except Exception:
            pass
        # Cleanup actions MUST run before any checks.
        if adv_fix_docker_daemon:
            try:
                log_f.write(f"{log_prefix}=== Advanced: Fix docker daemon for CORE ===\n")
            except Exception:
                pass
            _maybe_fix_docker_daemon()
            try:
                log_f.write(f"{log_prefix}Docker daemon.json updated and docker restarted.\n")
            except Exception:
                pass

        if adv_run_core_cleanup:
            try:
                log_f.write(f"{log_prefix}=== Advanced: Run core cleanup ===\n")
            except Exception:
                pass
            _maybe_core_cleanup()
            try:
                log_f.write(f"{log_prefix}CORE cleanup complete.\n")
            except Exception:
                pass

        if docker_remove_all_containers:
            try:
                log_f.write(f"{log_prefix}=== Docker: REMOVE ALL containers (DANGEROUS) ===\n")
                _write_sse_marker(
                    log_f,
                    'phase',
                    {
                        'stage': 'docker.remove_all_containers.prerun',
                        'detail': 'Removing ALL docker containers',
                    },
                )
            except Exception:
                pass
            try:
                payload = _run_remote_python_json(
                    core_cfg,
                    _remote_docker_remove_all_containers_script(core_cfg.get('ssh_password')),
                    logger=app.logger,
                    label='docker.remove_all_containers(prerun)',
                    timeout=900.0,
                )
                try:
                    if isinstance(payload, dict):
                        c = payload.get('containers') or {}
                        log_f.write(
                            f"{log_prefix}Remove-all summary: containers_found={c.get('found')} removed_attempted={c.get('removed_attempted')}\n"
                        )
                except Exception:
                    pass
            except Exception as exc:
                try:
                    log_f.write(f"{log_prefix}Docker remove-all-containers skipped/failed: {exc}\n")
                except Exception:
                    pass

        if docker_cleanup_before_run:
            try:
                log_f.write(f"{log_prefix}=== Docker: cleanup before run (containers + wrapper images) ===\n")
                _write_sse_marker(
                    log_f,
                    'phase',
                    {
                        'stage': 'docker.cleanup.prerun',
                        'detail': 'Stopping/removing vuln containers and wrapper images',
                    },
                )
            except Exception:
                pass
            try:
                status_payload = _run_remote_python_json(
                    core_cfg,
                    _remote_docker_status_script(core_cfg.get('ssh_password')),
                    logger=app.logger,
                    label='docker.status(for prerun cleanup)',
                    timeout=60.0,
                )
                names: list[str] = []
                if isinstance(status_payload, dict) and isinstance(status_payload.get('items'), list):
                    for it in status_payload.get('items') or []:
                        if isinstance(it, dict) and it.get('name'):
                            names.append(str(it.get('name')))
                if names:
                    try:
                        log_f.write(f"{log_prefix}Docker containers to cleanup: {', '.join(names[:20])}{' ...' if len(names) > 20 else ''}\n")
                    except Exception:
                        pass
                    _run_remote_python_json(
                        core_cfg,
                        _remote_docker_cleanup_script(names, core_cfg.get('ssh_password')),
                        logger=app.logger,
                        label='docker.cleanup(prerun)',
                        timeout=120.0,
                    )
                else:
                    try:
                        log_f.write(f"{log_prefix}No vuln docker-compose node names found for cleanup.\n")
                    except Exception:
                        pass

                payload = _run_remote_python_json(
                    core_cfg,
                    _remote_docker_remove_wrapper_images_script(core_cfg.get('ssh_password')),
                    logger=app.logger,
                    label='docker.wrapper_images.cleanup(prerun)',
                    timeout=180.0,
                )
                try:
                    removed = payload.get('removed') if isinstance(payload, dict) else None
                    if isinstance(removed, list) and removed:
                        log_f.write(f"{log_prefix}Removed wrapper images: {', '.join(str(x) for x in removed[:12])}{' ...' if len(removed) > 12 else ''}\n")
                    else:
                        log_f.write(f"{log_prefix}No wrapper images removed (or none found).\n")
                except Exception:
                    pass
            except Exception as exc:
                try:
                    log_f.write(f"{log_prefix}Pre-run docker cleanup skipped/failed: {exc}\n")
                except Exception:
                    pass

        # Determine active scenario name for tag scoping and CLI args.
        active_scenario_name = None
        if scenario_name_hint:
            active_scenario_name = scenario_name_hint
        elif scen_names and len(scen_names) > 0:
            active_scenario_name = scen_names[0]

        # Scope wrapper images by upload/scenario to avoid cross-scenario image reuse.
        try:
            out_dir_for_tag = os.path.dirname(xml_path) if xml_path else ''
            upload_base = os.path.basename(out_dir_for_tag) if out_dir_for_tag else ''
            parts = []
            if upload_base:
                parts.append(upload_base)
            if active_scenario_name:
                parts.append(active_scenario_name)
            if run_id:
                parts.append(str(run_id)[:8])
            scenario_tag = _safe_name('-'.join(parts) if parts else (active_scenario_name or 'scenario'))
        except Exception:
            scenario_tag = _safe_name(active_scenario_name or 'scenario')

        # Pre-run safety check: existing scenario-scoped images -> prompt abort/overwrite.
        existing_images: list[str] = []
        try:
            prefix_repo = f"coretg/{scenario_tag}-"
            cmd = "docker images --format '{{.Repository}}:{{.Tag}}'"
            rc, out, _err = _exec_sudo(cmd, timeout=35.0, stage='docker.images.check')
            if rc == 0:
                for ln in (out or '').splitlines():
                    ln = (ln or '').strip()
                    if not ln or ln.startswith('<none>:'):
                        continue
                    if ln.startswith(prefix_repo):
                        existing_images.append(ln)
        except Exception:
            existing_images = []
        if existing_images:
            try:
                log_f.write(
                    f"{log_prefix}Found {len(existing_images)} existing scenario image(s) on CORE VM for tag '{scenario_tag}'.\n"
                )
                log_f.write(
                    f"{log_prefix}Existing images: {', '.join(existing_images[:8])}{' ...' if len(existing_images) > 8 else ''}\n"
                )
            except Exception:
                pass
            if not overwrite_existing_images:
                try:
                    _write_sse_marker(
                        log_f,
                        'phase',
                        {
                            'stage': 'docker.images.precheck',
                            'detail': 'Existing scenario images detected; awaiting overwrite confirmation',
                            'scenario_tag': scenario_tag,
                            'existing_images': existing_images[:25],
                        },
                    )
                except Exception:
                    pass
                try:
                    remote_client.close()
                except Exception:
                    pass
                try:
                    log_f.close()
                except Exception:
                    pass
                _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
                return jsonify({
                    'error': 'Existing scenario-scoped Docker images detected on the CORE VM.',
                    'kind': 'existing_images',
                    'scenario_tag': scenario_tag,
                    'existing_images': existing_images,
                    'can_overwrite': True,
                }), 412

            # Overwrite: remove any containers using these images, then remove the images.
            try:
                log_f.write(f"{log_prefix}Overwrite confirmed; removing existing scenario images…\n")
                _write_sse_marker(
                    log_f,
                    'phase',
                    {
                        'stage': 'docker.images.overwrite',
                        'detail': 'Removing existing scenario images before execution',
                        'scenario_tag': scenario_tag,
                        'count': len(existing_images),
                    },
                )
            except Exception:
                pass
            try:
                images_json = json.dumps(existing_images)
                py_rm = (
                    "import os, json, subprocess\n"
                    "imgs=json.loads(os.environ.get('IMAGES_JSON','[]'))\n"
                    "removed=[]\nfailed=[]\n"
                    "for img in imgs:\n"
                    "  if not img or '<none>' in img: continue\n"
                    "  try:\n"
                    "    ids=subprocess.check_output(['docker','ps','-aq','--filter',f'ancestor={img}'], text=True, stderr=subprocess.STDOUT).split()\n"
                    "  except Exception:\n"
                    "    ids=[]\n"
                    "  if ids:\n"
                    "    try:\n"
                    "      subprocess.run(['docker','rm','-f']+ids, check=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n"
                    "    except Exception:\n"
                    "      pass\n"
                    "  p=subprocess.run(['docker','rmi','-f',img], check=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n"
                    "  if int(p.returncode or 0)==0:\n"
                    "    removed.append(img)\n"
                    "  else:\n"
                    "    failed.append({'image':img,'out':(p.stdout or '').strip()})\n"
                    "print('removed=' + str(len(removed)) + ' failed=' + str(len(failed)))\n"
                    "for item in failed[:5]:\n"
                    "  print('FAIL ' + item.get('image','') + ' ' + (item.get('out','')[:200]))\n"
                )
                cmd = f"IMAGES_JSON={shlex.quote(images_json)} python3 -c {shlex.quote(py_rm)}"
                _exec_sudo(cmd, timeout=180.0, stage='docker.images.overwrite')
            except Exception as exc:
                try:
                    log_f.write(f"{log_prefix}Failed removing existing scenario images: {exc}\n")
                except Exception:
                    pass
                try:
                    remote_client.close()
                except Exception:
                    pass
                try:
                    log_f.close()
                except Exception:
                    pass
                _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
                return jsonify({
                    'error': f'Failed removing existing scenario images on CORE VM: {exc}',
                    'kind': 'existing_images_overwrite_failed',
                    'scenario_tag': scenario_tag,
                }), 500

        # Now that SSH is confirmed and cleanup has run, block on active session conflicts.
        blocking_sessions = _collect_blocking_sessions()
        if blocking_sessions and not adv_auto_kill_sessions:
            count = len(blocking_sessions)
            message = (
                f"CORE VM {remote_desc} already has {count} active session(s); finish or stop the running scenario before starting another."
            )
            try:
                log_f.write(f"[remote] {message}\n")
            except Exception:
                pass
            try:
                remote_client.close()
            except Exception:
                pass
            try:
                log_f.close()
            except Exception:
                pass
            _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
            payload = {
                "error": message,
                "session_count": count,
                "core_host": core_host,
                "core_port": core_port,
                "active_sessions": [
                    {
                        "id": entry.get('id'),
                        "state": entry.get('state'),
                        "nodes": entry.get('nodes'),
                        "file": entry.get('file'),
                    }
                    for entry in blocking_sessions[:5]
                ],
            }
            return jsonify(payload), 423
        if blocking_sessions and adv_auto_kill_sessions:
            try:
                log_f.write(
                    f"[remote] NOTE: CORE VM {remote_desc} has {len(blocking_sessions)} active session(s); Advanced option will attempt to delete them before running.\n"
                )
            except Exception:
                pass

        # Checks run after cleanup actions.
        if adv_check_core_version:
            try:
                log_f.write(f"{log_prefix}=== Advanced: Check CORE version (expected 9.2.1) ===\n")
            except Exception:
                pass
            _check_core_version('9.2.1')
            try:
                log_f.write(f"{log_prefix}CORE version check passed.\n")
            except Exception:
                pass

        if adv_restart_core_daemon:
            try:
                log_f.write(f"{log_prefix}=== Advanced: Restart core-daemon ===\n")
            except Exception:
                pass
            _maybe_restart_core_daemon()
            try:
                log_f.write(f"{log_prefix}core-daemon restart requested.\n")
            except Exception:
                pass

        try:
            _write_sse_marker(
                log_f,
                'phase',
                {
                    'stage': 'core-daemon.precheck.begin',
                    'detail': 'Ensuring core-daemon is ready (pre-workspace)',
                },
            )
        except Exception:
            pass
        _check_remote_daemon_before_setup(
            client=remote_client,
            core_cfg=core_cfg,
            auto_start_allowed=auto_start_allowed,
            log_handle=log_f,
            log_prefix=log_prefix,
        )
        try:
            _write_sse_marker(
                log_f,
                'phase',
                {
                    'stage': 'core-daemon.precheck.done',
                    'detail': 'core-daemon verified (pre-workspace)',
                },
            )
        except Exception:
            pass
        try:
            pre_dir = os.path.join(out_dir or _outputs_dir(), 'core-pre')
            pre_saved = _grpc_save_current_session_xml_with_config(core_cfg, pre_dir)
        except Exception:
            pre_saved = None
        if pre_saved:
            app.logger.debug("[async] Pre-run session XML saved to %s", pre_saved)
    except CoreDaemonError as exc:
        try:
            log_f.write(f"{log_prefix}{exc}\n")
        except Exception:
            pass
        try:
            remote_client.close()
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
        status = 428 if isinstance(exc, CoreDaemonMissingError) else 409
        payload = {
            "error": str(exc),
            "daemon_missing": isinstance(exc, CoreDaemonMissingError),
            "daemon_conflict": isinstance(exc, CoreDaemonConflictError),
            "can_auto_start": bool(getattr(exc, 'can_auto_start', False)),
            "daemon_pids": getattr(exc, 'pids', []),
            "start_command": getattr(exc, 'start_command', CORE_DAEMON_START_COMMAND),
        }
        return jsonify(payload), status

    # Advanced: kill active sessions (if requested). This is done after core-daemon is confirmed,
    # to maximize chances that gRPC session deletion works.
    if adv_auto_kill_sessions:
        try:
            log_f.write(f"{log_prefix}=== Advanced: Auto-kill any running sessions ===\n")
        except Exception:
            pass
        deleted_ids, kill_errors = _maybe_kill_active_sessions()
        try:
            if deleted_ids:
                log_f.write(f"{log_prefix}Deleted sessions: {', '.join(str(x) for x in deleted_ids)}\n")
            else:
                log_f.write(f"{log_prefix}No sessions deleted.\n")
            for err in kill_errors:
                log_f.write(f"{log_prefix}{err}\n")
        except Exception:
            pass
        # Re-check session conflicts.
        blocking_sessions = _collect_blocking_sessions()
        if blocking_sessions:
            count = len(blocking_sessions)
            message = (
                f"CORE VM {remote_desc} still has {count} active session(s); cannot start another session."
            )
            try:
                log_f.write(f"{log_prefix}{message}\n")
            except Exception:
                pass
            try:
                remote_client.close()
            except Exception:
                pass
            try:
                log_f.close()
            except Exception:
                pass
            _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
            payload = {
                "error": message,
                "session_count": count,
                "core_host": core_host,
                "core_port": core_port,
                "deleted": deleted_ids,
                "errors": kill_errors,
                "active_sessions": [
                    {
                        "id": entry.get('id'),
                        "state": entry.get('state'),
                        "nodes": entry.get('nodes'),
                        "file": entry.get('file'),
                    }
                    for entry in blocking_sessions[:5]
                ],
            }
            return jsonify(payload), 423

    # Always emit a remote filesystem inventory early so Execute logs show whether
    # /tmp/vulns artifacts exist on the CORE VM even if the run fails before postrun.
    try:
        _log_remote_vulns_inventory_to_handle(core_cfg=core_cfg, log_handle=log_f, stage='pre_run')
    except Exception:
        pass

    try:
        remote_ctx = _prepare_remote_cli_context(
            client=remote_client,
            run_id=run_id,
            xml_path=xml_path,
            preview_plan_path=preview_plan_path,
            log_handle=log_f,
        )
        try:
            log_f.write(f"{log_prefix}Workspace: repo={remote_ctx.get('repo_dir')} run_dir={remote_ctx.get('run_dir')}\n")
            _write_sse_marker(
                log_f,
                'phase',
                {
                    'stage': 'remote.workspace',
                    'repo_dir': remote_ctx.get('repo_dir'),
                    'run_dir': remote_ctx.get('run_dir'),
                    'xml_path': remote_ctx.get('xml_path'),
                    'preview_plan_path': remote_ctx.get('preview_plan_path'),
                },
            )
        except Exception:
            pass
        daemon_pid = _ensure_remote_core_daemon_ready(
            client=remote_client,
            core_cfg=core_cfg,
            auto_start_allowed=auto_start_allowed,
            sudo_password=core_cfg.get('ssh_password'),
            logger=getattr(app, 'logger', logging.getLogger(__name__)),
            log_handle=log_f,
            log_prefix=log_prefix,
        )
        try:
            log_f.write(f"{log_prefix}core-daemon PID {daemon_pid} is active\n")
        except Exception:
            pass
        remote_python = _select_remote_python_interpreter(remote_client, core_cfg)
        try:
            log_f.write(f"{log_prefix}Using python interpreter: {remote_python}\n")
        except Exception:
            pass
    except RemoteRepoMissingError as exc:
        try:
            log_f.write(f"{log_prefix}{exc}\n")
        except Exception:
            pass
        _purge_remote_run_dir()
        try:
            remote_client.close()
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
        return jsonify({"error": str(exc), "missing_repo": exc.repo_path, "can_push_repo": True}), 409
    except CoreDaemonMissingError as exc:
        try:
            log_f.write(f"{log_prefix}{exc}\n")
        except Exception:
            pass
        _purge_remote_run_dir()
        try:
            remote_client.close()
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
        return jsonify({
            "error": str(exc),
            "daemon_missing": True,
            "can_auto_start": bool(getattr(exc, 'can_auto_start', False)),
            "start_command": getattr(exc, 'start_command', CORE_DAEMON_START_COMMAND),
        }), 428
    except CoreDaemonConflictError as exc:
        detail = str(exc)
        try:
            log_f.write(f"{log_prefix}{detail}\n")
        except Exception:
            pass
        _purge_remote_run_dir()
        try:
            remote_client.close()
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
        return jsonify({
            "error": detail,
            "daemon_conflict": True,
            "daemon_pids": getattr(exc, 'pids', []),
        }), 409
    except Exception as exc:
        try:
            log_f.write(f"{log_prefix}{exc}\n")
        except Exception:
            pass
        _purge_remote_run_dir()
        try:
            remote_client.close()
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
        return jsonify({"error": str(exc)}), 500
    # NOTE: docker cleanup steps are executed earlier in the run (before checks).
    cli_args = [
        remote_python,
        '-u',
        '-m',
        'core_topo_gen.cli',
        '--xml',
        remote_ctx['xml_path'] if remote_ctx else xml_path,
        '--host',
        core_host,
        '--port',
        str(core_port),
        '--verbose',
    ]
    if docker_remove_conflicts:
        cli_args.append('--docker-remove-conflicts')
    if seed is not None:
        cli_args.extend(['--seed', str(seed)])
    if active_scenario_name:
        cli_args.extend(['--scenario', active_scenario_name])
    if remote_ctx and remote_ctx.get('preview_plan_path'):
        cli_args.extend(['--preview-plan', remote_ctx['preview_plan_path']])
    elif preview_plan_path:
        # Fallback (should not happen if remote_ctx is populated)
        cli_args.extend(['--preview-plan', preview_plan_path])
    cli_cmd = ' '.join(shlex.quote(arg) for arg in cli_args)
    work_dir = remote_ctx['repo_dir'] if remote_ctx else '.'
    activate_prefix = ''
    venv_bin_remote = str(core_cfg.get('venv_bin') or '').strip()
    if venv_bin_remote:
        activate_path = posixpath.join(venv_bin_remote.rstrip('/'), 'activate')
        activate_prefix = f". {shlex.quote(activate_path)} >/dev/null 2>&1 || true; "
    # In remote CORE VM mode, docker operations happen on the remote host.
    # Many CORE VMs require sudo for docker access (no docker group membership).
    # We enable sudo mode for docker commands and, when a password is available,
    # provide it via stdin to avoid leaking secrets in the command line.
    docker_env_parts: list[str] = []
    try:
        if _coerce_bool(core_cfg.get('ssh_enabled')):
            docker_env_parts.append('CORETG_DOCKER_USE_SUDO=1')
            # Fail fast if docker compose pull fails (we surface the error and cancel the run).
            docker_env_parts.append('CORETG_DOCKER_STRICT_PULL=1')
            if core_cfg.get('ssh_password'):
                docker_env_parts.append('CORETG_DOCKER_SUDO_PASSWORD_STDIN=1')
    except Exception:
        docker_env_parts = []
    docker_env_prefix = (' '.join(docker_env_parts) + ' ') if docker_env_parts else ''
    flow_env_parts: list[str] = []
    # Deliver Flow generator artifacts into vuln containers by copying files in,
    # rather than bind-mounting directories from the host.
    flow_env_parts.append('CORETG_FLOW_ARTIFACTS_MODE=copy')
    try:
        if remote_ctx and remote_ctx.get('base_dir'):
            flow_env_parts.append(f"CORE_REMOTE_BASE_DIR={shlex.quote(str(remote_ctx.get('base_dir')))}")
    except Exception:
        pass
    flow_env_prefix = (' '.join(flow_env_parts) + ' ') if flow_env_parts else ''
    remote_command = (
        f"{activate_prefix}cd {shlex.quote(work_dir)} && "
        f"CORETG_SCENARIO_TAG={shlex.quote(scenario_tag)} {flow_env_prefix}{docker_env_prefix}PYTHONUNBUFFERED=1 {cli_cmd}"
    )
    try:
        log_f.write(f"{log_prefix}Launching CLI in {work_dir}\n")
        try:
            log_f.write(f"{log_prefix}Flow artifacts mode: copy (docker cp into containers)\n")
        except Exception:
            pass
        _write_sse_marker(
            log_f,
            'phase',
            {
                'stage': 'remote.cli.launch',
                'work_dir': work_dir,
                'command': cli_cmd,
                'scenario_tag': scenario_tag,
                'venv_bin': venv_bin_remote or None,
            },
        )
    except Exception:
        pass
    try:
        transport = remote_client.get_transport()
        if transport is None or not transport.is_active():
            raise RuntimeError('SSH transport unavailable while launching remote CLI')
        channel = transport.open_session()
        channel.set_combine_stderr(True)
        channel.exec_command(f"bash -lc {shlex.quote(remote_command)}")

        # If configured, send sudo password for docker helpers via stdin (single line).
        try:
            pw = core_cfg.get('ssh_password')
            if pw and _coerce_bool(core_cfg.get('ssh_enabled')):
                channel.send(str(pw) + "\n")
        except Exception:
            pass
        output_thread = threading.Thread(
            target=_relay_remote_channel_to_log,
            args=(channel, log_f),
            name=f'remote-cli-{run_id[:8]}',
            daemon=True,
        )
        output_thread.start()
        proc = _RemoteProcessHandle(channel=channel, client=remote_client)
        proc.attach_output_thread(output_thread)
    except Exception as exc:
        try:
            log_f.write(f"{log_prefix}Failed to start remote CLI: {exc}\n")
        except Exception:
            pass
        try:
            remote_client.close()
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        if ssh_tunnel:
            try:
                ssh_tunnel.close()
            except Exception:
                pass
        return jsonify({"error": f"Failed to launch remote CLI: {exc}"}), 500
    core_public = dict(core_cfg)
    core_public.pop('ssh_password', None)
    RUNS[run_id] = {
        'proc': proc,
        'log_path': log_path,
        'xml_path': xml_path,
        'done': False,
        'returncode': None,
        'pre_xml_path': pre_saved,
        'core_host': core_host,
        'core_port': core_port,
        'core_cfg': core_cfg,
        'core_cfg_public': core_public,
        'forward_host': conn_host,
        'forward_port': conn_port,
        'ssh_tunnel': ssh_tunnel,
        'scenario_names': scen_names,
        'scenario_name': active_scenario_name,
        'scenario_core': scenario_core_public,
        'post_xml_path': None,
        'history_added': False,
        'preview_plan_path': preview_plan_path,
        'summary_path': None,
        'remote': True,
        'remote_context': remote_ctx,
        'remote_run_dir': (remote_ctx or {}).get('run_dir'),
        'remote_repo_dir': (remote_ctx or {}).get('repo_dir'),
        'remote_xml_path': (remote_ctx or {}).get('xml_path'),
        'remote_base_dir': (remote_ctx or {}).get('base_dir'),
        'remote_preview_plan_path': (remote_ctx or {}).get('preview_plan_path'),
        'flow_artifacts_copied': False,
    }
    # Start a background finalizer so history is appended even if the UI does not poll /run_status
    def _wait_and_finalize_async(run_id_local: str):
        meta: Dict[str, Any] | None = None
        try:
            meta = RUNS.get(run_id_local)
            if not meta:
                return
            p = meta.get('proc')
            if not p:
                return
            rc = p.wait()
            meta['done'] = True
            meta['returncode'] = rc

            try:
                _maybe_copy_flow_artifacts_into_containers(meta, stage='postrun')
            except Exception:
                pass
            try:
                _sync_remote_artifacts(meta)
            except Exception:
                pass
            # mirror the logic in run_status to extract artifacts and append history
            try:
                xml_path_local = meta.get('xml_path')
                report_md = None
                txt = ''
                try:
                    lp = meta.get('log_path')
                    if lp and os.path.exists(lp):
                        with open(lp, 'r', encoding='utf-8', errors='ignore') as f:
                            txt = f.read()
                        report_md = _extract_report_path_from_text(txt)
                except Exception:
                    report_md = None
                if not report_md:
                    report_md = _find_latest_report_path()
                if report_md:
                    app.logger.info("[async-finalizer] Detected report path: %s", report_md)
                summary_json = _extract_summary_path_from_text(txt)
                if not summary_json:
                    summary_json = _derive_summary_from_report(report_md)
                if not summary_json and not report_md:
                    summary_json = _find_latest_summary_path()
                if summary_json and not os.path.exists(summary_json):
                    summary_json = None
                if summary_json:
                    meta['summary_path'] = summary_json
                    app.logger.info("[async-finalizer] Detected summary path: %s", summary_json)
                # Best-effort: capture post-run CORE session XML
                post_saved = None
                try:
                    out_dir = os.path.dirname(xml_path_local or '')
                    post_dir = os.path.join(out_dir, 'core-post') if out_dir else os.path.join(_outputs_dir(), 'core-post')
                    sid = _extract_session_id_from_text(txt)
                    scenario_label = meta.get('scenario_name') or active_scenario_name
                    if not scenario_label:
                        try:
                            sns_meta = meta.get('scenario_names') or []
                            if isinstance(sns_meta, list) and sns_meta:
                                scenario_label = sns_meta[0]
                        except Exception:
                            scenario_label = None
                    if sid:
                        _record_session_mapping(xml_path_local, sid, scenario_label)
                        try:
                            sid_int = int(str(sid).strip())
                            cfg_for_meta = meta.get('core_cfg') if isinstance(meta.get('core_cfg'), dict) else None
                            if cfg_for_meta:
                                _write_remote_session_scenario_meta(
                                    cfg_for_meta,
                                    session_id=sid_int,
                                    scenario_name=scenario_label,
                                    scenario_xml_basename=os.path.basename(xml_path_local or '') or None,
                                    logger=app.logger,
                                )
                        except Exception:
                            pass
                    cfg_for_post = meta.get('core_cfg') or {
                        'host': meta.get('core_host') or CORE_HOST,
                        'port': meta.get('core_port') or CORE_PORT,
                    }
                    post_saved = _grpc_save_current_session_xml_with_config(cfg_for_post, post_dir, session_id=sid)
                except Exception:
                    post_saved = None
                if post_saved:
                    meta['post_xml_path'] = post_saved
                    app.logger.debug("[async-finalizer] Post-run session XML saved to %s", post_saved)
                # Build single-scenario XML, then a Full Scenario bundle including scripts
                single_xml = None
                try:
                    single_xml = _write_single_scenario_xml(xml_path_local, active_scenario_name, out_dir=os.path.dirname(xml_path_local or ''))
                except Exception:
                    single_xml = None
                bundle_xml = single_xml or xml_path_local
                full_bundle = _build_full_scenario_archive(
                    os.path.dirname(bundle_xml or ''),
                    bundle_xml,
                    (report_md if (report_md and os.path.exists(report_md)) else None),
                    meta.get('pre_xml_path'),
                    post_saved,
                    summary_path=summary_json,
                    run_id=run_id_local,
                )
                if full_bundle:
                    meta['full_scenario_path'] = full_bundle
                session_xml_path = post_saved if (post_saved and os.path.exists(post_saved)) else None
                history_ok = _append_run_history({
                    'timestamp': datetime.datetime.utcnow().isoformat() + 'Z',
                    'mode': 'async',
                    'xml_path': xml_path_local,
                    'post_xml_path': session_xml_path,
                    'session_xml_path': session_xml_path,
                    'scenario_xml_path': xml_path_local,
                    'report_path': report_md if (report_md and os.path.exists(report_md)) else None,
                    'summary_path': summary_json if (summary_json and os.path.exists(summary_json)) else None,
                    'pre_xml_path': meta.get('pre_xml_path'),
                    'full_scenario_path': full_bundle,
                    'single_scenario_xml_path': single_xml,
                    'returncode': rc,
                    'run_id': run_id_local,
                    'scenario_names': meta.get('scenario_names') or [],
                    'scenario_name': meta.get('scenario_name'),
                    'preview_plan_path': meta.get('preview_plan_path'),
                    'core': meta.get('core_cfg_public') or _normalize_core_config(meta.get('core_cfg') or {}, include_password=False),
                    'scenario_core': meta.get('scenario_core'),
                })
                if history_ok:
                    meta['history_added'] = True
            except Exception as e_final:
                try:
                    app.logger.exception("[async-finalizer] failed finalizing run %s: %s", run_id_local, e_final)
                except Exception:
                    pass
            finally:
                try:
                    _cleanup_remote_workspace(meta)
                except Exception:
                    pass
        except Exception:
            # swallow all exceptions to avoid crashing the web server
            try:
                app.logger.exception("[async-finalizer] unexpected error for run %s", run_id_local)
            except Exception:
                pass
        finally:
            if meta:
                _close_async_run_tunnel(meta)

    try:
        t = threading.Thread(target=_wait_and_finalize_async, args=(run_id,), daemon=True)
        t.start()
        app.logger.debug("[async] Finalizer thread started for run_id=%s", run_id)
    except Exception:
        pass
    return jsonify({"run_id": run_id})


@app.route('/run_status/<run_id>', methods=['GET'])
def run_status(run_id: str):
    meta = RUNS.get(run_id)
    if not meta:
        return jsonify({"error": "not found"}), 404
    proc = meta.get('proc')
    if proc and meta.get('returncode') is None:
        rc = proc.poll()
        if rc is not None:
            meta['done'] = True
            meta['returncode'] = rc
            try:
                _maybe_copy_flow_artifacts_into_containers(meta, stage='postrun')
            except Exception:
                pass
            try:
                _sync_remote_artifacts(meta)
            except Exception:
                pass
            # Append history once (success or failure)
            if not meta.get('history_added'):
                try:
                    active_scenario_name = None
                    try:
                        sns = meta.get('scenario_names') or []
                        if isinstance(sns, list) and sns:
                            active_scenario_name = sns[0]
                    except Exception:
                        active_scenario_name = None
                    xml_path_local = meta.get('xml_path')
                    # Parse report path from log contents; fallback to latest under reports/
                    report_md = None
                    txt = ''
                    try:
                        lp = meta.get('log_path')
                        if lp and os.path.exists(lp):
                            with open(lp, 'r', encoding='utf-8', errors='ignore') as f:
                                txt = f.read()
                            report_md = _extract_report_path_from_text(txt)
                    except Exception:
                        report_md = None
                    if not report_md:
                        report_md = _find_latest_report_path()
                    if report_md:
                        app.logger.info("[async] Detected report path: %s", report_md)
                    summary_json = _extract_summary_path_from_text(txt)
                    if not summary_json:
                        summary_json = _derive_summary_from_report(report_md)
                    if not summary_json and not report_md:
                        summary_json = _find_latest_summary_path()
                    if summary_json and not os.path.exists(summary_json):
                        summary_json = None
                    if summary_json:
                        meta['summary_path'] = summary_json
                        app.logger.info("[async] Detected summary path: %s", summary_json)
                    # Best-effort: capture post-run CORE session XML
                    post_saved = None
                    try:
                        out_dir = os.path.dirname(xml_path_local or '')
                        post_dir = os.path.join(out_dir, 'core-post') if out_dir else os.path.join(_outputs_dir(), 'core-post')
                        # Parse session id from logs if available for precise save
                        sid = _extract_session_id_from_text(txt)
                        scenario_label = meta.get('scenario_name') or active_scenario_name
                        if not scenario_label:
                            try:
                                sns_meta = meta.get('scenario_names') or []
                                if isinstance(sns_meta, list) and sns_meta:
                                    scenario_label = sns_meta[0]
                            except Exception:
                                scenario_label = None
                        if sid:
                            _record_session_mapping(xml_path_local, sid, scenario_label)
                            try:
                                sid_int = int(str(sid).strip())
                                cfg_for_meta = meta.get('core_cfg') if isinstance(meta.get('core_cfg'), dict) else None
                                if cfg_for_meta:
                                    _write_remote_session_scenario_meta(
                                        cfg_for_meta,
                                        session_id=sid_int,
                                        scenario_name=scenario_label,
                                        scenario_xml_basename=os.path.basename(xml_path_local or '') or None,
                                        logger=app.logger,
                                    )
                            except Exception:
                                pass
                        cfg_for_post = meta.get('core_cfg') or {
                            'host': meta.get('core_host') or CORE_HOST,
                            'port': meta.get('core_port') or CORE_PORT,
                        }
                        post_saved = _grpc_save_current_session_xml_with_config(cfg_for_post, post_dir, session_id=sid)
                    except Exception:
                        post_saved = None
                    if post_saved:
                        meta['post_xml_path'] = post_saved
                        app.logger.debug("[async] Post-run session XML saved to %s", post_saved)
                    # Build single-scenario XML, then a Full Scenario bundle including scripts
                    single_xml = None
                    try:
                        single_xml = _write_single_scenario_xml(xml_path_local, active_scenario_name, out_dir=os.path.dirname(xml_path_local or ''))
                    except Exception:
                        single_xml = None
                    bundle_xml = single_xml or xml_path_local
                    full_bundle = _build_full_scenario_archive(
                        os.path.dirname(bundle_xml or ''),
                        bundle_xml,
                        (report_md if (report_md and os.path.exists(report_md)) else None),
                        meta.get('pre_xml_path'),
                        post_saved,
                        summary_path=summary_json,
                        run_id=run_id,
                    )
                    if full_bundle:
                        meta['full_scenario_path'] = full_bundle
                    session_xml_path = post_saved if (post_saved and os.path.exists(post_saved)) else None
                    history_ok = _append_run_history({
                        'timestamp': datetime.datetime.utcnow().isoformat() + 'Z',
                        'mode': 'async',
                        'xml_path': xml_path_local,
                        'post_xml_path': session_xml_path,
                        'session_xml_path': session_xml_path,
                        'scenario_xml_path': xml_path_local,
                        'report_path': report_md if (report_md and os.path.exists(report_md)) else None,
                        'summary_path': summary_json if (summary_json and os.path.exists(summary_json)) else None,
                        'pre_xml_path': meta.get('pre_xml_path'),
                        'full_scenario_path': full_bundle,
                        'single_scenario_xml_path': single_xml,
                        'returncode': rc,
                        'run_id': run_id,
                        'scenario_names': meta.get('scenario_names') or [],
                        'scenario_name': meta.get('scenario_name') or active_scenario_name,
                        'preview_plan_path': meta.get('preview_plan_path'),
                        'core': meta.get('core_cfg_public') or _normalize_core_config(meta.get('core_cfg') or {}, include_password=False),
                        'scenario_core': meta.get('scenario_core'),
                    })
                except Exception as e_hist:
                    try:
                        app.logger.exception("[async] failed appending run history: %s", e_hist)
                    except Exception:
                        pass
                finally:
                    if 'history_ok' in locals() and history_ok:
                        meta['history_added'] = True
                    _close_async_run_tunnel(meta)
                    try:
                        _cleanup_remote_workspace(meta)
                    except Exception:
                        pass
    if meta.get('done'):
        try:
            _sync_remote_artifacts(meta)
        except Exception:
            pass
    # Determine report path
    xml_path = meta.get('xml_path', '')
    out_dir = os.path.dirname(xml_path)
    # Determine report path (attempt to parse log each time so UI can link it without refresh)
    report_md = None
    txt = ''
    try:
        lp = meta.get('log_path')
        if lp and os.path.exists(lp):
            with open(lp, 'r', encoding='utf-8', errors='ignore') as f:
                txt = f.read()
            report_md = _extract_report_path_from_text(txt)
    except Exception:
        report_md = None

    docker_conflicts = _extract_docker_conflicts_from_text(txt)
    summary_json = _extract_summary_path_from_text(txt)
    if not summary_json:
        summary_json = _derive_summary_from_report(report_md)
    if not summary_json:
        summary_json = meta.get('summary_path')
    if not summary_json and not report_md:
        summary_json = _find_latest_summary_path()
    if summary_json and not os.path.exists(summary_json):
        summary_json = None
    if summary_json:
        meta['summary_path'] = summary_json
    return jsonify({
        'done': bool(meta.get('done')),
        'returncode': meta.get('returncode'),
        'docker_conflicts': docker_conflicts,
        'report_path': report_md if (report_md and os.path.exists(report_md)) else None,
        'summary_path': summary_json if (summary_json and os.path.exists(summary_json)) else None,
        'xml_path': (meta.get('post_xml_path') if meta.get('post_xml_path') and os.path.exists(meta.get('post_xml_path')) else None),
        'log_path': meta.get('log_path'),
        'scenario_xml_path': xml_path,
        'pre_xml_path': meta.get('pre_xml_path'),
        'full_scenario_path': (lambda p: p if (p and os.path.exists(p)) else None)(meta.get('full_scenario_path')),
        'core': meta.get('core_cfg_public') or _normalize_core_config(meta.get('core_cfg') or {}, include_password=False),
        'forward_host': meta.get('forward_host'),
        'forward_port': meta.get('forward_port'),
    })


@app.route('/upload_base', methods=['POST'])
def upload_base():
    user = _current_user()
    f = request.files.get('base_xml')
    if not f or f.filename == '':
        flash('No base scenario file selected.')
        return redirect(url_for('index'))
    filename = secure_filename(f.filename)
    base_dir = os.path.join(app.config['UPLOAD_FOLDER'], 'base')
    os.makedirs(base_dir, exist_ok=True)
    unique = datetime.datetime.now().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:8]
    saved_path = os.path.join(base_dir, f"{unique}-{filename}")
    f.save(saved_path)
    ok, errs = _validate_core_xml(saved_path)
    payload = _default_scenarios_payload()
    payload['base_upload'] = {
        'path': saved_path,
        'valid': bool(ok),
        'display_name': filename,
        'exists': True,
    }
    if not ok:
        flash('Base scenario XML is INVALID. See details link for errors.')
    else:
        flash('Base scenario uploaded and validated.')
        try:
            # set the base scenario file path on the first scenario for convenience
            payload['scenarios'][0]['base']['filepath'] = saved_path
            payload['scenarios'][0]['base']['display_name'] = filename
        except Exception:
            pass
    _attach_base_upload(payload)
    if payload.get('base_upload'):
        _save_base_upload_state(payload['base_upload'])
    payload = _prepare_payload_for_index(payload, user=user)
    return render_template('index.html', payload=payload, logs=(errs if not ok else ''), xml_preview='')

@app.route('/remove_base', methods=['POST'])
def remove_base():
    """Clear the base scenario file reference from the first scenario."""
    user = _current_user()
    try:
        payload = _default_scenarios_payload()
        # If scenarios_json posted, honor that to keep user edits
        data_str = request.form.get('scenarios_json')
        if data_str:
            try:
                data = json.loads(data_str)
                if isinstance(data, dict) and 'scenarios' in data:
                    payload['scenarios'] = data['scenarios']
            except Exception:
                pass
        # Clear the base filepath of first scenario
        try:
            if payload['scenarios'] and isinstance(payload['scenarios'][0], dict):
                payload['scenarios'][0].setdefault('base', {}).update({'filepath': '', 'display_name': ''})
        except Exception:
            pass
        flash('Base scenario removed.')
        _clear_base_upload_state()
        payload.pop('base_upload', None)
        # Do not attach base upload (cleared)
        payload = _prepare_payload_for_index(payload, user=user)
        return render_template('index.html', payload=payload, logs='', xml_preview='')
    except Exception as e:
        flash(f'Failed to remove base: {e}')
        return redirect(url_for('index'))


@app.route('/base_details')
def base_details():
    xml_path = request.args.get('path')
    if not xml_path or not os.path.exists(xml_path):
        return "File not found", 404
    ok, errs = _validate_core_xml(xml_path)
    summary = _analyze_core_xml(xml_path) if ok else {'error': errs}
    return render_template('base_details.html', xml_path=xml_path, valid=ok, errors=errs, summary=summary)

# ---------------- CORE Management (sessions and XMLs) ----------------

def _core_sessions_store_path() -> str:
    return os.path.join(_outputs_dir(), 'core_sessions.json')


def _load_core_sessions_store() -> dict:
    p = _core_sessions_store_path()
    try:
        if os.path.exists(p):
            with open(p, 'r', encoding='utf-8') as f:
                d = json.load(f)
                return d if isinstance(d, dict) else {}
    except Exception:
        pass
    return {}

def _session_store_entry_session_id(value: Any) -> Optional[int]:
    candidate: Any
    if isinstance(value, dict):
        candidate = value.get('session_id') or value.get('id')
    else:
        candidate = value
    if candidate in (None, ''):
        return None
    try:
        return int(candidate)
    except Exception:
        return None


def _session_store_entry_scenario_norm(value: Any) -> str:
    if not isinstance(value, dict):
        return ''
    raw = value.get('scenario_norm') or value.get('scenario_name') or value.get('scenario')
    if not raw:
        return ''
    return _normalize_scenario_label(raw)


def _session_store_entry_updated_at_epoch(value: Any) -> Optional[float]:
    """Best-effort parse of mapping update time.

    Stored as ISO8601 (usually with trailing 'Z'). Returns epoch seconds.
    """
    if not isinstance(value, dict):
        return None
    raw = value.get('updated_at') or value.get('recorded_at') or value.get('ts')
    if not raw:
        return None
    try:
        text = str(raw).strip()
    except Exception:
        return None
    if not text:
        return None
    try:
        # Accept both explicit Z and +00:00 offsets.
        if text.endswith('Z'):
            text = text[:-1] + '+00:00'
        dt = datetime.datetime.fromisoformat(text)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=datetime.timezone.utc)
        return dt.timestamp()
    except Exception:
        return None


def _safe_path_mtime_epoch(path_value: Any) -> Optional[float]:
    if not path_value:
        return None
    try:
        ap = os.path.abspath(str(path_value))
    except Exception:
        ap = str(path_value)
    try:
        return os.path.getmtime(ap)
    except Exception:
        return None


def _save_core_sessions_store(d: dict) -> None:
    try:
        os.makedirs(os.path.dirname(_core_sessions_store_path()), exist_ok=True)
        tmp = _core_sessions_store_path() + '.tmp'
        with open(tmp, 'w', encoding='utf-8') as f:
            json.dump(d, f, indent=2)
        os.replace(tmp, _core_sessions_store_path())
    except Exception:
        pass


def _session_store_entry_core_host(value: Any) -> str:
    if not isinstance(value, dict):
        return ''
    raw = value.get('core_host') or value.get('host')
    return str(raw).strip() if raw not in (None, '') else ''


def _session_store_entry_core_port(value: Any) -> Optional[int]:
    if not isinstance(value, dict):
        return None
    raw = value.get('core_port') or value.get('port')
    if raw in (None, ''):
        return None
    try:
        return int(raw)
    except Exception:
        return None


def _session_store_entry_matches_core(value: Any, host: str, port: int) -> bool:
    """True when the mapping entry belongs to the given CORE daemon."""
    entry_host = _session_store_entry_core_host(value)
    entry_port = _session_store_entry_core_port(value)
    if entry_host and entry_port is not None:
        return (entry_host == str(host).strip()) and (int(entry_port) == int(port))
    # Legacy entries without core_host/core_port are treated as non-matching to avoid
    # cross-VM session-id collisions.
    return False


def _migrate_core_sessions_store_with_core_targets(store: dict, history: list[dict]) -> dict:
    """Best-effort: tag legacy core_sessions.json entries with core_host/core_port.

    We use the stored scenario_norm to look up that scenario's CORE host/port from run history.
    """
    if not isinstance(store, dict) or not store:
        return store
    if not isinstance(history, list) or not history:
        return store

    # Build scenario_norm -> (core_host, core_port) map.
    scenario_to_core: dict[str, tuple[str, int]] = {}
    for entry in history:
        if not isinstance(entry, dict):
            continue
        scen = (entry.get('scenario_name') or '').strip()
        norm = _normalize_scenario_label(scen)
        if not norm or norm in scenario_to_core:
            continue
        try:
            cfg = _select_core_config_for_page(norm, history, include_password=False)
            chost = str(cfg.get('host') or CORE_HOST).strip()
            cport = int(cfg.get('port') or CORE_PORT)
            scenario_to_core[norm] = (chost, cport)
        except Exception:
            continue

    dirty = False
    migrated = dict(store)
    for path, value in migrated.items():
        if not isinstance(value, dict):
            continue
        # Add a best-effort timestamp for older entries so "latest mapping" selection
        # behaves sensibly even when keys are reused across runs.
        if not value.get('updated_at'):
            mt = _safe_path_mtime_epoch(path)
            if mt is not None:
                try:
                    value['updated_at'] = datetime.datetime.fromtimestamp(
                        mt,
                        tz=datetime.timezone.utc,
                    ).isoformat().replace('+00:00', 'Z')
                    dirty = True
                except Exception:
                    pass
        if _session_store_entry_core_host(value) and (_session_store_entry_core_port(value) is not None):
            continue
        norm = _session_store_entry_scenario_norm(value)
        if not norm:
            continue
        target = scenario_to_core.get(norm)
        if not target:
            continue
        chost, cport = target
        value['core_host'] = chost
        value['core_port'] = int(cport)
        dirty = True

    if dirty:
        try:
            _save_core_sessions_store(migrated)
        except Exception:
            pass
    return migrated


def _filter_core_sessions_store_for_core(store: dict, host: str, port: int) -> dict:
    if not isinstance(store, dict) or not store:
        return {}
    filtered: dict = {}
    for path, value in store.items():
        if _session_store_entry_matches_core(value, host, port):
            filtered[path] = value
    return filtered


def _update_xml_session_mapping(
    xml_path: str,
    session_id: int | None,
    *,
    scenario_name: Optional[str] = None,
    core_host: Optional[str] = None,
    core_port: Optional[int] = None,
) -> None:
    try:
        store = _load_core_sessions_store()
        key = os.path.abspath(xml_path)
        if session_id is None:
            if key in store:
                store.pop(key, None)
        else:
            scenario_label = (scenario_name or '').strip()
            entry: dict[str, Any] = {'session_id': int(session_id)}
            try:
                entry['updated_at'] = datetime.datetime.now(datetime.timezone.utc).isoformat().replace('+00:00', 'Z')
            except Exception:
                pass
            scenario_norm = _normalize_scenario_label(scenario_label)
            if scenario_norm:
                entry['scenario_norm'] = scenario_norm
            if scenario_label:
                entry['scenario_name'] = scenario_label
            if core_host not in (None, ''):
                entry['core_host'] = str(core_host).strip()
            if core_port is not None:
                try:
                    entry['core_port'] = int(core_port)
                except Exception:
                    pass
            store[key] = entry
        _save_core_sessions_store(store)
    except Exception:
        pass


def _record_session_mapping(
    xml_path: str | None,
    session_id: Any,
    scenario_name: Optional[str] = None,
    core_host: Optional[str] = None,
    core_port: Optional[int] = None,
) -> None:
    if not xml_path or session_id in (None, ''):
        return
    try:
        sid_int = int(str(session_id).strip())
    except Exception:
        return
    _update_xml_session_mapping(
        xml_path,
        sid_int,
        scenario_name=scenario_name,
        core_host=core_host,
        core_port=core_port,
    )


def _list_active_core_sessions(
    host: str,
    port: int,
    core_cfg: Optional[Dict[str, Any]] = None,
    *,
    errors: Optional[List[str]] = None,
    meta: Optional[Dict[str, Any]] = None,
) -> list[dict]:
    """Return list of active CORE sessions via gRPC. Best-effort if gRPC missing."""
    logger = getattr(app, 'logger', logging.getLogger(__name__))
    cfg = _normalize_core_config(core_cfg or {'host': host, 'port': port}, include_password=True)
    sessions_raw = _list_active_core_sessions_via_remote_python(cfg, errors=errors, meta=meta, logger=logger)
    items: list[dict] = []
    for entry in sessions_raw:
        try:
            sid = entry.get('id')
            state = entry.get('state')
            file_path = entry.get('file')
            sess_dir = entry.get('dir')
            # If remote session doesn't provide a usable file path *and* we don't even
            # have a session directory hint, fall back to our local store mapping.
            # Prefer paths under outputs/core-sessions (gRPC save_xml exports) when present.
            if (not file_path) and (not sess_dir) and sid is not None:
                try:
                    store_map = _load_core_sessions_store()
                    outputs_root = _outputs_dir()
                    preferred_prefix = os.path.join(outputs_root, 'core-sessions')
                    best_path: Optional[str] = None
                    best_ts: Optional[float] = None
                    best_is_preferred = False
                    legacy_candidates: list[str] = []
                    for pth, stored_entry in store_map.items():
                        try:
                            stored_sid = _session_store_entry_session_id(stored_entry)
                            if stored_sid is None or int(stored_sid) != int(sid):
                                continue
                            # Prefer exact CORE host/port match.
                            if not _session_store_entry_matches_core(stored_entry, host, port):
                                # Legacy entries without core_host/core_port are ambiguous across CORE VMs.
                                if (not _session_store_entry_core_host(stored_entry)) and (
                                    _session_store_entry_core_port(stored_entry) is None
                                ):
                                    legacy_candidates.append(pth)
                                continue

                            ts = _session_store_entry_updated_at_epoch(stored_entry)
                            if ts is None:
                                ts = _safe_path_mtime_epoch(pth)
                            if ts is None:
                                ts = 0.0

                            is_preferred = False
                            try:
                                ap = os.path.abspath(str(pth))
                            except Exception:
                                ap = str(pth)
                            try:
                                is_preferred = bool(ap and preferred_prefix and ap.startswith(preferred_prefix + os.sep))
                            except Exception:
                                is_preferred = False

                            if best_path is None:
                                best_path, best_ts, best_is_preferred = pth, ts, is_preferred
                                continue

                            # Prefer core-sessions exports over any other mapping path.
                            if is_preferred and (not best_is_preferred):
                                best_path, best_ts, best_is_preferred = pth, ts, True
                                continue
                            if is_preferred == best_is_preferred:
                                if best_ts is None or ts >= best_ts:
                                    best_path, best_ts = pth, ts
                        except Exception:
                            continue
                    if best_path:
                        file_path = best_path
                    elif len(legacy_candidates) == 1:
                        file_path = legacy_candidates[0]
                except Exception:
                    pass
            if (not file_path) and sess_dir and os.path.isdir(sess_dir):
                try:
                    for fn in os.listdir(sess_dir):
                        if fn.lower().endswith('.xml'):
                            file_path = os.path.join(sess_dir, fn)
                            break
                except Exception:
                    pass
            nodes_count = entry.get('nodes')
            items.append({
                'id': sid,
                'state': state,
                'nodes': nodes_count if nodes_count is not None else None,
                'file': file_path,
                'dir': sess_dir,
                'filename': None,
            })
        except Exception:
            logger.exception('[core.grpc] Failed processing remote session entry: %s', entry)
            continue
    return items


def _collect_node_ips(node: dict[str, Any]) -> list[str]:
    ips: list[str] = []
    try:
        interfaces = node.get('interfaces') or []
        for iface in interfaces:
            if not isinstance(iface, dict):
                continue
            ip4 = (iface.get('ipv4') or '').strip()
            ip4_mask = (iface.get('ipv4_mask') or '').strip()
            ip6 = (iface.get('ipv6') or '').strip()
            ip6_mask = (iface.get('ipv6_mask') or '').strip()
            if ip4:
                val = f"{ip4}/{ip4_mask}" if ip4_mask else ip4
                if val not in ips:
                    ips.append(val)
            if ip6:
                val6 = f"{ip6}/{ip6_mask}" if ip6_mask else ip6
                if val6 not in ips:
                    ips.append(val6)
    except Exception:
        pass
    return ips


def _collect_named_iface_ips(node: dict[str, Any], *, name_prefix: str) -> list[str]:
    ips: list[str] = []
    prefix = (name_prefix or '').strip().lower()
    if not prefix:
        return ips
    try:
        interfaces = node.get('interfaces') or []
        for iface in interfaces:
            if not isinstance(iface, dict):
                continue
            iface_name = (iface.get('name') or '').strip().lower()
            if not iface_name.startswith(prefix):
                continue
            ip4 = (iface.get('ipv4') or '').strip()
            ip4_mask = (iface.get('ipv4_mask') or '').strip()
            ip6 = (iface.get('ipv6') or '').strip()
            ip6_mask = (iface.get('ipv6_mask') or '').strip()
            if ip4:
                val = f"{ip4}/{ip4_mask}" if ip4_mask else ip4
                if val not in ips:
                    ips.append(val)
            if ip6:
                val6 = f"{ip6}/{ip6_mask}" if ip6_mask else ip6
                if val6 not in ips:
                    ips.append(val6)
    except Exception:
        pass
    return ips


_IPV4_ADDR_RE = re.compile(r"^(?P<ip>(?:\d{1,3}\.){3}\d{1,3})(?:/(?P<suffix>[^\s]+))?$")


def _force_ipv4_prefixlen(ips: list[str], prefixlen: str = "24") -> list[str]:
    forced: list[str] = []
    seen: set[str] = set()
    for raw in ips or []:
        if raw is None:
            continue
        text = str(raw).strip()
        if not text:
            continue
        m = _IPV4_ADDR_RE.match(text)
        if m:
            text = f"{m.group('ip')}/{prefixlen}"
        if text in seen:
            continue
        seen.add(text)
        forced.append(text)
    return forced


def _hitl_details_from_path(xml_path: str) -> list[dict[str, Any]]:
    try:
        abs_path = os.path.abspath(xml_path)
    except Exception:
        abs_path = xml_path
    if not abs_path:
        return []
    try:
        st = os.stat(abs_path)
        mtime = st.st_mtime
    except Exception:
        mtime = None
    cached = _SESSION_HITL_CACHE.get(abs_path)
    if cached and cached.get('mtime') == mtime:
        return cached.get('data', [])
    hitl_details: list[dict[str, Any]] = []
    try:
        summary = _analyze_core_xml(abs_path)
        nodes_summary = summary.get('nodes') or []
        has_rj45 = any(
            isinstance(node, dict) and str(node.get('type') or '').strip().lower() == 'rj45'
            for node in nodes_summary
        )
        has_hitl_nodes = bool(summary.get('hitl_nodes') or [])
        has_explicit_hitl_indicator = has_rj45 or has_hitl_nodes
        # Prefer the gateway/router-side interface IP for the RJ45 attachment.
        # Our generated scenarios name that interface "hitl<idx>", so we can reliably
        # pick only the gateway IP (not router uplinks or the RJ45 node IP).
        gateway_preferred: list[dict[str, Any]] = []
        for node in nodes_summary:
            if not isinstance(node, dict):
                continue
            node_type = (node.get('type') or '').strip()
            if node_type.lower() != 'router':
                continue
            ips = _collect_named_iface_ips(node, name_prefix='hitl')
            ips = _force_ipv4_prefixlen(ips, prefixlen='24')
            if not ips:
                continue
            name = (node.get('name') or node.get('id') or '').strip()
            gateway_preferred.append({'name': name or 'HITL gateway', 'type': node_type, 'ips': ips[:1]})

        # Only accept hitl* router interfaces as HITL if there is an explicit HITL indicator
        # in the XML analysis (e.g., an RJ45/external node or analyzer-provided hitl_nodes).
        if gateway_preferred and has_explicit_hitl_indicator:
            hitl_details = gateway_preferred
        else:
            # Legacy fallback: rely on explicit HITL nodes from the analyzer.
            # Avoid inferring HITL from RJ45 nodes, as some sessions include RJ45-like
            # devices even when HITL is not configured.
            nodes = summary.get('hitl_nodes') or []
            for node in nodes:
                if not isinstance(node, dict):
                    continue
                name = (node.get('name') or node.get('id') or '').strip()
                node_type = (node.get('type') or '').strip()
                ips = _force_ipv4_prefixlen(_collect_node_ips(node), prefixlen="24")
                hitl_details.append({
                    'name': name or node_type or 'HITL',
                    'type': node_type,
                    'ips': ips,
                })
    except Exception:
        hitl_details = []
    _SESSION_HITL_CACHE[abs_path] = {'mtime': mtime, 'data': hitl_details}
    return hitl_details


def _session_hitl_metadata(
    session: dict,
    *,
    core_cfg: Optional[Dict[str, Any]] = None,
    session_store: Optional[dict] = None,
) -> list[dict[str, Any]]:
    candidate_paths: list[str] = []
    file_path = session.get('file')
    # If the session already points at an existing XML, treat it as authoritative.
    # Avoid falling back to session-id-based store lookups which can be stale when
    # CORE session IDs are reused.
    if file_path:
        try:
            fp = str(file_path)
            if os.path.exists(fp):
                details = _hitl_details_from_path(fp)
                session['_hitl_source'] = 'session.file'
                return details
        except Exception:
            pass
        candidate_paths.append(str(file_path))
    store = session_store if isinstance(session_store, dict) else _load_core_sessions_store()
    sid = session.get('id')
    sid_int: Optional[int] = None
    if sid not in (None, ''):
        try:
            sid_int = int(sid)
        except Exception:
            sid_int = None

    # If the session has a file path but it's not accessible locally, prefer fetching
    # the current session XML via gRPC (when allowed) before consulting any stored
    # mappings by session id.
    if core_cfg and sid_int is not None and file_path:
        try:
            fp = str(file_path)
            if fp and (not os.path.exists(fp)):
                try:
                    out_dir = os.path.join(_outputs_dir(), 'core-sessions')
                except Exception:
                    out_dir = _outputs_dir()
                saved = _grpc_save_current_session_xml_with_config(core_cfg, out_dir, session_id=str(sid_int))
                if saved and os.path.exists(saved):
                    try:
                        _update_xml_session_mapping(
                            saved,
                            sid_int,
                            scenario_name=session.get('scenario_name') or None,
                            core_host=core_cfg.get('host', CORE_HOST) if isinstance(core_cfg, dict) else None,
                            core_port=core_cfg.get('port', CORE_PORT) if isinstance(core_cfg, dict) else None,
                        )
                    except Exception:
                        pass
                    session['file'] = saved
                    session['_hitl_source'] = 'grpc.save_xml'
                    return _hitl_details_from_path(saved)
        except Exception:
            pass

    if sid_int is not None and isinstance(store, dict):
        matches: list[str] = []
        for path, stored in store.items():
            if _session_store_entry_session_id(stored) == sid_int:
                matches.append(path)
        if matches:
            # Prefer the most recently modified existing file when multiple paths map to the same session id.
            best = None
            best_mtime = -1.0
            for p in matches:
                if not p:
                    continue
                try:
                    ap = os.path.abspath(str(p))
                except Exception:
                    ap = str(p)
                try:
                    if not os.path.exists(ap):
                        continue
                    mt = os.path.getmtime(ap)
                except Exception:
                    mt = -1.0
                if mt > best_mtime:
                    best_mtime = mt
                    best = ap
            if best:
                candidate_paths.append(best)
    seen: set[str] = set()
    for path in candidate_paths:
        if not path:
            continue
        try:
            abs_path = os.path.abspath(path)
        except Exception:
            abs_path = path
        if abs_path in seen:
            continue
        seen.add(abs_path)
        if not os.path.exists(abs_path):
            continue
        details = _hitl_details_from_path(abs_path)
        if details:
            try:
                # Only set a file path if the session didn't already have one.
                # Avoid overwriting a remote CORE VM path with a stale local store path.
                existing_file = session.get('file')
                if not existing_file:
                    session['file'] = abs_path
            except Exception:
                pass
            return details
    if core_cfg and sid_int is not None:
        try:
            out_dir = os.path.join(_outputs_dir(), 'core-sessions')
        except Exception:
            out_dir = _outputs_dir()
        try:
            saved = _grpc_save_current_session_xml_with_config(core_cfg, out_dir, session_id=str(sid_int))
        except Exception:
            saved = None
        if saved and os.path.exists(saved):
            try:
                _update_xml_session_mapping(
                    saved,
                    sid_int,
                    scenario_name=session.get('scenario_name') or None,
                    core_host=core_cfg.get('host', CORE_HOST) if isinstance(core_cfg, dict) else None,
                    core_port=core_cfg.get('port', CORE_PORT) if isinstance(core_cfg, dict) else None,
                )
            except Exception:
                pass
            try:
                # Prefer showing the fetched local XML path for details/debugging.
                session['file'] = saved
            except Exception:
                pass
            return _hitl_details_from_path(saved)
    return []


def _attach_participant_urls_to_sessions(
    sessions: list[dict],
    session_store: dict,
    scenario_paths: dict[str, set[str]],
    participant_urls: dict[str, str],
) -> None:
    if not sessions or not participant_urls:
        return
    path_to_norm: dict[str, str] = {}
    for norm_key, candidates in (scenario_paths or {}).items():
        for candidate in candidates or []:
            try:
                ap = os.path.abspath(str(candidate))
            except Exception:
                ap = str(candidate)
            if ap:
                path_to_norm[ap] = norm_key
    sessions_by_id: dict[int, tuple[dict, str]] = {}
    for path, entry in (session_store or {}).items():
        sid = _session_store_entry_session_id(entry)
        if sid is None:
            continue
        try:
            ap = os.path.abspath(path)
        except Exception:
            ap = str(path)
        sessions_by_id[sid] = (entry, ap)
    path_lookup_cache: dict[str, dict[str, str]] = {}
    for sess in sessions:
        scenario_norm = ''
        candidate_paths: list[str] = []
        sid_raw = sess.get('id')
        sid_int: Optional[int] = None
        if sid_raw not in (None, ''):
            try:
                sid_int = int(sid_raw)
            except Exception:
                sid_int = None
        if sid_int is not None and sid_int in sessions_by_id:
            entry, mapped_path = sessions_by_id[sid_int]
            if mapped_path:
                candidate_paths.append(mapped_path)
            scenario_norm = _session_store_entry_scenario_norm(entry)
            if (not scenario_norm) and mapped_path:
                scenario_norm = path_to_norm.get(mapped_path, '')
            if (not scenario_norm) and mapped_path:
                base = os.path.splitext(os.path.basename(mapped_path))[0]
                scenario_norm = _normalize_scenario_label(base)
        if not scenario_norm:
            scenario_name = sess.get('scenario_name')
            if scenario_name:
                scenario_norm = _normalize_scenario_label(scenario_name)
        if not scenario_norm:
            candidate_path = sess.get('file') or sess.get('dir')
            if candidate_path:
                try:
                    ap = os.path.abspath(candidate_path)
                except Exception:
                    ap = str(candidate_path)
                scenario_norm = path_to_norm.get(ap, '')
                if not scenario_norm:
                    base = os.path.splitext(os.path.basename(ap))[0]
                    scenario_norm = _normalize_scenario_label(base)
                if ap:
                    candidate_paths.append(ap)
        if scenario_norm:
            sess['scenario_norm'] = scenario_norm
            participant_url = participant_urls.get(scenario_norm)
            if not participant_url:
                for candidate_path in candidate_paths:
                    if not candidate_path:
                        continue
                    if candidate_path not in path_lookup_cache:
                        path_lookup_cache[candidate_path] = _participant_urls_from_xml(candidate_path)
                    candidate_url = path_lookup_cache[candidate_path].get(scenario_norm)
                    if candidate_url:
                        participant_url = candidate_url
                        participant_urls.setdefault(scenario_norm, candidate_url)
                        break
            if participant_url:
                sess['participant_proxmox_url'] = participant_url


def _attach_hitl_metadata_to_sessions(
    sessions: list[dict],
    core_cfg: Optional[Dict[str, Any]] = None,
    *,
    allow_remote_fetch: bool = False,
    session_store: Optional[Dict[str, Any]] = None,
) -> None:
    store = session_store if session_store is not None else _load_core_sessions_store()
    cfg = core_cfg if allow_remote_fetch else None
    for sess in sessions:
        sess['hitl'] = _session_hitl_metadata(sess, core_cfg=cfg, session_store=store)


def _scan_core_xmls(max_count: int = 200) -> list[dict]:
    """Scan for runnable CORE XMLs and exclude scenario editor saves.

    Include only:
      - uploads/core/*.xml (user-uploaded CORE XMLs)
      - outputs/core-sessions/**/*.xml (saved via gRPC from running sessions)

    Exclude:
      - outputs/scenarios-*/** (scenario editor saves)

    Returns list of dicts: { path, name, size, mtime, valid } sorted by mtime desc.
    """
    uploads_core = os.path.join(_uploads_dir(), 'core')
    outputs_sessions = os.path.join(_outputs_dir(), 'core-sessions')
    allowed_roots = [uploads_core, outputs_sessions]
    paths: list[str] = []
    for root in allowed_roots:
        try:
            if not root or not os.path.exists(root):
                continue
            for dp, _dn, files in os.walk(root):
                for fn in files:
                    if fn.lower().endswith('.xml'):
                        paths.append(os.path.join(dp, fn))
        except Exception:
            continue
    # Dedup and sort by mtime desc
    seen = set()
    recs: list[tuple[float, dict]] = []
    for p in paths:
        ap = os.path.abspath(p)
        if ap in seen:
            continue
        seen.add(ap)
        try:
            st = os.stat(ap)
            mt = st.st_mtime
            size = st.st_size
        except Exception:
            mt = 0.0
            size = -1
        valid = False
        ok, _errs = _validate_core_xml(ap)
        valid = bool(ok)
        recs.append((mt, {'path': ap, 'name': os.path.basename(ap), 'size': size, 'mtime': mt, 'valid': valid}))
    recs.sort(key=lambda x: x[0], reverse=True)
    return [r for _mt, r in recs[:max_count]]


@app.route('/core')
def core_page():
    history = _load_run_history()
    current_user = _current_user()
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
    scenario_participant_urls = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
    participant_url_flags = {
        norm: bool(url)
        for norm, url in scenario_participant_urls.items()
        if isinstance(norm, str) and norm
    }
    scenario_query = request.args.get('scenario', '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    # Enforce per-scenario view: default to the first available scenario when unset/invalid.
    if scenario_names:
        if not scenario_norm or not any(_normalize_scenario_label(n) == scenario_norm for n in scenario_names):
            scenario_norm = _normalize_scenario_label(scenario_names[0])
    active_scenario = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query)
    core_cfg = _select_core_config_for_page(scenario_norm, history, include_password=True)
    core_cfg = _ensure_core_vm_metadata(core_cfg)
    host = core_cfg.get('host', CORE_HOST)
    port = int(core_cfg.get('port', CORE_PORT))
    core_vm_configured, core_vm_summary = _build_core_vm_summary(core_cfg)
    core_errors: list[str] = []
    app.logger.info(
        '[core.page] scenario=%s host=%s:%s ssh=%s@%s:%s',
        active_scenario or '<none>',
        host,
        port,
        (core_cfg.get('ssh_username') or '').strip() or '<none>',
        core_cfg.get('ssh_host'),
        core_cfg.get('ssh_port'),
    )
    # Active sessions via gRPC
    session_meta: Dict[str, Any] = {}
    sessions = _list_active_core_sessions(host, port, core_cfg, errors=core_errors, meta=session_meta)
    grpc_command = session_meta.get('grpc_command')
    app.logger.info('[core.page] session_count=%d', len(sessions))
    # Known XMLs
    xmls = _scan_core_xmls()
    # Map running by file path, with fallback to local store. Scope by CORE VM.
    mapping = _load_core_sessions_store()
    mapping = _migrate_core_sessions_store_with_core_targets(mapping, history)
    mapping = _filter_core_sessions_store_for_core(mapping, host, port)
    session_label_map = _build_session_scenario_labels(mapping, scenario_names, scenario_paths)
    scenario_session_ids = _session_ids_for_scenario(mapping, scenario_norm, scenario_paths) if scenario_norm else set()
    _annotate_sessions_with_scenarios(
        sessions,
        session_label_map,
        scenario_norm,
        scenario_names,
        scenario_paths,
        scenario_query,
        scenario_session_ids,
    )
    if scenario_norm:
        filtered_sessions, matched = _filter_sessions_by_scenario(sessions, scenario_norm, scenario_paths, scenario_session_ids)
        sessions = filtered_sessions
        if not matched:
            app.logger.info('[core.page] scenario=%s produced no session matches', scenario_norm)
        filtered_xmls, xml_matched = _filter_xmls_by_scenario(xmls, scenario_norm, scenario_paths, mapping)
        if xml_matched:
            xmls = filtered_xmls
        else:
            app.logger.info('[core.page] scenario=%s produced no XML matches; showing all XMLs', scenario_norm)
    # Provide desired display filenames for the UI: <scenario-name><timestamp>.xml.
    # For pycore sessions, the CORE-VM session-meta is the authoritative scenario source.
    try:
        all_sids: list[int] = []
        for s in sessions:
            try:
                sid = s.get('id')
                if sid in (None, ''):
                    continue
                all_sids.append(int(sid))
            except Exception:
                continue
        meta_by_sid = _read_remote_session_scenario_meta_bulk(core_cfg, session_ids=all_sids, logger=app.logger) if all_sids else {}
    except Exception:
        meta_by_sid = {}

    for s in sessions:
        try:
            sid = s.get('id')
            sid_int = int(sid) if sid not in (None, '') else None
        except Exception:
            sid_int = None

        # Prefer meta scenario label when present.
        if sid_int is not None:
            try:
                meta = meta_by_sid.get(sid_int) or {}
                meta_label = (meta.get('scenario_name') or '').strip() if isinstance(meta, dict) else ''
                if meta_label:
                    s['scenario_name'] = meta_label
            except Exception:
                pass

        try:
            scen = (s.get('scenario_name') or '').strip() or active_scenario or ''
            ts_epoch = None
            if sid_int is not None:
                try:
                    meta = meta_by_sid.get(sid_int) or {}
                    if isinstance(meta, dict) and meta.get('written_at_epoch') not in (None, ''):
                        ts_epoch = float(meta.get('written_at_epoch'))
                except Exception:
                    ts_epoch = None
            if ts_epoch is None and sid_int is not None:
                ts_epoch = _session_store_updated_at_for_session_id(mapping, sid_int, host=host, port=port)
            s['filename'] = _scenario_timestamped_filename(scen or None, ts_epoch)
        except Exception:
            pass

    _attach_hitl_metadata_to_sessions(sessions, core_cfg, allow_remote_fetch=False, session_store=mapping)
    _attach_participant_urls_to_sessions(sessions, mapping, scenario_paths, scenario_participant_urls)
    file_to_sid: dict[str, int] = {}
    # From gRPC session summaries (file path may be absolute)
    for s in sessions:
        f = s.get('file')
        sid = s.get('id')
        if f and sid is not None:
            file_to_sid[os.path.abspath(f)] = int(sid)
    # Merge with prior mappings
    for k, v in mapping.items():
        sid = _session_store_entry_session_id(v)
        if sid is None:
            continue
        file_to_sid.setdefault(os.path.abspath(k), sid)
    # Annotate xmls
    for x in xmls:
        sid = file_to_sid.get(x['path'])
        x['session_id'] = sid
        x['running'] = sid is not None
    return render_template(
        'core.html',
        sessions=sessions,
        xmls=xmls,
        host=host,
        port=port,
        scenarios=scenario_names,
        active_scenario=active_scenario,
        core_errors=core_errors,
        core_grpc_command=grpc_command,
        core_vm_configured=core_vm_configured,
        core_vm_summary=core_vm_summary,
        core_log_entries=_current_core_ui_logs(),
        participant_url_flags=participant_url_flags,
    )


@app.route('/core/data')
def core_data():
    def _query_bool_param(name: str, default: bool) -> bool:
        raw = request.args.get(name)
        if raw is None:
            return default
        try:
            v = str(raw).strip().lower()
        except Exception:
            return default
        if v in ('1', 'true', 'yes', 'on'):
            return True
        if v in ('0', 'false', 'no', 'off'):
            return False
        return default

    history = _load_run_history()
    current_user = _current_user()
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
    scenario_participant_urls = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
    participant_url_flags = {
        norm: bool(url)
        for norm, url in scenario_participant_urls.items()
        if isinstance(norm, str) and norm
    }
    scenario_query = request.args.get('scenario', '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    # Enforce per-scenario view: default to the first available scenario when unset/invalid.
    if scenario_names:
        if not scenario_norm or not any(_normalize_scenario_label(n) == scenario_norm for n in scenario_names):
            scenario_norm = _normalize_scenario_label(scenario_names[0])
    scenario_display = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query)
    core_cfg = _select_core_config_for_page(scenario_norm, history, include_password=True)
    core_cfg = _ensure_core_vm_metadata(core_cfg)
    host = core_cfg.get('host', CORE_HOST)
    port = int(core_cfg.get('port', CORE_PORT))
    core_vm_configured, core_vm_summary = _build_core_vm_summary(core_cfg)
    core_errors: list[str] = []
    app.logger.info(
        '[core.data] scenario=%s host=%s:%s ssh=%s@%s:%s',
        scenario_display or '<none>',
        host,
        port,
        (core_cfg.get('ssh_username') or '').strip() or '<none>',
        core_cfg.get('ssh_host'),
        core_cfg.get('ssh_port'),
    )
    session_meta: Dict[str, Any] = {}
    include_xmls = _query_bool_param('include_xmls', True)

    sessions = _list_active_core_sessions(host, port, core_cfg, errors=core_errors, meta=session_meta)
    grpc_command = session_meta.get('grpc_command')
    app.logger.info('[core.data] session_count=%d', len(sessions))
    xmls: Optional[list[dict]] = None
    if include_xmls:
        xmls = _scan_core_xmls()
    # annotate xmls with running/session_id best-effort mapping, as in core_page. Scope by CORE VM.
    mapping = _load_core_sessions_store()
    mapping = _migrate_core_sessions_store_with_core_targets(mapping, history)
    mapping = _filter_core_sessions_store_for_core(mapping, host, port)
    session_label_map = _build_session_scenario_labels(mapping, scenario_names, scenario_paths)
    scenario_session_ids = _session_ids_for_scenario(mapping, scenario_norm, scenario_paths) if scenario_norm else set()
    _annotate_sessions_with_scenarios(
        sessions,
        session_label_map,
        scenario_norm,
        scenario_names,
        scenario_paths,
        scenario_query,
        scenario_session_ids,
    )

    # If session file/dir paths are on the CORE VM (common for webapp not running on CORE),
    # local path-based inference is disabled. Use CORE-VM session-meta as a fallback.
    missing_sids: list[int] = []
    for s in sessions:
        try:
            scen = (s.get('scenario_name') or '').strip()
            sid = s.get('id')
            if sid in (None, ''):
                continue
            candidate_path = ''
            try:
                candidate_path = str(s.get('file') or s.get('dir') or '').strip()
            except Exception:
                candidate_path = ''
            is_pycore = False
            try:
                if candidate_path.replace('\\', '/').startswith('/tmp/pycore.') or '/tmp/pycore.' in candidate_path.replace('\\', '/'):
                    is_pycore = True
            except Exception:
                is_pycore = False
            # Always try remote meta for pycore sessions because their filenames are generic.
            if is_pycore or (not scen):
                missing_sids.append(int(sid))
        except Exception:
            continue
    remote_meta_by_sid: dict[int, dict[str, Any]] = {}
    if missing_sids:
        try:
            remote_meta_by_sid = _read_remote_session_scenario_meta_bulk(core_cfg, session_ids=missing_sids, logger=app.logger)
        except Exception:
            remote_meta_by_sid = {}
    if remote_meta_by_sid:
        for s in sessions:
            try:
                sid = s.get('id')
                sid_int = int(sid) if sid not in (None, '') else None
            except Exception:
                sid_int = None
            if sid_int is None:
                continue
            try:
                meta = remote_meta_by_sid.get(sid_int) or {}
                label = (meta.get('scenario_name') or '').strip() if isinstance(meta, dict) else ''
                if not label:
                    continue

                # If the session file path is remote/unavailable locally, treat the
                # CORE-VM meta label as authoritative (it was written at session start).
                candidate_path = ''
                try:
                    candidate_path = str(s.get('file') or s.get('dir') or '').strip()
                except Exception:
                    candidate_path = ''
                is_remote = False
                try:
                    if candidate_path and (candidate_path.startswith('/tmp/pycore.') or candidate_path.startswith('\\tmp\\pycore.')):
                        is_remote = True
                except Exception:
                    is_remote = False
                if not is_remote:
                    try:
                        if candidate_path and (not os.path.exists(candidate_path)):
                            is_remote = True
                    except Exception:
                        is_remote = True

                if is_remote or (not (s.get('scenario_name') or '').strip()):
                    s['scenario_name'] = label
            except Exception:
                continue

    # Provide counts for *all* scenarios so the UI can indicate which scenario(s)
    # currently have active sessions, even though this endpoint is scenario-filtered.
    active_session_counts: dict[str, int] = {}
    try:
        norm_to_display: dict[str, str] = {}
        for name in scenario_names or []:
            try:
                norm_to_display[_normalize_scenario_label(name)] = str(name)
            except Exception:
                continue
        for s in sessions:
            try:
                lbl_norm = _normalize_scenario_label((s.get('scenario_name') or '').strip())
            except Exception:
                lbl_norm = ''
            if not lbl_norm:
                continue
            disp = norm_to_display.get(lbl_norm)
            if not disp:
                continue
            active_session_counts[disp] = active_session_counts.get(disp, 0) + 1
    except Exception:
        active_session_counts = {}

    # Scenario "items" = saved artifacts known for that scenario (e.g., saved XMLs).
    # This is intentionally derived from the scenario catalog (paths) to avoid extra
    # filesystem scanning cost during polling.
    scenario_item_counts: dict[str, int] = {}
    try:
        for name in scenario_names or []:
            try:
                disp = str(name)
                norm = _normalize_scenario_label(name)
                paths = scenario_paths.get(norm) if isinstance(scenario_paths, dict) else None
                scenario_item_counts[disp] = len(paths or [])
            except Exception:
                continue
    except Exception:
        scenario_item_counts = {}

    if scenario_norm:
        # Prefer scenario_name equality when we have it (including from remote meta).
        filtered: list[dict] = []
        matched = False
        for s in sessions:
            try:
                lbl = _normalize_scenario_label((s.get('scenario_name') or '').strip())
            except Exception:
                lbl = ''
            if lbl and lbl == scenario_norm:
                filtered.append(s)
                matched = True
        if not matched:
            # Fall back to the legacy path/id-based filter only when we couldn't match by label.
            filtered, matched = _filter_sessions_by_scenario(sessions, scenario_norm, scenario_paths, scenario_session_ids)
        sessions = filtered
        if not matched:
            app.logger.info('[core.data] scenario=%s produced no session matches', scenario_norm)

        # Finalize display label: if the session is in this scenario view by owner-id or
        # by path, ensure the badge reflects the selected scenario. This avoids stale
        # labels from remote CORE paths or session-meta when session IDs are reused.
        for s in sessions:
            try:
                sid = s.get('id')
                sid_int = int(sid) if sid not in (None, '') else None
            except Exception:
                sid_int = None
            try:
                is_owned = sid_int is not None and sid_int in (scenario_session_ids or set())
            except Exception:
                is_owned = False
            try:
                is_path_match = _path_matches_scenario(s.get('file'), scenario_norm, scenario_paths) or _path_matches_scenario(s.get('dir'), scenario_norm, scenario_paths)
            except Exception:
                is_path_match = False
            if is_owned or is_path_match:
                # If CORE-VM meta already supplied a scenario label, keep it.
                try:
                    sid = s.get('id')
                    sid_int = int(sid) if sid not in (None, '') else None
                except Exception:
                    sid_int = None
                if sid_int is not None:
                    try:
                        meta = remote_meta_by_sid.get(sid_int) or {}
                        meta_label = (meta.get('scenario_name') or '').strip() if isinstance(meta, dict) else ''
                    except Exception:
                        meta_label = ''
                    if meta_label:
                        continue
                try:
                    s['scenario_name'] = scenario_display or s.get('scenario_name') or ''
                except Exception:
                    pass
        if include_xmls and xmls is not None:
            filtered_xmls, xml_matched = _filter_xmls_by_scenario(xmls, scenario_norm, scenario_paths, mapping)
            if xml_matched:
                xmls = filtered_xmls
            else:
                app.logger.info('[core.data] scenario=%s produced no XML matches; showing all XMLs', scenario_norm)
    # Provide the requested generated filename: <scenario-name><timestamp>.xml.
    for s in sessions:
        try:
            sid = s.get('id')
            sid_int = int(sid) if sid not in (None, '') else None
        except Exception:
            sid_int = None
        try:
            scen = (s.get('scenario_name') or '').strip() or scenario_display or ''
            ts_epoch = None
            if sid_int is not None and sid_int in remote_meta_by_sid:
                meta = remote_meta_by_sid.get(sid_int) or {}
                if isinstance(meta, dict) and meta.get('written_at_epoch') not in (None, ''):
                    try:
                        ts_epoch = float(meta.get('written_at_epoch'))
                    except Exception:
                        ts_epoch = None
            if ts_epoch is None and sid_int is not None:
                ts_epoch = _session_store_updated_at_for_session_id(mapping, sid_int, host=host, port=port)
            s['filename'] = _scenario_timestamped_filename(scen or None, ts_epoch)
        except Exception:
            pass

    _attach_hitl_metadata_to_sessions(sessions, core_cfg, allow_remote_fetch=True, session_store=mapping)
    _attach_participant_urls_to_sessions(sessions, mapping, scenario_paths, scenario_participant_urls)
    file_to_sid: dict[str, int] = {}
    for s in sessions:
        f = s.get('file')
        sid = s.get('id')
        if f and sid is not None:
            file_to_sid[os.path.abspath(f)] = int(sid)
    for k, v in mapping.items():
        sid = _session_store_entry_session_id(v)
        if sid is None:
            continue
        file_to_sid.setdefault(os.path.abspath(k), sid)
    if include_xmls and xmls is not None:
        for x in xmls:
            sid = file_to_sid.get(x['path'])
            x['session_id'] = sid
            x['running'] = sid is not None

    core_modal_href = url_for('index', core_modal=1, scenario=scenario_display) if scenario_display else url_for('index', core_modal=1)
    payload: dict[str, Any] = {
        'sessions': sessions,
        'scenarios': scenario_names,
        'active_scenario': scenario_display,
        'participant_url_flags': participant_url_flags,
        'active_session_counts': active_session_counts,
        'scenario_item_counts': scenario_item_counts,
        'host': host,
        'port': port,
        'core_vm_configured': bool(core_vm_configured),
        'core_vm_summary': core_vm_summary or {},
        'core_modal_href': core_modal_href,
        'errors': core_errors,
        'grpc_command': grpc_command,
        'logs': _current_core_ui_logs(),
    }
    if include_xmls:
        payload['xmls'] = xmls or []
    return jsonify(payload)


@app.route('/core/upload', methods=['POST'])
def core_upload():
    f = request.files.get('xml_file')
    if not f or f.filename == '':
        flash('No file selected.')
        return _redirect_core_page_with_scenario()
    filename = secure_filename(f.filename)
    if not filename.lower().endswith('.xml'):
        flash('Only .xml allowed.')
        return _redirect_core_page_with_scenario()
    dest_dir = os.path.join(app.config['UPLOAD_FOLDER'], 'core')
    os.makedirs(dest_dir, exist_ok=True)
    unique = datetime.datetime.now().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:6]
    path = os.path.join(dest_dir, f"{unique}-{filename}")
    f.save(path)
    ok, errs = _validate_core_xml(path)
    if not ok:
        try: os.remove(path)
        except Exception: pass
        flash(f'Invalid CORE XML: {errs}')
        return _redirect_core_page_with_scenario()
    flash('XML uploaded and validated.')
    return _redirect_core_page_with_scenario()


def _redirect_core_page_with_scenario(*, scenario_hint: Optional[str] = None):
    """Redirect back to /core while preserving the current scenario filter.

    Many /core actions are POSTs from core.html. If we drop the scenario on redirect,
    the page will default back to the first scenario (often "Scenario 1").
    """
    try:
        scenario = (scenario_hint or request.values.get('scenario') or request.values.get('scenario_name') or '').strip()
    except Exception:
        scenario = (scenario_hint or '').strip() if isinstance(scenario_hint, str) else ''
    if scenario:
        return redirect(url_for('core_page', scenario=scenario))
    return redirect(url_for('core_page'))


@app.route('/core/push_repo', methods=['POST'])
def core_push_repo_route():
    """Upload the local repository snapshot to the remote CORE host."""

    xml_path: Optional[str] = None
    scenario_name_hint: Optional[str] = None
    scenario_index_hint: Optional[int] = None
    core_override: Optional[Dict[str, Any]] = None
    scenario_core_override: Optional[Dict[str, Any]] = None

    if request.form:
        xml_path = request.form.get('xml_path') or None
        scenario_name_hint = request.form.get('scenario') or request.form.get('scenario_name') or None
        raw_index = request.form.get('scenario_index')
        if raw_index not in (None, ''):
            try:
                scenario_index_hint = int(raw_index)
            except Exception:
                scenario_index_hint = None
        core_json = request.form.get('core_json')
        if core_json:
            try:
                core_override = json.loads(core_json)
            except Exception:
                core_override = None
        hitl_core_json = request.form.get('hitl_core_json')
        if hitl_core_json:
            try:
                scenario_core_override = json.loads(hitl_core_json)
            except Exception:
                scenario_core_override = None
    else:
        payload = request.get_json(silent=True) or {}
        if isinstance(payload, dict):
            xml_path = payload.get('xml_path') or payload.get('scenario_xml_path') or xml_path
            scenario_name_hint = (
                payload.get('scenario')
                or payload.get('scenario_name')
                or payload.get('active_scenario')
                or scenario_name_hint
            )
            if 'scenario_index' in payload:
                try:
                    scenario_index_hint = int(payload.get('scenario_index'))
                except Exception:
                    scenario_index_hint = None
            if isinstance(payload.get('core'), dict):
                core_override = payload.get('core')
            elif isinstance(payload.get('core_json'), str):
                try:
                    core_override = json.loads(payload.get('core_json') or '{}')
                except Exception:
                    core_override = None
            if isinstance(payload.get('hitl_core'), dict):
                scenario_core_override = payload.get('hitl_core')
            elif isinstance(payload.get('hitl_core_json'), str):
                try:
                    scenario_core_override = json.loads(payload.get('hitl_core_json') or '{}')
                except Exception:
                    scenario_core_override = None

    payload_for_core: Optional[Dict[str, Any]] = None
    scenario_payload: Optional[Dict[str, Any]] = None
    if xml_path:
        xml_path = os.path.abspath(xml_path)
        if os.path.exists(xml_path):
            try:
                payload_for_core = _parse_scenarios_xml(xml_path)
            except Exception:
                payload_for_core = None
        else:
            app.logger.warning('[core.push_repo] XML path not found: %s', xml_path)
    if payload_for_core:
        scen_list = payload_for_core.get('scenarios') or []
        if isinstance(scen_list, list) and scen_list:
            if scenario_name_hint:
                for scen_entry in scen_list:
                    if isinstance(scen_entry, dict) and str(scen_entry.get('name') or '').strip() == str(scenario_name_hint).strip():
                        scenario_payload = scen_entry
                        break
            if scenario_payload is None and scenario_index_hint is not None:
                if 0 <= scenario_index_hint < len(scen_list):
                    candidate = scen_list[scenario_index_hint]
                    if isinstance(candidate, dict):
                        scenario_payload = candidate
            if scenario_payload is None:
                for scen_entry in scen_list:
                    if isinstance(scen_entry, dict):
                        scenario_payload = scen_entry
                        break
    scenario_core_saved = None
    if scenario_payload and isinstance(scenario_payload.get('hitl'), dict):
        scenario_core_saved = scenario_payload['hitl'].get('core')
    global_core_saved = (
        payload_for_core.get('core') if (payload_for_core and isinstance(payload_for_core.get('core'), dict)) else None
    )
    core_cfg = _merge_core_configs(
        global_core_saved,
        scenario_core_saved,
        core_override if isinstance(core_override, dict) else None,
        scenario_core_override if isinstance(scenario_core_override, dict) else None,
        include_password=True,
    )
    progress_id = uuid.uuid4().hex
    _init_repo_push_progress(progress_id, stage='queued', detail='Queued repository sync…', status='queued', percent=0.0)
    try:
        _schedule_repo_push_to_remote(progress_id, core_cfg, logger=app.logger)
    except _SSHTunnelError as exc:
        _update_repo_push_progress(progress_id, status='error', stage='error', detail=str(exc))
        return jsonify({'error': str(exc)}), 400
    except Exception as exc:
        app.logger.exception('[core.push_repo] Failed syncing repo: %s', exc)
        _update_repo_push_progress(progress_id, status='error', stage='error', detail=str(exc))
        return jsonify({'error': f'Failed pushing repo: {exc}', 'progress_id': progress_id}), 500
    return jsonify({'ok': True, 'progress_id': progress_id})


@app.route('/core/push_repo/status/<progress_id>', methods=['GET'])
def core_push_repo_status(progress_id: str):
    payload = _get_repo_push_progress(progress_id)
    if not payload:
        return jsonify({'progress_id': progress_id, 'status': 'unknown'}), 404
    response = {
        'progress_id': progress_id,
        'status': payload.get('status') or 'pending',
        'stage': payload.get('stage'),
        'detail': payload.get('detail'),
        'percent': payload.get('percent'),
        'updated_at': payload.get('updated_at'),
        'created_at': payload.get('created_at'),
    }
    return jsonify(response)


@app.route('/core/push_repo/cancel/<progress_id>', methods=['POST'])
def core_push_repo_cancel(progress_id: str):
    payload = _get_repo_push_progress(progress_id)
    if not payload:
        return jsonify({'progress_id': progress_id, 'status': 'unknown'}), 404
    status = (payload.get('status') or '').strip().lower()
    if status in ('complete', 'error', 'cancelled'):
        return jsonify({'ok': True, 'progress_id': progress_id, 'status': status, 'noop': True})
    _update_repo_push_progress(
        progress_id,
        cancel_requested=True,
        status='cancelled',
        stage='cancelled',
        detail='Cancelled by user.',
    )

    # Best-effort: attempt to stop any in-flight remote finalize by killing its PID.
    kill_info: Dict[str, Any] = {
        'attempted': False,
        'pidfile': None,
        'pidfile_found': False,
        'pid': None,
        'term_sent': False,
        'kill_sent': False,
        'pidfile_removed': False,
        'archive': None,
        'archive_existed_before': None,
        'archive_exists_after': None,
    }
    try:
        ctx = _get_repo_push_cancel_ctx(progress_id)
        if isinstance(ctx, dict):
            core_cfg = ctx.get('core_cfg')
            pidfile = ctx.get('remote_pidfile')
            remote_archive = ctx.get('remote_archive')
            if isinstance(core_cfg, dict):
                kill_info['attempted'] = True
                if isinstance(pidfile, str) and pidfile.strip():
                    kill_info['pidfile'] = pidfile
                if isinstance(remote_archive, str) and remote_archive.strip():
                    kill_info['archive'] = remote_archive

                client = _open_ssh_client(core_cfg)
                try:
                    kill_script = (
                        "set -e; "
                        f"pidfile={shlex.quote(pidfile or '')}; "
                        f"archive={shlex.quote(remote_archive or '')}; "
                        "pidfile_found=0; pid=''; "
                        "if [ -n \"$pidfile\" ] && [ -f \"$pidfile\" ]; then pidfile_found=1; pid=$(cat \"$pidfile\" 2>/dev/null || true); fi; "
                        "term_sent=0; kill_sent=0; "
                        "if [ -n \"$pid\" ]; then "
                        "kill -TERM \"$pid\" 2>/dev/null && term_sent=1 || term_sent=0; "
                        "sleep 0.5; "
                        "kill -KILL \"$pid\" 2>/dev/null && kill_sent=1 || kill_sent=0; "
                        "fi; "
                        "pidfile_removed=0; "
                        "if [ -n \"$pidfile\" ]; then rm -f -- \"$pidfile\" 2>/dev/null && pidfile_removed=1 || pidfile_removed=0; fi; "
                        "archive_existed_before=''; archive_exists_after=''; "
                        "if [ -n \"$archive\" ]; then "
                        "if [ -f \"$archive\" ]; then archive_existed_before=1; else archive_existed_before=0; fi; "
                        "rm -f -- \"$archive\" 2>/dev/null || true; "
                        "if [ -f \"$archive\" ]; then archive_exists_after=1; else archive_exists_after=0; fi; "
                        "fi; "
                        "echo PIDFILE_FOUND=\"$pidfile_found\"; "
                        "echo PID=\"$pid\"; "
                        "echo TERM_SENT=\"$term_sent\"; "
                        "echo KILL_SENT=\"$kill_sent\"; "
                        "echo PIDFILE_REMOVED=\"$pidfile_removed\"; "
                        "echo ARCHIVE_EXISTED_BEFORE=\"$archive_existed_before\"; "
                        "echo ARCHIVE_EXISTS_AFTER=\"$archive_exists_after\""
                    )
                    _code, out, _err = _exec_ssh_command(client, f"sh -lc {shlex.quote(kill_script)}", timeout=25.0)
                    for line in (out or '').splitlines():
                        if '=' not in line:
                            continue
                        k, v = line.split('=', 1)
                        k = k.strip().upper()
                        v = v.strip().strip('"')
                        if k == 'PIDFILE_FOUND':
                            kill_info['pidfile_found'] = v == '1'
                        elif k == 'PID':
                            kill_info['pid'] = v or None
                        elif k == 'TERM_SENT':
                            kill_info['term_sent'] = v == '1'
                        elif k == 'KILL_SENT':
                            kill_info['kill_sent'] = v == '1'
                        elif k == 'PIDFILE_REMOVED':
                            kill_info['pidfile_removed'] = v == '1'
                        elif k == 'ARCHIVE_EXISTED_BEFORE':
                            kill_info['archive_existed_before'] = None if v == '' else (v == '1')
                        elif k == 'ARCHIVE_EXISTS_AFTER':
                            kill_info['archive_exists_after'] = None if v == '' else (v == '1')
                finally:
                    try:
                        client.close()
                    except Exception:
                        pass
    except Exception:
        pass

    return jsonify({'ok': True, 'progress_id': progress_id, 'status': 'cancelled', 'remote': kill_info})


@app.route('/core/start', methods=['POST'])
def core_start():
    xml_path = request.form.get('path')
    if not xml_path:
        flash('Missing XML path')
        return _redirect_core_page_with_scenario()
    scenario_label = (request.form.get('scenario') or '').strip()
    ap = os.path.abspath(xml_path)
    if not os.path.exists(ap):
        flash('File not found')
        return _redirect_core_page_with_scenario(scenario_hint=scenario_label)
    ok, errs = _validate_core_xml(ap)
    if not ok:
        flash(f'Invalid CORE XML: {errs}')
        return _redirect_core_page_with_scenario(scenario_hint=scenario_label)
    core_cfg = _core_config_for_request(include_password=True)
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    address = f"{cfg.get('host') or CORE_HOST}:{cfg.get('port') or CORE_PORT}"
    remote_xml_path: Optional[str] = None
    try:
        remote_xml_path = _upload_file_to_core_host(cfg, ap)
    except Exception as exc:
        flash(f'Failed to upload XML to CORE host: {exc}')
        return _redirect_core_page_with_scenario(scenario_hint=scenario_label)
    try:
        script = _remote_core_open_xml_script(address, remote_xml_path, auto_start=True)
        command_desc = (
            f"remote ssh {ssh_user}@{ssh_host} -> CoreGrpcClient.open_xml {address} ({os.path.basename(ap)})"
        )
        payload = _run_remote_python_json(
            cfg,
            script,
            logger=app.logger,
            label='core.open_xml',
            command_desc=command_desc,
        )
    except Exception as exc:
        flash(f'Failed to start CORE session: {exc}')
        return _redirect_core_page_with_scenario(scenario_hint=scenario_label)
    finally:
        if remote_xml_path:
            _remove_remote_file(cfg, remote_xml_path)
    if payload.get('error'):
        msg = payload.get('error')
        app.logger.warning('[core.start] remote error: %s', msg)
        flash(f'CORE rejected the XML: {msg}')
        tb = payload.get('traceback')
        if tb:
            app.logger.debug('[core.start] traceback: %s', tb)
        return _redirect_core_page_with_scenario(scenario_hint=scenario_label)
    sid = payload.get('session_id')
    if sid is None:
        flash('Remote CORE did not return a session id.')
        return _redirect_core_page_with_scenario(scenario_hint=scenario_label)
    try:
        sid_int = int(sid)
    except Exception:
        sid_int = sid
    app.logger.info('[core.start] Started session %s via %s (scenario=%r)', sid_int, address, scenario_label)
    _update_xml_session_mapping(
        ap,
        sid_int,
        scenario_name=scenario_label or None,
        core_host=cfg.get('host', CORE_HOST) if isinstance(cfg, dict) else None,
        core_port=cfg.get('port', CORE_PORT) if isinstance(cfg, dict) else None,
    )
    # Also persist a session->scenario tag on the CORE VM itself so a remote
    # session XML path (e.g., /tmp/pycore.<sid>/...) can be mapped back to a scenario.
    try:
        _write_remote_session_scenario_meta(
            cfg,
            session_id=int(sid_int) if isinstance(sid_int, int) else int(str(sid_int)),
            scenario_name=scenario_label or None,
            scenario_xml_basename=os.path.basename(ap),
            logger=app.logger,
        )
    except Exception:
        pass
    flash(f'Started session {sid_int}.')
    return _redirect_core_page_with_scenario(scenario_hint=scenario_label)


@app.route('/core/stop', methods=['POST'])
def core_stop():
    sid = request.form.get('session_id')
    if not sid:
        flash('Missing session id')
        return _redirect_core_page_with_scenario()
    try:
        sid_int = int(sid)
    except Exception:
        flash('Invalid session id')
        return _redirect_core_page_with_scenario()
    core_cfg = _core_config_for_request(include_password=True)
    try:
        _execute_remote_core_session_action(core_cfg, 'stop', sid_int, logger=app.logger)
        # Best-effort: cleanup Docker artifacts generated by this tool after stopping a session.
        # - Containers are derived from compose_assignments.json on the CORE VM (vuln docker-compose nodes).
        # - Images are limited to tool wrapper images under coretg/*:iproute2*.
        cleanup_containers = 0
        cleanup_images = 0
        cleanup_notes: list[str] = []
        try:
            status_payload = _run_remote_python_json(
                core_cfg,
                _remote_docker_status_script(core_cfg.get('ssh_password')),
                logger=app.logger,
                label='docker.status(for stop cleanup)',
                timeout=60.0,
            )
            names: list[str] = []
            if isinstance(status_payload, dict) and isinstance(status_payload.get('items'), list):
                for it in status_payload.get('items') or []:
                    if isinstance(it, dict) and it.get('name'):
                        names.append(str(it.get('name')))
            if names:
                payload = _run_remote_python_json(
                    core_cfg,
                    _remote_docker_cleanup_script(names, core_cfg.get('ssh_password')),
                    logger=app.logger,
                    label='docker.cleanup(on stop)',
                    timeout=120.0,
                )
                if isinstance(payload, dict) and isinstance(payload.get('results'), list):
                    cleanup_containers = len(payload.get('results') or [])
            else:
                cleanup_notes.append('no docker-compose node containers to cleanup')
        except Exception as exc:
            cleanup_notes.append(f'container cleanup skipped/failed: {exc}')

        try:
            payload = _run_remote_python_json(
                core_cfg,
                _remote_docker_remove_wrapper_images_script(core_cfg.get('ssh_password')),
                logger=app.logger,
                label='docker.wrapper_images.cleanup(on stop)',
                timeout=180.0,
            )
            if isinstance(payload, dict) and isinstance(payload.get('removed'), list):
                cleanup_images = len(payload.get('removed') or [])
        except Exception as exc:
            cleanup_notes.append(f'wrapper image cleanup skipped/failed: {exc}')

        msg = f'Stopped session {sid_int}.'
        extra = []
        if cleanup_containers:
            extra.append(f'docker containers cleaned={cleanup_containers}')
        if cleanup_images:
            extra.append(f'wrapper images removed={cleanup_images}')
        if cleanup_notes:
            extra.append('; '.join(cleanup_notes)[:240])
        if extra:
            msg = msg + ' ' + ' · '.join(extra)
        flash(msg)
    except Exception as exc:
        flash(f'Failed to stop session: {exc}')
    return _redirect_core_page_with_scenario()


@app.route('/core/kill_active_sessions_api', methods=['POST'])
def core_kill_active_sessions_api():
    """Delete active CORE sessions.

    Intended for the Execute flow: when a run is blocked due to active sessions,
    the UI can prompt the user to kill the previous session(s) and retry.
    """
    payload = request.get_json(silent=True) or {}
    kill_all = bool(payload.get('kill_all'))
    session_ids_raw = payload.get('session_ids')

    core_cfg = _core_config_for_request(include_password=True)
    core_host = core_cfg.get('host', CORE_HOST)
    try:
        core_port = int(core_cfg.get('port', CORE_PORT))
    except Exception:
        core_port = CORE_PORT

    session_ids: list[int] = []
    if not kill_all:
        if isinstance(session_ids_raw, list):
            for item in session_ids_raw:
                try:
                    session_ids.append(int(str(item).strip()))
                except Exception:
                    continue

    if kill_all or not session_ids:
        try:
            sessions = _list_active_core_sessions(core_host, int(core_port), core_cfg, errors=[], meta={})
        except Exception:
            sessions = []
        for entry in sessions:
            sid = entry.get('id')
            if sid in (None, ''):
                continue
            try:
                session_ids.append(int(str(sid).strip()))
            except Exception:
                continue

    # De-dupe while preserving order
    seen: set[int] = set()
    ordered_ids: list[int] = []
    for sid in session_ids:
        if sid in seen:
            continue
        seen.add(sid)
        ordered_ids.append(sid)

    deleted: list[int] = []
    errors: list[str] = []
    for sid in ordered_ids:
        try:
            _execute_remote_core_session_action(core_cfg, 'delete', sid, logger=app.logger)
            deleted.append(sid)
        except Exception as exc:
            errors.append(f"Failed deleting session {sid}: {exc}")

    return jsonify({
        'ok': not errors,
        'deleted': deleted,
        'errors': errors,
        'core_host': core_host,
        'core_port': core_port,
    }), 200


@app.route('/core/stop_duplicate_daemons_api', methods=['POST'])
def core_stop_duplicate_daemons_api():
    """Stop duplicate core-daemon processes and restart core-daemon.

    Intended for the Execute flow: when a run is blocked due to multiple core-daemon
    processes, the UI can prompt the user to stop them and retry.

    Notes:
    - Requires SSH access and sudo privileges on the CORE VM.
    - If specific PIDs are provided, they are targeted first, but we still
      perform a best-effort cleanup (systemctl stop + pkill) to ensure a single daemon.
    """
    payload = request.get_json(silent=True) or {}
    pids_raw = payload.get('pids')
    requested_pids: list[int] = []
    if isinstance(pids_raw, list):
        for item in pids_raw:
            try:
                requested_pids.append(int(str(item).strip()))
            except Exception:
                continue

    core_cfg = _core_config_for_request(include_password=True)
    ssh_password = core_cfg.get('ssh_password')
    if not ssh_password:
        return jsonify({
            'ok': False,
            'error': 'Stopping core-daemon requires sudo; provide an SSH password.',
            'can_stop_daemons': False,
        }), 400
    if paramiko is None:
        return jsonify({'ok': False, 'error': 'Paramiko unavailable; cannot SSH to CORE VM.'}), 500
    _ensure_paramiko_available()

    ssh_client = paramiko.SSHClient()  # type: ignore[assignment]
    ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
    try:
        ssh_client.connect(
            hostname=str(core_cfg.get('ssh_host') or core_cfg.get('host') or 'localhost'),
            port=int(core_cfg.get('ssh_port') or 22),
            username=str(core_cfg.get('ssh_username') or ''),
            password=ssh_password,
            look_for_keys=False,
            allow_agent=False,
            timeout=10.0,
            banner_timeout=10.0,
            auth_timeout=10.0,
        )
        before = _collect_remote_core_daemon_pids(ssh_client)
        # Prefer requested pids (e.g. from conflict error), else operate on what we see.
        target = requested_pids or before
        _stop_remote_core_daemon_conflict(
            ssh_client,
            sudo_password=ssh_password,
            pids=target,
            logger=app.logger,
        )
        try:
            time.sleep(1.0)
        except Exception:
            pass
        after = _collect_remote_core_daemon_pids(ssh_client)
        return jsonify({
            'ok': True,
            'daemon_pids_before': before,
            'daemon_pids_after': after,
        }), 200
    except Exception as exc:
        return jsonify({'ok': False, 'error': str(exc)}), 500
    finally:
        try:
            ssh_client.close()
        except Exception:
            pass


@app.route('/core/start_session', methods=['POST'])
def core_start_session():
    sid = request.form.get('session_id')
    if not sid:
        flash('Missing session id')
        return _redirect_core_page_with_scenario()
    try:
        sid_int = int(sid)
    except Exception:
        flash('Invalid session id')
        return _redirect_core_page_with_scenario()
    core_cfg = _core_config_for_request(include_password=True)
    try:
        _execute_remote_core_session_action(core_cfg, 'start', sid_int, logger=app.logger)
        flash(f'Started session {sid_int}.')
    except Exception as exc:
        flash(f'Failed to start session: {exc}')
    return _redirect_core_page_with_scenario()


@app.route('/core/delete', methods=['POST'])
def core_delete():
    # Delete session (if provided) and/or delete XML file under uploads/ or outputs/
    sid = request.form.get('session_id')
    xml_path = request.form.get('path')
    if sid:
        try:
            sid_int = int(sid)
            core_cfg = _core_config_for_request(include_password=True)
            _execute_remote_core_session_action(core_cfg, 'delete', sid_int, logger=app.logger)
            flash(f'Deleted session {sid_int}.')
        except Exception as e:
            flash(f'Failed to delete session: {e}')
    if xml_path:
        ap = os.path.abspath(xml_path)
        # Safety: only delete inside uploads/ or outputs/
        try:
            allowed = [os.path.abspath(_uploads_dir()), os.path.abspath(_outputs_dir())]
            if any(ap.startswith(a + os.sep) or ap == a for a in allowed):
                try:
                    os.remove(ap)
                    flash('Deleted XML file.')
                except FileNotFoundError:
                    pass
                except Exception as e:
                    flash(f'Failed deleting XML: {e}')
                # clear mapping
                _update_xml_session_mapping(ap, None)
            else:
                flash('Refusing to delete file outside outputs/ or uploads/.')
        except Exception:
            pass
    return _redirect_core_page_with_scenario()



@app.route('/core/details')
def core_details():
    scenario_param = (request.args.get('scenario_name') or request.args.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_param)
    history = _load_run_history()
    current_user = _current_user()
    scenario_names, scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
    scenario_label = _resolve_scenario_display(scenario_norm, scenario_names, scenario_param)
    core_cfg = _core_config_for_request(include_password=True)
    core_host = core_cfg.get('host', CORE_HOST)
    core_port = int(core_cfg.get('port', CORE_PORT))
    xml_param = request.args.get('path')
    xml_path = os.path.abspath(xml_param) if xml_param else None
    # Safety: only allow inspecting local files we manage.
    if xml_path:
        try:
            allowed_roots = [os.path.abspath(_uploads_dir()), os.path.abspath(_outputs_dir())]
            if not any(xml_path == root or xml_path.startswith(root + os.sep) for root in allowed_roots):
                xml_path = None
        except Exception:
            xml_path = None
    if xml_path and not os.path.exists(xml_path):
        xml_path = None
    sid = request.args.get('session_id')
    xml_summary = None
    xml_valid = False
    errors = ''
    classification = None  # 'scenario' | 'session' | 'unknown' | 'planner'
    container_flag = False
    planner_bundle = False
    # If no XML path given but we have a session id, attempt to export the session XML so we can show details
    if not xml_path and sid:
        try:
            out_dir = os.path.join(_outputs_dir(), 'core-sessions')
            os.makedirs(out_dir, exist_ok=True)
            saved = _grpc_save_current_session_xml_with_config(core_cfg, out_dir, session_id=str(sid))
            if saved and os.path.exists(saved):
                xml_path = saved
        except Exception:
            pass
    if not xml_path and scenario_norm:
        fallback = _select_existing_path(scenario_paths.get(scenario_norm))
        if fallback:
            xml_path = fallback
            try:
                app.logger.info('[core.details] Using scenario fallback %s for %s', xml_path, scenario_label or scenario_norm)
            except Exception:
                pass
    if xml_path and os.path.exists(xml_path):
        try:
            # Lightweight classification: scenario XML should have <Scenarios>, session XML will have <session> and possibly <container>
            import xml.etree.ElementTree as _ET
            with open(xml_path, 'rb') as f:
                data_head = f.read(4096)
            try:
                root = _ET.fromstring(data_head + b"</dummy>")
            except Exception:
                try:
                    tree = _ET.parse(xml_path)
                    root = tree.getroot()
                except Exception:
                    root = None
            if root is not None:
                tag_lower = root.tag.lower()
                if 'scenarios' in tag_lower:
                    if root.find('.//ScenarioEditor') is not None:
                        planner_bundle = True
                        classification = 'planner'
                    else:
                        classification = 'scenario'
                elif 'session' in tag_lower:
                    classification = 'session'
                else:
                    classification = 'unknown'
                if root.find('.//container') is not None:
                    container_flag = True
                    if classification != 'scenario':
                        classification = 'session'
            if planner_bundle:
                xml_valid = True
                errors = ''
                xml_summary = _summarize_planner_scenarios(xml_path)
            else:
                ok, errs = _validate_core_xml(xml_path)
                if ok:
                    xml_valid = True
                else:
                    if classification == 'session':
                        xml_valid = True
                        # Suppress schema errors for session exports; treat as advisory only.
                    else:
                        xml_valid = False
                        if errs and not errors:
                            errors = errs
                # Always attempt analysis so graph can render even for invalid/session XML; mark summary with invalid flag
                try:
                    xml_summary = _analyze_core_xml(xml_path)
                    if xml_summary is None:
                        xml_summary = {}
                    if classification == 'session':
                        xml_summary['__session_export'] = True
                    if not xml_valid:
                        xml_summary['__invalid'] = True
                except Exception:
                    # On total failure keep prior xml_summary (None)
                    xml_summary = xml_summary or None
        except Exception as _e:
            errors = errors or f'XML inspection failed: {_e}'
    session_info = None
    if sid:
        try:
            sid_int = int(sid)
            # lookup session info via gRPC
            sessions = _list_active_core_sessions(core_host, core_port, core_cfg)
            for s in sessions:
                if int(s.get('id')) == sid_int:
                    session_info = s
                    break
        except Exception:
            session_info = None
    try:
        if xml_summary is not None:
            app.logger.debug(
                "[core_details] xml_path=%s classification=%s valid=%s nodes=%s switch_nodes=%s links_detail=%s",
                xml_path, classification, xml_valid,
                len(xml_summary.get('nodes') or []),
                len(xml_summary.get('switch_nodes') or []),
                len(xml_summary.get('links_detail') or [])
            )
        else:
            app.logger.debug(
                "[core_details] xml_path=%s classification=%s valid=%s (no summary)",
                xml_path, classification, xml_valid
            )
    except Exception:
        pass
    # Render without legacy approved-plan context
    return render_template(
        'core_details.html',
        xml_path=xml_path,
        valid=xml_valid,
        errors=errors,
        summary=xml_summary,
        session=session_info,
        classification=classification,
        container_flag=container_flag,
        scenario_label=scenario_label,
    )


@app.route('/admin/cleanup_pycore', methods=['POST'])
def admin_cleanup_pycore():
    """Remove stale /tmp/pycore.* directories not associated with active sessions.

    Returns JSON summary: {removed: [...], kept: [...]}"""
    try:
        core_cfg = _core_config_for_request(include_password=True)
        core_host = core_cfg.get('host', CORE_HOST)
        core_port = int(core_cfg.get('port', CORE_PORT))
        active_ids = set()
        try:
            sessions = _list_active_core_sessions(core_host, core_port, core_cfg)
            for s in sessions:
                try:
                    active_ids.add(int(s.get('id')))
                except Exception:
                    continue
        except Exception:
            pass
        removed = []
        kept = []
        for p in Path('/tmp').glob('pycore.*'):
            try:
                sid = int(p.name.split('.')[-1])
            except Exception:
                kept.append(str(p))
                continue
            if sid in active_ids:
                kept.append(str(p))
                continue
            # Only remove if directory exists and not recently modified (older than 30s) to avoid race with creation
            try:
                age = time.time() - p.stat().st_mtime
            except Exception:
                age = 999
            if age < 30:
                kept.append(str(p))
                continue
            try:
                shutil.rmtree(p)
                removed.append(str(p))
            except Exception:
                kept.append(str(p))
        return jsonify({'ok': True, 'removed': removed, 'kept': kept, 'active_session_ids': sorted(active_ids)})
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)})


@app.route('/core/save_xml', methods=['POST'])
def core_save_xml():
    sid = request.form.get('session_id')
    try:
        sid_int = int(sid) if sid is not None else None
    except Exception:
        sid_int = None
    out_dir = os.path.join(_outputs_dir(), 'core-sessions')
    os.makedirs(out_dir, exist_ok=True)
    core_cfg = _core_config_for_request(include_password=True)
    try:
        saved = _grpc_save_current_session_xml_with_config(
            core_cfg,
            out_dir,
            session_id=str(sid_int) if sid_int is not None else None,
        )
        if not saved or not os.path.exists(saved):
            return Response('Failed to save session XML', status=500)
        # Stream back as a download so frontend can save via blob
        return send_file(saved, as_attachment=True, download_name=os.path.basename(saved), mimetype='application/xml')
    except Exception as e:
        return Response(f'Error saving session XML: {e}', status=500)


@app.route('/core/session_scenario', methods=['GET'])
def core_session_scenario():
    """Resolve a CORE session (by sid or by remote path) to a scenario label.

    Query params:
      - sid: CORE session id
      - path: remote CORE VM path (e.g. /tmp/pycore.17/Scenario_1.xml)
    """
    sid_raw = (request.args.get('sid') or '').strip()
    path_raw = (request.args.get('path') or '').strip()
    sid: int | None = None
    if sid_raw:
        try:
            sid = int(sid_raw)
        except Exception:
            sid = None
    if sid is None and path_raw:
        sid = _extract_session_id_from_core_path(path_raw)
    if sid is None:
        return jsonify({'ok': False, 'error': 'Provide sid or path.'}), 400

    history = _load_run_history()
    current_user = _current_user()
    scenario_names, scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
    core_cfg = _core_config_for_request(include_password=True)
    host = core_cfg.get('host', CORE_HOST)
    try:
        port = int(core_cfg.get('port', CORE_PORT))
    except Exception:
        port = CORE_PORT

    # 1) Prefer local mapping store (already scoped by CORE host/port and timestamps).
    scenario_label: str | None = None
    try:
        store = _load_core_sessions_store()
        store = _migrate_core_sessions_store_with_core_targets(store, history)
        store = _filter_core_sessions_store_for_core(store, host, port)
        scenario_label = _session_store_scenario_for_session_id(store, int(sid), host=host, port=port)
    except Exception:
        scenario_label = None

    # 2) Fallback to CORE-VM-resident meta file.
    remote_meta: dict[str, Any] | None = None
    if not scenario_label:
        try:
            remote_meta = _read_remote_session_scenario_meta(core_cfg, session_id=int(sid), logger=app.logger)
            if isinstance(remote_meta, dict):
                scenario_label = (remote_meta.get('scenario_name') or '').strip() or None
        except Exception:
            remote_meta = None

    scenario_norm = _normalize_scenario_label(scenario_label or '') if scenario_label else ''
    # Enforce assignment-based access for restricted roles.
    allowed_norms = _builder_allowed_norms(current_user)
    if allowed_norms is not None and scenario_norm and scenario_norm not in allowed_norms:
        return jsonify({'ok': False, 'error': 'Scenario not assigned.'}), 403
    scenario_display = _resolve_scenario_display(scenario_norm, scenario_names, scenario_label or '') if scenario_norm else ''
    return jsonify({
        'ok': True,
        'session_id': int(sid),
        'scenario_name': scenario_display or (scenario_label or ''),
        'scenario_norm': scenario_norm,
        'core_host': host,
        'core_port': port,
        'source': 'local_store' if scenario_label and not remote_meta else ('remote_meta' if remote_meta else 'unknown'),
    })


@app.route('/core/session/<int:sid>')
def core_session(sid: int):
    """Convenience route to view a specific session's details.
    Attempts to look up the session and its file path, then reuses the core_details template.
    """
    session_info = None
    xml_path = None
    core_cfg = _core_config_for_request(include_password=True)
    try:
        sessions = _list_active_core_sessions(
            core_cfg.get('host', CORE_HOST),
            int(core_cfg.get('port', CORE_PORT)),
            core_cfg,
        )
        for s in sessions:
            if int(s.get('id')) == int(sid):
                session_info = s
                xml_path = s.get('file')
                break
    except Exception:
        session_info = None
    xml_valid = False
    errors = ''
    xml_summary = None
    if xml_path and os.path.exists(xml_path):
        ok, errs = _validate_core_xml(xml_path)
        xml_valid = bool(ok)
        errors = errs if not ok else ''
        xml_summary = _analyze_core_xml(xml_path) if ok else None
    return render_template('core_details.html', xml_path=xml_path, valid=xml_valid, errors=errors, summary=xml_summary, session=session_info)


@app.route('/test_core', methods=['POST'])
def test_core():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'ok': False, 'error': 'Admin privileges required'}), 403
    try:
        data: Dict[str, Any] = {}
        if request.is_json:
            data = request.get_json(silent=True) or {}
        else:
            data = {
                "host": request.form.get('host'),
                "port": request.form.get('port'),
                "ssh_enabled": request.form.get('ssh_enabled'),
                "ssh_host": request.form.get('ssh_host'),
                "ssh_port": request.form.get('ssh_port'),
                "ssh_username": request.form.get('ssh_username'),
                "ssh_password": request.form.get('ssh_password'),
                "core": request.form.get('core_json'),
            }
            try:
                if isinstance(data.get('core'), str) and data['core']:
                    data['core'] = json.loads(data['core'])
            except Exception:
                data['core'] = None
        direct_override = {
            key: data.get(key)
            for key in ('host', 'port', 'ssh_enabled', 'ssh_host', 'ssh_port', 'ssh_username', 'ssh_password')
            if data.get(key) not in (None, '')
        }
        scenario_core_raw = data.get('hitl_core')
        if not scenario_core_raw and request.form.get('hitl_core_json'):
            try:
                scenario_core_raw = json.loads(request.form.get('hitl_core_json') or '')
            except Exception:
                scenario_core_raw = None
        scenario_core_dict = scenario_core_raw if isinstance(scenario_core_raw, dict) else {}
        scenario_vm_key = str(scenario_core_dict.get('vm_key') or '').strip()
        scenario_vm_node = str(scenario_core_dict.get('vm_node') or '').strip()
        scenario_vm_name = str(scenario_core_dict.get('vm_name') or '').strip()
        scenario_vm_id_raw = scenario_core_dict.get('vmid') or scenario_core_dict.get('vm_id')
        scenario_vm_id = str(scenario_vm_id_raw).strip() if isinstance(scenario_vm_id_raw, (str, int)) else ''
        if not scenario_vm_node and scenario_vm_key and '::' in scenario_vm_key:
            scenario_vm_node = scenario_vm_key.split('::', 1)[0].strip()
        cfg = _merge_core_configs(
            data.get('core') if isinstance(data.get('core'), dict) else None,
            scenario_core_dict,
            direct_override if direct_override else None,
            include_password=True,
        )
        if not scenario_vm_key:
            return jsonify({'ok': False, 'error': 'Select a CORE VM before validating the connection.'}), 400
        cfg['vm_key'] = scenario_vm_key
        if scenario_vm_node:
            cfg['vm_node'] = scenario_vm_node
        if scenario_vm_name:
            cfg.setdefault('vm_name', scenario_vm_name)
        if scenario_vm_id:
            cfg.setdefault('vmid', scenario_vm_id)
        core_secret_id = str(cfg.get('core_secret_id') or '').strip()
        stored_secret = None
        if core_secret_id:
            stored_secret = _load_core_credentials(core_secret_id)
            if not stored_secret:
                return jsonify({'ok': False, 'error': 'Stored CORE credentials are unavailable. Re-enter the SSH password for the selected CORE VM.'}), 400
            stored_vm_key = str(stored_secret.get('vm_key') or '').strip()
            if stored_vm_key and stored_vm_key != scenario_vm_key:
                stored_vm_name = str(stored_secret.get('vm_name') or stored_secret.get('vm_key') or 'previous VM')
                mismatch_message = (
                    f'Stored CORE credentials target {stored_vm_name}, ' \
                    f'but Step 2 is configured for {scenario_vm_name or scenario_vm_key}. '
                    'Clear the Step 2 credentials and validate with the selected CORE VM.'
                )
                return jsonify({'ok': False, 'error': mismatch_message, 'vm_mismatch': True}), 409
            stored_vm_node = str(stored_secret.get('vm_node') or '').strip()
            if stored_vm_node and scenario_vm_node and stored_vm_node != scenario_vm_node:
                mismatch_message = (
                    f'Stored CORE credentials reference node {stored_vm_node}, '
                    f'but the selected CORE VM resides on node {scenario_vm_node}. '
                    'Clear the Step 2 credentials and validate with the selected CORE VM.'
                )
                return jsonify({'ok': False, 'error': mismatch_message, 'vm_mismatch': True}), 409
        auto_start_daemon = bool(cfg.get('auto_start_daemon'))
        install_custom_services = bool(cfg.get('install_custom_services'))
        stop_duplicate_daemons = bool(cfg.get('stop_duplicate_daemons'))
        adv_fix_docker_daemon = bool(cfg.get('adv_fix_docker_daemon'))
        adv_run_core_cleanup = bool(cfg.get('adv_run_core_cleanup'))
        adv_check_core_version = bool(cfg.get('adv_check_core_version'))
        adv_restart_core_daemon = bool(cfg.get('adv_restart_core_daemon'))
        adv_start_core_daemon = bool(cfg.get('adv_start_core_daemon'))
        adv_auto_kill_sessions = bool(cfg.get('adv_auto_kill_sessions'))
        is_pytest = bool(os.environ.get('PYTEST_CURRENT_TEST') or ('pytest' in sys.modules))
        daemon_pids: List[int] = []
        install_meta: Optional[Dict[str, Any]] = None

        advanced_checks: Dict[str, Dict[str, Any]] = {}
        advanced_warnings: List[str] = []
        if (adv_fix_docker_daemon or adv_run_core_cleanup or adv_check_core_version or adv_restart_core_daemon or adv_start_core_daemon or adv_auto_kill_sessions):
            # These checks require remote access.
            if is_pytest:
                app.logger.info('[core] advanced checks enabled but skipping remote execution (pytest)')
                advanced_checks = _run_core_connection_advanced_checks(
                    cfg,
                    adv_fix_docker_daemon=False,
                    adv_run_core_cleanup=False,
                    adv_check_core_version=False,
                    adv_restart_core_daemon=False,
                    adv_start_core_daemon=False,
                    adv_auto_kill_sessions=False,
                )
            else:
                advanced_checks = _run_core_connection_advanced_checks(
                    cfg,
                    adv_fix_docker_daemon=adv_fix_docker_daemon,
                    adv_run_core_cleanup=adv_run_core_cleanup,
                    adv_check_core_version=adv_check_core_version,
                    adv_restart_core_daemon=adv_restart_core_daemon,
                    adv_start_core_daemon=adv_start_core_daemon,
                    adv_auto_kill_sessions=adv_auto_kill_sessions,
                )
                failures = [
                    (key, res)
                    for key, res in (advanced_checks or {}).items()
                    if isinstance(res, dict) and res.get('enabled') and (res.get('ok') is False)
                ]
                if failures:
                    parts = []
                    for key, res in failures:
                        msg = str(res.get('message') or '').strip()
                        parts.append(f"{key}: {msg or 'failed'}")
                    advanced_warnings.append('Advanced checks failed: ' + '; '.join(parts))
        if is_pytest and not (auto_start_daemon or install_custom_services or stop_duplicate_daemons):
            # Unit tests mock the tunnel/socket checks and should not depend on real DNS/SSH.
            app.logger.info('[core] skipping core-daemon SSH inspection (pytest)')
        elif paramiko is None:
            if auto_start_daemon:
                app.logger.warning('[core] Paramiko unavailable; cannot auto-start or inspect core-daemon remotely.')
            if install_custom_services:
                app.logger.warning('[core] Paramiko unavailable; cannot install custom services remotely.')
        else:
            _ensure_paramiko_available()
            ssh_client = paramiko.SSHClient()  # type: ignore[assignment]
            ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
            try:
                ssh_client.connect(
                    hostname=str(cfg.get('ssh_host') or cfg.get('host') or 'localhost'),
                    port=int(cfg.get('ssh_port') or 22),
                    username=str(cfg.get('ssh_username') or ''),
                    password=cfg.get('ssh_password'),
                    look_for_keys=False,
                    allow_agent=False,
                    timeout=10.0,
                    banner_timeout=10.0,
                    auth_timeout=10.0,
                )
                daemon_pids = _collect_remote_core_daemon_pids(ssh_client)
                if len(daemon_pids) > 1:
                    if stop_duplicate_daemons:
                        try:
                            _stop_remote_core_daemon_conflict(
                                ssh_client,
                                sudo_password=cfg.get('ssh_password'),
                                pids=daemon_pids,
                                logger=app.logger,
                            )
                            try:
                                time.sleep(1.0)
                            except Exception:
                                pass
                            daemon_pids = _collect_remote_core_daemon_pids(ssh_client)
                        except Exception as exc:
                            msg = (
                                'Multiple core-daemon processes are running on the CORE VM, and the automatic stop failed. '
                                f'PIDs: {", ".join(str(pid) for pid in daemon_pids)}. Error: {exc}'
                            )
                            return jsonify({
                                'ok': False,
                                'error': msg,
                                'daemon_conflict': True,
                                'daemon_pids': daemon_pids,
                                'can_stop_daemons': bool(cfg.get('ssh_password')),
                                'code': 'core_daemon_conflict',
                            }), 409
                    if len(daemon_pids) > 1:
                        msg = (
                            'Multiple core-daemon processes are running on the CORE VM. '
                            f'PIDs: {", ".join(str(pid) for pid in daemon_pids)}. '
                            'Stop duplicate daemons before continuing.'
                        )
                        return jsonify({
                            'ok': False,
                            'error': msg,
                            'daemon_conflict': True,
                            'daemon_pids': daemon_pids,
                            'can_stop_daemons': bool(cfg.get('ssh_password')),
                            'code': 'core_daemon_conflict',
                        }), 409

                if install_custom_services:
                    app.logger.info('[core] Installing custom services on CORE VM...')
                    install_meta = _install_custom_services_to_core_vm(
                        ssh_client,
                        sudo_password=cfg.get('ssh_password'),
                        logger=app.logger,
                    )
                    app.logger.info(
                        '[core] Custom services installed: modules=%s target=%s',
                        ','.join(install_meta.get('modules') or []),
                        install_meta.get('services_dir'),
                    )
                    try:
                        time.sleep(1.0)
                    except Exception:
                        pass
                    daemon_pids = _collect_remote_core_daemon_pids(ssh_client)
                    if len(daemon_pids) > 1:
                        if stop_duplicate_daemons:
                            try:
                                _stop_remote_core_daemon_conflict(
                                    ssh_client,
                                    sudo_password=cfg.get('ssh_password'),
                                    pids=daemon_pids,
                                    logger=app.logger,
                                )
                                try:
                                    time.sleep(1.0)
                                except Exception:
                                    pass
                                daemon_pids = _collect_remote_core_daemon_pids(ssh_client)
                            except Exception as exc:
                                msg = (
                                    'Multiple core-daemon processes are running on the CORE VM after installing services, '
                                    'and the automatic stop failed. '
                                    f'PIDs: {", ".join(str(pid) for pid in daemon_pids)}. Error: {exc}'
                                )
                                return jsonify({
                                    'ok': False,
                                    'error': msg,
                                    'daemon_conflict': True,
                                    'daemon_pids': daemon_pids,
                                    'can_stop_daemons': bool(cfg.get('ssh_password')),
                                    'code': 'core_daemon_conflict',
                                }), 409
                        if len(daemon_pids) > 1:
                            msg = (
                                'Multiple core-daemon processes are running on the CORE VM after installing services. '
                                f'PIDs: {", ".join(str(pid) for pid in daemon_pids)}. '
                                'Stop duplicate daemons before continuing.'
                            )
                            return jsonify({
                                'ok': False,
                                'error': msg,
                                'daemon_conflict': True,
                                'daemon_pids': daemon_pids,
                                'can_stop_daemons': bool(cfg.get('ssh_password')),
                                'code': 'core_daemon_conflict',
                            }), 409

                if auto_start_daemon:
                    if daemon_pids:
                        app.logger.info('[core] Skipping auto-start; core-daemon already running (PID %s).', daemon_pids[0])
                    else:
                        _start_remote_core_daemon(ssh_client, cfg.get('ssh_password'), app.logger)
                        try:
                            time.sleep(1.0)
                        except Exception:
                            pass
                        daemon_pids = _collect_remote_core_daemon_pids(ssh_client)
                        if len(daemon_pids) != 1:
                            msg = 'core-daemon auto-start attempted but a single running process was not detected.'
                            app.logger.warning('[core] %s (pids=%s)', msg, daemon_pids)
                            return jsonify({'ok': False, 'error': msg, 'daemon_conflict': True}), 502
                else:
                    if not daemon_pids:
                        app.logger.warning('[core] No core-daemon process detected during validation.')
            except Exception as conn_exc:
                app.logger.warning('[core] core-daemon SSH inspection failed: %s', conn_exc)
            finally:
                try:
                    ssh_client.close()
                except Exception:
                    pass
        if is_pytest:
            app.logger.info('[core] skipping daemon listening check (pytest)')
        elif paramiko is None:
            app.logger.warning('[core] skipping daemon listening check (paramiko unavailable)')
        else:
            try:
                _ensure_core_daemon_listening(cfg, timeout=5.0)
            except Exception as exc:
                app.logger.warning('[core] daemon listening check failed: %s', exc)
                return jsonify({'ok': False, 'error': f'core-daemon is not reachable on {cfg.get("host")}:{cfg.get("port")}: {exc}'}), 502
        remote_desc = f"{cfg.get('host')}:{cfg.get('port')}"
        import socket
        forwarded_host = ''
        forwarded_port = 0
        with _core_connection(cfg) as (conn_host, conn_port):
            forwarded_host = conn_host
            forwarded_port = int(conn_port)
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2.0)
            try:
                sock.connect((forwarded_host, forwarded_port))
            finally:
                try:
                    sock.close()
                except Exception:
                    pass
        scenario_index_raw = data.get('scenario_index')
        try:
            scenario_index_val = int(scenario_index_raw) if scenario_index_raw not in (None, '') else None
        except Exception:
            scenario_index_val = None
        scenario_name_val = str(data.get('scenario_name') or data.get('scenario') or '').strip()
        secret_payload = {
            'scenario_name': scenario_name_val,
            'scenario_index': scenario_index_val,
            'grpc_host': cfg.get('host'),
            'grpc_port': cfg.get('port'),
            'ssh_host': cfg.get('ssh_host'),
            'ssh_port': cfg.get('ssh_port'),
            'ssh_username': cfg.get('ssh_username'),
            'ssh_password': cfg.get('ssh_password'),
            'ssh_enabled': cfg.get('ssh_enabled'),
            'venv_bin': cfg.get('venv_bin'),
        }
        if isinstance(scenario_core_raw, dict):
            secret_payload.update({
                'vm_key': scenario_core_raw.get('vm_key'),
                'vm_name': scenario_core_raw.get('vm_name'),
                'vm_node': scenario_core_raw.get('vm_node'),
                'vmid': scenario_core_raw.get('vmid'),
                'proxmox_secret_id': scenario_core_raw.get('proxmox_secret_id') or scenario_core_raw.get('secret_id'),
                'proxmox_target': scenario_core_raw.get('proxmox_target') if isinstance(scenario_core_raw.get('proxmox_target'), dict) else None,
            })
        try:
            stored_meta = _save_core_credentials(secret_payload)
        except RuntimeError as exc:
            return jsonify({"ok": False, "error": str(exc)}), 500
        except Exception as exc:
            app.logger.exception('[core] failed to persist credentials: %s', exc)
            return jsonify({"ok": False, "error": 'CORE connection succeeded but credentials could not be stored'}), 500
        summary_vm_label = stored_meta.get('vm_name') or stored_meta.get('vm_key')
        if summary_vm_label:
            summary_message = (
                f"Validated CORE access for {stored_meta['ssh_username']} @ "
                f"{stored_meta['ssh_host']}:{stored_meta['ssh_port']} (VM {summary_vm_label})"
            )
        else:
            summary_message = f"Validated CORE access for {stored_meta['ssh_username']} @ {stored_meta['ssh_host']}:{stored_meta['ssh_port']}"

        # Persist a safe shared hint so builders/participants can see the validated state.
        try:
            if scenario_name_val:
                _merge_hitl_validation_into_scenario_catalog(
                    scenario_name_val,
                    core={
                        'core_secret_id': stored_meta.get('identifier'),
                        'validated': bool(stored_meta.get('identifier')),
                        'last_validated_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
                        'grpc_host': stored_meta.get('grpc_host') or stored_meta.get('host'),
                        'grpc_port': stored_meta.get('grpc_port') or stored_meta.get('port'),
                        'ssh_host': stored_meta.get('ssh_host'),
                        'ssh_port': stored_meta.get('ssh_port'),
                        'vm_key': stored_meta.get('vm_key'),
                        'vm_name': stored_meta.get('vm_name'),
                        'vm_node': stored_meta.get('vm_node'),
                        'stored_at': stored_meta.get('stored_at'),
                    },
                )
        except Exception:
            pass
        return jsonify({
            "ok": True,
            "forward_host": forwarded_host,
            "forward_port": forwarded_port,
            "remote": remote_desc,
            "ssh_enabled": bool(cfg.get('ssh_enabled')),
            "host": cfg.get('host'),
            "port": int(cfg.get('port', 0)) if cfg.get('port') is not None else None,
            "daemon_pids": daemon_pids,
            "install_custom_services": install_meta,
            "advanced_checks": advanced_checks,
            "warnings": advanced_warnings,
            "core": _normalize_core_config(cfg, include_password=False),
            "core_secret_id": stored_meta['identifier'],
            "core_summary": stored_meta,
            "scenario_index": scenario_index_val,
            "scenario_name": scenario_name_val,
            "message": summary_message,
        })
    except _SSHTunnelError as e:
        return jsonify({"ok": False, "error": str(e), "ssh_error": True}), 200
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 200


@app.route('/test_core_venv', methods=['POST'])
def test_core_venv():
    payload = request.get_json(silent=True) or {}
    raw_path = payload.get('venv_bin') or payload.get('path') or ''
    sanitized = _sanitize_venv_bin_path(raw_path)
    if not sanitized:
        return jsonify({"ok": False, "error": 'Provide the CORE virtualenv bin path to test.'}), 400
    if not os.path.isabs(sanitized):
        return jsonify({"ok": False, "error": f'CORE venv bin must be an absolute path: {sanitized}'}), 400
    ssh_host = str(payload.get('ssh_host') or payload.get('host') or '').strip()
    if not ssh_host:
        return jsonify({"ok": False, "error": 'Provide the SSH host for the CORE VM to test the virtualenv.'}), 400
    try:
        ssh_port = int(payload.get('ssh_port') or 22)
    except Exception:
        return jsonify({"ok": False, "error": 'SSH port must be an integer.'}), 400
    ssh_username = str(payload.get('ssh_username') or '').strip()
    if not ssh_username:
        return jsonify({"ok": False, "error": 'Enter the SSH username before testing the CORE virtualenv.'}), 400
    ssh_password_raw = payload.get('ssh_password')
    ssh_password = '' if ssh_password_raw in (None, '') else str(ssh_password_raw)
    if not ssh_password:
        return jsonify({"ok": False, "error": 'Enter the SSH password before testing the CORE virtualenv.'}), 400
    try:
        _ensure_paramiko_available()
    except RuntimeError as exc:
        return jsonify({"ok": False, "error": str(exc)}), 500
    client = paramiko.SSHClient()  # type: ignore[assignment]
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
    try:
        client.connect(
            hostname=ssh_host,
            port=ssh_port,
            username=ssh_username,
            password=ssh_password,
            look_for_keys=False,
            allow_agent=False,
            timeout=15.0,
            banner_timeout=15.0,
            auth_timeout=15.0,
        )
    except Exception as exc:
        return jsonify({"ok": False, "error": f'Failed to open SSH session to {ssh_host}:{ssh_port}: {exc}'}), 502
    python_candidates = [
        os.path.join(sanitized, exe_name)
        for exe_name in PYTHON_EXECUTABLE_NAMES
    ]
    candidate_literal = ' '.join(shlex.quote(path) for path in python_candidates)
    python_probe = textwrap.dedent(
        """
        import json
        import sys

        result = {
            "python": sys.executable,
            "version": sys.version.split()[0],
        }
        try:
            import core  # type: ignore  # noqa: F401
            import core.api.grpc.client  # type: ignore  # noqa: F401
        except Exception as exc:  # pragma: no cover - remote execution
            result["status"] = "error"
            result["error"] = repr(exc)
        else:
            result["status"] = "ok"
        print("::VENVCHECK::" + json.dumps(result))
        if result.get("status") != "ok":
            sys.exit(3)
        """
    ).strip()
    missing_payload = json.dumps({
        "status": "error",
        "error": f"No python executable found in {sanitized}",
    })
    remote_cmd = textwrap.dedent(
        f"""
        CANDIDATES=({candidate_literal})
        FOUND=0
        for candidate in "${{CANDIDATES[@]}}"; do
            if [ -x "$candidate" ]; then
                FOUND=1
                "$candidate" - <<'PY'
{python_probe}
PY
                exit $?
            fi
        done
        if [ $FOUND -eq 0 ]; then
            echo "::VENVCHECK::{missing_payload}"
            exit 10
        fi
        """
    ).strip()
    try:
        stdin, stdout, stderr = client.exec_command(remote_cmd, timeout=30.0)
        try:
            stdout_data = stdout.read()
            stderr_data = stderr.read()
        finally:
            try:
                stdin.close()
            except Exception:
                pass
        exit_code = stdout.channel.recv_exit_status() if hasattr(stdout, 'channel') else 0
    except Exception as exc:
        try:
            client.close()
        except Exception:
            pass
        return jsonify({"ok": False, "error": f'Failed to probe CORE virtualenv via SSH: {exc}'}), 500
    finally:
        try:
            client.close()
        except Exception:
            pass
    def _decode(data: Any) -> str:
        if isinstance(data, bytes):
            return data.decode('utf-8', 'ignore')
        return str(data or '')
    stdout_text = _decode(stdout_data)
    stderr_text = _decode(stderr_data)
    summary: Dict[str, Any] | None = None
    for blob in (stdout_text, stderr_text):
        if not blob:
            continue
        for line in blob.splitlines():
            line = line.strip()
            if not line:
                continue
            if not line.startswith('::VENVCHECK::'):
                continue
            payload_text = line.split('::VENVCHECK::', 1)[-1]
            try:
                summary = json.loads(payload_text)
                break
            except Exception:
                continue
        if summary:
            break
    python_version = summary.get('version') if isinstance(summary, dict) else None
    python_path = summary.get('python') if isinstance(summary, dict) else None
    status = summary.get('status') if isinstance(summary, dict) else None
    error_detail = summary.get('error') if isinstance(summary, dict) else None
    if exit_code == 0 and status == 'ok':
        message = summary.get('message') or f"Python {python_version or ''} imported core.api.grpc successfully.".strip()
        return jsonify({
            "ok": True,
            "message": message,
            "venv_bin": sanitized,
            "python_executable": python_path,
            "python_version": python_version,
            "ssh_host": ssh_host,
            "ssh_port": ssh_port,
            "stdout": stdout_text.strip(),
            "stderr": stderr_text.strip(),
        })
    error_message = error_detail or stderr_text.strip() or stdout_text.strip() or 'core.api.grpc import failed in this environment.'
    return jsonify({
        "ok": False,
        "error": error_message,
        "venv_bin": sanitized,
        "python_executable": python_path,
        "python_version": python_version,
        "ssh_host": ssh_host,
        "ssh_port": ssh_port,
        "stdout": stdout_text.strip(),
        "stderr": stderr_text.strip(),
        "returncode": exit_code,
    }), 400


@app.route('/stream/<run_id>')
def stream_logs(run_id: str):
    meta = RUNS.get(run_id)
    if not meta:
        return Response('event: error\ndata: not found\n\n', mimetype='text/event-stream')
    log_path = meta.get('log_path')

    marker_re = re.compile(
        r'^' + re.escape(_SSE_MARKER_PREFIX) + r'\s+(?P<event>[a-zA-Z0-9_\-]+)\s+(?P<data>\{.*\})\s*$'
    )

    def _emit_line(line: str):
        """Translate marker lines into typed SSE events; otherwise emit as message."""
        try:
            m = marker_re.match(line or '')
            if m:
                ev_name = m.group('event')
                payload_text = m.group('data')
                yield f"event: {ev_name}\n"
                yield f"data: {payload_text}\n\n"
                return
        except Exception:
            pass
        yield f"data: {line}\n\n"

    def generate():
        # 1) Send existing backlog first for immediate context
        last_pos = 0
        try:
            with open(log_path, 'r', encoding='utf-8', errors='ignore') as f_init:
                backlog = f_init.read()
                last_pos = f_init.tell()
            if backlog:
                for line in backlog.splitlines():
                    yield from _emit_line(line)
        except FileNotFoundError:
            pass
        # 2) Tail incremental additions
        while True:
            try:
                with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
                    f.seek(last_pos)
                    chunk = f.read()
                    if chunk:
                        last_pos = f.tell()
                        # Split into lines to keep events reasonable
                        for line in chunk.splitlines():
                            yield from _emit_line(line)
            except FileNotFoundError:
                pass
            # Check process status
            proc = meta.get('proc')
            rc = None
            if proc:
                rc = proc.poll()
                if rc is not None and meta.get('returncode') is None:
                    meta['returncode'] = rc
                    meta['done'] = True
            if meta.get('done'):
                # Drain any remaining buffered log lines before ending so late
                # cleanup markers are visible to the client.
                try:
                    with open(log_path, 'r', encoding='utf-8', errors='ignore') as f_final:
                        f_final.seek(last_pos)
                        tail = f_final.read()
                        if tail:
                            last_pos = f_final.tell()
                            for line in tail.splitlines():
                                yield from _emit_line(line)
                except FileNotFoundError:
                    pass
                # Signal end regardless; client will stop listening
                yield "event: end\ndata: done\n\n"
                break
            time.sleep(0.5)

    headers = {
        'Cache-Control': 'no-cache',
        'X-Accel-Buffering': 'no',  # for some proxies
        'Content-Type': 'text/event-stream',
        'Connection': 'keep-alive',
    }
    return Response(generate(), headers=headers)


@app.route('/flag_generators_test/cleanup/<run_id>', methods=['POST'])
def flag_generators_test_cleanup(run_id: str):
    """Delete all artifacts for a flag-generator test run.

    This is intentionally scoped to outputs/ to avoid deleting arbitrary paths.
    """
    t0 = time.time()
    app.logger.info("[flaggen_test] POST /flag_generators_test/cleanup run_id=%s", run_id)
    meta = RUNS.get(run_id)
    if meta and meta.get('kind') != 'flag_generator_test':
        app.logger.info("[flaggen_test] cleanup refusing: kind=%r", meta.get('kind'))
        return jsonify({'ok': False, 'error': 'not found'}), 404

    run_dir = meta.get('run_dir') if isinstance(meta, dict) else None
    if not isinstance(run_dir, str) or not run_dir:
        run_dir = _flaggen_run_dir_for_id(run_id)
    if not isinstance(run_dir, str) or not run_dir:
        app.logger.warning("[flaggen_test] cleanup missing run dir run_id=%s", run_id)
        return jsonify({'ok': False, 'error': 'missing run dir'}), 500

    abs_run_dir = os.path.abspath(run_dir)
    outputs_root = os.path.abspath(_outputs_dir())
    if not (abs_run_dir == outputs_root or abs_run_dir.startswith(outputs_root + os.sep)):
        app.logger.warning(
            "[flaggen_test] cleanup refusing run_id=%s abs_run_dir=%s outputs_root=%s",
            run_id,
            abs_run_dir,
            outputs_root,
        )
        return jsonify({'ok': False, 'error': 'refusing'}), 400

    app.logger.info(
        "[flaggen_test] cleanup resolved run_id=%s run_dir=%s exists=%s",
        run_id,
        abs_run_dir,
        os.path.isdir(abs_run_dir),
    )

    # Best-effort: stop any still-running runner process.
    try:
        if isinstance(meta, dict):
            meta['cleanup_requested'] = True
        proc = meta.get('proc') if isinstance(meta, dict) else None
        if proc and hasattr(proc, 'poll') and proc.poll() is None:
            try:
                app.logger.info("[flaggen_test] cleanup terminating runner run_id=%s", run_id)
                proc.terminate()
                proc.wait(timeout=5)
            except Exception:
                try:
                    app.logger.info("[flaggen_test] cleanup killing runner run_id=%s", run_id)
                    proc.kill()
                except Exception:
                    pass
    except Exception:
        pass

    # Emit cleanup markers to the log if possible.
    try:
        lp = meta.get('log_path') if isinstance(meta, dict) else os.path.join(abs_run_dir, 'run.log')
        if isinstance(lp, str) and lp:
            with open(lp, 'a', encoding='utf-8') as log_f:
                _write_sse_marker(log_f, 'phase', {'phase': 'cleanup_start', 'run_id': run_id})
    except Exception:
        pass

    removed = False
    try:
        if os.path.isdir(abs_run_dir):
            shutil.rmtree(abs_run_dir, ignore_errors=True)
        removed = True
    except Exception:
        removed = False

    app.logger.info(
        "[flaggen_test] cleanup removed=%s run_id=%s elapsed_ms=%d",
        removed,
        run_id,
        int((time.time() - t0) * 1000),
    )

    try:
        if isinstance(meta, dict):
            meta['done'] = True
    except Exception:
        pass

    try:
        if isinstance(meta, dict):
            lp = meta.get('log_path')
        else:
            lp = os.path.join(abs_run_dir, 'run.log')
        if isinstance(lp, str) and lp and os.path.exists(lp):
            with open(lp, 'a', encoding='utf-8') as log_f2:
                _write_sse_marker(log_f2, 'phase', {'phase': 'cleanup_done', 'run_id': run_id, 'removed': removed})
    except Exception:
        pass

    try:
        RUNS.pop(run_id, None)
    except Exception:
        pass

    app.logger.info(
        "[flaggen_test] cleanup complete run_id=%s elapsed_ms=%d",
        run_id,
        int((time.time() - t0) * 1000),
    )

    return jsonify({'ok': True, 'removed': removed}), 200


@app.route('/cancel_run/<run_id>', methods=['POST'])
def cancel_run(run_id: str):
    meta = RUNS.get(run_id)
    if not meta:
        return jsonify({"error": "not found"}), 404
    proc = meta.get('proc')
    try:
        if proc and proc.poll() is None:
            # Append a cancel marker to log, then terminate
            lp = meta.get('log_path')
            try:
                with open(lp, 'a', encoding='utf-8') as f:
                    f.write("\n== Run cancelled by user ==\n")
            except Exception:
                pass
            proc.terminate()
            try:
                proc.wait(timeout=5)
            except Exception:
                proc.kill()
        meta['done'] = True
        if meta.get('returncode') is None:
            meta['returncode'] = -1
        try:
            _cleanup_remote_workspace(meta)
        except Exception:
            pass
        _close_async_run_tunnel(meta)
        return jsonify({"ok": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ---------------- Data Sources -----------------
@app.route('/data_sources')
def data_sources_page():
    state = _load_data_sources_state()
    return render_template('data_sources.html', sources=state.get('sources', []))

@app.route('/data_sources/upload', methods=['POST'])
def data_sources_upload():
    f = request.files.get('csv_file')
    if not f or f.filename == '':
        flash('No file selected.')
        return redirect(url_for('data_sources_page'))
    filename = secure_filename(f.filename)
    if not filename.lower().endswith('.csv'):
        flash('Only .csv allowed.')
        return redirect(url_for('data_sources_page'))
    unique = datetime.datetime.now().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:6]
    dest_dir = os.path.join(DATA_SOURCES_DIR)
    os.makedirs(dest_dir, exist_ok=True)
    path = os.path.join(dest_dir, f"{unique}-{filename}")
    f.save(path)
    ok, note, norm_rows, skipped = _validate_and_normalize_data_source_csv(path, skip_invalid=True)
    if not ok:
        try: os.remove(path)
        except Exception: pass
        flash(f'Invalid CSV: {note}')
        return redirect(url_for('data_sources_page'))
    # Write back normalized CSV to ensure required/optional columns are present
    try:
        tmp = path + '.tmp'
        with open(tmp, 'w', encoding='utf-8', newline='') as f:
            w = csv.writer(f)
            for r in norm_rows:
                w.writerow(r)
        os.replace(tmp, path)
    except Exception:
        pass
    state = _load_data_sources_state()
    entry = {
        "id": uuid.uuid4().hex[:12],
        "name": filename,
        "path": path,
        "enabled": True,
        "rows": note,
        "uploaded": datetime.datetime.utcnow().isoformat() + 'Z'
    }
    state['sources'].append(entry)
    _save_data_sources_state(state)
    if ok and skipped:
        flash(f'CSV imported with {len(skipped)} invalid row(s) skipped.')
    else:
        flash('CSV imported.')
    return redirect(url_for('data_sources_page'))

@app.route('/data_sources/toggle/<sid>', methods=['POST'])
def data_sources_toggle(sid):
    state = _load_data_sources_state()
    for s in state.get('sources', []):
        if s.get('id') == sid:
            s['enabled'] = not s.get('enabled', False)
            break
    _save_data_sources_state(state)
    return redirect(url_for('data_sources_page'))

@app.route('/data_sources/delete/<sid>', methods=['POST'])
def data_sources_delete(sid):
    state = _load_data_sources_state()
    new_sources = []
    for s in state.get('sources', []):
        if s.get('id') == sid:
            try:
                if os.path.exists(s.get('path','')):
                    os.remove(s['path'])
            except Exception:
                pass
            continue
        new_sources.append(s)
    state['sources'] = new_sources
    _save_data_sources_state(state)
    flash('Deleted.')
    return redirect(url_for('data_sources_page'))

@app.route('/data_sources/refresh/<sid>', methods=['POST'])
def data_sources_refresh(sid):
    state = _load_data_sources_state()
    for s in state.get('sources', []):
        if s.get('id') == sid:
            ok, note, norm_rows, skipped = _validate_and_normalize_data_source_csv(s.get('path',''), skip_invalid=True)
            if ok and norm_rows:
                # Write back normalized CSV
                try:
                    p = s.get('path','')
                    tmp = p + '.tmp'
                    with open(tmp, 'w', encoding='utf-8', newline='') as f:
                        w = csv.writer(f)
                        for r in norm_rows:
                            w.writerow(r)
                    os.replace(tmp, p)
                except Exception:
                    pass
            if ok and skipped:
                note = note + f" (skipped {len(skipped)} invalid)"
            s['rows'] = note if ok else f"ERR: {note}"
            break
    _save_data_sources_state(state)
    return redirect(url_for('data_sources_page'))

@app.route('/data_sources/download/<sid>')
def data_sources_download(sid):
    state = _load_data_sources_state()
    for s in state.get('sources', []):
        if s.get('id') == sid and os.path.exists(s.get('path','')):
            return send_file(s['path'], as_attachment=True, download_name=os.path.basename(s['name']))
    flash('Not found')
    return redirect(url_for('data_sources_page'))

@app.route('/data_sources/export_all')
def data_sources_export_all():
    import io, zipfile
    state = _load_data_sources_state()
    mem = io.BytesIO()
    with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
        for s in state.get('sources', []):
            p = s.get('path')
            if p and os.path.exists(p):
                z.write(p, arcname=os.path.basename(p))
    mem.seek(0)
    return send_file(mem, as_attachment=True, download_name='data_sources.zip')


# ---------------- Flag Catalog (mirror of Vulnerability Catalog) -----------------


@app.route('/flag_catalog')
def flag_catalog_page():
    state = _load_flag_generator_sources_state()
    return render_template('flag_catalog.html', sources=state.get('sources', []), active_page='flag_catalog')


@app.route('/flag_catalog_data')
def flag_catalog_data():
    """Return aggregated flag catalog from enabled flag sources."""
    try:
        items, meta = _flag_catalog_items_from_enabled_sources()
        return jsonify({
            'types': meta.get('types', []),
            'security_profiles': meta.get('security_profiles', []),
            'items': items,
        })
    except Exception as e:
        return jsonify({'error': str(e), 'types': [], 'security_profiles': [], 'items': []}), 500


@app.route('/flag_sources/upload', methods=['POST'])
def flag_sources_upload():
    f = request.files.get('json_file')
    if not f or f.filename == '':
        flash('No file selected.')
        return redirect(url_for('flag_catalog_page'))
    filename = secure_filename(f.filename)
    if not filename.lower().endswith('.json'):
        flash('Only .json allowed.')
        return redirect(url_for('flag_catalog_page'))
    unique = datetime.datetime.now().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:6]
    os.makedirs(FLAG_SOURCES_DIR, exist_ok=True)
    path = os.path.join(FLAG_SOURCES_DIR, f"{unique}-{filename}")
    f.save(path)
    ok, note, items, skipped = _validate_and_normalize_flag_source_json(path)
    if not ok:
        try:
            os.remove(path)
        except Exception:
            pass
        flash(f'Invalid JSON: {note}')
        return redirect(url_for('flag_catalog_page'))
    # Write back normalized JSON
    try:
        tmp = path + '.tmp'
        with open(tmp, 'w', encoding='utf-8') as fh:
            json.dump(items, fh, indent=2)
        os.replace(tmp, path)
    except Exception:
        pass
    state = _load_flag_sources_state()
    entry = {
        'id': uuid.uuid4().hex[:12],
        'name': filename,
        'path': path,
        'enabled': True,
        'rows': note,
        'uploaded': datetime.datetime.utcnow().isoformat() + 'Z',
    }
    state['sources'].append(entry)
    _save_flag_sources_state(state)
    if ok and skipped:
        flash(f'JSON imported with {len(skipped)} invalid row(s) skipped.')
    else:
        flash('JSON imported.')
    return redirect(url_for('flag_catalog_page'))


@app.route('/flag_sources/toggle/<sid>', methods=['POST'])
def flag_sources_toggle(sid):
    state = _load_flag_sources_state()
    for s in state.get('sources', []):
        if s.get('id') == sid:
            s['enabled'] = not s.get('enabled', False)
            break
    _save_flag_sources_state(state)
    return redirect(url_for('flag_catalog_page'))


@app.route('/flag_sources/delete/<sid>', methods=['POST'])
def flag_sources_delete(sid):
    state = _load_flag_sources_state()
    kept = []
    for s in state.get('sources', []):
        if s.get('id') == sid:
            try:
                if os.path.exists(s.get('path', '')):
                    os.remove(s['path'])
            except Exception:
                pass
            continue
        kept.append(s)
    state['sources'] = kept
    _save_flag_sources_state(state)
    flash('Deleted.')
    return redirect(url_for('flag_catalog_page'))


@app.route('/flag_sources/refresh/<sid>', methods=['POST'])
def flag_sources_refresh(sid):
    state = _load_flag_sources_state()
    for s in state.get('sources', []):
        if s.get('id') == sid:
            ok, note, items, skipped = _validate_and_normalize_flag_source_json(s.get('path', ''))
            if ok and items is not None:
                try:
                    p = s.get('path', '')
                    tmp = p + '.tmp'
                    with open(tmp, 'w', encoding='utf-8') as fh:
                        json.dump(items, fh, indent=2)
                    os.replace(tmp, p)
                except Exception:
                    pass
            if ok and skipped:
                note = note + f" (skipped {len(skipped)} invalid)"
            s['rows'] = note if ok else f"ERR: {note}"
            break
    _save_flag_sources_state(state)
    return redirect(url_for('flag_catalog_page'))


@app.route('/flag_sources/download/<sid>')
def flag_sources_download(sid):
    state = _load_flag_sources_state()
    for s in state.get('sources', []):
        if s.get('id') == sid and os.path.exists(s.get('path', '')):
            return send_file(s['path'], as_attachment=True, download_name=os.path.basename(s.get('name') or 'flag_catalog.json'))
    flash('Not found')
    return redirect(url_for('flag_catalog_page'))


@app.route('/flag_sources/export_all')
def flag_sources_export_all():
    import io, zipfile
    state = _load_flag_sources_state()
    mem = io.BytesIO()
    with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
        for s in state.get('sources', []):
            p = s.get('path')
            if p and os.path.exists(p):
                z.write(p, arcname=os.path.basename(p))
    mem.seek(0)
    return send_file(mem, as_attachment=True, download_name='flag_sources.zip')


@app.route('/flag_sources/edit/<sid>')
def flag_sources_edit(sid):
    state = _load_flag_sources_state()
    target = None
    for s in state.get('sources', []):
        if s.get('id') == sid:
            target = s
            break
    if not target:
        flash('Source not found')
        return redirect(url_for('flag_catalog_page'))
    path = target.get('path')
    if not path or not os.path.exists(path):
        flash('File missing')
        return redirect(url_for('flag_catalog_page'))
    ok, note, items, _skipped = _validate_and_normalize_flag_source_json(path)
    if not ok:
        flash(f'Invalid source JSON: {note}')
        items = []
    name = target.get('name') or os.path.basename(path)
    return render_template('flag_source_edit.html', sid=sid, name=name, path=path, items=items, columns=FLAG_ALL_FIELDS, active_page='flag_catalog')


@app.route('/flag_sources/save/<sid>', methods=['POST'])
def flag_sources_save(sid):
    """Save edited flag source JSON content.

    Expects JSON payload: { rows: string[][] }
    First row is header; remaining rows are values.
    """
    try:
        data = request.get_json(silent=True)
        if not isinstance(data, dict) or 'rows' not in data:
            return jsonify({'ok': False, 'error': 'Invalid payload'}), 400
        rows = data.get('rows')
        if not isinstance(rows, list) or any(not isinstance(r, list) for r in rows):
            return jsonify({'ok': False, 'error': 'Rows must be a list of lists'}), 400
        if not rows:
            return jsonify({'ok': False, 'error': 'No rows provided'}), 400
        header = [str(c).strip() for c in (rows[0] or [])]
        if not header:
            return jsonify({'ok': False, 'error': 'Missing header'}), 400
        # Build list of objects
        items: list[dict] = []
        for r in rows[1:]:
            rec = {}
            for i, h in enumerate(header):
                if not h:
                    continue
                rec[h] = r[i] if i < len(r) else ''
            # Skip completely empty rows
            if not any(str(v).strip() for v in rec.values()):
                continue
            items.append(rec)

        state = _load_flag_sources_state()
        target = None
        for s in state.get('sources', []):
            if s.get('id') == sid:
                target = s
                break
        if not target:
            return jsonify({'ok': False, 'error': 'Source not found'}), 404
        path = target.get('path')
        if not path:
            return jsonify({'ok': False, 'error': 'Missing file path'}), 400

        # Validate by writing a preview file and reusing canonical validator
        tmp_preview = path + '.editpreview'
        try:
            with open(tmp_preview, 'w', encoding='utf-8') as fh:
                json.dump(items, fh, indent=2)
            ok2, note2, norm_items, skipped2 = _validate_and_normalize_flag_source_json(tmp_preview)
        finally:
            try:
                os.remove(tmp_preview)
            except Exception:
                pass
        if not ok2:
            return jsonify({'ok': False, 'error': note2}), 200

        tmp = path + '.tmp'
        with open(tmp, 'w', encoding='utf-8') as fh:
            json.dump(norm_items, fh, indent=2)
        os.replace(tmp, path)

        # Update state row count
        note = note2
        if ok2 and skipped2:
            note = note + f" (skipped {len(skipped2)} invalid)"
        target['rows'] = note
        _save_flag_sources_state(state)
        return jsonify({'ok': True, 'skipped': len(skipped2) if ok2 else 0})
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)}), 500


# ---------------- Flag Generators (v3 plugin catalog) -----------------


def _flag_generators_from_enabled_sources() -> tuple[list[dict], list[dict]]:
    """Aggregate generators from enabled generator sources.

    Returns: (generators, errors)
    """
    state = _load_flag_generator_sources_state()
    sources = state.get('sources') or []
    all_gens: list[dict] = []
    errors: list[dict] = []
    seen_ids: set[str] = set()
    for s in sources:
        if not isinstance(s, dict):
            continue
        if not s.get('enabled', False):
            continue
        path = s.get('path')
        ok, note, doc, skipped = _validate_and_normalize_flag_generator_source_json(path)
        if not ok or not doc:
            errors.append({'source': s.get('name'), 'path': path, 'error': note})
            continue
        gens, gerrs = _v3_catalog_to_generator_views(doc)
        for e in gerrs:
            try:
                errors.append({'source': s.get('name'), 'path': path, **(e if isinstance(e, dict) else {'warning': str(e)})})
            except Exception:
                continue
        for gen in gens:
            if not isinstance(gen, dict):
                continue
            gid = str(gen.get('id') or '')
            if not gid or gid in seen_ids:
                continue
            seen_ids.add(gid)
            gen2 = dict(gen)
            gen2['_source_name'] = s.get('name')
            gen2['_source_path'] = path
            all_gens.append(gen2)
        if skipped:
            errors.append({'source': s.get('name'), 'path': path, 'warning': f"skipped {len(skipped)} invalid generator(s)"})
    all_gens.sort(key=lambda g: (str(g.get('name') or '').lower(), str(g.get('id') or '')))
    return all_gens, errors


def _load_flag_node_generator_sources_state() -> dict:
    """Load flag node-generator sources state.

    This mirrors the flag-generator catalog schema, but is stored separately under
    data_sources/flag_node_generators/_state.json.
    """
    try:
        if not os.path.exists(FLAG_NODE_GENERATORS_STATE_PATH):
            state = {'sources': []}
            try:
                tmp = FLAG_NODE_GENERATORS_STATE_PATH + '.tmp'
                with open(tmp, 'w', encoding='utf-8') as fh:
                    json.dump(state, fh, indent=2)
                os.replace(tmp, FLAG_NODE_GENERATORS_STATE_PATH)
            except Exception:
                pass
            return state
        with open(FLAG_NODE_GENERATORS_STATE_PATH, 'r', encoding='utf-8') as fh:
            state = json.load(fh)
        if not isinstance(state, dict):
            return {'sources': []}
        if not isinstance(state.get('sources'), list):
            state['sources'] = []
        return state
    except Exception:
        return {'sources': []}


def _flag_node_generators_from_enabled_sources() -> tuple[list[dict], list[dict]]:
    """Aggregate node-generators from enabled node-generator sources."""
    state = _load_flag_node_generator_sources_state()
    sources = state.get('sources') or []
    all_gens: list[dict] = []
    errors: list[dict] = []
    seen_ids: set[str] = set()
    for s in sources:
        if not isinstance(s, dict):
            continue
        if not s.get('enabled', False):
            continue
        path = s.get('path')
        ok, note, doc, skipped = _validate_and_normalize_flag_generator_source_json(path)
        if not ok or not doc:
            errors.append({'source': s.get('name'), 'path': path, 'error': note})
            continue
        gens, gerrs = _v3_catalog_to_generator_views(doc)
        for e in gerrs:
            try:
                errors.append({'source': s.get('name'), 'path': path, **(e if isinstance(e, dict) else {'warning': str(e)})})
            except Exception:
                continue
        for gen in gens:
            if not isinstance(gen, dict):
                continue
            gid = str(gen.get('id') or '')
            if not gid or gid in seen_ids:
                continue
            seen_ids.add(gid)
            gen2 = dict(gen)
            gen2['_source_name'] = s.get('name')
            gen2['_source_path'] = path
            all_gens.append(gen2)
        if skipped:
            errors.append({'source': s.get('name'), 'path': path, 'warning': f"skipped {len(skipped)} invalid generator(s)"})
    all_gens.sort(key=lambda g: (str(g.get('name') or '').lower(), str(g.get('id') or '')))
    return all_gens, errors


@app.route('/flag_generators_data')
def flag_generators_data():
    try:
        generators, errors = _flag_generators_from_enabled_sources()
        return jsonify({'generators': generators, 'errors': errors})
    except Exception as e:
        return jsonify({'generators': [], 'errors': [{'error': str(e)}]}), 500


@app.route('/flag_node_generators_data')
def flag_node_generators_data():
    try:
        generators, errors = _flag_node_generators_from_enabled_sources()
        return jsonify({'generators': generators, 'errors': errors})
    except Exception as e:
        return jsonify({'generators': [], 'errors': [{'error': str(e)}]}), 500


@app.route('/flag_generators_sources/upload', methods=['POST'])
def flag_generators_sources_upload():
    f = request.files.get('json_file')
    if not f or f.filename == '':
        flash('No file selected.')
        return redirect(url_for('flag_catalog_page'))
    filename = secure_filename(f.filename)
    if not filename.lower().endswith('.json'):
        flash('Only .json allowed.')
        return redirect(url_for('flag_catalog_page'))
    unique = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:6]
    os.makedirs(FLAG_GENERATORS_SOURCES_DIR, exist_ok=True)
    path = os.path.join(FLAG_GENERATORS_SOURCES_DIR, f"{unique}-{filename}")
    f.save(path)
    ok, note, normalized_doc, _skipped = _validate_and_normalize_flag_generator_source_json(path)
    if not ok or not normalized_doc:
        try:
            os.remove(path)
        except Exception:
            pass
        flash(f'Invalid generator JSON: {note}')
        return redirect(url_for('flag_catalog_page'))
    try:
        tmp = path + '.tmp'
        with open(tmp, 'w', encoding='utf-8') as fh:
            json.dump(normalized_doc, fh, indent=2)
        os.replace(tmp, path)
    except Exception:
        pass
    state = _load_flag_generator_sources_state()
    state.setdefault('sources', [])
    state['sources'].append({
        'id': uuid.uuid4().hex[:12],
        'name': filename,
        'path': path,
        'enabled': True,
        'rows': note,
        'uploaded': datetime.datetime.utcnow().isoformat() + 'Z',
    })
    _save_flag_generator_sources_state(state)
    flash('Generator JSON imported.')
    return redirect(url_for('flag_catalog_page'))


@app.route('/flag_generators_sources/toggle/<sid>', methods=['POST'])
def flag_generators_sources_toggle(sid):
    state = _load_flag_generator_sources_state()
    for s in state.get('sources', []):
        if isinstance(s, dict) and s.get('id') == sid:
            s['enabled'] = not s.get('enabled', False)
            break
    _save_flag_generator_sources_state(state)
    return redirect(url_for('flag_catalog_page'))


@app.route('/flag_generators_sources/delete/<sid>', methods=['POST'])
def flag_generators_sources_delete(sid):
    state = _load_flag_generator_sources_state()
    kept = []
    for s in state.get('sources', []):
        if isinstance(s, dict) and s.get('id') == sid:
            try:
                p = s.get('path')
                if p and os.path.exists(p):
                    os.remove(p)
            except Exception:
                pass
            continue
        kept.append(s)
    state['sources'] = kept
    _save_flag_generator_sources_state(state)
    flash('Deleted.')
    return redirect(url_for('flag_catalog_page'))


@app.route('/flag_generators_sources/refresh/<sid>', methods=['POST'])
def flag_generators_sources_refresh(sid):
    state = _load_flag_generator_sources_state()
    for s in state.get('sources', []):
        if isinstance(s, dict) and s.get('id') == sid:
            path = s.get('path')
            ok, note, normalized_doc, _skipped = _validate_and_normalize_flag_generator_source_json(path)
            if ok and normalized_doc:
                try:
                    tmp = path + '.tmp'
                    with open(tmp, 'w', encoding='utf-8') as fh:
                        json.dump(normalized_doc, fh, indent=2)
                    os.replace(tmp, path)
                except Exception:
                    pass
            s['rows'] = note if ok else f"ERR: {note}"
            break
    _save_flag_generator_sources_state(state)
    return redirect(url_for('flag_catalog_page'))


@app.route('/flag_generators_sources/download/<sid>')
def flag_generators_sources_download(sid):
    state = _load_flag_generator_sources_state()
    for s in state.get('sources', []):
        if isinstance(s, dict) and s.get('id') == sid:
            p = s.get('path')
            if p and os.path.exists(p):
                download_name = s.get('name') or os.path.basename(p)
                return send_file(p, as_attachment=True, download_name=download_name)
    flash('Not found')
    return redirect(url_for('flag_catalog_page'))


@app.route('/flag_generators_sources/export_all')
def flag_generators_sources_export_all():
    import io, zipfile
    state = _load_flag_generator_sources_state()
    mem = io.BytesIO()
    with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
        for s in state.get('sources', []):
            if not isinstance(s, dict):
                continue
            p = s.get('path')
            if p and os.path.exists(p):
                z.write(p, arcname=os.path.basename(p))
    mem.seek(0)
    return send_file(mem, as_attachment=True, download_name='flag_generators_sources.zip')


@app.route('/flag_generators_sources/edit/<sid>')
def flag_generators_sources_edit(sid):
    state = _load_flag_generator_sources_state()
    target = None
    for s in state.get('sources', []):
        if isinstance(s, dict) and s.get('id') == sid:
            target = s
            break
    if not target:
        flash('Source not found')
        return redirect(url_for('flag_catalog_page'))
    path = target.get('path')
    if not path or not os.path.exists(path):
        flash('File missing')
        return redirect(url_for('flag_catalog_page'))
    try:
        raw_text = open(path, 'r', encoding='utf-8').read()
    except Exception:
        raw_text = ''
    return render_template(
        'flag_generator_source_edit.html',
        sid=sid,
        name=target.get('name') or os.path.basename(path),
        path=path,
        raw_text=raw_text,
        active_page='flag_catalog',
    )


@app.route('/flag_generators_sources/save/<sid>', methods=['POST'])
def flag_generators_sources_save(sid):
    """Save raw generator source JSON.

    Payload: {"raw": "{...}"}
    """
    try:
        data = request.get_json(silent=True) or {}
        raw = data.get('raw')
        if not isinstance(raw, str):
            return jsonify({'ok': False, 'error': 'Missing raw JSON'}), 400
        state = _load_flag_generator_sources_state()
        target = None
        for s in state.get('sources', []):
            if isinstance(s, dict) and s.get('id') == sid:
                target = s
                break
        if not target:
            return jsonify({'ok': False, 'error': 'Source not found'}), 404
        path = target.get('path')
        if not path:
            return jsonify({'ok': False, 'error': 'Missing file path'}), 400

        # Validate using a temp file
        tmp_preview = path + '.editpreview'
        try:
            with open(tmp_preview, 'w', encoding='utf-8') as fh:
                fh.write(raw)
            ok, note, normalized_doc, _skipped = _validate_and_normalize_flag_generator_source_json(tmp_preview)
        finally:
            try:
                os.remove(tmp_preview)
            except Exception:
                pass
        if not ok or not normalized_doc:
            return jsonify({'ok': False, 'error': note}), 200

        tmp = path + '.tmp'
        with open(tmp, 'w', encoding='utf-8') as fh:
            json.dump(normalized_doc, fh, indent=2)
        os.replace(tmp, path)
        target['rows'] = note
        _save_flag_generator_sources_state(state)
        return jsonify({'ok': True})
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)}), 500


def _flag_generators_runs_dir() -> str:
    d = os.path.join(_outputs_dir(), 'flag_generators_runs')
    os.makedirs(d, exist_ok=True)
    return d


def _flaggen_run_dir_for_id(run_id: str) -> str:
    """Resolve a local flag generator run directory for a run_id.

    This allows artifact browsing/downloading even if the webapp restarted and
    the in-memory RUNS registry was lost.
    """
    rid = (run_id or '').strip()
    # Basic sanitization: avoid path traversal.
    rid = rid.replace('..', '').replace('\\', '/').strip('/').strip()
    return os.path.join(_flag_generators_runs_dir(), rid)


def _flag_node_generators_runs_dir() -> str:
    d = os.path.join(_outputs_dir(), 'flag_node_generators_runs')
    os.makedirs(d, exist_ok=True)
    return d


def _flagnodegen_run_dir_for_id(run_id: str) -> str:
    rid = (run_id or '').strip()
    rid = rid.replace('..', '').replace('\\', '/').strip('/').strip()
    return os.path.join(_flag_node_generators_runs_dir(), rid)


def _find_enabled_node_generator_by_id(generator_id: str) -> Optional[dict]:
    gid = (generator_id or '').strip()
    if not gid:
        return None
    try:
        generators, _errors = _flag_node_generators_from_enabled_sources()
    except Exception:
        return None
    for g in generators:
        try:
            if str(g.get('id') or '').strip() == gid:
                return g
        except Exception:
            continue
    return None


def _find_enabled_generator_by_id(generator_id: str) -> Optional[dict]:
    gid = (generator_id or '').strip()
    if not gid:
        return None
    try:
        generators, _errors = _flag_generators_from_enabled_sources()
    except Exception:
        return None
    for g in generators:
        try:
            if str(g.get('id') or '').strip() == gid:
                return g
        except Exception:
            continue
    return None


@app.route('/flag_generators_test/run', methods=['POST'])
def flag_generators_test_run():
    """Start a generator test run.

    Accepts text inputs and file uploads based on the generator's declared inputs.
    Writes a run log and streams it via /stream/<run_id>.
    """
    t0 = time.time()
    generator_id = (request.form.get('generator_id') or '').strip()
    try:
        app.logger.info("[flaggen_test] POST /flag_generators_test/run generator_id=%s", generator_id)
    except Exception:
        pass
    gen = _find_enabled_generator_by_id(generator_id)
    if not gen:
        try:
            app.logger.warning("[flaggen_test] generator not found: %s", generator_id)
        except Exception:
            pass
        return jsonify({'ok': False, 'error': 'Generator not found (must be in an enabled source).'}), 404

    run_id = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:10]
    run_dir = os.path.join(_flag_generators_runs_dir(), run_id)
    inputs_dir = os.path.join(run_dir, 'inputs')
    os.makedirs(inputs_dir, exist_ok=True)
    log_path = os.path.join(run_dir, 'run.log')

    # Build config object from declared inputs.
    cfg: dict[str, Any] = {}
    inputs = gen.get('inputs') if isinstance(gen, dict) else None
    inputs_list = inputs if isinstance(inputs, list) else []
    for inp in inputs_list:
        if not isinstance(inp, dict):
            continue
        name = str(inp.get('name') or '').strip()
        if not name:
            continue
        f = request.files.get(name)
        if f and getattr(f, 'filename', ''):
            safe_name = secure_filename(f.filename) or 'upload'
            stored = f"{name}__{safe_name}"
            dest = os.path.join(inputs_dir, stored)
            try:
                f.save(dest)
                # Expose the in-container path.
                cfg[name] = f"/inputs/{stored}"
            except Exception:
                return jsonify({'ok': False, 'error': f"Failed saving file input: {name}"}), 400
            continue
        raw_val = request.form.get(name)
        if raw_val is not None:
            cfg[name] = raw_val

    try:
        app.logger.info("[flaggen_test] inputs keys=%s", sorted(list(cfg.keys())))
    except Exception:
        pass

    # Validate required inputs minimally (only presence).
    missing: list[str] = []
    for inp in inputs_list:
        if not isinstance(inp, dict):
            continue
        try:
            if not inp.get('required'):
                continue
            name = str(inp.get('name') or '').strip()
            if not name:
                continue
            val = cfg.get(name)
            if val is None or (isinstance(val, str) and not val.strip()):
                missing.append(name)
        except Exception:
            continue
    if missing:
        try:
            app.logger.warning("[flaggen_test] missing required inputs: %s", missing)
        except Exception:
            pass
        return jsonify({'ok': False, 'error': f"Missing required input(s): {', '.join(missing)}"}), 400

    # Launch runner.
    repo_root = _get_repo_root()
    runner_path = os.path.join(repo_root, 'scripts', 'run_flag_generator.py')
    cmd = [
        sys.executable or 'python',
        runner_path,
        '--generator-id',
        generator_id,
        '--out-dir',
        run_dir,
        '--config',
        json.dumps(cfg, ensure_ascii=False),
        '--repo-root',
        repo_root,
    ]

    try:
        with open(log_path, 'a', encoding='utf-8') as log_f:
            log_f.write(f"[flaggen] starting {generator_id}\n")
            _write_sse_marker(log_f, 'phase', {'phase': 'starting', 'generator_id': generator_id, 'run_id': run_id})
    except Exception:
        pass

    try:
        log_handle = open(log_path, 'a', encoding='utf-8')
        proc = subprocess.Popen(
            cmd,
            cwd=repo_root,
            stdout=log_handle,
            stderr=log_handle,
            text=True,
        )
    except Exception as exc:
        try:
            with open(log_path, 'a', encoding='utf-8') as log_f:
                log_f.write(f"[flaggen] failed to start: {exc}\n")
                _write_sse_marker(log_f, 'phase', {'phase': 'error', 'error': str(exc)})
        except Exception:
            pass
        return jsonify({'ok': False, 'error': f"Failed launching generator: {exc}"}), 500

    try:
        app.logger.info(
            "[flaggen_test] spawned pid=%s run_id=%s run_dir=%s elapsed_ms=%s",
            getattr(proc, 'pid', None),
            run_id,
            run_dir,
            int((time.time() - t0) * 1000),
        )
    except Exception:
        pass

    RUNS[run_id] = {
        'proc': proc,
        'log_path': log_path,
        'done': False,
        'returncode': None,
        'run_dir': run_dir,
        'kind': 'flag_generator_test',
        'generator_id': generator_id,
    }

    def _finalize_flaggen(run_id_local: str, log_handle_local: Any):
        try:
            meta = RUNS.get(run_id_local)
            if not meta:
                return
            p = meta.get('proc')
            if not p:
                return
            rc = p.wait()
            meta['done'] = True
            meta['returncode'] = rc
            try:
                with open(meta.get('log_path'), 'a', encoding='utf-8') as log_f:
                    _write_sse_marker(log_f, 'phase', {'phase': 'done', 'returncode': rc})
            except Exception:
                pass
        finally:
            try:
                log_handle_local.close()
            except Exception:
                pass

    threading.Thread(
        target=_finalize_flaggen,
        args=(run_id, log_handle),
        name=f'flaggen-{run_id[:8]}',
        daemon=True,
    ).start()

    try:
        app.logger.info(
            "[flaggen_test] responding ok run_id=%s elapsed_ms=%s",
            run_id,
            int((time.time() - t0) * 1000),
        )
    except Exception:
        pass
    return jsonify({'ok': True, 'run_id': run_id})


@app.route('/flag_node_generators_test/run', methods=['POST'])
def flag_node_generators_test_run():
    """Start a node-generator test run."""
    t0 = time.time()
    generator_id = (request.form.get('generator_id') or '').strip()
    try:
        app.logger.info("[flagnodegen_test] POST /flag_node_generators_test/run generator_id=%s", generator_id)
    except Exception:
        pass

    gen = _find_enabled_node_generator_by_id(generator_id)
    if not gen:
        return jsonify({'ok': False, 'error': 'Generator not found (must be in an enabled source).'}), 404

    run_id = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:10]
    run_dir = os.path.join(_flag_node_generators_runs_dir(), run_id)
    inputs_dir = os.path.join(run_dir, 'inputs')
    os.makedirs(inputs_dir, exist_ok=True)
    log_path = os.path.join(run_dir, 'run.log')

    cfg: dict[str, Any] = {}
    inputs = gen.get('inputs') if isinstance(gen, dict) else None
    inputs_list = inputs if isinstance(inputs, list) else []
    for inp in inputs_list:
        if not isinstance(inp, dict):
            continue
        name = str(inp.get('name') or '').strip()
        if not name:
            continue
        f = request.files.get(name)
        if f and getattr(f, 'filename', ''):
            safe_name = secure_filename(f.filename) or 'upload'
            stored = f"{name}__{safe_name}"
            dest = os.path.join(inputs_dir, stored)
            try:
                f.save(dest)
                cfg[name] = f"/inputs/{stored}"
            except Exception:
                return jsonify({'ok': False, 'error': f"Failed saving file input: {name}"}), 400
            continue
        raw_val = request.form.get(name)
        if raw_val is not None:
            cfg[name] = raw_val

    missing: list[str] = []
    for inp in inputs_list:
        if not isinstance(inp, dict):
            continue
        try:
            if not inp.get('required'):
                continue
            name = str(inp.get('name') or '').strip()
            if not name:
                continue
            val = cfg.get(name)
            if val is None or (isinstance(val, str) and not val.strip()):
                missing.append(name)
        except Exception:
            continue
    if missing:
        return jsonify({'ok': False, 'error': f"Missing required input(s): {', '.join(missing)}"}), 400

    repo_root = _get_repo_root()
    runner_path = os.path.join(repo_root, 'scripts', 'run_flag_generator.py')
    cmd = [
        sys.executable or 'python',
        runner_path,
        '--catalog',
        'flag_node_generators',
        '--generator-id',
        generator_id,
        '--out-dir',
        run_dir,
        '--config',
        json.dumps(cfg, ensure_ascii=False),
        '--repo-root',
        repo_root,
    ]

    try:
        with open(log_path, 'a', encoding='utf-8') as log_f:
            log_f.write(f"[flagnodegen] starting {generator_id}\n")
            _write_sse_marker(log_f, 'phase', {'phase': 'starting', 'generator_id': generator_id, 'run_id': run_id})
    except Exception:
        pass

    try:
        log_handle = open(log_path, 'a', encoding='utf-8')
        proc = subprocess.Popen(
            cmd,
            cwd=repo_root,
            stdout=log_handle,
            stderr=log_handle,
            text=True,
        )
    except Exception as exc:
        try:
            with open(log_path, 'a', encoding='utf-8') as log_f:
                log_f.write(f"[flagnodegen] failed to start: {exc}\n")
                _write_sse_marker(log_f, 'phase', {'phase': 'error', 'error': str(exc)})
        except Exception:
            pass
        return jsonify({'ok': False, 'error': f"Failed launching generator: {exc}"}), 500

    RUNS[run_id] = {
        'proc': proc,
        'log_path': log_path,
        'done': False,
        'returncode': None,
        'run_dir': run_dir,
        'kind': 'flag_node_generator_test',
        'generator_id': generator_id,
    }

    def _finalize(run_id_local: str, log_handle_local: Any):
        try:
            meta = RUNS.get(run_id_local)
            if not meta:
                return
            p = meta.get('proc')
            if not p:
                return
            rc = p.wait()
            meta['done'] = True
            meta['returncode'] = rc
            try:
                with open(meta.get('log_path'), 'a', encoding='utf-8') as log_f:
                    _write_sse_marker(log_f, 'phase', {'phase': 'done', 'returncode': rc})
            except Exception:
                pass
        finally:
            try:
                log_handle_local.close()
            except Exception:
                pass

    threading.Thread(
        target=_finalize,
        args=(run_id, log_handle),
        name=f'flagnodegen-{run_id[:8]}',
        daemon=True,
    ).start()

    try:
        app.logger.info(
            "[flagnodegen_test] spawned pid=%s run_id=%s run_dir=%s elapsed_ms=%s",
            getattr(proc, 'pid', None),
            run_id,
            run_dir,
            int((time.time() - t0) * 1000),
        )
    except Exception:
        pass
    return jsonify({'ok': True, 'run_id': run_id})


@app.route('/flag_node_generators_test/outputs/<run_id>')
def flag_node_generators_test_outputs(run_id: str):
    meta = RUNS.get(run_id)
    if meta and meta.get('kind') != 'flag_node_generator_test':
        return jsonify({'ok': False, 'error': 'not found'}), 404
    run_dir = meta.get('run_dir') if isinstance(meta, dict) else None
    if not isinstance(run_dir, str) or not run_dir:
        run_dir = _flagnodegen_run_dir_for_id(run_id)
    if not isinstance(run_dir, str) or not run_dir:
        return jsonify({'ok': False, 'error': 'missing run dir'}), 500
    abs_run_dir = os.path.abspath(run_dir)
    outputs_root = os.path.abspath(_outputs_dir())
    if not (abs_run_dir == outputs_root or abs_run_dir.startswith(outputs_root + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400
    if not os.path.isdir(abs_run_dir):
        done = bool(meta.get('done')) if isinstance(meta, dict) else False
        returncode = meta.get('returncode') if isinstance(meta, dict) else None
        return jsonify({'ok': True, 'files': [], 'done': done, 'returncode': returncode}), 200

    input_files: list[dict[str, Any]] = []
    output_files: list[dict[str, Any]] = []
    misc_files: list[dict[str, Any]] = []
    for root, _dirs, filenames in os.walk(abs_run_dir):
        rel_root = os.path.relpath(root, abs_run_dir).replace('\\', '/')
        for fn in filenames:
            abs_path = os.path.join(root, fn)
            try:
                st = os.stat(abs_path)
                rel = os.path.relpath(abs_path, abs_run_dir).replace('\\', '/')
                entry = {'path': rel, 'name': fn, 'size': st.st_size}
            except Exception:
                continue
            if rel_root == 'inputs' or rel_root.startswith('inputs/'):
                input_files.append(entry)
            elif rel == 'run.log':
                misc_files.append(entry)
            else:
                output_files.append(entry)

    input_files.sort(key=lambda x: x.get('path') or '')
    output_files.sort(key=lambda x: x.get('path') or '')
    misc_files.sort(key=lambda x: x.get('path') or '')
    done = bool(meta.get('done')) if isinstance(meta, dict) else True
    returncode = meta.get('returncode') if isinstance(meta, dict) else None
    return jsonify({'ok': True, 'inputs': input_files, 'outputs': output_files, 'misc': misc_files, 'done': done, 'returncode': returncode}), 200


@app.route('/flag_node_generators_test/download/<run_id>')
def flag_node_generators_test_download(run_id: str):
    meta = RUNS.get(run_id)
    if meta and meta.get('kind') != 'flag_node_generator_test':
        return jsonify({'ok': False, 'error': 'not found'}), 404
    run_dir = meta.get('run_dir') if isinstance(meta, dict) else None
    if not isinstance(run_dir, str) or not run_dir:
        run_dir = _flagnodegen_run_dir_for_id(run_id)
    if not isinstance(run_dir, str) or not run_dir:
        return jsonify({'ok': False, 'error': 'missing run dir'}), 500
    rel = (request.args.get('p') or '').strip().lstrip('/').replace('\\', '/')
    if not rel:
        return jsonify({'ok': False, 'error': 'invalid path'}), 400
    abs_run_dir = os.path.abspath(run_dir)
    outputs_root = os.path.abspath(_outputs_dir())
    if not (abs_run_dir == outputs_root or abs_run_dir.startswith(outputs_root + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400
    abs_path = os.path.abspath(os.path.join(abs_run_dir, rel))
    if not (abs_path == abs_run_dir or abs_path.startswith(abs_run_dir + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400
    if not os.path.exists(abs_path) or not os.path.isfile(abs_path):
        return jsonify({'ok': False, 'error': 'missing file'}), 404
    return send_file(abs_path, as_attachment=True, download_name=os.path.basename(abs_path))


@app.route('/flag_node_generators_test/cleanup/<run_id>', methods=['POST'])
def flag_node_generators_test_cleanup(run_id: str):
    """Delete all artifacts for a flag-node-generator test run (scoped to outputs/)."""
    meta = RUNS.get(run_id)
    if meta and meta.get('kind') != 'flag_node_generator_test':
        return jsonify({'ok': False, 'error': 'not found'}), 404
    run_dir = meta.get('run_dir') if isinstance(meta, dict) else None
    if not isinstance(run_dir, str) or not run_dir:
        run_dir = _flagnodegen_run_dir_for_id(run_id)
    if not isinstance(run_dir, str) or not run_dir:
        return jsonify({'ok': False, 'error': 'missing run dir'}), 500
    abs_run_dir = os.path.abspath(run_dir)
    outputs_root = os.path.abspath(_outputs_dir())
    if not (abs_run_dir == outputs_root or abs_run_dir.startswith(outputs_root + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400

    try:
        proc = meta.get('proc') if isinstance(meta, dict) else None
        if proc and hasattr(proc, 'poll') and proc.poll() is None:
            try:
                proc.terminate()
                proc.wait(timeout=5)
            except Exception:
                try:
                    proc.kill()
                except Exception:
                    pass
    except Exception:
        pass

    removed = False
    try:
        if os.path.isdir(abs_run_dir):
            shutil.rmtree(abs_run_dir, ignore_errors=True)
        removed = True
    except Exception:
        removed = False

    try:
        RUNS.pop(run_id, None)
    except Exception:
        pass
    return jsonify({'ok': True, 'removed': removed}), 200


@app.route('/flag_generators_test/outputs/<run_id>')
def flag_generators_test_outputs(run_id: str):
    meta = RUNS.get(run_id)
    if meta and meta.get('kind') != 'flag_generator_test':
        return jsonify({'ok': False, 'error': 'not found'}), 404

    run_dir = meta.get('run_dir') if isinstance(meta, dict) else None
    if not isinstance(run_dir, str) or not run_dir:
        run_dir = _flaggen_run_dir_for_id(run_id)
    if not isinstance(run_dir, str) or not run_dir:
        return jsonify({'ok': False, 'error': 'missing run dir'}), 500
    abs_run_dir = os.path.abspath(run_dir)
    outputs_root = os.path.abspath(_outputs_dir())
    if not (abs_run_dir == outputs_root or abs_run_dir.startswith(outputs_root + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400
    if not os.path.isdir(abs_run_dir):
        done = bool(meta.get('done')) if isinstance(meta, dict) else False
        returncode = meta.get('returncode') if isinstance(meta, dict) else None
        return jsonify({'ok': True, 'files': [], 'done': done, 'returncode': returncode}), 200

    input_files: list[dict[str, Any]] = []
    output_files: list[dict[str, Any]] = []
    misc_files: list[dict[str, Any]] = []

    for root, _dirs, filenames in os.walk(abs_run_dir):
        rel_root = os.path.relpath(root, abs_run_dir).replace('\\', '/')
        for fn in filenames:
            abs_path = os.path.join(root, fn)
            try:
                st = os.stat(abs_path)
                rel = os.path.relpath(abs_path, abs_run_dir).replace('\\', '/')
                entry = {'path': rel, 'name': fn, 'size': st.st_size}
            except Exception:
                continue

            if rel_root == 'inputs' or rel_root.startswith('inputs/'):
                input_files.append(entry)
            elif rel == 'run.log':
                misc_files.append(entry)
            else:
                output_files.append(entry)

    input_files.sort(key=lambda x: x.get('path') or '')
    output_files.sort(key=lambda x: x.get('path') or '')
    misc_files.sort(key=lambda x: x.get('path') or '')
    done = bool(meta.get('done')) if isinstance(meta, dict) else True
    returncode = meta.get('returncode') if isinstance(meta, dict) else None
    return jsonify({
        'ok': True,
        'inputs': input_files,
        'outputs': output_files,
        'misc': misc_files,
        'done': done,
        'returncode': returncode,
    }), 200


@app.route('/flag_generators_test/download/<run_id>')
def flag_generators_test_download(run_id: str):
    meta = RUNS.get(run_id)
    if meta and meta.get('kind') != 'flag_generator_test':
        return jsonify({'ok': False, 'error': 'not found'}), 404
    run_dir = meta.get('run_dir') if isinstance(meta, dict) else None
    if not isinstance(run_dir, str) or not run_dir:
        run_dir = _flaggen_run_dir_for_id(run_id)
    if not isinstance(run_dir, str) or not run_dir:
        return jsonify({'ok': False, 'error': 'missing run dir'}), 500
    rel = (request.args.get('p') or '').strip().lstrip('/').replace('\\', '/')
    if not rel:
        return jsonify({'ok': False, 'error': 'invalid path'}), 400
    abs_run_dir = os.path.abspath(run_dir)
    outputs_root = os.path.abspath(_outputs_dir())
    if not (abs_run_dir == outputs_root or abs_run_dir.startswith(outputs_root + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400
    abs_path = os.path.abspath(os.path.join(abs_run_dir, rel))
    if not (abs_path == abs_run_dir or abs_path.startswith(abs_run_dir + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400
    if not os.path.exists(abs_path) or not os.path.isfile(abs_path):
        return jsonify({'ok': False, 'error': 'missing file'}), 404
    return send_file(abs_path, as_attachment=True, download_name=os.path.basename(abs_path))


@app.route('/flag_generators_test/bundle/<run_id>')
def flag_generators_test_bundle(run_id: str):
    return jsonify({'ok': False, 'error': 'bundle.zip output disabled'}), 404


def _flag_resolve_path(raw_path: str) -> str:
    """Resolve a flag path that may be repo-relative."""
    p = (raw_path or '').strip()
    if not p:
        return ''
    try:
        if p.startswith('file://'):
            p = p[len('file://'):]
    except Exception:
        pass
    if os.path.isabs(p):
        return p
    try:
        return os.path.abspath(os.path.join(_get_repo_root(), p))
    except Exception:
        return os.path.abspath(p)


@app.route('/flag_compose/status', methods=['POST'])
def flag_compose_status():
    """Return status for a list of flag catalog items."""
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_flag_base_dir())
        os.makedirs(base_out, exist_ok=True)
        for it in items:
            name = (it.get('name') or it.get('Name') or '').strip()
            path_raw = (it.get('path') or it.get('Path') or '').strip()
            compose_name = (it.get('compose_name') or it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'flag')
            fdir = os.path.join(base_out, safe)
            gh = _parse_github_url(path_raw)
            compose_file = None
            base_dir = fdir
            exists = False

            # Local file path support (repo-relative)
            local_path = _flag_resolve_path(path_raw)
            if local_path and os.path.exists(local_path) and local_path.lower().endswith(('.yml', '.yaml')):
                compose_file = local_path
                base_dir = os.path.dirname(local_path)
                exists = True
                try:
                    logs.append(f"[status] {name}: local compose={compose_file}")
                except Exception:
                    pass
            elif gh.get('is_github'):
                repo_dir = os.path.join(fdir, _vuln_repo_subdir())
                sub = gh.get('subpath') or ''
                is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                if is_file_sub:
                    compose_file = os.path.join(repo_dir, sub)
                    base_dir = os.path.dirname(compose_file)
                    exists = os.path.exists(compose_file)
                else:
                    base_dir = os.path.join(repo_dir, sub) if sub else repo_dir
                    exists = os.path.isdir(base_dir)
                if exists and compose_name and not compose_file:
                    p = os.path.join(base_dir, compose_name)
                    if os.path.exists(p):
                        compose_file = p
                if not compose_file:
                    cand = _compose_candidates(base_dir)
                    compose_file = cand[0] if cand else None
                try:
                    logs.append(f"[status] {name}: github base={base_dir} exists={exists} compose={compose_name}")
                except Exception:
                    pass
            else:
                # Legacy direct download into outputs/flags/<safe>/compose_name
                compose_file = os.path.join(fdir, compose_name)
                exists = os.path.exists(compose_file)
                base_dir = fdir

            pulled = False
            if exists and compose_file and shutil.which('docker'):
                try:
                    proc = subprocess.run(['docker', 'compose', '-f', compose_file, 'config', '--images'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=30)
                    if proc.returncode == 0:
                        images = [ln.strip() for ln in (proc.stdout or '').splitlines() if ln.strip()]
                        if images:
                            present = []
                            for img in images:
                                p2 = subprocess.run(['docker', 'image', 'inspect', img], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                                present.append(p2.returncode == 0)
                            pulled = all(present)
                except Exception:
                    pulled = False
            out.append({
                'name': name,
                'path': path_raw,
                'compose_name': compose_name,
                'compose_path': compose_file,
                'exists': bool(exists),
                'pulled': bool(pulled),
                'dir': base_dir,
            })
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/flag_compose/download', methods=['POST'])
def flag_compose_download():
    """Download/clone docker-compose assets for the given flag catalog items."""
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_flag_base_dir())
        os.makedirs(base_out, exist_ok=True)

        try:
            from core_topo_gen.utils.vuln_process import _github_tree_to_raw as _to_raw
        except Exception:
            def _to_raw(base_url: str, filename: str) -> str | None:
                try:
                    from urllib.parse import urlparse
                    u = urlparse(base_url)
                    if u.netloc.lower() != 'github.com':
                        return None
                    parts = [p for p in u.path.strip('/').split('/') if p]
                    if len(parts) < 4 or parts[2] != 'tree':
                        return None
                    owner, repo, _tree, branch = parts[:4]
                    rest = '/'.join(parts[4:])
                    return f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{rest}/{filename}"
                except Exception:
                    return None

        import urllib.request
        import shlex
        for it in items:
            name = (it.get('name') or it.get('Name') or '').strip()
            path_raw = (it.get('path') or it.get('Path') or '').strip()
            compose_name = (it.get('compose_name') or it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'flag')
            fdir = os.path.join(base_out, safe)
            os.makedirs(fdir, exist_ok=True)

            # Local compose path support: nothing to download.
            local_path = _flag_resolve_path(path_raw)
            if local_path and os.path.exists(local_path) and local_path.lower().endswith(('.yml', '.yaml')):
                out.append({'name': name, 'path': path_raw, 'ok': True, 'dir': os.path.dirname(local_path), 'message': 'local compose file'})
                continue

            gh = _parse_github_url(path_raw)
            if gh.get('is_github'):
                if not shutil.which('git'):
                    logs.append(f"[download] {name}: git not available in PATH")
                    out.append({'name': name, 'path': path_raw, 'ok': False, 'dir': fdir, 'message': 'git not available'})
                    continue
                repo_dir = os.path.join(fdir, _vuln_repo_subdir())
                if os.path.isdir(os.path.join(repo_dir, '.git')):
                    base_dir = os.path.join(repo_dir, gh.get('subpath') or '') if gh.get('subpath') else repo_dir
                    out.append({'name': name, 'path': path_raw, 'ok': True, 'dir': base_dir, 'message': 'already downloaded'})
                    continue
                try:
                    if os.path.exists(repo_dir):
                        shutil.rmtree(repo_dir)
                except Exception:
                    pass
                cmd = ['git', 'clone', '--depth', '1']
                if gh.get('branch'):
                    cmd += ['--branch', gh.get('branch')]
                cmd += [gh.get('git_url'), repo_dir]
                try:
                    logs.append(f"[download] {name}: running: {' '.join(shlex.quote(c) for c in cmd)}")
                    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=120)
                    if proc.returncode == 0 and os.path.isdir(repo_dir):
                        base_dir = os.path.join(repo_dir, gh.get('subpath') or '') if gh.get('subpath') else repo_dir
                        out.append({'name': name, 'path': path_raw, 'ok': True, 'dir': base_dir, 'message': 'downloaded'})
                    else:
                        msg = (proc.stdout or '').strip()
                        out.append({'name': name, 'path': path_raw, 'ok': False, 'dir': fdir, 'message': msg[-1000:] if msg else 'git clone failed'})
                except Exception as e:
                    out.append({'name': name, 'path': path_raw, 'ok': False, 'dir': fdir, 'message': str(e)})
            else:
                raw = _to_raw(path_raw, compose_name) or (path_raw.rstrip('/') + '/' + compose_name)
                yml_path = os.path.join(fdir, compose_name)
                try:
                    logs.append(f"[download] {name}: GET {raw}")
                    with urllib.request.urlopen(raw, timeout=30) as resp:
                        data_bin = resp.read(1_000_000)
                    with open(yml_path, 'wb') as fh:
                        fh.write(data_bin)
                    out.append({'name': name, 'path': path_raw, 'ok': True, 'dir': fdir, 'message': 'downloaded', 'compose_name': compose_name})
                except Exception as e:
                    out.append({'name': name, 'path': path_raw, 'ok': False, 'dir': fdir, 'message': str(e), 'compose_name': compose_name})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/flag_compose/pull', methods=['POST'])
def flag_compose_pull():
    """Run docker compose pull for the given flag catalog items."""
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_flag_base_dir())
        for it in items:
            name = (it.get('name') or it.get('Name') or '').strip()
            path_raw = (it.get('path') or it.get('Path') or '').strip()
            compose_name = (it.get('compose_name') or it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'flag')
            fdir = os.path.join(base_out, safe)

            local_path = _flag_resolve_path(path_raw)
            if local_path and os.path.exists(local_path) and local_path.lower().endswith(('.yml', '.yaml')):
                yml_path = local_path
            else:
                gh = _parse_github_url(path_raw)
                if gh.get('is_github'):
                    repo_dir = os.path.join(fdir, _vuln_repo_subdir())
                    sub = gh.get('subpath') or ''
                    is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                    base_dir = os.path.join(repo_dir, os.path.dirname(sub)) if is_file_sub else (os.path.join(repo_dir, sub) if sub else repo_dir)
                    yml_path = os.path.join(repo_dir, sub) if is_file_sub else os.path.join(base_dir, compose_name)
                    if not os.path.exists(yml_path):
                        cand = _compose_candidates(base_dir)
                        yml_path = cand[0] if cand else None
                else:
                    yml_path = os.path.join(fdir, compose_name)

            if not yml_path or not os.path.exists(yml_path):
                out.append({'name': name, 'path': path_raw, 'ok': False, 'message': 'compose file missing', 'compose_name': compose_name})
                continue
            if not shutil.which('docker'):
                out.append({'name': name, 'path': path_raw, 'ok': False, 'message': 'docker not available', 'compose_name': compose_name})
                continue
            try:
                proc = subprocess.run(['docker', 'compose', '-f', yml_path, 'pull'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                logs.append(f"[pull] {name}: rc={proc.returncode} file={yml_path}")
                ok = proc.returncode == 0
                msg = 'ok' if ok else ((proc.stdout or '')[-1000:] if proc.stdout else 'failed')
                out.append({'name': name, 'path': path_raw, 'ok': ok, 'message': msg, 'compose_name': compose_name})
            except Exception as e:
                out.append({'name': name, 'path': path_raw, 'ok': False, 'message': str(e), 'compose_name': compose_name})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/flag_compose/remove', methods=['POST'])
def flag_compose_remove():
    """Remove compose assets and any local outputs for the given flag catalog items."""
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_flag_base_dir())
        for it in items:
            name = (it.get('name') or it.get('Name') or '').strip()
            path_raw = (it.get('path') or it.get('Path') or '').strip()
            compose_name = (it.get('compose_name') or it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'flag')
            fdir = os.path.join(base_out, safe)

            # Prefer local compose file if path points at one.
            local_path = _flag_resolve_path(path_raw)
            yml_path = None
            if local_path and os.path.exists(local_path) and local_path.lower().endswith(('.yml', '.yaml')):
                yml_path = local_path
            else:
                gh = _parse_github_url(path_raw)
                if gh.get('is_github'):
                    repo_dir = os.path.join(fdir, _vuln_repo_subdir())
                    sub = gh.get('subpath') or ''
                    is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                    base_dir = os.path.join(repo_dir, os.path.dirname(sub)) if is_file_sub else (os.path.join(repo_dir, sub) if sub else repo_dir)
                    yml_path = os.path.join(repo_dir, sub) if is_file_sub else os.path.join(base_dir, compose_name)
                    if not os.path.exists(yml_path):
                        cand = _compose_candidates(base_dir)
                        yml_path = cand[0] if cand else None
                else:
                    yml_path = os.path.join(fdir, compose_name)

            if yml_path and os.path.exists(yml_path) and shutil.which('docker'):
                try:
                    logs.append(f"[remove] {name}: docker compose down file={yml_path}")
                except Exception:
                    pass
                try:
                    subprocess.run(['docker', 'compose', '-f', yml_path, 'down', '--volumes', '--remove-orphans'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                except Exception:
                    pass

            # Remove downloaded dirs under outputs/flags only (never delete repo-local templates)
            try:
                gh = _parse_github_url(path_raw)
                if gh.get('is_github'):
                    repo_dir = os.path.join(fdir, _vuln_repo_subdir())
                    if os.path.isdir(repo_dir):
                        shutil.rmtree(repo_dir, ignore_errors=True)
                        logs.append(f"[remove] {name}: deleted {repo_dir}")
                else:
                    yml = os.path.join(fdir, compose_name)
                    if os.path.exists(yml):
                        try:
                            os.remove(yml)
                            logs.append(f"[remove] {name}: deleted {yml}")
                        except Exception:
                            pass
                try:
                    if os.path.isdir(fdir) and not os.listdir(fdir):
                        os.rmdir(fdir)
                except Exception:
                    pass
            except Exception as e:
                try:
                    logs.append(f"[remove] cleanup error: {e}")
                except Exception:
                    pass

            out.append({'name': name, 'path': path_raw, 'ok': True, 'message': 'removed', 'compose_name': compose_name})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/vuln_catalog')
def vuln_catalog():
    """Return vulnerability catalog built from enabled data source CSVs.

    Response JSON:
      {
        "types": [str],
        "vectors": [str],
        "items": [ {"Name","Path","Type","Startup","Vector","CVE","Description","References"} ]
      }
    Only includes rows from enabled data sources that validate.
    """
    try:
        state = _load_data_sources_state()
        types = set()
        vectors = set()
        items = []
        for s in state.get('sources', []):
            if not s.get('enabled'): continue
            p = s.get('path')
            if not p or not os.path.exists(p): continue
            ok, note, norm_rows, _skipped = _validate_and_normalize_data_source_csv(p, skip_invalid=True)
            if not ok or not norm_rows or len(norm_rows) < 2: continue
            header = norm_rows[0]
            idx = {name: header.index(name) for name in header if name in header}
            for r in norm_rows[1:]:
                try:
                    rec = {
                        'Name': r[idx.get('Name')],
                        'Path': r[idx.get('Path')],
                        'Type': r[idx.get('Type')],
                        'Startup': r[idx.get('Startup')],
                        'Vector': r[idx.get('Vector')],
                        'CVE': r[idx.get('CVE')] if 'CVE' in idx else 'n/a',
                        'Description': r[idx.get('Description')] if 'Description' in idx else 'n/a',
                        'References': r[idx.get('References')] if 'References' in idx else 'n/a',
                    }
                    # keep only non-empty mandatory values
                    if not rec['Name'] or not rec['Path']:
                        continue
                    items.append(rec)
                    if rec['Type']: types.add(rec['Type'])
                    if rec['Vector']: vectors.add(rec['Vector'])
                except Exception:
                    continue
        return jsonify({
            'types': sorted(types),
            'vectors': sorted(vectors),
            'items': items,
        })
    except Exception as e:
        return jsonify({'error': str(e), 'types': [], 'vectors': [], 'items': []}), 500


# ------------ Vulnerability compose helpers (GitHub-aware) ---------------
def _safe_name(s: str) -> str:
    try:
        return re.sub(r"[^a-z0-9_.-]+", "-", (s or '').strip().lower())[:80] or 'vuln'
    except Exception:
        return 'vuln'


def _parse_github_url(url: str):
    """Parse a GitHub URL. Supports formats:
    - https://github.com/owner/repo/tree/<branch>/<subpath>
    - https://github.com/owner/repo/blob/<branch>/<file_or_subpath>
    - https://github.com/owner/repo (no branch; default branch)

    Returns dict with keys:
      { 'is_github': bool, 'git_url': str|None, 'branch': str|None, 'subpath': str|None, 'mode': 'tree'|'blob'|'root' }
    """
    try:
        from urllib.parse import urlparse
        u = urlparse(url)
        if u.netloc.lower() != 'github.com':
            return {'is_github': False}
        parts = [p for p in u.path.strip('/').split('/') if p]
        if len(parts) < 2:
            return {'is_github': False}
        owner, repo = parts[0], parts[1]
        git_url = f"https://github.com/{owner}/{repo}.git"
        if len(parts) == 2:
            return {'is_github': True, 'git_url': git_url, 'branch': None, 'subpath': '', 'mode': 'root'}
        mode = parts[2]
        if mode not in ('tree', 'blob') or len(parts) < 4:
            # Unknown path mode; treat as root
            return {'is_github': True, 'git_url': git_url, 'branch': None, 'subpath': '', 'mode': 'root'}
        branch = parts[3]
        rest = '/'.join(parts[4:])
        return {'is_github': True, 'git_url': git_url, 'branch': branch, 'subpath': rest, 'mode': mode}
    except Exception:
        return {'is_github': False}


def _compose_candidates(base_dir: str):
    """Return possible compose file paths under base_dir in priority order."""
    cands = ['docker-compose.yml', 'docker-compose.yaml', 'compose.yml', 'compose.yaml']
    out = []
    try:
        for name in cands:
            p = os.path.join(base_dir, name)
            if os.path.exists(p):
                out.append(p)
    except Exception:
        pass
    return out

@app.route('/vuln_compose/status', methods=['POST'])
def vuln_compose_status():
    """Return status for a list of catalog items: whether compose file is downloaded and images pulled.

    Payload: { items: [{Name, Path}] }
    Returns: { items: [{Name, Path, exists: bool, pulled: bool, dir: str}] }
    """
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_vuln_base_dir())
        os.makedirs(base_out, exist_ok=True)
        for it in items:
            name = (it.get('Name') or '').strip()
            path = (it.get('Path') or '').strip()
            compose_name = (it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'vuln')
            vdir = os.path.join(base_out, safe)
            gh = _parse_github_url(path)
            base_dir = vdir
            compose_file = None
            if gh.get('is_github'):
                try:
                    logs.append(f"[status] {name}: Path={path}")
                    logs.append(f"[status] {name}: git_url={gh.get('git_url')} branch={gh.get('branch')} subpath={gh.get('subpath')} mode={gh.get('mode')}")
                except Exception:
                    pass
                repo_dir = os.path.join(vdir, _vuln_repo_subdir())
                sub = gh.get('subpath') or ''
                # If subpath looks like a compose file, resolve directly
                is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                if is_file_sub:
                    compose_file = os.path.join(repo_dir, sub)
                    base_dir = os.path.dirname(compose_file)
                    exists = os.path.exists(compose_file)
                else:
                    base_dir = os.path.join(repo_dir, sub) if sub else repo_dir
                    exists = os.path.isdir(base_dir)
                try:
                    logs.append(f"[status] {name}: base={base_dir} exists={exists} compose={compose_name}")
                except Exception:
                    pass
                # prefer provided compose name
                if exists and compose_name and not compose_file:
                    p = os.path.join(base_dir, compose_name)
                    if os.path.exists(p):
                        compose_file = p
                # log compose candidates
                try:
                    cands = _compose_candidates(base_dir) if exists else []
                    logs.append(f"[status] {name}: compose candidates={cands[:4]}")
                except Exception:
                    pass
                if not compose_file:
                    # find compose candidates within base_dir
                    cand = _compose_candidates(base_dir)
                    compose_file = cand[0] if cand else None
            else:
                # legacy direct download to vdir/docker-compose.yml
                compose_file = os.path.join(vdir, compose_name or 'docker-compose.yml')
                exists = os.path.exists(compose_file)
                try:
                    logs.append(f"[status] {name}: non-github Path={path} compose_path={compose_file} exists={exists}")
                except Exception:
                    pass
            pulled = False
            if exists and compose_file and shutil.which('docker'):
                try:
                    proc = subprocess.run(['docker', 'compose', '-f', compose_file, 'config', '--images'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=30)
                    try:
                        logs.append(f"[status] docker compose config --images rc={proc.returncode}")
                    except Exception:
                        pass
                    if proc.returncode == 0:
                        images = [ln.strip() for ln in (proc.stdout or '').splitlines() if ln.strip()]
                        try:
                            logs.append(f"[status] images discovered: {len(images)}")
                            logs.append(f"[status] images sample: {images[:4]}")
                        except Exception:
                            pass
                        if images:
                            present = []
                            for img in images:
                                p2 = subprocess.run(['docker', 'image', 'inspect', img], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                                try:
                                    logs.append(f"[status] image inspect {img} rc={p2.returncode}")
                                except Exception:
                                    pass
                                present.append(p2.returncode == 0)
                            pulled = all(present)
                except Exception:
                    pulled = False
            out.append({'Name': name, 'Path': path, 'compose': compose_name, 'compose_path': compose_file, 'exists': bool(exists), 'pulled': bool(pulled), 'dir': base_dir})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/vuln_compose/status_images', methods=['POST'])
def vuln_compose_status_images():
    """Fast-ish status for a list of catalog items: whether compose is downloaded and images exist locally.

    This endpoint is optimized for UI gating (vuln picker):
    - Resolve the compose file path similarly to `/vuln_compose/status`.
    - Prefer parsing docker-compose YAML (`services.*.image`) to avoid invoking `docker compose config`.
    - Check local images via `docker image inspect`.
    - Fallback to `docker compose -f <file> config --images` only when YAML parsing yields no images.

    Payload: { items: [{Name, Path}] }
    Returns: { items: [{Name, Path, exists: bool, pulled: bool, compose_path: str|None, images: [str], missing_images: [str]}] }
    """
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out: list[dict] = []

        base_out = os.path.abspath(_vuln_base_dir())
        docker_ok = bool(shutil.which('docker'))

        # Optional dependency (pinned in requirements, but keep best-effort).
        try:
            import yaml  # type: ignore
        except Exception:  # pragma: no cover
            yaml = None  # type: ignore

        def _resolve_compose_path(name: str, path: str, compose_name: str) -> tuple[bool, str | None]:
            safe = _safe_name(name or 'vuln')
            vdir = os.path.join(base_out, safe)
            gh = _parse_github_url(path)
            compose_file: str | None = None
            exists = False

            if gh.get('is_github'):
                repo_dir = os.path.join(vdir, _vuln_repo_subdir())
                sub = gh.get('subpath') or ''
                is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                if is_file_sub:
                    compose_file = os.path.join(repo_dir, sub)
                    exists = os.path.exists(compose_file)
                else:
                    base_dir = os.path.join(repo_dir, sub) if sub else repo_dir
                    exists = os.path.isdir(base_dir)
                    if exists:
                        preferred = os.path.join(base_dir, compose_name)
                        if os.path.exists(preferred):
                            compose_file = preferred
                        else:
                            cand = _compose_candidates(base_dir)
                            compose_file = cand[0] if cand else None
                return bool(exists and compose_file and os.path.exists(compose_file)), compose_file

            # Non-github: legacy direct download
            compose_file = os.path.join(vdir, compose_name)
            exists = os.path.exists(compose_file)
            return bool(exists), compose_file if exists else compose_file

        def _images_from_compose_yaml(compose_path: str) -> list[str]:
            if not yaml:
                return []
            try:
                with open(compose_path, 'r', encoding='utf-8', errors='ignore') as f:
                    doc = yaml.safe_load(f) or {}
                if not isinstance(doc, dict):
                    return []
                services = doc.get('services')
                if not isinstance(services, dict):
                    return []
                images: list[str] = []
                for _svc_name, svc in services.items():
                    if not isinstance(svc, dict):
                        continue
                    img = svc.get('image')
                    if isinstance(img, str) and img.strip():
                        images.append(img.strip())
                # preserve stable order but dedupe
                seen: set[str] = set()
                out_imgs: list[str] = []
                for img in images:
                    if img in seen:
                        continue
                    seen.add(img)
                    out_imgs.append(img)
                return out_imgs
            except Exception:
                return []

        for it in items:
            name = (it.get('Name') or '').strip()
            path = (it.get('Path') or '').strip()
            compose_name = (it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'

            exists, compose_path = _resolve_compose_path(name, path, compose_name)
            images: list[str] = []
            missing: list[str] = []
            pulled = False

            if exists and compose_path and docker_ok:
                images = _images_from_compose_yaml(compose_path)

                # Fallback: ask docker compose to render images list (slower).
                if not images:
                    try:
                        proc = subprocess.run(
                            ['docker', 'compose', '-f', compose_path, 'config', '--images'],
                            stdout=subprocess.PIPE,
                            stderr=subprocess.STDOUT,
                            text=True,
                            timeout=20,
                        )
                        if proc.returncode == 0:
                            images = [ln.strip() for ln in (proc.stdout or '').splitlines() if ln.strip()]
                    except Exception:
                        images = []

                if images:
                    for img in images:
                        try:
                            p2 = subprocess.run(
                                ['docker', 'image', 'inspect', img],
                                stdout=subprocess.DEVNULL,
                                stderr=subprocess.DEVNULL,
                            )
                            if p2.returncode != 0:
                                missing.append(img)
                        except Exception:
                            missing.append(img)
                    pulled = len(missing) == 0

            out.append({
                'Name': name,
                'Path': path,
                'compose': compose_name,
                'compose_path': compose_path,
                'exists': bool(exists),
                'pulled': bool(pulled),
                'images': images,
                'missing_images': missing,
                'docker_available': docker_ok,
            })

        return jsonify({'items': out})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/vuln_compose/download', methods=['POST'])
def vuln_compose_download():
    """Download docker-compose.yml for the given catalog items.

    Payload: { items: [{Name, Path}] }
    Returns: { items: [{Name, Path, ok: bool, dir: str, message: str}] }
    """
    try:
        try:
            from core_topo_gen.utils.vuln_process import _github_tree_to_raw as _to_raw
        except Exception as _imp_err:
            # Fallback: minimal tree->raw converter for GitHub tree URLs
            def _to_raw(base_url: str, filename: str) -> str | None:
                try:
                    from urllib.parse import urlparse
                    u = urlparse(base_url)
                    if u.netloc.lower() != 'github.com':
                        return None
                    parts = [p for p in u.path.strip('/').split('/') if p]
                    if len(parts) < 4 or parts[2] != 'tree':
                        return None
                    owner, repo, _tree, branch = parts[:4]
                    rest = '/'.join(parts[4:])
                    return f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{rest}/{filename}"
                except Exception:
                    return None
            try:
                app.logger.warning("[download] fallback _to_raw used due to import error: %s", _imp_err)
            except Exception:
                pass
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_vuln_base_dir())
        os.makedirs(base_out, exist_ok=True)
        import urllib.request
        import shlex
        for it in items:
            name = (it.get('Name') or '').strip()
            path = (it.get('Path') or '').strip()
            compose_name = (it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'vuln')
            vdir = os.path.join(base_out, safe)
            os.makedirs(vdir, exist_ok=True)
            gh = _parse_github_url(path)
            if gh.get('is_github'):
                # Clone the repo; use branch if provided
                if not shutil.which('git'):
                    try:
                        logs.append(f"[download] {name}: git not available in PATH")
                    except Exception:
                        pass
                    out.append({'Name': name, 'Path': path, 'ok': False, 'dir': vdir, 'message': 'git not available'})
                    continue
                repo_dir = os.path.join(vdir, _vuln_repo_subdir())
                try:
                    logs.append(f"[download] {name}: Path={path}")
                    logs.append(f"[download] {name}: git_url={gh.get('git_url')} branch={gh.get('branch')} subpath={gh.get('subpath')} -> repo_dir={repo_dir}")
                except Exception:
                    pass
                # If already cloned and looks valid, skip re-clone
                if os.path.isdir(os.path.join(repo_dir, '.git')):
                    try:
                        logs.append(f"[download] {name}: repo exists {repo_dir}")
                    except Exception:
                        pass
                    base_dir = os.path.join(repo_dir, gh.get('subpath') or '') if gh.get('subpath') else repo_dir
                    try:
                        logs.append(f"[download] {name}: base_dir={base_dir}")
                        # limited directory listing
                        if os.path.isdir(base_dir):
                            entries = []
                            for nm in os.listdir(base_dir)[:10]:
                                p = os.path.join(base_dir, nm)
                                kind = 'dir' if os.path.isdir(p) else 'file'
                                entries.append(f"{nm}({kind})")
                            logs.append(f"[download] {name}: base_dir entries: {entries}")
                    except Exception:
                        pass
                    out.append({'Name': name, 'Path': path, 'ok': True, 'dir': base_dir, 'message': 'already downloaded'})
                    continue
                # Ensure empty directory
                try:
                    if os.path.exists(repo_dir):
                        shutil.rmtree(repo_dir)
                except Exception:
                    pass
                cmd = ['git', 'clone', '--depth', '1']
                if gh.get('branch'):
                    cmd += ['--branch', gh.get('branch')]
                cmd += [gh.get('git_url'), repo_dir]
                try:
                    try:
                        logs.append(f"[download] {name}: running: {' '.join(shlex.quote(c) for c in cmd)}")
                    except Exception:
                        pass
                    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=120)
                    try:
                        logs.append(f"[download] git clone rc={proc.returncode} dir={repo_dir}")
                        if proc.stdout:
                            for ln in proc.stdout.splitlines()[:100]:
                                logs.append(f"[git] {ln}")
                    except Exception:
                        pass
                    if proc.returncode == 0 and os.path.isdir(repo_dir):
                        base_dir = os.path.join(repo_dir, gh.get('subpath') or '') if gh.get('subpath') else repo_dir
                        try:
                            logs.append(f"[download] {name}: base_dir={base_dir}")
                            # limited directory listing
                            if os.path.isdir(base_dir):
                                entries = []
                                for nm in os.listdir(base_dir)[:10]:
                                    p = os.path.join(base_dir, nm)
                                    kind = 'dir' if os.path.isdir(p) else 'file'
                                    entries.append(f"{nm}({kind})")
                                logs.append(f"[download] {name}: base_dir entries: {entries}")
                        except Exception:
                            pass
                        out.append({'Name': name, 'Path': path, 'ok': True, 'dir': base_dir, 'message': 'downloaded'})
                    else:
                        msg = (proc.stdout or '').strip()
                        out.append({'Name': name, 'Path': path, 'ok': False, 'dir': vdir, 'message': msg[-1000:] if msg else 'git clone failed'})
                except Exception as e:
                    out.append({'Name': name, 'Path': path, 'ok': False, 'dir': vdir, 'message': str(e)})
            else:
                # Legacy: direct download of compose file (use provided compose name)
                raw = _to_raw(path, compose_name) or (path.rstrip('/') + '/' + compose_name)
                yml_path = os.path.join(vdir, compose_name)
                try:
                    try:
                        logs.append(f"[download] {name}: Path={path}")
                        logs.append(f"[download] {name}: GET {raw}")
                    except Exception:
                        pass
                    with urllib.request.urlopen(raw, timeout=30) as resp:
                        status = getattr(resp, 'status', None) or getattr(resp, 'code', None)
                        data_bin = resp.read(1_000_000)
                        try:
                            logs.append(f"[download] {name}: HTTP {status} bytes={len(data_bin) if data_bin else 0}")
                        except Exception:
                            pass
                    with open(yml_path, 'wb') as f:
                        f.write(data_bin)
                    out.append({'Name': name, 'Path': path, 'ok': True, 'dir': vdir, 'message': 'downloaded', 'compose': compose_name})
                except Exception as e:
                    out.append({'Name': name, 'Path': path, 'ok': False, 'dir': vdir, 'message': str(e), 'compose': compose_name})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/vuln_compose/pull', methods=['POST'])
def vuln_compose_pull():
    """Run docker compose pull for the given catalog items (assumes docker-compose.yml is present).

    Payload: { items: [{Name, Path}] }
    Returns: { items: [{Name, Path, ok: bool, message: str}] }
    """
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_vuln_base_dir())
        for it in items:
            name = (it.get('Name') or '').strip()
            path = (it.get('Path') or '').strip()
            compose_name = (it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'vuln')
            vdir = os.path.join(base_out, safe)
            gh = _parse_github_url(path)
            if gh.get('is_github'):
                repo_dir = os.path.join(vdir, _vuln_repo_subdir())
                sub = gh.get('subpath') or ''
                # blob file path -> direct compose path
                is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                base_dir = os.path.join(repo_dir, os.path.dirname(sub)) if is_file_sub else (os.path.join(repo_dir, sub) if sub else repo_dir)
                try:
                    logs.append(
                        f"[pull] {name}: git_url={gh.get('git_url')} branch={gh.get('branch')} subpath={gh.get('subpath')} base_dir={base_dir}"
                    )
                except Exception:
                    pass
                # prefer provided compose name
                yml_path = os.path.join(repo_dir, sub) if is_file_sub else os.path.join(base_dir, compose_name)
                if not os.path.exists(yml_path):
                    cand = _compose_candidates(base_dir)
                    yml_path = cand[0] if cand else None
                try:
                    logs.append(f"[pull] {name}: yml_path={yml_path}")
                except Exception:
                    pass
            else:
                yml_path = os.path.join(vdir, compose_name)
                try:
                    logs.append(f"[pull] {name}: non-github base_dir={vdir}")
                except Exception:
                    pass
            if not yml_path or not os.path.exists(yml_path):
                out.append({'Name': name, 'Path': path, 'ok': False, 'message': 'compose file missing', 'compose': compose_name})
                continue
            if not shutil.which('docker'):
                out.append({'Name': name, 'Path': path, 'ok': False, 'message': 'docker not available', 'compose': compose_name})
                continue
            try:
                proc = subprocess.run(['docker', 'compose', '-f', yml_path, 'pull'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                try:
                    logs.append(f"[pull] {name}: docker compose pull rc={proc.returncode} file={yml_path}")
                    if proc.stdout:
                        for ln in proc.stdout.splitlines()[:200]:
                            logs.append(f"[docker] {ln}")
                except Exception:
                    pass
                ok = proc.returncode == 0
                msg = 'ok' if ok else ((proc.stdout or '')[-1000:] if proc.stdout else 'failed')
                out.append({'Name': name, 'Path': path, 'ok': ok, 'message': msg, 'compose': compose_name})
            except Exception as e:
                out.append({'Name': name, 'Path': path, 'ok': False, 'message': str(e), 'compose': compose_name})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/vuln_compose/remove', methods=['POST'])
def vuln_compose_remove():
    """Remove docker-compose assets and containers/images for the given catalog items.

    Steps per item:
    - Resolve compose file path (like status/pull)
    - docker compose down --volumes --remove-orphans
    - Optionally remove images referenced by compose (best-effort)
    - Remove downloaded directories (repo dir or compose file directory) under outputs

    Payload: { items: [{Name, Path}] }
    Returns: { items: [{Name, Path, ok: bool, message: str}] }
    """
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_vuln_base_dir())
        for it in items:
            name = (it.get('Name') or '').strip()
            path = (it.get('Path') or '').strip()
            compose_name = (it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'vuln')
            vdir = os.path.join(base_out, safe)
            gh = _parse_github_url(path)
            yml_path = None
            base_dir = vdir
            try:
                logs.append(f"[remove] {name}: Path={path}")
            except Exception:
                pass
            if gh.get('is_github'):
                repo_dir = os.path.join(vdir, _vuln_repo_subdir())
                sub = gh.get('subpath') or ''
                is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                base_dir = os.path.join(repo_dir, os.path.dirname(sub)) if is_file_sub else (os.path.join(repo_dir, sub) if sub else repo_dir)
                yml_path = os.path.join(repo_dir, sub) if is_file_sub else os.path.join(base_dir, compose_name)
                if not os.path.exists(yml_path):
                    cand = _compose_candidates(base_dir)
                    yml_path = cand[0] if cand else None
            else:
                yml_path = os.path.join(vdir, compose_name)
            # Bring down compose stack
            if yml_path and os.path.exists(yml_path) and shutil.which('docker'):
                try:
                    logs.append(f"[remove] {name}: docker compose down file={yml_path}")
                except Exception:
                    pass
                try:
                    proc = subprocess.run(['docker', 'compose', '-f', yml_path, 'down', '--volumes', '--remove-orphans'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                    try:
                        logs.append(f"[remove] docker compose down rc={proc.returncode}")
                        if proc.stdout:
                            for ln in proc.stdout.splitlines()[:200]:
                                logs.append(f"[docker] {ln}")
                    except Exception:
                        pass
                except Exception as e:
                    try: logs.append(f"[remove] compose down error: {e}")
                    except Exception: pass
                # Attempt to remove images referenced by compose (best-effort)
                try:
                    proc2 = subprocess.run(['docker', 'compose', '-f', yml_path, 'config', '--images'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                    if proc2.returncode == 0:
                        images = [ln.strip() for ln in (proc2.stdout or '').splitlines() if ln.strip()]
                        for img in images:
                            p3 = subprocess.run(['docker', 'image', 'rm', '-f', img], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                            try: logs.append(f"[remove] image rm {img} rc={p3.returncode}")
                            except Exception: pass
                except Exception:
                    pass
            # Remove downloaded files/dirs under outputs for this item
            try:
                if gh.get('is_github'):
                    repo_dir = os.path.join(vdir, _vuln_repo_subdir())
                    if os.path.isdir(repo_dir):
                        shutil.rmtree(repo_dir, ignore_errors=True)
                        logs.append(f"[remove] {name}: deleted {repo_dir}")
                else:
                    # legacy direct compose path
                    yml = os.path.join(vdir, compose_name)
                    if os.path.exists(yml):
                        try:
                            os.remove(yml)
                            logs.append(f"[remove] {name}: deleted {yml}")
                        except Exception:
                            pass
                # Remove vdir if empty
                try:
                    if os.path.isdir(vdir) and not os.listdir(vdir):
                        os.rmdir(vdir)
                        logs.append(f"[remove] {name}: cleaned empty {vdir}")
                except Exception:
                    pass
            except Exception as e:
                try: logs.append(f"[remove] cleanup error: {e}")
                except Exception: pass
            out.append({'Name': name, 'Path': path, 'ok': True, 'message': 'removed', 'compose': compose_name})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/data_sources/edit/<sid>')
def data_sources_edit(sid):
    """Render an editable view of the CSV source in a simple table.
    """
    state = _load_data_sources_state()
    target = None
    for s in state.get('sources', []):
        if s.get('id') == sid:
            target = s
            break
    if not target:
        flash('Source not found')
        return redirect(url_for('data_sources_page'))
    path = target.get('path')
    if not path or not os.path.exists(path):
        flash('File missing')
        return redirect(url_for('data_sources_page'))
    # Read CSV safely
    rows = []
    with open(path, 'r', encoding='utf-8', errors='replace', newline='') as f:
        rdr = csv.reader(f)
        for r in rdr:
            rows.append(r)
    name = target.get('name') or os.path.basename(path)
    return render_template('data_source_edit.html', sid=sid, name=name, path=path, rows=rows)

@app.route('/data_sources/save/<sid>', methods=['POST'])
def data_sources_save(sid):
    """Save edited CSV content coming from the editor page.
    Expects JSON payload: { rows: string[][] }
    """
    try:
        data = request.get_json(silent=True)
        if not isinstance(data, dict) or 'rows' not in data:
            return jsonify({"ok": False, "error": "Invalid payload"}), 400
        rows = data.get('rows')
        if not isinstance(rows, list) or any(not isinstance(r, list) for r in rows):
            return jsonify({"ok": False, "error": "Rows must be a list of lists"}), 400
        # Basic row length normalization (pad shorter rows to header length)
        maxw = max((len(r) for r in rows), default=0)
        norm = []
        for r in rows:
            if len(r) < maxw:
                r = r + [''] * (maxw - len(r))
            norm.append([str(c) if c is not None else '' for c in r])
        state = _load_data_sources_state()
        target = None
        for s in state.get('sources', []):
            if s.get('id') == sid:
                target = s
                break
        if not target:
            return jsonify({"ok": False, "error": "Source not found"}), 404
        path = target.get('path')
        if not path:
            return jsonify({"ok": False, "error": "Missing file path"}), 400
        # Validate and normalize according to schema
        # Write temp to validate with the same function used for uploads
        tmp_preview = path + '.editpreview'
        try:
            with open(tmp_preview, 'w', encoding='utf-8', newline='') as f:
                w = csv.writer(f)
                for r in norm:
                    w.writerow(r)
            ok2, note2, norm_rows2, skipped2 = _validate_and_normalize_data_source_csv(tmp_preview, skip_invalid=True)
        finally:
            try: os.remove(tmp_preview)
            except Exception: pass
        if not ok2:
            return jsonify({"ok": False, "error": note2}), 200
        # Atomic write normalized rows
        tmp = path + '.tmp'
        with open(tmp, 'w', encoding='utf-8', newline='') as f:
            w = csv.writer(f)
            for r in (norm_rows2 or norm):
                w.writerow(r)
        os.replace(tmp, path)
        # Update state row count
        ok, note = _validate_csv(path)
        if ok2 and skipped2:
            note_extra = f" (skipped {len(skipped2)} invalid)"
        else:
            note_extra = ''
        target['rows'] = (note if ok else f"ERR: {note}") + note_extra
        _save_data_sources_state(state)
        return jsonify({"ok": True, "skipped": len(skipped2) if ok2 else 0})
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500

def _purge_run_history_for_scenario(scenario_name: str, delete_artifacts: bool = True) -> int:
    """Remove any run history entries whose scenario_names contains scenario_name.
    Optionally delete associated artifact files (xml/report/pre-session xml) under outputs/.
    Returns number of entries removed.
    """
    try:
        if not os.path.exists(RUN_HISTORY_PATH):
            return 0
        with open(RUN_HISTORY_PATH, 'r', encoding='utf-8') as f:
            hist = json.load(f)
        if not isinstance(hist, list):
            return 0
        kept = []
        removed = 0
        target_key = ''
        try:
            target_key = _scenario_match_key(scenario_name)
        except Exception:
            target_key = ''
        for entry in hist:
            scen_list = []
            try:
                if 'scenario_names' in entry:
                    scen_list = entry.get('scenario_names') or []
                else:
                    scen_list = _scenario_names_from_xml(entry.get('xml_path'))
            except Exception:
                scen_list = []
            match = False
            if target_key and scen_list:
                try:
                    for nm in scen_list:
                        if _scenario_match_key(nm) == target_key:
                            match = True
                            break
                except Exception:
                    match = False
            else:
                match = bool(scenario_name in (scen_list or []))

            if match:
                removed += 1
                if delete_artifacts:
                    for key in ('xml_path','report_path','pre_xml_path','post_xml_path','scenario_xml_path'):
                        p = entry.get(key)
                        if p and isinstance(p,str) and os.path.exists(p):
                            # Only delete if inside outputs directory for safety
                            try:
                                out_abs = os.path.abspath(_outputs_dir())
                                p_abs = os.path.abspath(p)
                                if p_abs.startswith(out_abs):
                                    try: os.remove(p_abs)
                                    except Exception: pass
                                    # Attempt to remove directory if empty afterwards
                                    try:
                                        parent = os.path.dirname(p_abs)
                                        if parent.startswith(out_abs) and os.path.isdir(parent) and not os.listdir(parent):
                                            os.rmdir(parent)
                                    except Exception:
                                        pass
                            except Exception:
                                pass
                continue
            kept.append(entry)
        if removed:
            tmp = RUN_HISTORY_PATH + '.tmp'
            with open(tmp, 'w', encoding='utf-8') as f:
                json.dump(kept, f, indent=2)
            os.replace(tmp, RUN_HISTORY_PATH)
        return removed
    except Exception:
        return 0

@app.route('/purge_history_for_scenario', methods=['POST'])
def purge_history_for_scenario():
    try:
        data = request.get_json(silent=True) or {}
        name = (data.get('name') or '').strip()
        if not name:
            return jsonify({'removed': 0}), 200
        removed = _purge_run_history_for_scenario(name, delete_artifacts=True)
        return jsonify({'removed': removed})
    except Exception as e:
        return jsonify({'removed': 0, 'error': str(e)}), 200


@app.route('/delete_scenarios', methods=['POST'])
def delete_scenarios():
    """Persist scenario deletions across refresh.

    This updates outputs/scenario_catalog.json, which seeds the Scenario Editor list
    on initial page load.
    """
    try:
        data = request.get_json(silent=True) or {}
        raw_names = data.get('names')
        if isinstance(raw_names, str):
            names = [raw_names]
        elif isinstance(raw_names, list):
            names = [n for n in raw_names if isinstance(n, (str, int, float))]
        else:
            names = []
        names = [str(n).strip() for n in names if str(n).strip()]
        if not names:
            return jsonify({'ok': False, 'error': 'Missing scenario names'}), 400
        result = _remove_scenarios_from_catalog(names)
        # Also delete any saved Scenario_*.xml artifacts and remove the scenario(s)
        # from all editor snapshots, otherwise they will be re-discovered on refresh.
        artifacts = _delete_saved_scenario_xml_artifacts(names)
        snapshots = _remove_scenarios_from_all_editor_snapshots(names)
        history_removed = 0
        try:
            for nm in names:
                try:
                    history_removed += _purge_run_history_for_scenario(str(nm), delete_artifacts=True)
                except Exception:
                    continue
        except Exception:
            history_removed = history_removed
        return jsonify({'ok': True, **result, **artifacts, **snapshots, 'history_removed': history_removed})
    except Exception as e:
        try:
            app.logger.exception('[delete_scenarios] failed: %s', e)
        except Exception:
            pass
        return jsonify({'ok': False, 'error': str(e)}), 500

if __name__ == '__main__':
    try:
        port = int(os.environ.get('PORT') or os.environ.get('WEBAPP_PORT') or '9090')
    except Exception:
        port = 9090
    host = os.environ.get('CORETG_HOST') or '0.0.0.0'
    debug = _env_flag('CORETG_DEBUG', False) or _env_flag('FLASK_DEBUG', False)
    use_reloader = _env_flag('CORETG_USE_RELOADER', debug)
    try:
        did_scrub = _scrub_hitl_validation_usernames_in_scenario_catalog()
        if did_scrub:
            app.logger.info('[hitl_validation] scrubbed usernames from scenario_catalog.json')
    except Exception:
        pass
    try:
        did_backfill = _backfill_hitl_config_from_editor_snapshots()
        if did_backfill:
            app.logger.info('[hitl_config] backfilled hitl_config from editor snapshots')
    except Exception:
        pass
    try:
        did_scrub_cfg = _scrub_unverified_hitl_config_in_scenario_catalog()
        if did_scrub_cfg:
            app.logger.info('[hitl_config] scrubbed unverified hitl_config entries from scenario_catalog.json')
    except Exception:
        pass
    app.run(host=host, port=port, debug=debug, use_reloader=use_reloader)
