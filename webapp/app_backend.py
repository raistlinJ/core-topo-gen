from __future__ import annotations

import os
import base64
import hashlib
import sys
import re
import shutil
import subprocess
import io
import json
import datetime
import time
import uuid
import threading
import csv
import logging
import zipfile
import tarfile
import tempfile
import secrets
import socket
import hashlib
import socketserver
import select
import contextlib
import textwrap
import shlex
import posixpath
import fnmatch
import copy
from urllib.parse import urlparse, parse_qs

from collections import deque

from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Iterator, TextIO, Iterable

from core_topo_gen.utils.flow_seed import flow_generator_seed as _flow_generator_seed_impl

try:
    import psutil  # type: ignore
except ImportError:  # pragma: no cover - psutil is optional for tests
    psutil = None  # type: ignore
from collections import defaultdict
from types import SimpleNamespace
import xml.etree.ElementTree as ET

from flask import Flask, render_template, request, redirect, url_for, flash, send_file, Response, jsonify, session, g, has_request_context, abort
from werkzeug.utils import secure_filename
from werkzeug.security import generate_password_hash, check_password_hash

try:
    from flask_login import login_required, current_user
except ImportError:
    # If flask_login is missing, define a dummy decorator to avoid crashes,
    # though the app likely depends on it.
    def login_required(f):
        return f
    current_user = None

try:
    from proxmoxer import ProxmoxAPI  # type: ignore
except ImportError:  # pragma: no cover - optional dependency
    ProxmoxAPI = None  # type: ignore

try:
    import paramiko  # type: ignore
except ImportError:  # pragma: no cover - optional dependency
    paramiko = None  # type: ignore

try:
    from cryptography.fernet import Fernet, InvalidToken  # type: ignore
except ImportError:  # pragma: no cover - optional dependency
    Fernet = None  # type: ignore

    class InvalidToken(Exception):
        """Fallback InvalidToken if cryptography is unavailable."""
        pass
from lxml import etree as LET  # XML validation
ALLOWED_EXTENSIONS = {'xml'}


_SSE_MARKER_PREFIX = '__SSE_EVENT__'


def _write_sse_marker(log_handle, event: str, payload) -> None:
    """Write a marker line into the run log that /stream/<run_id> can translate
    into a typed SSE event.

    This avoids adding additional shared state between the runner thread and the
    streaming endpoint.
    """
    if log_handle is None:
        return
    try:
        safe_event = re.sub(r'[^a-zA-Z0-9_\-]+', '', str(event or 'phase')) or 'phase'
        data = json.dumps(payload or {}, ensure_ascii=False, separators=(',', ':'))
        log_handle.write(f"{_SSE_MARKER_PREFIX} {safe_event} {data}\n")
    except Exception:
        return


def _attack_graph_for_chain(
    *,
    chain_nodes: list[dict[str, Any]],
    scenario_label: str,
    flag_assignments: list[dict[str, Any]] | None = None,
) -> dict[str, Any]:
    """Build a simple attack-graph JSON for a linear chain.

    Format (v1):
    {
      "schema_version": 1,
      "scenario": "...",
      "nodes": [{"id","label","type","is_vuln","ipv4","generator":{...}}],
      "edges": [{"source","target","artifacts"}]
    }
    """
    assignment_by_node_id: dict[str, dict[str, Any]] = {}
    try:
        for fa in (flag_assignments or []):
            if not isinstance(fa, dict):
                continue
            nid = str(fa.get('node_id') or '').strip()
            if nid and nid not in assignment_by_node_id:
                assignment_by_node_id[nid] = fa
    except Exception:
        assignment_by_node_id = {}

    nodes_out: list[dict[str, Any]] = []
    edges_out: list[dict[str, Any]] = []

    def _node_ipv4(n: dict[str, Any]) -> str:
        try:
            return _first_valid_ipv4(n.get('ipv4') or n.get('ip4') or n.get('ip') or '')
        except Exception:
            return ''

    def _outputs_for_assignment(a: dict[str, Any] | None) -> list[str]:
        if not isinstance(a, dict):
            return []
        out: list[str] = []
        try:
            outs = a.get('outputs')
            if isinstance(outs, list):
                out.extend([str(x or '').strip() for x in outs if str(x or '').strip()])
        except Exception as exc:
            try:
                app.logger.exception('[flow.prepare_preview_for_execute] generator run failure: %s', exc)
            except Exception:
                pass
            return jsonify({
                'ok': False,
                'error': f'Generator execution failed during resolve: {exc}',
                'scenario': scenario_label or scenario_norm,
                'length': length,
                'stats': stats,
                'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or '')} for n in chain_nodes],
                'flag_assignments': flag_assignments,
                'base_preview_plan_path': base_plan_path,
                'best_effort': bool(best_effort),
            }), 500
        try:
            prod = a.get('produces')
            if isinstance(prod, list):
                out.extend([str(x or '').strip() for x in prod if str(x or '').strip()])
        except Exception:
            pass
        try:
            fields = a.get('output_fields')
            if isinstance(fields, list):
                out.extend([str(x or '').strip() for x in fields if str(x or '').strip()])
        except Exception:
            pass
        # De-dupe
        seen: set[str] = set()
        uniq: list[str] = []
        for k in out:
            if not k or k in seen:
                continue
            seen.add(k)
            uniq.append(k)
        return uniq

    def _resolved_map(a: dict[str, Any] | None, key: str) -> dict[str, Any]:
        if not isinstance(a, dict):
            return {}
        try:
            val = a.get(key)
            return val if isinstance(val, dict) else {}
        except Exception:
            return {}

    def _resolved_kv_list(resolved_map: dict[str, Any]) -> list[str]:
        items: list[str] = []
        if not isinstance(resolved_map, dict):
            return items
        for k, v in resolved_map.items():
            try:
                if isinstance(v, str):
                    vs = v.strip()
                else:
                    vs = json.dumps(v, ensure_ascii=False)
            except Exception:
                vs = str(v)
            kk = str(k or '').strip()
            if not kk:
                continue
            if vs:
                items.append(f"{kk}={vs}")
            else:
                items.append(kk)
        return items

    for node in (chain_nodes or []):
        if not isinstance(node, dict):
            continue
        nid = str(node.get('id') or '').strip()
        if not nid:
            continue
        fa = assignment_by_node_id.get(nid)
        gen = {}
        if isinstance(fa, dict):
            gen = {
                'id': str(fa.get('id') or ''),
                'name': str(fa.get('name') or ''),
                'kind': str(fa.get('type') or ''),
                'source': str(fa.get('flag_generator') or ''),
                'catalog': str(fa.get('generator_catalog') or ''),
                'chain': [
                    {
                        'id': str(n.get('id') or ''),
                        'name': str(n.get('name') or ''),
                        'type': str(n.get('type') or ''),
                        'is_vuln': bool(n.get('is_vuln')),
                        'ip4': str(n.get('ip4') or ''),
                        'ipv4': str(n.get('ipv4') or ''),
                        'interfaces': list(n.get('interfaces') or []) if isinstance(n.get('interfaces'), list) else [],
                    }
                    for n in chain_nodes
                ],
                'resolved_outputs': _resolved_map(fa, 'resolved_outputs'),
                'flag_value': fa.get('flag_value'),
            }
        nodes_out.append({
            'id': nid,
            'label': str(node.get('name') or nid),
            'type': str(node.get('type') or ''),
            'is_vuln': bool(node.get('is_vuln')),
            'ipv4': _node_ipv4(node) or None,
            'generator': gen or None,
        })

    for i in range(len(chain_nodes) - 1):
        src = chain_nodes[i]
        tgt = chain_nodes[i + 1]
        if not isinstance(src, dict) or not isinstance(tgt, dict):
            continue
        src_id = str(src.get('id') or '').strip()
        tgt_id = str(tgt.get('id') or '').strip()
        if not src_id or not tgt_id:
            continue
        fa = assignment_by_node_id.get(src_id)
        resolved_out = _resolved_map(fa, 'resolved_outputs')
        edges_out.append({
            'source': src_id,
            'target': tgt_id,
            'artifacts': _outputs_for_assignment(fa),
            'artifacts_resolved': resolved_out,
            'artifacts_resolved_kv': _resolved_kv_list(resolved_out),
        })

    return {
        'schema_version': 1,
        'scenario': str(scenario_label or ''),
        'nodes': nodes_out,
        'edges': edges_out,
    }


def _attack_graph_dot(attack_graph: dict[str, Any]) -> str:
    """Return a Graphviz DOT representation of the attack graph."""
    nodes = attack_graph.get('nodes') if isinstance(attack_graph, dict) else None
    edges = attack_graph.get('edges') if isinstance(attack_graph, dict) else None
    if not isinstance(nodes, list):
        nodes = []
    if not isinstance(edges, list):
        edges = []

    def _esc(text: Any) -> str:
        s = str(text or '')
        s = s.replace('"', '\\"')
        s = s.replace('\n', '\\n')
        return s

    lines: list[str] = ['digraph attack_graph {']
    lines.append('  rankdir=LR;')
    lines.append('  node [shape=box, fontsize=10];')
    for n in nodes:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if not nid:
            continue
        label = str(n.get('label') or nid)
        gen = n.get('generator') if isinstance(n.get('generator'), dict) else None
        gen_name = str((gen or {}).get('name') or '').strip()
        gen_kind = str((gen or {}).get('kind') or '').strip()
        ip = str(n.get('ipv4') or '').strip()
        parts = [label]
        if ip:
            parts.append(f"@ {ip}")
        if gen_name or gen_kind:
            gline = gen_name if gen_name else ''
            if gen_kind:
                gline = f"{gline} [{gen_kind}]" if gline else f"[{gen_kind}]"
            parts.append(gline)
        try:
            rin = (gen or {}).get('resolved_inputs') if isinstance(gen, dict) else None
            if isinstance(rin, dict) and rin:
                items = []
                for k, v in list(rin.items())[:3]:
                    try:
                        vs = v.strip() if isinstance(v, str) else json.dumps(v, ensure_ascii=False)
                    except Exception:
                        vs = str(v)
                    kk = str(k or '').strip()
                    if kk:
                        items.append(f"{kk}={vs}" if vs else kk)
                if items:
                    parts.append("in: " + ", ".join(items))
        except Exception:
            pass
        try:
            rout = (gen or {}).get('resolved_outputs') if isinstance(gen, dict) else None
            if isinstance(rout, dict) and rout:
                items = []
                for k, v in list(rout.items())[:3]:
                    try:
                        vs = v.strip() if isinstance(v, str) else json.dumps(v, ensure_ascii=False)
                    except Exception:
                        vs = str(v)
                    kk = str(k or '').strip()
                    if kk:
                        items.append(f"{kk}={vs}" if vs else kk)
                if items:
                    parts.append("out: " + ", ".join(items))
        except Exception:
            pass
        node_label = "\\n".join([p for p in parts if p])
        lines.append(f"  \"{_esc(nid)}\" [label=\"{_esc(node_label)}\"];" )
    for e in edges:
        if not isinstance(e, dict):
            continue
        src = str(e.get('source') or '').strip()
        tgt = str(e.get('target') or '').strip()
        if not src or not tgt:
            continue
        artifacts = e.get('artifacts') if isinstance(e.get('artifacts'), list) else []
        resolved_kv = e.get('artifacts_resolved_kv') if isinstance(e.get('artifacts_resolved_kv'), list) else []
        label = ""
        try:
            items = [str(x or '').strip() for x in artifacts if str(x or '').strip()]
            items = items + [str(x or '').strip() for x in resolved_kv if str(x or '').strip()]
            if items:
                label = "\\n".join(items[:6])
                if len(items) > 6:
                    label = label + "\\n..."
        except Exception:
            label = ""
        if label:
            lines.append(f"  \"{_esc(src)}\" -> \"{_esc(tgt)}\" [label=\"{_esc(label)}\"];" )
        else:
            lines.append(f"  \"{_esc(src)}\" -> \"{_esc(tgt)}\";" )
    lines.append('}')
    return "\n".join(lines)


def _attack_graph_pdf_base64(dot_text: str) -> str | None:
    """Return base64-encoded PDF for a DOT string, or None if unavailable."""
    try:
        if not dot_text.strip():
            return None
    except Exception:
        return None
    try:
        if not shutil.which('dot'):
            return None
    except Exception:
        return None
    try:
        proc = subprocess.run(
            ['dot', '-Tpdf'],
            input=dot_text.encode('utf-8'),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=False,
            timeout=5,
        )
        if proc.returncode != 0:
            return None
        if not proc.stdout:
            return None
        return base64.b64encode(proc.stdout).decode('ascii')
    except Exception:
        return None


def _env_flag(name: str, default: bool = False) -> bool:
    value = os.environ.get(name)
    if value is None:
        return default
    return value.strip().lower() in {'1', 'true', 'yes', 'on'}


HITL_DISABLE_HOST_ENUM = _env_flag('HITL_DISABLE_HOST_ENUM', False)
NON_PHYSICAL_INTERFACE_NAMES = {
    'lo',
    'lo0',
    'loopback',
}
NON_PHYSICAL_INTERFACE_PREFIXES = (
    'awdl',
    'bridge',
    'br-',
    'docker',
    'gif',
    'ham',
    'llw',
    'p2p',
    'rmnet',
    'stf',
    'tap',
    'tailscale',
    'tun',
    'utun',
    'vboxnet',
    'veth',
    'virbr',
    'vmnet',
    'wg',
    'zt',
)
NON_PHYSICAL_INTERFACE_SUBSTRINGS = (
    'tailscale',
    'tunnel',
    'zerotier',
)

FULL_PREVIEW_ARTIFACT_VERSION = 2

_HITL_ATTACHMENT_ALLOWED = {
    "existing_router",
    "existing_switch",
    "new_router",
    "proxmox_vm",
}

_DEFAULT_HITL_ATTACHMENT = "existing_router"

_CORE_FIELD_KEYS = (
    'host',
    'port',
    'ssh_enabled',
    'ssh_host',
    'ssh_port',
    'ssh_username',
    'ssh_password',
    'venv_bin',
    'venv_user_override',
)

DEFAULT_CORE_VENV_BIN = '/opt/core/venv/bin/python'
CORE_DAEMON_START_COMMAND = 'sudo systemctl start core-daemon'
PYTHON_EXECUTABLE_NAMES = ('core-python', 'python3', 'python')
REMOTE_BASE_DIR_ENV = os.environ.get('CORE_REMOTE_BASE_DIR', '/tmp/core-topo-gen')
REMOTE_STATIC_REPO_ENV = os.environ.get('CORE_REMOTE_STATIC_REPO', '/tmp/core-topo-gen')
REMOTE_RUNS_SUBDIR = 'runs'
REMOTE_LOG_CHUNK_SIZE = 8192
REPO_PUSH_SKIP_IF_UNCHANGED = os.environ.get('CORETG_REPO_SKIP_IF_UNCHANGED', '1')

REPO_PUSH_EXCLUDE_DIRS = {
    '.git',
    '.hg',
    '.svn',
    '.mypy_cache',
    '.pytest_cache',
    '.ruff_cache',
    '.venv',
    'venv',
    'env',
    '.conda',
    '.conda-py311',
    '__pycache__',
    'node_modules',
    'dist',
    'build',
    'outputs',
    'uploads',
    'tmp_hitl',
}
REPO_PUSH_EXCLUDE_PATTERNS = ('*.pyc', '*.pyo', '*.pyd', '*.log', '*.tmp', '*.swp', '*.swo')
REPO_PUSH_ALLOWED_OUTPUTS_ENV = os.environ.get('CORETG_REPO_PUSH_ALLOWED_OUTPUTS', '')

_SESSION_HITL_CACHE: Dict[str, Dict[str, Any]] = {}

_REPO_PUSH_PROGRESS: Dict[str, Dict[str, Any]] = {}
_REPO_PUSH_PROGRESS_LOCK = threading.Lock()
_REPO_PUSH_PROGRESS_TTL_SECONDS = 600.0

# Best-effort cancellation context for long-running remote repo finalize.
# Stored in-memory only; entries are removed when finalize completes/errors/cancels.
_REPO_PUSH_CANCEL_CTX: Dict[str, Dict[str, Any]] = {}
_REPO_PUSH_CANCEL_CTX_LOCK = threading.Lock()


def _set_repo_push_cancel_ctx(progress_id: Optional[str], ctx: Dict[str, Any]) -> None:
    if not progress_id:
        return
    with _REPO_PUSH_CANCEL_CTX_LOCK:
        _REPO_PUSH_CANCEL_CTX[progress_id] = dict(ctx or {})


def _get_repo_push_cancel_ctx(progress_id: Optional[str]) -> Optional[Dict[str, Any]]:
    if not progress_id:
        return None
    with _REPO_PUSH_CANCEL_CTX_LOCK:
        payload = _REPO_PUSH_CANCEL_CTX.get(progress_id)
        return dict(payload) if isinstance(payload, dict) else None


def _pop_repo_push_cancel_ctx(progress_id: Optional[str]) -> Optional[Dict[str, Any]]:
    if not progress_id:
        return None
    with _REPO_PUSH_CANCEL_CTX_LOCK:
        payload = _REPO_PUSH_CANCEL_CTX.pop(progress_id, None)
        return dict(payload) if isinstance(payload, dict) else None


def _schedule_repo_push_to_remote(progress_id: str, core_cfg: Dict[str, Any], *, logger: Optional[logging.Logger] = None) -> None:
    log = logger or getattr(app, 'logger', logging.getLogger(__name__))

    def _worker() -> None:
        try:
            # _push_repo_to_remote will update progress_id throughout packaging/uploading
            # and then queue remote finalization (also progress tracked).
            _push_repo_to_remote(
                core_cfg,
                logger=log,
                progress_id=progress_id,
                finalize_async=True,
                upload_only_injected_artifacts=False,
            )
        except Exception as exc:
            try:
                log.exception('[core.push_repo] background sync failed: %s', exc)
            except Exception:
                pass
            _update_repo_push_progress(progress_id, status='error', stage='error', detail=str(exc))

    try:
        threading.Thread(target=_worker, daemon=True, name=f'core-repo-push-{progress_id[:8]}').start()
    except Exception as exc:
        _update_repo_push_progress(progress_id, status='error', stage='error', detail=f'Failed to schedule repo push: {exc}')


def _init_repo_push_progress(
    progress_id: str,
    *,
    stage: str,
    detail: str,
    status: str = 'initializing',
    percent: Optional[float] = None,
) -> None:
    now = time.time()
    payload = {
        'progress_id': progress_id,
        'status': status,
        'stage': stage,
        'detail': detail,
        'percent': percent,
        'created_at': now,
        'updated_at': now,
    }
    with _REPO_PUSH_PROGRESS_LOCK:
        _REPO_PUSH_PROGRESS[progress_id] = payload


def _update_repo_push_progress(progress_id: Optional[str], **fields: Any) -> None:
    if not progress_id:
        return
    now = time.time()
    with _REPO_PUSH_PROGRESS_LOCK:
        entry = _REPO_PUSH_PROGRESS.get(progress_id)
        if not entry:
            entry = {
                'progress_id': progress_id,
                'created_at': now,
            }
            _REPO_PUSH_PROGRESS[progress_id] = entry
        entry.update(fields)
        entry['updated_at'] = now


def _expire_repo_push_progress() -> None:
    cutoff = time.time() - _REPO_PUSH_PROGRESS_TTL_SECONDS
    with _REPO_PUSH_PROGRESS_LOCK:
        stale = [
            progress_id
            for progress_id, payload in _REPO_PUSH_PROGRESS.items()
            if payload.get('status') in ('complete', 'error') and payload.get('updated_at', 0) < cutoff
        ]
        for progress_id in stale:
            _REPO_PUSH_PROGRESS.pop(progress_id, None)


def _get_repo_push_progress(progress_id: str) -> Optional[Dict[str, Any]]:
    _expire_repo_push_progress()
    with _REPO_PUSH_PROGRESS_LOCK:
        payload = _REPO_PUSH_PROGRESS.get(progress_id)
        if not payload:
            return None
        return dict(payload)


def _sanitize_venv_bin_path(path_value: Any) -> Optional[str]:
    if path_value in (None, '', False):
        return None
    try:
        text = str(path_value)
    except Exception:
        return None
    candidate = os.path.expanduser(text.strip())
    if not candidate:
        return None
    sanitized = candidate.rstrip('/\\')
    return sanitized or None


def _find_python_in_venv_bin(bin_dir: str) -> Optional[str]:
    for exe_name in PYTHON_EXECUTABLE_NAMES:
        candidate = os.path.join(bin_dir, exe_name)
        try:
            if os.path.isfile(candidate) and os.access(candidate, os.X_OK):
                return candidate
        except Exception:
            continue
    return None


def _venv_is_explicit(core_cfg: Dict[str, Any], preferred_venv_bin: Optional[str]) -> bool:
    sanitized = _sanitize_venv_bin_path(preferred_venv_bin)
    if not sanitized:
        return False

        # If the session has a file path but it's not accessible locally (common when the
        # webapp is not running on the CORE VM), prefer fetching the current session XML via
        # gRPC rather than using any stored mapping-by-session-id (which can be stale).
        sid = session.get('id')
        sid_int: Optional[int] = None
        if sid not in (None, ''):
            try:
                sid_int = int(sid)
            except Exception:
                sid_int = None
        if core_cfg and sid_int is not None and file_path:
            try:
                # Only do this when the provided file path doesn't exist locally.
                fp = str(file_path)
                if fp and (not os.path.exists(fp)):
                    out_dir = os.path.join(_outputs_dir(), 'core-sessions')
                    saved = _grpc_save_current_session_xml_with_config(core_cfg, out_dir, session_id=str(sid_int))
                    if saved and os.path.exists(saved):
                        try:
                            _update_xml_session_mapping(
                                saved,
                                sid_int,
                                scenario_name=session.get('scenario_name') or None,
                                core_host=core_cfg.get('host', CORE_HOST) if isinstance(core_cfg, dict) else None,
                                core_port=core_cfg.get('port', CORE_PORT) if isinstance(core_cfg, dict) else None,
                            )
                        except Exception:
                            pass
                        session['file'] = saved
                        session['_hitl_source'] = 'grpc.save_xml'
                        return _hitl_details_from_path(saved)
            except Exception:
                pass
        store = session_store if isinstance(session_store, dict) else _load_core_sessions_store()


class _SSHTunnelError(RuntimeError):
    """Raised when SSH tunneling operations fail."""


class _ForwardServer(socketserver.ThreadingTCPServer):
    daemon_threads = True
    allow_reuse_address = True


class RemoteRepoMissingError(RuntimeError):
    """Raised when the remote CORE repo directory does not exist."""

    def __init__(self, repo_path: str):
        self.repo_path = repo_path
        super().__init__(f'Remote repo not found at {repo_path}')


class CoreDaemonError(RuntimeError):
    """Base exception for remote core-daemon orchestration failures."""


class CoreDaemonMissingError(CoreDaemonError):
    def __init__(
        self,
        message: str,
        *,
        can_auto_start: bool = False,
        start_command: Optional[str] = None,
    ):
        super().__init__(message)
        self.can_auto_start = can_auto_start
        self.start_command = start_command or CORE_DAEMON_START_COMMAND


class CoreDaemonConflictError(CoreDaemonError):
    def __init__(self, message: str, *, pids: Optional[List[int]] = None):
        super().__init__(message)
        self.pids = list(pids or [])


def _ensure_paramiko_available() -> None:
    if paramiko is None:  # pragma: no cover - dependency missing
        raise RuntimeError('SSH tunneling requires the paramiko package. Install paramiko and retry.')


def _make_forward_handler(transport: Any, remote_host: str, remote_port: int):
    class _ForwardHandler(socketserver.BaseRequestHandler):
        def handle(self) -> None:
            logger = getattr(app, 'logger', logging.getLogger(__name__))
            try:
                if transport is None or not transport.is_active():
                    raise _SSHTunnelError('SSH transport is not active')
                chan = transport.open_channel(
                    kind='direct-tcpip',
                    dest_addr=(remote_host, remote_port),
                    src_addr=self.request.getpeername(),
                )
            except Exception as exc:
                try:
                    logger.error('SSH tunnel channel open failed to %s:%s', remote_host, remote_port, exc_info=True)
                except Exception:
                    pass
                raise _SSHTunnelError(f'Failed to open SSH channel to {remote_host}:{remote_port}: {exc}') from exc
            if chan is None:
                raise _SSHTunnelError('SSH channel creation returned None')
            try:
                while True:
                    rlist, _, _ = select.select([self.request, chan], [], [])
                    if self.request in rlist:
                        data = self.request.recv(1024)
                        if not data:
                            break
                        chan.sendall(data)
                    if chan in rlist:
                        data = chan.recv(1024)
                        if not data:
                            break
                        self.request.sendall(data)
            finally:
                try:
                    chan.close()
                except Exception:
                    logger.debug('Failed closing SSH channel', exc_info=True)
                try:
                    self.request.close()
                except Exception:
                    logger.debug('Failed closing local socket', exc_info=True)

    return _ForwardHandler


class _SshTunnel:
    def __init__(
        self,
        *,
        ssh_host: str,
        ssh_port: int,
        username: str,
        password: str | None,
        remote_host: str,
        remote_port: int,
        timeout: float = 15.0,
    ) -> None:
        _ensure_paramiko_available()
        self.ssh_host = ssh_host
        self.ssh_port = ssh_port
        self.username = username
        self.password = password or ''
        self.remote_host = remote_host
        self.remote_port = remote_port
        self.timeout = max(1.0, float(timeout))
        self.client: Any | None = None
        self.transport: Any | None = None
        self.server: _ForwardServer | None = None
        self.thread: threading.Thread | None = None
        self.local_port: int | None = None

    def start(self) -> Tuple[str, int]:
        logger = getattr(app, 'logger', logging.getLogger(__name__))
        self.client = paramiko.SSHClient()  # type: ignore[assignment]
        self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
        try:
            self.client.connect(
                hostname=self.ssh_host,
                port=int(self.ssh_port),
                username=self.username,
                password=self.password,
                look_for_keys=False,
                allow_agent=False,
                timeout=self.timeout,
                banner_timeout=self.timeout,
                auth_timeout=self.timeout,
            )
        except Exception as exc:
            raise _SSHTunnelError(f'Failed to establish SSH connection to {self.ssh_host}:{self.ssh_port}: {exc}') from exc
        self.transport = self.client.get_transport()
        if self.transport is None or not self.transport.is_active():
            self.close()
            raise _SSHTunnelError('SSH transport unavailable after connect')
        try:
            self.transport.set_keepalive(30)
        except Exception:
            pass
        handler = _make_forward_handler(self.transport, self.remote_host, self.remote_port)
        try:
            self.server = _ForwardServer(('127.0.0.1', 0), handler)
        except Exception as exc:
            self.close()
            raise _SSHTunnelError(f'Failed to create local forwarding server: {exc}') from exc
        self.local_port = int(self.server.server_address[1])
        self.thread = threading.Thread(target=self.server.serve_forever, name='core-ssh-forward', daemon=True)
        self.thread.start()
        logger.info('Established SSH tunnel %s@%s:%s -> %s:%s (local %s)', self.username, self.ssh_host, self.ssh_port, self.remote_host, self.remote_port, self.local_port)
        return '127.0.0.1', self.local_port

    def close(self) -> None:
        logger = getattr(app, 'logger', logging.getLogger(__name__))
        if self.server:
            try:
                self.server.shutdown()
            except Exception:
                logger.debug('Tunnel server shutdown failed', exc_info=True)
            try:
                self.server.server_close()
            except Exception:
                logger.debug('Tunnel server close failed', exc_info=True)
        self.server = None
        if self.client:
            try:
                self.client.close()
            except Exception:
                logger.debug('SSH client close failed', exc_info=True)
        self.client = None
        self.transport = None
        self.thread = None
        self.local_port = None


class _RemoteProcessHandle:
    """Popen-like wrapper for an SSH channel running the CLI remotely."""

    def __init__(self, *, channel: Any, client: Any) -> None:
        self.channel = channel
        self.client = client
        self._returncode: Optional[int] = None
        self._cleanup_done = False
        self._lock = threading.Lock()
        self._output_thread: threading.Thread | None = None

    def attach_output_thread(self, thread: threading.Thread) -> None:
        self._output_thread = thread

    def poll(self) -> Optional[int]:
        if self._returncode is not None:
            return self._returncode
        if self.channel is None:
            return self._returncode
        try:
            if self.channel.exit_status_ready():
                self._returncode = self.channel.recv_exit_status()
                self._cleanup()
        except Exception:
            self._returncode = self._returncode or -1
            self._cleanup()
        return self._returncode

    def wait(self, timeout: float | None = None) -> int:
        start = time.time()
        while True:
            rc = self.poll()
            if rc is not None:
                return rc
            if timeout is not None and (time.time() - start) >= timeout:
                raise TimeoutError('Remote process wait timed out')
            time.sleep(0.2)

    def terminate(self) -> None:
        with self._lock:
            try:
                if self.channel and not self.channel.closed:
                    self.channel.close()
            except Exception:
                pass
            self._returncode = self._returncode if self._returncode is not None else -1
            self._cleanup()

    def kill(self) -> None:
        self.terminate()

    def _cleanup(self) -> None:
        if self._cleanup_done:
            return
        self._cleanup_done = True
        try:
            if self._output_thread and self._output_thread.is_alive():
                self._output_thread.join(timeout=2.0)
        except Exception:
            pass
        try:
            if self.client:
                self.client.close()
        except Exception:
            pass
        self.client = None
        self.channel = None


_ANSI_ESCAPE_RE = re.compile(r"\x1B\[[0-?]*[ -/]*[@-~]")


def _strip_ansi(text: str) -> str:
    try:
        return _ANSI_ESCAPE_RE.sub('', text)
    except Exception:
        return text


def _relay_remote_channel_to_log(channel: Any, log_handle: Any) -> None:
    """Stream bytes from an SSH channel into the local log file."""

    try:
        while True:
            try:
                if channel.recv_ready():
                    chunk = channel.recv(REMOTE_LOG_CHUNK_SIZE)
                elif channel.exit_status_ready():
                    chunk = channel.recv(REMOTE_LOG_CHUNK_SIZE)
                    if not chunk:
                        break
                else:
                    time.sleep(0.2)
                    continue
            except Exception:
                break
            if not chunk:
                if channel.exit_status_ready():
                    break
                continue
            try:
                text = chunk.decode('utf-8', 'replace')
            except Exception:
                text = chunk.decode('latin-1', 'replace')
            log_handle.write(_strip_ansi(text))
    finally:
        try:
            log_handle.flush()
        except Exception:
            pass


def _exec_ssh_command(
    client: Any,
    command: str,
    *,
    timeout: float | None = 120.0,
    cancel_check: Any = None,
    check: bool = False,
) -> tuple[int, str, str]:
    """Execute a command over SSH and capture stdout/stderr.

    Uses a wall-clock timeout to avoid hanging on blocking reads.
    When check=True, raises RuntimeError on non-zero exit codes.
    """

    if command:
        _append_core_ui_log('DEBUG', f'[ssh.exec] COMMAND START\n{command}')

    start = time.time()
    stdin, stdout, stderr = client.exec_command(command)
    channel = getattr(stdout, 'channel', None)

    stdout_chunks: list[bytes] = []
    stderr_chunks: list[bytes] = []

    try:
        if channel is not None:
            try:
                channel.settimeout(1.0)
            except Exception:
                pass
        while True:
            if channel is None:
                break
            try:
                if cancel_check is not None and bool(cancel_check()):
                    try:
                        if not channel.closed:
                            channel.close()
                    except Exception:
                        pass
                    raise TimeoutError('SSH command cancelled')
            except TimeoutError:
                raise
            except Exception:
                # Never allow cancel_check bugs to wedge execution.
                pass
            try:
                if channel.recv_ready():
                    stdout_chunks.append(channel.recv(REMOTE_LOG_CHUNK_SIZE))
                if channel.recv_stderr_ready():
                    stderr_chunks.append(channel.recv_stderr(REMOTE_LOG_CHUNK_SIZE))
            except Exception:
                # Don't wedge on transient read errors.
                pass

            try:
                if channel.exit_status_ready():
                    # Drain remaining buffered output then exit.
                    if not channel.recv_ready() and not channel.recv_stderr_ready():
                        break
            except Exception:
                break

            if timeout is not None:
                if (time.time() - start) >= max(0.1, float(timeout)):
                    try:
                        if channel is not None and not channel.closed:
                            channel.close()
                    except Exception:
                        pass
                    raise TimeoutError(f'SSH command timed out after {timeout:.0f}s')

            time.sleep(0.15)
    finally:
        try:
            stdin.close()
        except Exception:
            pass

    exit_code = 0
    try:
        if channel is not None:
            exit_code = int(channel.recv_exit_status())
    except Exception:
        exit_code = 0

    def _decode(blob: Any) -> str:
        if isinstance(blob, bytes):
            return blob.decode('utf-8', 'ignore')
        return str(blob or '')

    stdout_text = _decode(b''.join(stdout_chunks))
    stderr_text = _decode(b''.join(stderr_chunks))

    log_lines = [f'[ssh.exec] EXIT {exit_code}']
    stdout_tag = 'stdout (empty)' if not stdout_text else f'stdout:\n{stdout_text}'
    stderr_tag = 'stderr (empty)' if not stderr_text else f'stderr:\n{stderr_text}'
    log_lines.append(stdout_tag)
    log_lines.append(stderr_tag)
    log_level = 'WARN' if exit_code != 0 else 'DEBUG'
    _append_core_ui_log(log_level, '\n'.join(log_lines))

    if check and exit_code != 0:
        detail = stderr_text.strip() or stdout_text.strip() or f'Exit {exit_code}'
        raise RuntimeError(f'SSH command failed: {detail}')

    return exit_code, stdout_text, stderr_text


def _is_repo_push_cancel_requested(progress_id: Optional[str]) -> bool:
    if not progress_id:
        return False
    payload = _get_repo_push_progress(progress_id)
    if not payload:
        return False
    if payload.get('status') == 'cancelled':
        return True
    return bool(payload.get('cancel_requested'))


def _open_ssh_client(core_cfg: Dict[str, Any]) -> Any:
    cfg = _require_core_ssh_credentials(core_cfg)
    _ensure_paramiko_available()
    client = paramiko.SSHClient()  # type: ignore[assignment]
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
    client.connect(
        hostname=cfg.get('ssh_host') or cfg.get('host') or 'localhost',
        port=int(cfg.get('ssh_port') or 22),
        username=cfg.get('ssh_username'),
        password=cfg.get('ssh_password') or '',
        look_for_keys=False,
        allow_agent=False,
        timeout=30.0,
        banner_timeout=30.0,
        auth_timeout=30.0,
    )
    return client


def _remote_expand_path(sftp: Any, path: str | None) -> str:
    raw = (path or '').strip() or '.'
    try:
        if raw.startswith('~/'):
            home = sftp.normalize('.')
            return posixpath.normpath(posixpath.join(home, raw[2:]))
        if raw.startswith('~'):
            return posixpath.normpath(sftp.normalize(raw))
        if raw.startswith('/'):
            return posixpath.normpath(raw)
        home = sftp.normalize('.')
        return posixpath.normpath(posixpath.join(home, raw))
    except Exception:
        return raw


def _remote_base_dir(sftp: Any) -> str:
    return _remote_expand_path(sftp, REMOTE_BASE_DIR_ENV)


def _remote_static_repo_dir(sftp: Any) -> str:
    return _remote_expand_path(sftp, REMOTE_STATIC_REPO_ENV)


def _remote_path_join(*parts: str) -> str:
    cleaned = [p for p in parts if p not in (None, '', '.')]
    if not cleaned:
        return '/'
    return posixpath.normpath(posixpath.join(*cleaned))


def _remote_mkdirs(client: Any, path: str) -> None:
    quoted = shlex.quote(path)
    _exec_ssh_command(client, f"mkdir -p {quoted}")


def _remote_remove_path(client: Any, path: str) -> None:
    quoted = shlex.quote(path)
    _exec_ssh_command(client, f"rm -rf {quoted}")


def _resolve_repo_push_allowed_outputs(upload_only_injected_artifacts: bool) -> List[str]:
    if REPO_PUSH_ALLOWED_OUTPUTS_ENV:
        out: List[str] = []
        for raw in REPO_PUSH_ALLOWED_OUTPUTS_ENV.split(','):
            item = raw.strip().replace('\\', '/')
            if item:
                out.append(item)
        return out
    if upload_only_injected_artifacts:
        return [
            'outputs/installed_vuln_catalogs',
            'outputs/installed_generators',
        ]
    return [
        'outputs/installed_vuln_catalogs',
        'outputs/installed_generators',
        'outputs/scenarios-',
        'outputs/tmp-exec-',
        'outputs/plans',
    ]


def _flow_required_installed_generator_outputs(
    flag_assignments: list[dict[str, Any]] | None,
    *,
    repo_root: str,
) -> List[str]:
    required: set[str] = set()
    try:
        if _coerce_bool(os.environ.get('CORETG_FLOW_SYNC_VULN_CATALOGS', '0')):
            required.add('outputs/installed_vuln_catalogs')
    except Exception:
        pass
    if not flag_assignments:
        return sorted(required)

    try:
        gens, _ = _flag_generators_from_enabled_sources()
    except Exception:
        gens = []
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
    except Exception:
        node_gens = []

    gen_by_key: dict[tuple[str, str], dict[str, Any]] = {}
    for g in (gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid:
            gen_by_key[('flag_generators', gid)] = g
    for g in (node_gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid:
            gen_by_key[('flag_node_generators', gid)] = g

    repo_root_abs = os.path.abspath(repo_root)
    for fa in (flag_assignments or []):
        if not isinstance(fa, dict):
            continue
        gid = str(fa.get('id') or fa.get('generator_id') or '').strip()
        if not gid:
            continue
        cat = str(fa.get('generator_catalog') or '').strip()
        if not cat:
            kind = str(fa.get('type') or '').strip().lower()
            if kind == 'flag-node-generator':
                cat = 'flag_node_generators'
            else:
                cat = 'flag_generators'
        if cat not in {'flag_generators', 'flag_node_generators'}:
            continue
        gen = gen_by_key.get((cat, gid))
        if not isinstance(gen, dict):
            continue
        src_path = str(gen.get('_source_path') or gen.get('source', {}).get('path') or '').strip()
        if not src_path:
            continue
        try:
            if os.path.isabs(src_path):
                src_abs = os.path.abspath(src_path)
            else:
                src_abs = os.path.abspath(os.path.join(repo_root_abs, src_path))
            if os.path.commonpath([repo_root_abs, src_abs]) == repo_root_abs:
                rel = os.path.relpath(src_abs, repo_root_abs)
            else:
                rel = src_path
        except Exception:
            rel = src_path
        rel = rel.replace('\\', '/').lstrip('/')
        if rel.endswith('manifest.yaml') or rel.endswith('manifest.yml'):
            rel = posixpath.dirname(rel)
        if rel.startswith('outputs/installed_generators/'):
            required.add(rel)

    return sorted(required)


def _flow_required_generator_repo_paths(
    flag_assignments: list[dict[str, Any]] | None,
    *,
    repo_root: str,
) -> List[str]:
    """Return repo-relative paths required to run the selected generators."""
    required: set[str] = set()
    required.add('scripts/run_flag_generator.py')
    # core_topo_gen is always required for manifest loading on remote VMs
    required.add('core_topo_gen')
    if not flag_assignments:
        return sorted(required)

    try:
        gens, _ = _flag_generators_from_enabled_sources()
    except Exception as exc:
        app.logger.error('[flow.generator] failed to load flag_generators: %s', exc)
        gens = []
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
    except Exception as exc:
        app.logger.error('[flow.generator] failed to load flag_node_generators: %s', exc)
        node_gens = []

    gen_by_key: dict[tuple[str, str], dict[str, Any]] = {}
    for g in (gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid:
            gen_by_key[('flag_generators', gid)] = g
    for g in (node_gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid:
            gen_by_key[('flag_node_generators', gid)] = g

    # Track lookup failures for diagnostics
    lookup_failures: list[str] = []
    path_failures: list[str] = []

    repo_root_abs = os.path.abspath(repo_root)
    for fa in (flag_assignments or []):
        if not isinstance(fa, dict):
            continue
        gid = str(fa.get('id') or fa.get('generator_id') or '').strip()
        if not gid:
            continue
        cat = str(fa.get('generator_catalog') or '').strip()
        if not cat:
            kind = str(fa.get('type') or '').strip().lower()
            if kind == 'flag-node-generator':
                cat = 'flag_node_generators'
            else:
                cat = 'flag_generators'
        if cat not in {'flag_generators', 'flag_node_generators'}:
            continue
        gen = gen_by_key.get((cat, gid))
        if not isinstance(gen, dict):
            lookup_failures.append(f'{cat}/{gid}')
            continue
        src_path = str(gen.get('_source_path') or gen.get('source', {}).get('path') or '').strip()
        if not src_path:
            path_failures.append(f'{cat}/{gid} (no _source_path or source.path)')
            continue
        try:
            if os.path.isabs(src_path):
                src_abs = os.path.abspath(src_path)
            else:
                src_abs = os.path.abspath(os.path.join(repo_root_abs, src_path))
            if os.path.commonpath([repo_root_abs, src_abs]) == repo_root_abs:
                rel = os.path.relpath(src_abs, repo_root_abs)
            else:
                rel = src_path
        except Exception:
            rel = src_path
        rel = rel.replace('\\', '/').lstrip('/')
        if rel.endswith('manifest.yaml') or rel.endswith('manifest.yml'):
            rel = posixpath.dirname(rel)
        if rel:
            required.add(rel)

    # Log diagnostic information about failures
    if lookup_failures:
        available_keys = sorted(gen_by_key.keys())
        app.logger.error(
            '[flow.generator] generator lookup failed for %d assignment(s): %s. '
            'Available generators: %s',
            len(lookup_failures),
            lookup_failures[:5],  # Limit to avoid huge logs
            available_keys[:20] if len(available_keys) <= 20 else f'{available_keys[:10]} ... ({len(available_keys)} total)',
        )
    if path_failures:
        app.logger.error(
            '[flow.generator] generator source path missing for: %s',
            path_failures,
        )

    return sorted(required)


def _should_exclude_repo_member(rel_path: str, *, allowed_outputs: Optional[List[str]] = None) -> bool:
    if not rel_path:
        return False
    parts = [part for part in Path(rel_path).parts if part not in ('', '.')]
    if not parts:
        return False
    # Allow required subsets of outputs/ to be pushed to the remote host.
    # Keep repo sync lean while ensuring execute has the needed artifacts.
    try:
        rel_norm = rel_path.replace('\\', '/').lstrip('/')
        if rel_norm.startswith('outputs'):
            if rel_norm == 'outputs':
                return False
            if not allowed_outputs:
                allowed_outputs = _resolve_repo_push_allowed_outputs(False)
            if any(
                rel_norm == p
                or rel_norm.startswith(p + '/')
                for p in allowed_outputs
            ):
                return False
            return True
    except Exception:
        pass
    for part in parts:
        if part in REPO_PUSH_EXCLUDE_DIRS:
            return True
    leaf = parts[-1]
    for pattern in REPO_PUSH_EXCLUDE_PATTERNS:
        if fnmatch.fnmatch(leaf, pattern):
            return True
    return False


def _create_local_repo_archive(src_dir: str, dest_basename: str, *, allowed_outputs: Optional[List[str]] = None) -> str:
    base_name = dest_basename.strip('/') or 'core-topo-gen'
    tmp_fd, tmp_path = tempfile.mkstemp(prefix='coretg_repo_', suffix='.tar.gz')
    os.close(tmp_fd)

    def _filter(member: tarfile.TarInfo) -> tarfile.TarInfo | None:
        rel_name = member.name
        prefix = f"{base_name}/"
        if rel_name.startswith(prefix):
            rel_name = rel_name[len(prefix):]
        elif rel_name == base_name:
            rel_name = ''
        if rel_name and _should_exclude_repo_member(rel_name, allowed_outputs=allowed_outputs):
            return None
        return member

    with tarfile.open(tmp_path, 'w:gz') as tar:
        tar.add(src_dir, arcname=base_name, filter=_filter)
    return tmp_path


def _repo_skip_if_unchanged_enabled() -> bool:
    try:
        return str(REPO_PUSH_SKIP_IF_UNCHANGED).strip().lower() not in ('0', 'false', 'no', 'off')
    except Exception:
        return True


def _compute_repo_fingerprint(repo_root: str, *, allowed_outputs: Optional[List[str]] = None) -> str:
    """Compute a lightweight fingerprint of repo contents (paths + size + mtime)."""
    import hashlib
    h = hashlib.sha256()
    root = os.path.abspath(repo_root)
    for dirpath, dirnames, filenames in os.walk(root):
        rel_dir = os.path.relpath(dirpath, root)
        if rel_dir == '.':
            rel_dir = ''
        # Filter directories in-place to avoid walking excluded trees
        dirnames[:] = [
            d for d in dirnames
            if not _should_exclude_repo_member(os.path.join(rel_dir, d), allowed_outputs=allowed_outputs)
        ]
        for fname in filenames:
            rel_path = os.path.join(rel_dir, fname) if rel_dir else fname
            if _should_exclude_repo_member(rel_path, allowed_outputs=allowed_outputs):
                continue
            full_path = os.path.join(dirpath, fname)
            try:
                st = os.stat(full_path)
            except Exception:
                continue
            h.update(rel_path.replace('\\', '/').encode('utf-8', 'ignore'))
            h.update(b'\x00')
            h.update(str(int(st.st_size)).encode('utf-8'))
            h.update(b'\x00')
            h.update(str(int(st.st_mtime)).encode('utf-8'))
            h.update(b'\n')
    return h.hexdigest()


def _compute_repo_fingerprint_includes(repo_root: str, include_paths: list[str]) -> str:
    """Compute fingerprint for specific relative paths only."""
    import hashlib

    h = hashlib.sha256()
    root = os.path.abspath(repo_root)
    for raw in include_paths or []:
        rel = str(raw or '').replace('\\', '/').lstrip('/')
        if not rel:
            continue
        full_path = os.path.join(root, rel)
        if not os.path.exists(full_path):
            continue
        if os.path.isdir(full_path):
            for dirpath, dirnames, filenames in os.walk(full_path):
                rel_dir = os.path.relpath(dirpath, root)
                if rel_dir == '.':
                    rel_dir = ''
                dirnames[:] = [d for d in dirnames if not d.startswith('.')]  # keep minimal
                for fname in filenames:
                    rel_path = os.path.join(rel_dir, fname) if rel_dir else fname
                    full_file = os.path.join(dirpath, fname)
                    try:
                        st = os.stat(full_file)
                    except Exception:
                        continue
                    h.update(rel_path.replace('\\', '/').encode('utf-8', 'ignore'))
                    h.update(b'\x00')
                    h.update(str(int(st.st_size)).encode('utf-8'))
                    h.update(b'\x00')
                    h.update(str(int(st.st_mtime)).encode('utf-8'))
                    h.update(b'\n')
        else:
            try:
                st = os.stat(full_path)
            except Exception:
                continue
            h.update(rel.replace('\\', '/').encode('utf-8', 'ignore'))
            h.update(b'\x00')
            h.update(str(int(st.st_size)).encode('utf-8'))
            h.update(b'\x00')
            h.update(str(int(st.st_mtime)).encode('utf-8'))
            h.update(b'\n')
    return h.hexdigest()


def _create_local_repo_archive_from_paths(
    src_dir: str,
    dest_basename: str,
    include_paths: list[str],
) -> str:
    base_name = dest_basename.strip('/') or 'core-topo-gen'
    tmp_fd, tmp_path = tempfile.mkstemp(prefix='coretg_repo_', suffix='.tar.gz')
    os.close(tmp_fd)

    root = os.path.abspath(src_dir)
    with tarfile.open(tmp_path, 'w:gz') as tar:
        for raw in include_paths or []:
            rel = str(raw or '').replace('\\', '/').lstrip('/')
            if not rel:
                continue
            full = os.path.join(root, rel)
            if not os.path.exists(full):
                continue
            arcname = posixpath.join(base_name, rel)
            tar.add(full, arcname=arcname)
    return tmp_path


def _read_remote_repo_hash(sftp: Any, remote_repo: str) -> Optional[str]:
    try:
        remote_hash_path = _remote_path_join(remote_repo, '.coretg_repo_hash')
        with sftp.open(remote_hash_path, 'r') as rf:
            raw = rf.read().decode('utf-8', 'ignore').strip()
            return raw or None
    except Exception:
        return None


def _write_remote_repo_hash(client: Any, remote_repo: str, hash_value: str) -> None:
    try:
        remote_hash_path = _remote_path_join(remote_repo, '.coretg_repo_hash')
        cmd = f"printf %s {shlex.quote(hash_value)} > {shlex.quote(remote_hash_path)}"
        _exec_ssh_command(client, cmd)
    except Exception:
        pass


def _push_repo_to_remote(
    core_cfg: Dict[str, Any],
    *,
    logger: Optional[logging.Logger] = None,
    progress_id: Optional[str] = None,
    finalize_async: bool = False,
    upload_only_injected_artifacts: bool = False,
    allowed_outputs_override: Optional[List[str]] = None,
    include_repo_paths: Optional[List[str]] = None,
    log_handle: Any | None = None,
) -> Dict[str, Any]:
    cfg = _require_core_ssh_credentials(core_cfg)
    repo_root = _get_repo_root()
    log = logger or getattr(app, 'logger', logging.getLogger(__name__))
    log.info('[remote-sync] starting repo sync to CORE VM')
    _update_repo_push_progress(progress_id, status='packaging', stage='packaging', percent=2.0, detail='Creating repository archive')
    client = _open_ssh_client(cfg)
    sftp = None
    archive_path = None
    try:
        sftp = client.open_sftp()
        log.info('[remote-sync] SFTP session opened')
        remote_repo = _remote_static_repo_dir(sftp)
        log.info('[remote-sync] remote_repo=%s', remote_repo)
        remote_parent = posixpath.dirname(remote_repo.rstrip('/')) or '/'
        base_name = os.path.basename(remote_repo.rstrip('/')) or 'core-topo-gen'
        allowed_outputs = allowed_outputs_override
        if not allowed_outputs:
            allowed_outputs = _resolve_repo_push_allowed_outputs(bool(upload_only_injected_artifacts))
        include_repo_paths = [
            p for p in (include_repo_paths or [])
            if isinstance(p, str) and str(p).strip()
        ]
        log.info('[remote-sync] allowed_outputs=%s', allowed_outputs)
        if include_repo_paths:
            log.info('[remote-sync] include_paths=%s', include_repo_paths)
        try:
            if log_handle:
                log_handle.write(f"[remote] Repo upload: remote_repo={remote_repo}\n")
                log_handle.write(f"[remote] Repo upload: allowed_outputs={allowed_outputs}\n")
                if include_repo_paths:
                    log_handle.write(f"[remote] Repo upload: include_paths={include_repo_paths}\n")
                log_handle.flush()
        except Exception:
            pass
        if _repo_skip_if_unchanged_enabled():
            try:
                if include_repo_paths:
                    local_hash = _compute_repo_fingerprint_includes(repo_root, include_repo_paths)
                else:
                    local_hash = _compute_repo_fingerprint(repo_root, allowed_outputs=allowed_outputs)
                log.info('[remote-sync] computed local hash')
                remote_hash = _read_remote_repo_hash(sftp, remote_repo)
                log.info('[remote-sync] remote hash=%s', remote_hash or 'missing')
                if remote_hash and local_hash == remote_hash:
                    _update_repo_push_progress(progress_id, status='complete', stage='complete', percent=100.0, detail='Repository unchanged; upload skipped.')
                    log.info('[remote-sync] Repository unchanged; skipping upload to %s', remote_repo)
                    return {'repo_path': remote_repo, 'progress_id': progress_id, 'skipped': True}
            except Exception:
                pass
        try:
            if log_handle:
                log_handle.write("[remote] Repo upload: packaging snapshot\n")
                log_handle.flush()
        except Exception:
            pass
        log.info('[remote-sync] creating archive snapshot')
        t0 = time.monotonic()
        if include_repo_paths:
            archive_path = _create_local_repo_archive_from_paths(repo_root, base_name, include_repo_paths)
        else:
            archive_path = _create_local_repo_archive(repo_root, base_name, allowed_outputs=allowed_outputs)
        t1 = time.monotonic()
        try:
            archive_size = os.path.getsize(archive_path) if archive_path else 0
        except Exception:
            archive_size = 0
        log.info('[remote-sync] archive ready: %s (size=%s bytes, took=%.2fs)', archive_path, archive_size, (t1 - t0))
        _update_repo_push_progress(progress_id, status='packaging', stage='packaging', percent=8.0, detail='Repository archive ready.')
        remote_archive = _remote_path_join(remote_parent, f"{uuid.uuid4().hex}.tar.gz")
        try:
            if log_handle:
                log_handle.write(f"[remote] Repo upload: uploading snapshot to {remote_archive}\n")
                log_handle.flush()
        except Exception:
            pass
        log.info('[remote-sync] uploading archive to %s', remote_archive)
        _update_repo_push_progress(progress_id, status='uploading', stage='uploading', percent=12.0, detail='Uploading snapshot to CORE host')
        sftp.put(archive_path, remote_archive)
        log.info('[remote-sync] upload complete')
        _update_repo_push_progress(progress_id, status='uploading', stage='uploaded', percent=40.0, detail='Upload complete; preparing remote finalize')
        if finalize_async:
            _update_repo_push_progress(progress_id, status='finalizing', stage='remote', percent=45.0, detail='Remote finalization queued')
            _schedule_remote_repo_finalize(
                progress_id,
                cfg,
                remote_repo=remote_repo,
                remote_parent=remote_parent,
                remote_archive=remote_archive,
                logger=log,
            )
            return {'repo_path': remote_repo, 'progress_id': progress_id, 'finalizing': True}
        extract_script = (
            f"set -euo pipefail; mkdir -p {shlex.quote(remote_parent)}; "
            f"rm -rf {shlex.quote(remote_repo)}; "
            f"tar -xzf {shlex.quote(remote_archive)} -C {shlex.quote(remote_parent)}; "
            f"rm -f {shlex.quote(remote_archive)}"
        )
        try:
            if log_handle:
                log_handle.write("[remote] Repo upload: extracting snapshot on CORE host\n")
                log_handle.flush()
        except Exception:
            pass
        log.info('[remote-sync] extracting archive on CORE VM')
        _update_repo_push_progress(progress_id, status='finalizing', stage='remote', percent=60.0, detail='Extracting snapshot on CORE host')
        _exec_ssh_command(client, f"bash -lc {shlex.quote(extract_script)}", timeout=None, check=True)
        log.info('[remote-sync] extract complete')
        _update_repo_push_progress(progress_id, status='finalizing', stage='remote', percent=95.0, detail='Cleaning temporary archive')
        log.info('[remote-sync] Repository uploaded to %s', remote_repo)
        try:
            if log_handle:
                log_handle.write("[remote] Repo upload: snapshot extracted and ready.\n")
                log_handle.flush()
        except Exception:
            pass
        try:
            if _repo_skip_if_unchanged_enabled():
                if include_repo_paths:
                    local_hash = _compute_repo_fingerprint_includes(repo_root, include_repo_paths)
                else:
                    local_hash = _compute_repo_fingerprint(repo_root, allowed_outputs=allowed_outputs)
                _write_remote_repo_hash(client, remote_repo, local_hash)
        except Exception:
            pass
        _update_repo_push_progress(progress_id, status='complete', stage='complete', percent=100.0, detail='Repository ready on remote host.')
        return {'repo_path': remote_repo, 'progress_id': progress_id}
    finally:
        try:
            if archive_path and os.path.exists(archive_path):
                os.remove(archive_path)
        except Exception:
            pass
        try:
            if sftp:
                sftp.close()
        except Exception:
            pass
        try:
            client.close()
        except Exception:
            pass


def _schedule_remote_repo_finalize(
    progress_id: Optional[str],
    core_cfg: Dict[str, Any],
    *,
    remote_repo: str,
    remote_parent: str,
    remote_archive: str,
    logger: Optional[logging.Logger] = None,
) -> None:
    if not progress_id:
        # No async tracking requested; fall back to synchronous finalize.
        extract_script = (
            f"set -euo pipefail; mkdir -p {shlex.quote(remote_parent)}; "
            f"rm -rf {shlex.quote(remote_repo)}; "
            f"tar -xzf {shlex.quote(remote_archive)} -C {shlex.quote(remote_parent)}; "
            f"rm -f {shlex.quote(remote_archive)}"
        )
        try:
            client = _open_ssh_client(core_cfg)
            try:
                _exec_ssh_command(client, f"bash -lc {shlex.quote(extract_script)}", timeout=None, check=True)
            finally:
                client.close()
        except Exception:
            pass
        return

    def _worker() -> None:
        client: Any | None = None
        pidfile: Optional[str] = None
        try:
            if _is_repo_push_cancel_requested(progress_id):
                _update_repo_push_progress(progress_id, status='cancelled', stage='cancelled', detail='Cancelled by user.')
                return
            _update_repo_push_progress(progress_id, status='finalizing', stage='cleanup', percent=55.0, detail='Removing previous repository')
            client = _open_ssh_client(core_cfg)

            # Track the remote tar PID so a cancel request can kill it.
            try:
                pidfile = _remote_path_join(
                    posixpath.dirname(remote_archive.rstrip('/')) or remote_parent or '/',
                    f"coretg_finalize_{progress_id}.pid" if progress_id else f"coretg_finalize_{uuid.uuid4().hex}.pid",
                )
            except Exception:
                pidfile = _remote_path_join(remote_parent, f"coretg_finalize_{uuid.uuid4().hex}.pid")

            try:
                _set_repo_push_cancel_ctx(
                    progress_id,
                    {
                        'core_cfg': dict(core_cfg),
                        'remote_pidfile': pidfile,
                        'remote_archive': remote_archive,
                        'remote_parent': remote_parent,
                        'remote_repo': remote_repo,
                    },
                )
            except Exception:
                pass

            _exec_ssh_command(
                client,
                f"mkdir -p {shlex.quote(remote_parent)}",
                timeout=None,
                cancel_check=lambda: _is_repo_push_cancel_requested(progress_id),
                check=True,
            )
            _exec_ssh_command(
                client,
                f"rm -rf {shlex.quote(remote_repo)}",
                timeout=None,
                cancel_check=lambda: _is_repo_push_cancel_requested(progress_id),
                check=True,
            )
            _update_repo_push_progress(progress_id, status='finalizing', stage='extract', percent=75.0, detail='Extracting new snapshot on CORE host')

            # Run tar under a tracked pidfile.
            tar_script = (
                "set -e; "
                f"pidfile={shlex.quote(pidfile or '')}; "
                "rm -f -- \"$pidfile\" 2>/dev/null || true; "
                f"( tar -xzf {shlex.quote(remote_archive)} -C {shlex.quote(remote_parent)} ) & "
                "pid=$!; "
                "echo \"$pid\" > \"$pidfile\"; "
                "wait \"$pid\"; "
                "rc=$?; "
                "rm -f -- \"$pidfile\" 2>/dev/null || true; "
                "exit \"$rc\""
            )
            _exec_ssh_command(
                client,
                f"sh -lc {shlex.quote(tar_script)}",
                timeout=None,
                cancel_check=lambda: _is_repo_push_cancel_requested(progress_id),
                check=True,
            )
            _update_repo_push_progress(progress_id, status='finalizing', stage='cleanup', percent=90.0, detail='Cleaning temporary archive')
            _exec_ssh_command(
                client,
                f"rm -f {shlex.quote(remote_archive)}",
                timeout=None,
                cancel_check=lambda: _is_repo_push_cancel_requested(progress_id),
                check=True,
            )
            _update_repo_push_progress(progress_id, status='complete', stage='complete', percent=100.0, detail='Repository ready on remote host.')
            if logger:
                logger.info('[remote-sync] Repository finalized at %s', remote_repo)
        except TimeoutError as exc:
            # Used both for true timeouts and cooperative cancel.
            if 'cancelled' in str(exc).lower():
                _update_repo_push_progress(progress_id, status='cancelled', stage='cancelled', detail='Cancelled by user.')
                try:
                    if client:
                        _exec_ssh_command(client, f"rm -f {shlex.quote(remote_archive)}", timeout=30.0)
                except Exception:
                    pass
                return
            raise
        except Exception as exc:
            try:
                if _is_repo_push_cancel_requested(progress_id):
                    _update_repo_push_progress(progress_id, status='cancelled', stage='cancelled', detail='Cancelled by user.')
                    try:
                        if client and pidfile:
                            _exec_ssh_command(client, f"rm -f {shlex.quote(pidfile)}", timeout=10.0)
                    except Exception:
                        pass
                    return
            except Exception:
                pass
            if logger:
                logger.exception('[remote-sync] finalize failed: %s', exc)
            _update_repo_push_progress(progress_id, status='error', stage='error', detail=str(exc))
            try:
                if client:
                    _exec_ssh_command(client, f"rm -f {shlex.quote(remote_archive)}", timeout=60.0)
            except Exception:
                pass
        finally:
            try:
                _pop_repo_push_cancel_ctx(progress_id)
            except Exception:
                pass
            if client:
                try:
                    client.close()
                except Exception:
                    pass

    try:
        threading.Thread(target=_worker, daemon=True).start()
    except Exception:
        _update_repo_push_progress(progress_id, status='error', stage='error', detail='Failed to schedule remote finalization')


def _iter_values_by_key(obj: Any, keys: set[str]) -> Iterator[Any]:
    if isinstance(obj, dict):
        for k, v in obj.items():
            if k in keys:
                yield v
            yield from _iter_values_by_key(v, keys)
    elif isinstance(obj, list):
        for item in obj:
            yield from _iter_values_by_key(item, keys)


def _normalize_inject_dest_dir_for_validation(raw: str, *, default: str = '/tmp') -> str:
    s = str(raw or '').strip()
    if not s:
        return default
    if not s.startswith('/'):
        return default
    parts = [p for p in s.split('/') if p]
    if any(p == '..' for p in parts):
        return default
    return '/' + '/'.join(parts) if parts else default


def _normalize_inject_dest_path_for_validation(raw: str, src_path: str | None, *, default_dir: str = '/tmp') -> str:
    src_base = os.path.basename(str(src_path or '').strip()) if src_path else ''
    dest = str(raw or '').strip()
    if not dest or not dest.startswith('/'):
        return f"{default_dir.rstrip('/')}/{src_base}" if src_base else default_dir
    parts = [p for p in dest.split('/') if p]
    if any(p == '..' for p in parts):
        return f"{default_dir.rstrip('/')}/{src_base}" if src_base else default_dir
    normalized = '/' + '/'.join(parts) if parts else default_dir
    if dest.endswith('/'):
        if src_base:
            normalized = normalized.rstrip('/') + '/' + src_base
    return normalized


def _extract_inject_dirs_from_payload(payload: Dict[str, Any]) -> List[str]:
    if not payload or not isinstance(payload, dict):
        return []
    full = payload.get('full_preview') if isinstance(payload.get('full_preview'), dict) else None
    root = full if isinstance(full, dict) else payload

    inject_specs: List[str] = []
    for v in _iter_values_by_key(root, {'inject_files'}):
        if isinstance(v, list):
            for item in v:
                s = str(item or '').strip()
                if s:
                    inject_specs.append(s)
        elif isinstance(v, str) and v.strip():
            inject_specs.append(v.strip())

    if not inject_specs:
        return []

    dests: List[str] = []
    for raw in inject_specs:
        dest = ''
        if '->' in raw:
            _, dest = raw.split('->', 1)
        elif '=>' in raw:
            _, dest = raw.split('=>', 1)
        dests.append(_normalize_inject_dest_dir_for_validation(dest))

    seen: set[str] = set()
    out: List[str] = []
    for d in dests:
        if not d or d in seen:
            continue
        seen.add(d)
        out.append(d)
    return out


def _extract_inject_files_from_payload(payload: Dict[str, Any]) -> List[str]:
    if not payload or not isinstance(payload, dict):
        return []
    full = payload.get('full_preview') if isinstance(payload.get('full_preview'), dict) else None
    root = full if isinstance(full, dict) else payload

    inject_specs: List[str] = []
    for v in _iter_values_by_key(root, {'inject_files'}):
        if isinstance(v, list):
            for item in v:
                s = str(item or '').strip()
                if s:
                    inject_specs.append(s)
        elif isinstance(v, str) and v.strip():
            inject_specs.append(v.strip())

    if not inject_specs:
        return []

    files: List[str] = []
    def _looks_like_output_key(text: str) -> bool:
        s = str(text or '').strip()
        if not s:
            return False
        if '/' in s:
            return False
        return '(' in s and ')' in s

    for raw in inject_specs:
        src = raw
        dest = ''
        if '->' in raw:
            src, dest = raw.split('->', 1)
        elif '=>' in raw:
            src, dest = raw.split('=>', 1)
        src = str(src or '').strip()
        if _looks_like_output_key(src):
            # Skip symbolic keys like File(path) when no outputs are resolved.
            continue
        dest_path = _normalize_inject_dest_path_for_validation(dest, src)
        if dest_path:
            files.append(dest_path)

    seen: set[str] = set()
    out: List[str] = []
    for f in files:
        if not f or f in seen:
            continue
        seen.add(f)
        out.append(f)
    return out


def _load_plan_preview_payload_from_path(path: str, scenario_label: str | None) -> Dict[str, Any] | None:
    if not path:
        return None
    try:
        ap = os.path.abspath(str(path))
    except Exception:
        ap = str(path)
    if not ap or not os.path.exists(ap):
        return None
    try:
        if ap.lower().endswith('.xml'):
            return _load_plan_preview_from_xml(ap, scenario_label)
        with open(ap, 'r', encoding='utf-8') as f:
            payload = json.load(f)
        return payload if isinstance(payload, dict) else None
    except Exception:
        return None


def _extract_inject_dirs_from_plan_xml(scenario_xml_path: str, scenario_label: str | None) -> List[str]:
    payload = None
    try:
        payload = _load_plan_preview_from_xml(scenario_xml_path, scenario_label)
    except Exception:
        payload = None
    dirs_from_plan = _extract_inject_dirs_from_payload(payload) if isinstance(payload, dict) else []

    # Merge in Flow sequencing inject overrides saved in FlagSequencing/FlowState.
    inject_specs: List[str] = []
    flow_has_artifacts = False
    try:
        flow_state = _flow_state_from_xml_path(scenario_xml_path, scenario_label)
        if isinstance(flow_state, dict):
            for v in _iter_values_by_key(flow_state, {'inject_files_override', 'inject_files'}):
                if isinstance(v, list):
                    for item in v:
                        s = str(item or '').strip()
                        if s:
                            inject_specs.append(s)
                elif isinstance(v, str) and v.strip():
                    inject_specs.append(v.strip())
            try:
                assigns = flow_state.get('flag_assignments') if isinstance(flow_state, dict) else None
                if isinstance(assigns, list):
                    for entry in assigns:
                        if not isinstance(entry, dict):
                            continue
                        ro = entry.get('resolved_outputs')
                        oo = entry.get('output_overrides')
                        if isinstance(ro, dict) and ro:
                            flow_has_artifacts = True
                            break
                        if isinstance(oo, dict) and oo:
                            flow_has_artifacts = True
                            break
            except Exception:
                flow_has_artifacts = flow_has_artifacts
    except Exception:
        pass

    dirs_from_flow: List[str] = []
    if inject_specs:
        dests: List[str] = []
        for raw in inject_specs:
            dest = ''
            if '->' in raw:
                _, dest = raw.split('->', 1)
            elif '=>' in raw:
                _, dest = raw.split('=>', 1)
            dests.append(_normalize_inject_dest_dir_for_validation(dest))
        seen: set[str] = set()
        for d in dests:
            if not d or d in seen:
                continue
            seen.add(d)
            dirs_from_flow.append(d)
    if flow_has_artifacts and not dirs_from_flow:
        dirs_from_flow.append('/flow_artifacts')

    merged: List[str] = []
    seen_all: set[str] = set()
    for src in (dirs_from_plan, dirs_from_flow):
        for d in src:
            if d in seen_all:
                continue
            seen_all.add(d)
            merged.append(d)
    return merged

    if not inject_specs:
        return []


def _extract_inject_files_from_plan_xml(scenario_xml_path: str, scenario_label: str | None) -> List[str]:
    payload = None
    try:
        payload = _load_plan_preview_from_xml(scenario_xml_path, scenario_label)
    except Exception:
        payload = None
    files_from_plan = _extract_inject_files_from_payload(payload) if isinstance(payload, dict) else []

    inject_specs: List[str] = []
    try:
        flow_state = _flow_state_from_xml_path(scenario_xml_path, scenario_label)
        if isinstance(flow_state, dict):
            for v in _iter_values_by_key(flow_state, {'inject_files_override', 'inject_files'}):
                if isinstance(v, list):
                    for item in v:
                        s = str(item or '').strip()
                        if s:
                            inject_specs.append(s)
                elif isinstance(v, str) and v.strip():
                    inject_specs.append(v.strip())
    except Exception:
        pass

    files_from_flow: List[str] = []
    if inject_specs:
        for raw in inject_specs:
            src = raw
            dest = ''
            if '->' in raw:
                src, dest = raw.split('->', 1)
            elif '=>' in raw:
                src, dest = raw.split('=>', 1)
            src = str(src or '').strip()
            dest_path = _normalize_inject_dest_path_for_validation(dest, src)
            if dest_path:
                files_from_flow.append(dest_path)

    merged: List[str] = []
    seen_all: set[str] = set()
    for src in (files_from_plan, files_from_flow):
        for f in src:
            if f in seen_all:
                continue
            seen_all.add(f)
            merged.append(f)
    return merged


def _extract_inject_specs_from_flow_state(scenario_xml_path: str, scenario_label: str | None) -> List[str]:
    specs: List[str] = []
    try:
        flow_state = _flow_state_from_xml_path(scenario_xml_path, scenario_label)
    except Exception:
        flow_state = None
    if not isinstance(flow_state, dict):
        return specs
    assigns = flow_state.get('flag_assignments') if isinstance(flow_state.get('flag_assignments'), list) else []
    chain_ids = []
    try:
        chain_ids = [str(x).strip() for x in (flow_state.get('chain_ids') or []) if str(x).strip()]
    except Exception:
        chain_ids = []
    chain_set = set(chain_ids)
    for entry in assigns or []:
        if not isinstance(entry, dict):
            continue
        nid_raw = str(entry.get('node_id') or '').strip()
        if chain_set and nid_raw and nid_raw not in chain_set:
            continue
        for key in ('inject_files_override', 'inject_files'):
            vals = entry.get(key)
            if isinstance(vals, list):
                for item in vals:
                    s = str(item or '').strip()
                    if s:
                        specs.append(s)
            elif isinstance(vals, str) and vals.strip():
                specs.append(vals.strip())
    return specs


def _extract_inject_expected_by_node(scenario_xml_path: str, scenario_label: str | None) -> Dict[str, List[str]]:
    expected: Dict[str, List[str]] = {}
    try:
        flow_state = _flow_state_from_xml_path(scenario_xml_path, scenario_label)
    except Exception:
        flow_state = None
    if not isinstance(flow_state, dict):
        return expected
    chain_ids = []
    try:
        chain_ids = [str(x).strip() for x in (flow_state.get('chain_ids') or []) if str(x).strip()]
    except Exception:
        chain_ids = []
    chain_set = set(chain_ids)
    assigns = flow_state.get('flag_assignments') if isinstance(flow_state.get('flag_assignments'), list) else []

    def _basename(path: str) -> str:
        parts = str(path or '').replace('\\', '/').split('/')
        parts = [p for p in parts if p]
        return parts[-1] if parts else ''

    def _looks_like_output_key(text: str) -> bool:
        s = str(text or '').strip()
        if not s:
            return False
        if '/' in s:
            return False
        return '(' in s and ')' in s

    for entry in assigns or []:
        if not isinstance(entry, dict):
            continue
        nid_raw = str(entry.get('node_id') or '').strip()
        if not nid_raw:
            continue
        if chain_set and nid_raw not in chain_set:
            continue
        node_name = None
        try:
            exp = _expected_from_plan_preview(scenario_xml_path, scenario_label).get(int(nid_raw)) or {}
            node_name = str(exp.get('name') or '').strip() or None
        except Exception:
            node_name = None
        node_key = node_name or f"node-{nid_raw}"

        per_node: List[str] = []
        detail_list = entry.get('inject_files_detail') if isinstance(entry.get('inject_files_detail'), list) else []
        if detail_list:
            for item in detail_list:
                if not isinstance(item, dict):
                    continue
                path = str(item.get('path') or '').strip()
                if path:
                    per_node.append(path)
        else:
            injects = entry.get('inject_files') if isinstance(entry.get('inject_files'), list) else []
            for raw in injects:
                text = str(raw or '').strip()
                if not text:
                    continue
                if _looks_like_output_key(text):
                    continue
                if '->' in text or '=>' in text:
                    sep = '->' if '->' in text else '=>'
                    src, dest = text.split(sep, 1)
                    src = str(src or '').strip()
                    dest = str(dest or '').strip()
                    dest_dir = dest if dest.startswith('/') else '/tmp'
                    base = _basename(src)
                    if base:
                        per_node.append(f"{dest_dir.rstrip('/')}/{base}")
                    else:
                        per_node.append(dest_dir)
                    continue
                if text.startswith('/'):
                    per_node.append(text)
                else:
                    per_node.append(f"/tmp/{text.lstrip('./')}")
        if per_node:
            seen = set()
            unique = []
            for p in per_node:
                if p in seen:
                    continue
                seen.add(p)
                unique.append(p)
            expected[node_key] = unique
    return expected


def _extract_inject_node_ids_from_flow_state(scenario_xml_path: str, scenario_label: str | None) -> set[int]:
    node_ids: set[int] = set()
    try:
        flow_state = _flow_state_from_xml_path(scenario_xml_path, scenario_label)
    except Exception:
        flow_state = None
    if not isinstance(flow_state, dict):
        return node_ids
    chain_ids = []
    try:
        chain_ids = [str(x).strip() for x in (flow_state.get('chain_ids') or []) if str(x).strip()]
    except Exception:
        chain_ids = []
    chain_set = set(chain_ids)
    assigns = flow_state.get('flag_assignments') if isinstance(flow_state.get('flag_assignments'), list) else []
    for entry in assigns or []:
        if not isinstance(entry, dict):
            continue
        nid_raw = str(entry.get('node_id') or '').strip()
        if not nid_raw:
            continue
        if chain_set and nid_raw not in chain_set:
            continue
        inj = entry.get('inject_files')
        if not (isinstance(inj, list) and any(str(x or '').strip() for x in inj)):
            continue
        try:
            node_ids.add(int(nid_raw))
        except Exception:
            continue
    return node_ids

    dests: List[str] = []
    for raw in inject_specs:
        dest = ''
        if '->' in raw:
            _, dest = raw.split('->', 1)
        elif '=>' in raw:
            _, dest = raw.split('=>', 1)
        dests.append(_normalize_inject_dest_dir_for_validation(dest))

    seen: set[str] = set()
    out: List[str] = []
    for d in dests:
        if not d or d in seen:
            continue
        seen.add(d)
        out.append(d)
    return out


def _extract_expected_docker_and_vuln_nodes_from_plan_xml(
    scenario_xml_path: str,
    scenario_label: str | None,
) -> tuple[List[str], List[str]]:
    payload = None
    try:
        payload = _load_plan_preview_from_xml(scenario_xml_path, scenario_label)
    except Exception:
        payload = None
    if not payload or not isinstance(payload, dict):
        return [], []
    full = payload.get('full_preview') if isinstance(payload.get('full_preview'), dict) else None
    root = full if isinstance(full, dict) else payload
    hosts = root.get('hosts') if isinstance(root, dict) else None
    if not isinstance(hosts, list):
        return [], []

    expected_docker: List[str] = []
    expected_vuln: List[str] = []
    for h in hosts:
        if not isinstance(h, dict):
            continue
        name = str(h.get('name') or '').strip()
        if not name:
            continue
        role = str(h.get('role') or '').strip().lower()
        kind = str(h.get('kind') or '').strip().lower()
        metadata = h.get('metadata') if isinstance(h.get('metadata'), dict) else {}
        has_flow = isinstance(metadata.get('flow_flag'), dict)
        is_docker = role == 'docker' or kind == 'docker' or has_flow
        if is_docker:
            expected_docker.append(name)
        vulns = h.get('vulnerabilities')
        if isinstance(vulns, list) and any(vulns):
            expected_vuln.append(name)

    def _dedupe(vals: List[str]) -> List[str]:
        seen: set[str] = set()
        out: List[str] = []
        for v in vals:
            if v in seen:
                continue
            seen.add(v)
            out.append(v)
        return out

    return _dedupe(expected_docker), _dedupe(expected_vuln)


def _extract_flow_artifact_dirs_from_plan(preview_plan_path: str, *, prefer_mount_dir: bool = False) -> List[str]:
    """Extract local artifact directories referenced by the plan.

    By default we look for `artifacts_dir` keys.
    When `prefer_mount_dir` is True, we first look for `mount_dir` keys (typically
    pointing at a filtered `.../injected` directory). If none are found, we
    fall back to `artifacts_dir` to preserve backwards compatibility.
    """
    payload = None
    try:
        if str(preview_plan_path).lower().endswith('.xml'):
            payload = _load_plan_preview_from_xml(preview_plan_path, None)
    except Exception:
        payload = None
    if not payload or not isinstance(payload, dict):
        return []

    full = payload.get('full_preview') if isinstance(payload, dict) else None
    root = full if isinstance(full, dict) else payload

    dirs: List[str] = []
    if prefer_mount_dir:
        for v in _iter_values_by_key(root, {'mount_dir'}):
            if isinstance(v, str) and v:
                dirs.append(v)
        if not dirs:
            for v in _iter_values_by_key(root, {'artifacts_dir'}):
                if isinstance(v, str) and v:
                    dirs.append(v)
    else:
        for v in _iter_values_by_key(root, {'artifacts_dir'}):
            if isinstance(v, str) and v:
                dirs.append(v)

    # Fallback: scan metadata.flow (PlanPreview metadata) and FlowState in XML.
    if not dirs:
        try:
            meta = payload.get('metadata') if isinstance(payload, dict) else None
            flow = meta.get('flow') if isinstance(meta, dict) else None
            if isinstance(flow, dict):
                if prefer_mount_dir:
                    for v in _iter_values_by_key(flow, {'mount_dir'}):
                        if isinstance(v, str) and v:
                            dirs.append(v)
                    if not dirs:
                        for v in _iter_values_by_key(flow, {'artifacts_dir'}):
                            if isinstance(v, str) and v:
                                dirs.append(v)
                else:
                    for v in _iter_values_by_key(flow, {'artifacts_dir'}):
                        if isinstance(v, str) and v:
                            dirs.append(v)
        except Exception:
            pass
    if not dirs and str(preview_plan_path).lower().endswith('.xml'):
        try:
            flow_state = _flow_state_from_xml_path(preview_plan_path, None)
            if isinstance(flow_state, dict):
                if prefer_mount_dir:
                    for v in _iter_values_by_key(flow_state, {'mount_dir'}):
                        if isinstance(v, str) and v:
                            dirs.append(v)
                    if not dirs:
                        for v in _iter_values_by_key(flow_state, {'artifacts_dir'}):
                            if isinstance(v, str) and v:
                                dirs.append(v)
                else:
                    for v in _iter_values_by_key(flow_state, {'artifacts_dir'}):
                        if isinstance(v, str) and v:
                            dirs.append(v)
        except Exception:
            pass

    seen: set[str] = set()
    out: List[str] = []
    for d in dirs:
        if d not in seen:
            seen.add(d)
            out.append(d)
    return out


def _upload_flow_artifacts_for_plan_to_remote(
    *,
    client: Any,
    sftp: Any,
    preview_plan_path: str,
    log_handle: Any,
    upload_only_injected_artifacts: bool = False,
) -> None:
    """Upload any locally-generated flow artifacts to the CORE VM.

    The preview plan references absolute paths (typically under /tmp/vulns/...).
    When running the CLI on a remote CORE VM, those directories must exist on
    the remote filesystem or bind mounts / docker cp will see nothing.
    """
    artifact_dirs = _extract_flow_artifact_dirs_from_plan(
        preview_plan_path,
        prefer_mount_dir=bool(upload_only_injected_artifacts),
    )
    if not artifact_dirs:
        try:
            log_handle.write('[remote] No flow artifact dirs referenced in preview plan\n')
        except Exception:
            pass
        return

    try:
        if upload_only_injected_artifacts:
            log_handle.write('[remote] Flow artifacts upload mode: mount_dir only (prefer injected)\n')
    except Exception:
        pass

    repo_root = _get_repo_root()
    outputs_root = os.path.join(repo_root, 'outputs')
    allowed_prefixes = (
        '/tmp/vulns',
        os.path.join(outputs_root, 'tmp-exec-'),
        os.path.join(outputs_root, 'scenarios-'),
    )
    upload_dirs = [
        d
        for d in artifact_dirs
        if any(d == p or d.startswith(p + '/') for p in allowed_prefixes)
    ]
    skipped = [d for d in artifact_dirs if d not in upload_dirs]
    if skipped:
        try:
            log_handle.write(f"[remote] Skipping {len(skipped)} artifact dirs (outside /tmp/vulns)\n")
        except Exception:
            pass
    if not upload_dirs:
        return

    made_dirs: set[str] = set()
    copied_files = 0
    copied_bytes = 0
    for local_dir in upload_dirs:
        if not os.path.isdir(local_dir):
            try:
                log_handle.write(f"[remote] flow.artifacts.upload skip (missing): {local_dir}\n")
            except Exception:
                pass
            continue

        for root, dirs, files in os.walk(local_dir):
            rel = os.path.relpath(root, local_dir)
            rel = '' if rel == '.' else rel
            remote_root = local_dir if not rel else _remote_path_join(local_dir, rel)
            if remote_root not in made_dirs:
                try:
                    _remote_mkdirs(client, remote_root)
                    made_dirs.add(remote_root)
                except Exception:
                    pass

            for dn in dirs:
                rp_dir = _remote_path_join(remote_root, dn)
                if rp_dir in made_dirs:
                    continue
                try:
                    _remote_mkdirs(client, rp_dir)
                    made_dirs.add(rp_dir)
                except Exception:
                    pass

            for fn in files:
                lp = os.path.join(root, fn)
                if not os.path.isfile(lp):
                    continue
                rp = _remote_path_join(remote_root, fn)
                try:
                    sftp.put(lp, rp)
                    copied_files += 1
                    try:
                        copied_bytes += int(os.path.getsize(lp))
                    except Exception:
                        pass
                except Exception:
                    try:
                        log_handle.write(f"[remote] flow.artifacts.upload failed: {lp} -> {rp}\n")
                    except Exception:
                        pass

        try:
            log_handle.write(f"[remote] flow.artifacts.uploaded dir={local_dir}\n")
        except Exception:
            pass

    try:
        log_handle.write(f"[remote] flow.artifacts.upload complete files={copied_files} bytes={copied_bytes}\n")
    except Exception:
        pass


def _prepare_remote_cli_context(
    *,
    client: Any,
    run_id: str,
    xml_path: str,
    preview_plan_path: str | None,
    log_handle: Any,
    upload_only_injected_artifacts: bool = False,
) -> Dict[str, Any]:
    """Upload required artifacts before starting the remote CLI."""

    sftp = client.open_sftp()
    try:
        base_dir = _remote_base_dir(sftp)
        run_dir = _remote_path_join(base_dir, REMOTE_RUNS_SUBDIR, run_id)
        _remote_mkdirs(client, run_dir)
        repo_dir = _remote_static_repo_dir(sftp)
        try:
            sftp.stat(repo_dir)
        except Exception as exc:
            raise RemoteRepoMissingError(repo_dir) from exc
        package_dir = _remote_path_join(repo_dir, 'core_topo_gen')
        package_init = _remote_path_join(package_dir, '__init__.py')
        try:
            sftp.stat(package_dir)
            sftp.stat(package_init)
        except Exception as exc:
            raise RemoteRepoMissingError(repo_dir) from exc
        try:
            log_handle.write(f"[remote] Using repo at {repo_dir}\n")
        except Exception:
            pass
        # Ensure reports/outputs/uploads directories exist for CLI outputs
        for subdir in ('reports', 'outputs', 'uploads'):
            _remote_mkdirs(client, _remote_path_join(repo_dir, subdir))
        remote_xml_path = _remote_path_join(run_dir, os.path.basename(xml_path))
        sftp.put(xml_path, remote_xml_path)
        remote_preview_plan = None
        if preview_plan_path:
            remote_preview_plan = _remote_path_join(run_dir, os.path.basename(preview_plan_path))
            sftp.put(preview_plan_path, remote_preview_plan)
            # If the preview/flow plan references local /tmp/vulns artifact directories,
            # upload them to the CORE VM so the remote run can use them.
            _upload_flow_artifacts_for_plan_to_remote(
                client=client,
                sftp=sftp,
                preview_plan_path=preview_plan_path,
                log_handle=log_handle,
                upload_only_injected_artifacts=bool(upload_only_injected_artifacts),
            )
        context = {
            'base_dir': base_dir,
            'run_dir': run_dir,
            'repo_dir': repo_dir,
            'xml_path': remote_xml_path,
            'preview_plan_path': remote_preview_plan,
        }
        if not preview_plan_path:
            try:
                _upload_flow_artifacts_for_plan_to_remote(
                    client=client,
                    sftp=sftp,
                    preview_plan_path=xml_path,
                    log_handle=log_handle,
                    upload_only_injected_artifacts=bool(upload_only_injected_artifacts),
                )
            except Exception:
                pass
        return context
    finally:
        try:
            sftp.close()
        except Exception:
            pass


def _build_python_probe_command(
    host: str,
    port: int,
    timeout: float,
    *,
    interpreter: str = 'python3',
) -> str:
    host_literal = json.dumps(host)
    interpreter_safe = shlex.quote(interpreter)
    script = textwrap.dedent(
        f"""{interpreter_safe} - <<'PY'
import socket
import sys
host = {host_literal}
port = {int(port)}
timeout = {timeout:.2f}
try:
    sock = socket.create_connection((host, port), timeout=timeout)
except Exception as exc:
    print("ERROR:", exc)
    sys.exit(1)
else:
    sock.close()
    print("OK")
PY
"""
    )
    return script


def _candidate_remote_python_interpreters(core_cfg: Dict[str, Any]) -> List[str]:
    venv_bin = _sanitize_venv_bin_path((core_cfg or {}).get('venv_bin'))
    candidates: List[str] = []
    if venv_bin:
        sanitized = venv_bin.rstrip('/\\')
        if sanitized:
            for exe_name in ('python3', 'python'):
                candidates.append(posixpath.join(sanitized, exe_name))
    candidates.extend(['python3', 'python'])
    ordered: List[str] = []
    seen: set[str] = set()
    for entry in candidates:
        normalized = str(entry or '').strip()
        if not normalized or normalized in seen:
            continue
        seen.add(normalized)
        ordered.append(normalized)
    return ordered


def _select_remote_python_interpreter(client: Any, core_cfg: Dict[str, Any]) -> str:
    candidates = _candidate_remote_python_interpreters(core_cfg)
    if not candidates:
        candidates = ['python3', 'python']
    last_error = None
    for candidate in candidates:
        cmd = f"{shlex.quote(candidate)} -V"
        code, _out, err = _exec_ssh_command(client, cmd)
        if code == 0:
            return candidate
        last_error = err or f"{candidate} unavailable"
    detail = f" ({last_error})" if last_error else ''
    raise RuntimeError(f"Unable to locate a python interpreter on the CORE host{detail}")


def _resolve_remote_artifact_path(
    sftp: Any,
    candidate: str,
    *,
    repo_dir: str | None,
    run_dir: str | None,
) -> str:
    path = (candidate or '').strip()
    if not path:
        return ''
    if path.startswith('/'):
        return posixpath.normpath(path)
    options: list[str] = []
    if repo_dir:
        options.append(_remote_path_join(repo_dir, path))
    if run_dir:
        options.append(_remote_path_join(run_dir, path))
    options.append(posixpath.normpath(path))
    for option in options:
        try:
            sftp.stat(option)
            return posixpath.normpath(option)
        except Exception:
            continue
    return posixpath.normpath(options[0])


def _sync_remote_artifacts(meta: Dict[str, Any]) -> None:
    if not meta.get('remote'):
        return
    if meta.get('remote_artifacts_synced'):
        return
    log_path = meta.get('log_path')
    if not log_path or not os.path.exists(log_path):
        return
    try:
        with open(log_path, 'r', encoding='utf-8', errors='ignore') as fh:
            log_text = fh.read()
    except Exception:
        return
    remote_report = _extract_report_path_from_text(log_text, require_exists=False)
    remote_summary = _extract_summary_path_from_text(log_text, require_exists=False)
    if not remote_report and not remote_summary:
        meta['remote_artifacts_synced'] = True
        return
    core_cfg = meta.get('core_cfg') if isinstance(meta.get('core_cfg'), dict) else None
    if not core_cfg:
        return
    try:
        client = _open_ssh_client(core_cfg)
        sftp = client.open_sftp()
    except Exception as exc:
        try:
            app.logger.warning('[remote-sync] SSH connect failed: %s', exc)
        except Exception:
            pass
        return
    downloaded: list[tuple[str, str]] = []
    try:
        repo_dir = meta.get('remote_repo_dir')
        run_dir = meta.get('remote_run_dir')
        if remote_report:
            resolved_report = _resolve_remote_artifact_path(sftp, remote_report, repo_dir=repo_dir, run_dir=run_dir)
            local_report = os.path.join(_reports_dir(), os.path.basename(resolved_report))
            sftp.get(resolved_report, local_report)
            downloaded.append(('report', local_report))
        if remote_summary:
            resolved_summary = _resolve_remote_artifact_path(sftp, remote_summary, repo_dir=repo_dir, run_dir=run_dir)
            local_summary = os.path.join(_reports_dir(), os.path.basename(resolved_summary))
            sftp.get(resolved_summary, local_summary)
            downloaded.append(('summary', local_summary))
    except Exception as exc:
        try:
            app.logger.warning('[remote-sync] Failed copying artifacts: %s', exc)
        except Exception:
            pass
        return
    finally:
        try:
            sftp.close()
        except Exception:
            pass
        try:
            client.close()
        except Exception:
            pass
    if downloaded:
        try:
            with open(log_path, 'a', encoding='utf-8') as fh:
                for kind, local_path in downloaded:
                    if kind == 'report':
                        fh.write(f"Scenario report written to {local_path}\n")
                    elif kind == 'summary':
                        fh.write(f"Scenario summary written to {local_path}\n")
        except Exception:
            pass
    meta['remote_artifacts_synced'] = True


def _cleanup_remote_workspace(meta: Dict[str, Any]) -> None:
    if not meta.get('remote'):
        return
    if meta.get('remote_workspace_cleaned'):
        return
    run_dir = meta.get('remote_run_dir')
    if not run_dir:
        meta['remote_workspace_cleaned'] = True
        return
    core_cfg = meta.get('core_cfg') if isinstance(meta.get('core_cfg'), dict) else None
    if not core_cfg:
        return
    try:
        client = _open_ssh_client(core_cfg)
    except Exception:
        return
    try:
        _remote_remove_path(client, run_dir)
    except Exception:
        pass
    finally:
        try:
            client.close()
        except Exception:
            pass
    meta['remote_workspace_cleaned'] = True

@contextlib.contextmanager
def _core_connection_via_ssh(core_cfg: Dict[str, Any]) -> Iterator[Tuple[str, int]]:
    tunnel: _SshTunnel | None = None
    try:
        tunnel = _SshTunnel(
            ssh_host=str(core_cfg.get('ssh_host') or core_cfg.get('host') or 'localhost'),
            ssh_port=int(core_cfg.get('ssh_port') or 22),
            username=str(core_cfg.get('ssh_username') or ''),
            password=core_cfg.get('ssh_password'),
            remote_host=str(core_cfg.get('host') or 'localhost'),
            remote_port=int(core_cfg.get('port') or 50051),
        )
        host, port = tunnel.start()
        yield host, port
    except Exception as exc:
        ssh_host = str(core_cfg.get('ssh_host') or core_cfg.get('host') or 'localhost')
        ssh_port = int(core_cfg.get('ssh_port') or 22)
        remote_host = str(core_cfg.get('host') or 'localhost')
        remote_port = int(core_cfg.get('port') or 50051)
        ssh_user = str(core_cfg.get('ssh_username') or '')
        
        # Helper for common Docker misconfiguration
        hint = ""
        if remote_host in ('localhost', '127.0.0.1') and core_cfg.get('ssh_enabled'):
            hint = " (Hint: 'localhost' usually means THIS container, not the SSH server. Did you mean to use the SSH server's internal checking address?)"

        app.logger.error(
            "SSH tunnel failed: %s. Context: ssh=%s:%s user=%s target=%s:%s%s",
            exc, ssh_host, ssh_port, ssh_user, remote_host, remote_port, hint
        )
        raise exc
    finally:
        if tunnel:
            tunnel.close()


@contextlib.contextmanager
def _core_connection(core_cfg: Dict[str, Any]) -> Iterator[Tuple[str, int]]:
    """Yield a host/port tuple for connecting to CORE.

    If SSH is enabled, open a temporary tunnel; otherwise return the raw host/port.
    """

    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_enabled = _coerce_bool(cfg.get('ssh_enabled'))
    if ssh_enabled:
        cfg = _require_core_ssh_credentials(cfg)
        with _core_connection_via_ssh(cfg) as forwarded:
            yield forwarded
        return

    host = str(cfg.get('host') or '127.0.0.1')
    try:
        port = int(cfg.get('port') or CORE_PORT)
    except Exception:
        port = CORE_PORT
    yield host, port


def _coerce_bool(value: Any) -> bool:
    if isinstance(value, bool):
        return value
    if isinstance(value, (int, float)):
        return bool(value)
    if isinstance(value, str):
        return value.strip().lower() in {'1', 'true', 'yes', 'on', 'y'}
    return False


def _normalize_core_config(raw: Any, *, include_password: bool = True) -> Dict[str, Any]:
    base = raw if isinstance(raw, dict) else {}
    nested = base.get('ssh') if isinstance(base.get('ssh'), dict) else {}
    host = str(base.get('host') or CORE_HOST or 'localhost')
    try:
        port = int(base.get('port') if base.get('port') not in (None, '') else CORE_PORT)
    except Exception:
        port = CORE_PORT
    ssh_enabled = True
    ssh_host = str(base.get('ssh_host') or nested.get('host') or host)
    try:
        ssh_port = int(base.get('ssh_port') if base.get('ssh_port') not in (None, '') else (nested.get('port') or 22))
    except Exception:
        ssh_port = 22
    ssh_username = str(base.get('ssh_username') or nested.get('username') or '')
    ssh_password_val: str | None = None
    if include_password:
        pw_source = base.get('ssh_password')
        if pw_source in (None, '') and isinstance(nested, dict):
            pw_source = nested.get('password')
        ssh_password_val = str(pw_source) if pw_source not in (None, '') else ''
    env_override_raw = os.environ.get('CORE_REMOTE_VENV_BIN')
    env_override = _sanitize_venv_bin_path(env_override_raw)
    default_venv = _sanitize_venv_bin_path(DEFAULT_CORE_VENV_BIN)
    venv_candidate = base.get('venv_bin')
    if venv_candidate in (None, '') and isinstance(nested, dict):
        venv_candidate = nested.get('venv_bin') or nested.get('core_venv_bin')
    if venv_candidate in (None, ''):
        venv_candidate = base.get('core_venv_bin')
    if venv_candidate in (None, ''):
        venv_candidate = env_override or DEFAULT_CORE_VENV_BIN
    venv_bin = _sanitize_venv_bin_path(venv_candidate) or default_venv or DEFAULT_CORE_VENV_BIN
    venv_user_override = bool(base.get('venv_user_override'))
    if not venv_user_override and isinstance(nested, dict):
        venv_user_override = bool(nested.get('venv_user_override'))
    if not venv_user_override:
        if env_override and venv_bin == env_override:
            venv_user_override = True
        elif default_venv and venv_bin == default_venv:
            venv_user_override = False
        else:
            venv_user_override = bool(venv_bin and default_venv and venv_bin != default_venv)
    cfg: Dict[str, Any] = {
        'host': host,
        'port': port,
        'ssh_enabled': ssh_enabled,
        'ssh_host': ssh_host,
        'ssh_port': ssh_port,
        'ssh_username': ssh_username,
        'venv_bin': venv_bin,
        'venv_user_override': bool(venv_user_override),
    }
    if include_password:
        cfg['ssh_password'] = ssh_password_val or ''
    return cfg


def _require_core_ssh_credentials(core_cfg: Dict[str, Any]) -> Dict[str, Any]:
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_host = str(cfg.get('ssh_host') or cfg.get('host') or '').strip()
    if not ssh_host:
        raise RuntimeError('SSH host is required for CORE VM connection.')
    ssh_user = str(cfg.get('ssh_username') or '').strip()
    if not ssh_user:
        raise RuntimeError('SSH username is required for CORE VM connection.')
    ssh_password = cfg.get('ssh_password')
    if ssh_password is None or str(ssh_password).strip() == '':
        raise RuntimeError('SSH password is required for CORE VM connection.')
    return cfg


def _extract_optional_core_config(raw: Any, *, include_password: bool = False) -> Dict[str, Any] | None:
    if not isinstance(raw, dict):
        return None
    candidate = dict(raw)

    def _has_value(val: Any) -> bool:
        if val is None:
            return False
        if isinstance(val, str):
            return val.strip() != ''
        return True

    significant_keys = ('host', 'port', 'ssh_host', 'ssh_port', 'ssh_username')
    metadata_signal_keys = ('vm_key', 'vm_name', 'vm_node', 'core_secret_id')
    has_signal = any(_has_value(candidate.get(key)) for key in significant_keys)
    if not has_signal:
        nested = candidate.get('ssh') if isinstance(candidate.get('ssh'), dict) else {}
        has_signal = any(_has_value(nested.get(key)) for key in ('host', 'port', 'username', 'password'))
    if not has_signal:
        has_signal = any(_has_value(candidate.get(key)) for key in metadata_signal_keys)
    if include_password and not has_signal:
        has_signal = _has_value(candidate.get('ssh_password')) or (
            isinstance(candidate.get('ssh'), dict) and _has_value(candidate['ssh'].get('password'))
        )
    if not has_signal:
        return None

    provided_fields: Dict[str, Any] = {
        key: candidate.get(key)
        for key in ('host', 'port', 'ssh_host', 'ssh_port', 'ssh_username', 'venv_bin', 'venv_user_override')
    }
    if include_password:
        provided_fields['ssh_password'] = candidate.get('ssh_password')

    normalized = _normalize_core_config(candidate, include_password=include_password)
    extras: Dict[str, Any] = {}
    for key, value in candidate.items():
        if key in {'host', 'port', 'ssh_enabled', 'ssh_host', 'ssh_port', 'ssh_username', 'ssh_password', 'venv_bin', 'venv_user_override', 'ssh'}:
            continue
        extras[key] = value
    for field, original in provided_fields.items():
        if field == 'ssh_password' and not include_password:
            continue
        if not _has_value(original):
            normalized.pop(field, None)
    if extras:
        normalized.update(extras)
    if not include_password:
        normalized.pop('ssh_password', None)
    if not normalized and extras:
        normalized = extras
    return normalized

def _scrub_scenario_core_config(raw: Any) -> Dict[str, Any] | None:
    """Remove sensitive fields while preserving VM metadata for history/logging."""

    if not isinstance(raw, dict):
        return None
    cleaned: Dict[str, Any] = {}
    for key, value in raw.items():
        if key == 'ssh_password':
            continue
        if key == 'ssh' and isinstance(value, dict):
            nested = {k: v for k, v in value.items() if k != 'password'}
            if nested:
                cleaned['ssh'] = nested
            continue
        cleaned[key] = value
    normalized = _normalize_core_config(cleaned, include_password=False)
    extras: Dict[str, Any] = {}
    for key, value in cleaned.items():
        if key in _CORE_FIELD_KEYS:
            continue
        extras[key] = value
    if extras:
        normalized = dict(normalized)
        normalized.update(extras)
    return normalized


def _merge_core_configs(*configs: Any, include_password: bool = True) -> Dict[str, Any]:
    base = _core_backend_defaults(include_password=include_password)
    merged = dict(base)
    extras: Dict[str, Any] = {}
    core_secret_id: Optional[str] = None
    for cfg in configs:
        if cfg is None:
            continue
        normalized = _extract_optional_core_config(cfg, include_password=include_password)
        if not normalized:
            # Password-only override should still apply when include_password requested
            if include_password and isinstance(cfg, dict) and cfg.get('ssh_password') not in (None, ''):
                normalized = {'ssh_password': cfg.get('ssh_password')}
            else:
                continue
        for key, value in normalized.items():
            if key in _CORE_FIELD_KEYS:
                merged[key] = value
            elif key == 'core_secret_id' and value not in (None, ''):
                core_secret_id = str(value)
                extras['core_secret_id'] = core_secret_id
            else:
                extras[key] = value
    if include_password and (not merged.get('ssh_password')) and core_secret_id:
        try:
            secret_record = _load_core_credentials(core_secret_id)
        except RuntimeError:
            secret_record = None
        if secret_record:
            password_plain = secret_record.get('ssh_password_plain') or secret_record.get('password_plain') or ''
            if password_plain and not merged.get('ssh_password'):
                merged['ssh_password'] = password_plain
            for field in ('host', 'port', 'grpc_host', 'grpc_port', 'ssh_host', 'ssh_port', 'ssh_username', 'venv_bin'):
                stored_val = secret_record.get(field)
                if stored_val in (None, ''):
                    continue
                target_field = 'host' if field == 'grpc_host' else 'port' if field == 'grpc_port' else field
                if target_field in {'host', 'port'}:
                    if not merged.get(target_field):
                        merged[target_field] = stored_val
                else:
                    if merged.get(target_field) in (None, '', 0):
                        merged[target_field] = stored_val
            extras.setdefault('core_secret_id', core_secret_id)
            for meta_field in ('vm_key', 'vm_name', 'vm_node', 'vmid', 'proxmox_secret_id', 'proxmox_target'):
                if meta_field in extras:
                    continue
                stored_meta_val = secret_record.get(meta_field)
                if stored_meta_val in (None, ''):
                    continue
                extras[meta_field] = stored_meta_val
    merged = _normalize_core_config(merged, include_password=include_password)
    if extras:
        merged.update(extras)
    return merged


def _core_backend_defaults(*, include_password: bool = True) -> Dict[str, Any]:
    env_cfg = {
        'host': os.environ.get('CORE_HOST', CORE_HOST),
        'port': os.environ.get('CORE_PORT', CORE_PORT),
        'ssh_enabled': os.environ.get('CORE_SSH_ENABLED'),
        'ssh_host': os.environ.get('CORE_SSH_HOST'),
        'ssh_port': os.environ.get('CORE_SSH_PORT'),
        'ssh_username': os.environ.get('CORE_SSH_USERNAME'),
    }
    if include_password:
        env_cfg['ssh_password'] = os.environ.get('CORE_SSH_PASSWORD')
    cfg = _normalize_core_config(env_cfg, include_password=include_password)
    cfg['ssh_enabled'] = True
    return cfg


def _normalize_history_core_value(raw: Any) -> Dict[str, Any]:
    if isinstance(raw, dict):
        normalized = _extract_optional_core_config(raw, include_password=False)
        if normalized is not None:
            return normalized
        return _normalize_core_config({}, include_password=False)
    if isinstance(raw, str):
        text = raw.strip()
        if not text:
            return _normalize_core_config({}, include_password=False)
        try:
            parsed = json.loads(text)
            if isinstance(parsed, dict):
                return _normalize_core_config(parsed, include_password=False)
        except Exception:
            pass
        host_match = re.search(r'([A-Za-z0-9_.-]+)\s*:\s*(\d+)', text)
        host = host_match.group(1) if host_match else None
        try:
            port = int(host_match.group(2)) if host_match else None
        except Exception:
            port = None
        lower = text.lower()
        data: Dict[str, Any] = {}
        if host:
            data['host'] = host
        if port:
            data['port'] = port
        if 'ssh' in lower:
            disabled_markers = ('disabled', 'off', 'false', '0', 'no')
            ssh_enabled = not any(marker in lower for marker in disabled_markers)
            data['ssh_enabled'] = ssh_enabled
        return _normalize_core_config(data, include_password=False)
    return _normalize_core_config({}, include_password=False)


def _normalize_run_history_entry(entry: Any) -> Dict[str, Any]:
    if not isinstance(entry, dict):
        return {}
    normalized = dict(entry)

    # Per-scenario invariant: run history entries represent a single scenario.
    # Prefer explicit scenario_name; otherwise fall back to scenario_names[0].
    try:
        scenario_name = normalized.get('scenario_name')
        if isinstance(scenario_name, str):
            scenario_name = scenario_name.strip()
        else:
            scenario_name = ''
    except Exception:
        scenario_name = ''
    if not scenario_name:
        try:
            names = normalized.get('scenario_names')
            if isinstance(names, list) and names:
                first = names[0]
                scenario_name = first.strip() if isinstance(first, str) else str(first).strip()
        except Exception:
            scenario_name = ''
    if scenario_name:
        normalized['scenario_name'] = scenario_name
        normalized['scenario_names'] = [scenario_name]
    else:
        # Normalize legacy non-list forms and then truncate to one.
        sn = normalized.get('scenario_names')
        candidates: list[Any] = []
        if isinstance(sn, list):
            candidates = sn
        elif isinstance(sn, str):
            candidates = [sn]
        elif sn is not None:
            candidates = [sn]
        ordered: list[str] = []
        seen: set[str] = set()
        for entry in candidates:
            normalized_name = str(entry or '').strip()
            if not normalized_name or normalized_name in seen:
                continue
            seen.add(normalized_name)
            ordered.append(normalized_name)
        normalized['scenario_names'] = ordered
    return normalized


def _compose_remote_python_command(interpreter: str, script: str, activate_path: Optional[str] = None) -> str:
    activation = ''
    if activate_path:
        activation = f". {shlex.quote(activate_path)} >/dev/null 2>&1 || true; "
    return f"{activation}{shlex.quote(interpreter)} - <<'PY'\n{script}\nPY"


def _summarize_for_log(text: str, *, limit: int = 320, collapse_ws: bool = True) -> str:
    if not text:
        return ''
    sample = text.strip()
    if not sample:
        return ''
    if collapse_ws:
        sample = re.sub(r'\s+', ' ', sample)
    if len(sample) > limit:
        sample = sample[:limit - 1].rstrip() + ''
    return sample


def _current_core_ui_logs() -> list:
    if not has_request_context():
        return []
    try:
        return list(getattr(g, 'core_ui_logs', []) or [])
    except Exception:
        return []


def _append_core_ui_log(level: str, message: str) -> None:
    lvl = (level or 'INFO').upper()
    msg = message if isinstance(message, str) else str(message)
    try:
        logger = getattr(app, 'logger', logging.getLogger(__name__))
    except Exception:
        logger = logging.getLogger(__name__)
    log_fn = getattr(logger, lvl.lower(), logger.info)
    try:
        log_fn(msg)
    except Exception:
        logger.info(msg)
    if not has_request_context():
        return
    try:
        buf = getattr(g, 'core_ui_logs', None)
        if buf is None:
            buf = []
            g.core_ui_logs = buf
        buf.append({'level': lvl, 'message': msg})
    except Exception:
        pass


def _remote_core_sessions_script(address: str) -> str:
    address_literal = json.dumps(address)
    template = textwrap.dedent(
        """
import json
import traceback

from core.api.grpc.client import CoreGrpcClient

ADDRESS = __ADDRESS_LITERAL__


def serialize(session):
    state_obj = getattr(session, 'state', None)
    state = getattr(state_obj, 'name', None) if state_obj is not None else None
    if state is None:
        state = state_obj or getattr(session, 'state', None)
    return {
        'id': getattr(session, 'id', None),
        'state': state,
        'nodes': getattr(session, 'nodes', None),
        'file': getattr(session, 'file', None),
        'dir': getattr(session, 'dir', None),
    }


def main():
    result = {'sessions': [], 'error': None, 'traceback': None}
    client = None
    try:
        client = CoreGrpcClient(address=ADDRESS)
        client.connect()
        raw_sessions = client.get_sessions()
        items = []
        for sess in raw_sessions:
            entry = serialize(sess)
            sid = entry.get('id')
            needs_nodes = entry.get('nodes') in (None, 0)
            if sid is not None and needs_nodes:
                getter = getattr(client, 'get_nodes', None)
                if callable(getter):
                    try:
                        nodes = getter(int(sid))
                        if nodes is not None:
                            entry['nodes'] = len(nodes)
                    except Exception:
                        pass
            items.append(entry)
        result['sessions'] = items
    except Exception as exc:
        result['error'] = f"{exc.__class__.__name__}: {exc}"
        result['traceback'] = traceback.format_exc()
    finally:
        try:
            if client:
                client.close()
        except Exception:
            pass
    print(json.dumps(result))


if __name__ == '__main__':
    main()
"""
    )
    return template.replace('__ADDRESS_LITERAL__', address_literal)


def _remote_core_open_xml_script(address: str, xml_path: str, auto_start: bool = True) -> str:
    address_literal = json.dumps(address)
    xml_literal = json.dumps(xml_path)
    auto_literal = 'True' if auto_start else 'False'
    template = textwrap.dedent(
        """
import json
import traceback
from pathlib import Path

from core.api.grpc.client import CoreGrpcClient
try:
    from core.api.grpc import core_pb2 as _core_pb2
except Exception:
    _core_pb2 = None


class _SessionShim:
    __slots__ = ('id', 'session_id')

    def __init__(self, session_id):
        sid = int(session_id)
        self.id = sid
        self.session_id = sid

    def to_proto(self):
        if _core_pb2 is None:
            raise AttributeError('core_pb2 unavailable')
        msg = _core_pb2.Session()
        msg.id = int(self.id)
        return msg


def _start_session(client, session_id):
    session = None
    try:
        opener = getattr(client, 'open_session', None)
        if callable(opener):
            maybe = opener(int(session_id))
            if maybe is not None:
                session = maybe
    except Exception:
        pass
    if session is None:
        try:
            sessions = client.get_sessions()
            for sess in sessions or []:
                sid = getattr(sess, 'id', None) or getattr(sess, 'session_id', None)
                if sid is not None and int(sid) == int(session_id):
                    session = sess
                    break
        except Exception:
            session = None
    if session is not None:
        try:
            client.start_session(session=session)
            return
        except TypeError:
            pass
        try:
            client.start_session(session)
            return
        except TypeError:
            pass
    shim = _SessionShim(session_id)
    try:
        client.start_session(session=shim)
        return
    except Exception:
        pass
    try:
        client.start_session(shim)
        return
    except Exception:
        pass
    client.start_session(session_id)


def main():
    payload = {}
    client = None
    try:
        client = CoreGrpcClient(address=__ADDRESS_LITERAL__)
        connector = getattr(client, 'connect', None)
        if callable(connector):
            connector()
        success, session_id = client.open_xml(Path(__XML_PATH_LITERAL__), start=False)
        payload['result'] = bool(success)
        if session_id is not None:
            payload['session_id'] = int(session_id)
        if __AUTO_START_LITERAL__ and session_id is not None and success:
            _start_session(client, int(session_id))
    except Exception as exc:
        payload['error'] = str(exc)
        payload['traceback'] = traceback.format_exc()
    finally:
        try:
            if client is not None:
                closer = getattr(client, 'close', None)
                if callable(closer):
                    closer()
        except Exception:
            pass
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    )
    script = template.replace('__ADDRESS_LITERAL__', address_literal)
    script = script.replace('__XML_PATH_LITERAL__', xml_literal)
    script = script.replace('__AUTO_START_LITERAL__', auto_literal)
    return script


def _remote_core_session_action_script(address: str, action: str, session_id: int) -> str:
    address_literal = json.dumps(address)
    session_literal = json.dumps(int(session_id))
    action_literal = json.dumps(action)
    template = textwrap.dedent(
        """
import json
import traceback

from core.api.grpc.client import CoreGrpcClient
try:
    from core.api.grpc import core_pb2 as _core_pb2
except Exception:
    _core_pb2 = None


def _session_identifier(item):
    return getattr(item, 'id', None) or getattr(item, 'session_id', None)


class _SessionShim:
    __slots__ = ('id', 'session_id')

    def __init__(self, session_id):
        sid = int(session_id)
        self.id = sid
        self.session_id = sid

    def to_proto(self):
        if _core_pb2 is None:
            raise AttributeError('core_pb2 unavailable')
        msg = _core_pb2.Session()
        msg.id = int(self.id)
        return msg


def _resolve_session(client, session_id):
    try:
        opener = getattr(client, 'open_session', None)
        if callable(opener):
            session = opener(int(session_id))
            if session is not None:
                return session
    except Exception:
        pass
    try:
        sessions = client.get_sessions()
        for sess in sessions or []:
            sid = _session_identifier(sess)
            if sid is not None and int(sid) == int(session_id):
                return sess
    except Exception:
        pass
    return None


def _start_session(client, session_id):
    session = _resolve_session(client, session_id)
    if session is not None:
        try:
            client.start_session(session=session)
            return
        except TypeError:
            pass
        try:
            client.start_session(session)
            return
        except TypeError:
            pass
    shim = _SessionShim(session_id)
    try:
        client.start_session(session=shim)
        return
    except Exception:
        pass
    try:
        client.start_session(shim)
        return
    except Exception:
        pass
    client.start_session(session_id)


def main():
    payload = {}
    client = None
    try:
        client = CoreGrpcClient(address=__ADDRESS_LITERAL__)
        connector = getattr(client, 'connect', None)
        if callable(connector):
            connector()
        session_id = int(__SESSION_LITERAL__)
        action = __ACTION_LITERAL__
        if action == 'start':
            _start_session(client, session_id)
        elif action == 'stop':
            client.stop_session(session_id)
        elif action == 'delete':
            client.delete_session(session_id)
        else:
            raise ValueError(f'Unsupported action: {action}')
        payload['status'] = 'ok'
        payload['session_id'] = session_id
    except Exception as exc:
        payload['error'] = str(exc)
        payload['traceback'] = traceback.format_exc()
    finally:
        try:
            if client is not None:
                closer = getattr(client, 'close', None)
                if callable(closer):
                    closer()
        except Exception:
            pass
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    )
    script = template.replace('__ADDRESS_LITERAL__', address_literal)
    script = script.replace('__SESSION_LITERAL__', session_literal)
    script = script.replace('__ACTION_LITERAL__', action_literal)
    return script


def _remote_core_save_xml_script(address: str, session_id: Optional[str], dest_path: str) -> str:
    address_literal = json.dumps(address)
    dest_literal = json.dumps(dest_path)
    session_literal = 'None' if session_id is None else json.dumps(str(session_id))
    template = textwrap.dedent(
        """
import json
import traceback
from pathlib import Path

from core.api.grpc.client import CoreGrpcClient


def _session_identifier(item):
    return getattr(item, 'id', None) or getattr(item, 'session_id', None)


def main():
    payload = {}
    client = None
    try:
        client = CoreGrpcClient(address=__ADDRESS_LITERAL__)
        connector = getattr(client, 'connect', None)
        if callable(connector):
            connector()
        sessions = client.get_sessions() or []
        if not sessions:
            raise RuntimeError('No CORE sessions available')
        requested = __SESSION_LITERAL__
        target_id = None
        if requested is not None:
            for sess in sessions:
                sid = _session_identifier(sess)
                if sid is not None and str(sid) == str(requested):
                    target_id = sid
                    break
        if target_id is None:
            target_id = _session_identifier(sessions[0])
        if target_id is None:
            raise RuntimeError('Unable to determine session id to save')
        try:
            client.open_session(target_id)
        except Exception:
            pass
        out_path = Path(__DEST_LITERAL__)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        client.save_xml(session_id=target_id, file_path=out_path)
        payload['session_id'] = str(target_id)
        payload['output_path'] = str(out_path)
    except Exception as exc:
        payload['error'] = str(exc)
        payload['traceback'] = traceback.format_exc()
    finally:
        try:
            if client is not None:
                closer = getattr(client, 'close', None)
                if callable(closer):
                    closer()
        except Exception:
            pass
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    )
    script = template.replace('__ADDRESS_LITERAL__', address_literal)
    script = script.replace('__DEST_LITERAL__', dest_literal)
    script = script.replace('__SESSION_LITERAL__', session_literal)
    return script


def _run_remote_python_json(
    core_cfg: Dict[str, Any],
    script: str,
    *,
    logger: logging.Logger,
    label: str,
    meta: Optional[Dict[str, Any]] = None,
    command_desc: Optional[str] = None,
    timeout: float = 120.0,
) -> Dict[str, Any]:
    cfg = _normalize_core_config(core_cfg, include_password=True)
    if meta is not None and command_desc:
        meta['grpc_command'] = command_desc
        try:
            g.last_core_grpc_command = command_desc  # type: ignore[attr-defined]
        except Exception:
            pass
    try:
        client = _open_ssh_client(cfg)
    except Exception as exc:
        logger.warning('[core.remote] SSH connection failed for %s: %s', label, exc)
        raise
    try:
        interpreter_candidates = _candidate_remote_python_interpreters(cfg)
        if not interpreter_candidates:
            interpreter_candidates = ['python3', 'python']
        venv_bin = str(cfg.get('venv_bin') or '').strip()
        activate_path = posixpath.join(venv_bin, 'activate') if venv_bin else None
        last_error: Optional[str] = None
        last_stdout_text: str = ''
        last_stderr_text: str = ''
        last_interpreter: str = ''
        for interpreter in interpreter_candidates:
            command = _compose_remote_python_command(interpreter, script, activate_path)
            logger.info('[core.remote] (%s) executing via %s', label, interpreter)
            exit_code, stdout, stderr = _exec_ssh_command(client, command, timeout=timeout)
            stdout_text = (stdout or '').strip()
            stderr_text = (stderr or '').strip()
            last_stdout_text = stdout_text
            last_stderr_text = stderr_text
            last_interpreter = interpreter
            if stderr_text:
                logger.debug('[core.remote] (%s) stderr (%s): %s', label, interpreter, stderr_text)
            if exit_code != 0:
                last_error = f"interpreter={interpreter} exit={exit_code} stdout={_summarize_for_log(stdout_text)} stderr={_summarize_for_log(stderr_text)}"
                continue
            if not stdout_text:
                last_error = f"interpreter={interpreter} empty stdout"
                continue
            try:
                return json.loads(stdout_text)
            except json.JSONDecodeError as exc:
                logger.warning('[core.remote] (%s) invalid JSON via %s: %s', label, interpreter, exc)
                logger.debug('[core.remote] (%s) raw stdout: %s', label, stdout_text)
                last_error = f'json decode error: {exc}'
                continue
        detail = f' ({last_error})' if last_error else ''
        # Include a small amount of last output to aid troubleshooting (passwords are not logged).
        out_detail = ''
        if last_stdout_text or last_stderr_text:
            out_detail = (
                f" last_interpreter={last_interpreter}"
                f" last_stdout={_summarize_for_log(last_stdout_text)}"
                f" last_stderr={_summarize_for_log(last_stderr_text)}"
            )
        raise RuntimeError(f'Remote execution failed for {label}{detail}{out_detail}')
    finally:
        try:
            client.close()
        except Exception:
            pass


def _sftp_ensure_dir(sftp: Any, directory: str) -> None:
    directory = posixpath.normpath(directory)
    if not directory or directory == '/':
        return
    parts: list[str] = []
    current = directory
    while current not in ('', '/'):
        parts.append(current)
        current = posixpath.dirname(current)
    for path in reversed(parts):
        try:
            sftp.stat(path)
        except Exception:
            try:
                sftp.mkdir(path)
            except Exception:
                pass


def _upload_file_to_core_host(core_cfg: Dict[str, Any], local_path: str, *, remote_dir: str = '/tmp/core-topo-gen/uploads') -> str:
    if not os.path.exists(local_path):
        raise FileNotFoundError(local_path)
    cfg = _normalize_core_config(core_cfg, include_password=True)
    client = _open_ssh_client(cfg)
    sftp = None
    try:
        sftp = client.open_sftp()
        _sftp_ensure_dir(sftp, remote_dir)
        unique = f"{uuid.uuid4().hex}_{os.path.basename(local_path)}"
        remote_path = posixpath.normpath(posixpath.join(remote_dir, unique))
        sftp.put(local_path, remote_path)
        return remote_path
    finally:
        try:
            if sftp:
                sftp.close()
        except Exception:
            pass
        try:
            client.close()
        except Exception:
            pass


def _remove_remote_file(core_cfg: Dict[str, Any], remote_path: str) -> None:
    if not remote_path:
        return
    cfg = _normalize_core_config(core_cfg, include_password=True)
    try:
        client = _open_ssh_client(cfg)
    except Exception:
        return
    try:
        sftp = client.open_sftp()
    except Exception:
        try:
            client.close()
        except Exception:
            pass
        return
    try:
        try:
            sftp.remove(remote_path)
        except FileNotFoundError:
            pass
    finally:
        try:
            sftp.close()
        except Exception:
            pass
        try:
            client.close()
        except Exception:
            pass


def _download_remote_file(core_cfg: Dict[str, Any], remote_path: str, local_path: str) -> None:
    cfg = _normalize_core_config(core_cfg, include_password=True)
    client = _open_ssh_client(cfg)
    sftp = None
    try:
        sftp = client.open_sftp()
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        sftp.get(remote_path, local_path)
    finally:
        try:
            if sftp:
                sftp.close()
        except Exception:
            pass
        try:
            client.close()
        except Exception:
            pass


def _collect_remote_core_daemon_pids(
    client: Any,
    *,
    log_handle: Optional[TextIO] = None,
    log_prefix: str = '[remote] ',
    stage: str = 'probe',
) -> List[int]:
    # Use `timeout` to avoid SSH reads hanging indefinitely if the remote shell blocks.
    command = "sh -c 'timeout 5s pgrep -x core-daemon 2>/dev/null || true'"
    try:
        stdin, stdout, stderr = client.exec_command(command, timeout=8.0)
    except Exception:
        return []
    try:
        stdout_data = stdout.read()
        stderr_data = stderr.read()
    finally:
        try:
            stdin.close()
        except Exception:
            pass
    try:
        exit_code = stdout.channel.recv_exit_status() if hasattr(stdout, 'channel') else 0
    except Exception:
        exit_code = 0
    try:
        if log_handle is not None:
            out_preview = (
                stdout_data.decode('utf-8', 'ignore')
                if isinstance(stdout_data, (bytes, bytearray))
                else str(stdout_data or '')
            ).strip()
            err_preview = (
                stderr_data.decode('utf-8', 'ignore')
                if isinstance(stderr_data, (bytes, bytearray))
                else str(stderr_data or '')
            ).strip()
            # Keep logs readable.
            if len(out_preview) > 300:
                out_preview = out_preview[:300] + ''
            if len(err_preview) > 300:
                err_preview = err_preview[:300] + ''
            log_handle.write(
                f"{log_prefix}{stage}: {command} -> exit={exit_code} stdout={out_preview} stderr={err_preview}\n"
            )
            _write_sse_marker(
                log_handle,
                'phase',
                {
                    'stage': stage,
                    'kind': 'ssh',
                    'command': command,
                    'exit': exit_code,
                    'stdout': out_preview,
                    'stderr': err_preview,
                },
            )
    except Exception:
        pass
    if exit_code not in (0, 1):
        return []
    text = ''
    if isinstance(stdout_data, bytes):
        text = stdout_data.decode('utf-8', 'ignore')
    else:
        text = str(stdout_data or '')
    pids: List[int] = []
    for token in text.strip().split():
        try:
            pids.append(int(token))
        except Exception:
            continue
    return pids


def _start_remote_core_daemon(client: Any, sudo_password: str | None, logger: logging.Logger) -> tuple[int, str, str]:
    # Avoid hanging on an interactive sudo prompt: if no password is available,
    # use -n so sudo fails fast instead of blocking forever.
    if sudo_password:
        # Wrap in `timeout` so systemctl can't block forever.
        sudo_cmd = "sudo -S -p '' sh -c 'timeout 20s systemctl start core-daemon'"
    else:
        sudo_cmd = "sudo -n sh -c 'timeout 20s systemctl start core-daemon'"
    stdin = stdout = stderr = None
    try:
        stdin, stdout, stderr = client.exec_command(sudo_cmd, timeout=20.0, get_pty=True)
        if sudo_password:
            try:
                stdin.write(str(sudo_password) + '\n')
                stdin.flush()
            except Exception:
                pass
        stdout_data = stdout.read()
        stderr_data = stderr.read()
        exit_code = stdout.channel.recv_exit_status() if hasattr(stdout, 'channel') else 0
        out_text = stdout_data.decode('utf-8', 'ignore') if isinstance(stdout_data, bytes) else str(stdout_data or '')
        err_text = stderr_data.decode('utf-8', 'ignore') if isinstance(stderr_data, bytes) else str(stderr_data or '')
        logger.info('[core] auto-start daemon executed: exit=%d stdout=%s stderr=%s', exit_code, out_text.strip(), err_text.strip())
        return exit_code, out_text, err_text
    finally:
        try:
            if stdin:
                stdin.close()
        except Exception:
            pass


def _stop_remote_core_daemon_conflict(
    client: Any,
    *,
    sudo_password: str | None,
    pids: List[int],
    logger: logging.Logger,
) -> Dict[str, Any]:
    """Stop duplicate core-daemon processes.

    Implementation strategy:
    - Stop the systemd service (best-effort)
    - Kill the discovered PIDs
    - Start the service again

    This intentionally errs on the side of producing a single clean daemon.
    """

    if not pids:
        return {'status': 'noop', 'detail': 'no pids provided'}
    if not sudo_password:
        raise RuntimeError('Stopping core-daemon requires sudo; provide an SSH password.')

    def _sudo(cmd: str, *, timeout: float = 30.0) -> tuple[int, str, str]:
        wrapped = f"sudo -S -p '' sh -c {shlex.quote(cmd)}"
        stdin = stdout = stderr = None
        try:
            stdin, stdout, stderr = client.exec_command(wrapped, timeout=timeout, get_pty=True)
            try:
                stdin.write(str(sudo_password) + '\n')
                stdin.flush()
            except Exception:
                pass
            out = stdout.read() if stdout else b''
            err = stderr.read() if stderr else b''
            try:
                exit_code = stdout.channel.recv_exit_status() if (stdout and hasattr(stdout, 'channel')) else 0
            except Exception:
                exit_code = 0
            out_text = out.decode('utf-8', 'ignore') if isinstance(out, (bytes, bytearray)) else str(out or '')
            err_text = err.decode('utf-8', 'ignore') if isinstance(err, (bytes, bytearray)) else str(err or '')
            return exit_code, out_text, err_text
        finally:
            try:
                if stdin:
                    stdin.close()
            except Exception:
                pass

    pid_args = ' '.join(str(int(pid)) for pid in pids if str(pid).strip().isdigit())
    if not pid_args:
        return {'status': 'noop', 'detail': 'no valid pids provided'}

    steps: List[Dict[str, Any]] = []
    for label, cmd in (
        ('systemctl_stop', 'timeout 15s systemctl stop core-daemon || true'),
        ('kill_term', f'kill -TERM {pid_args} 2>/dev/null || true'),
        ('sleep', 'sleep 1'),
        ('kill_kill', f'kill -KILL {pid_args} 2>/dev/null || true'),
        ('systemctl_start', 'timeout 20s systemctl start core-daemon || true'),
    ):
        code, out, err = _sudo(cmd, timeout=35.0)
        steps.append({'step': label, 'command': cmd, 'exit': code, 'stdout': (out or '').strip(), 'stderr': (err or '').strip()})
        try:
            logger.info('[core] stop duplicate daemons: %s exit=%s', label, code)
        except Exception:
            pass
    return {'status': 'attempted', 'pids': pids, 'steps': steps}


def _local_custom_services_dir() -> str:
    repo_root = _get_repo_root()
    return os.path.join(repo_root, 'on_core_machine', 'custom_services')


def _local_custom_service_files() -> list[str]:
    base = _local_custom_services_dir()
    try:
        if not os.path.isdir(base):
            return []
    except Exception:
        return []
    out: list[str] = []
    try:
        for name in os.listdir(base):
            if not name.endswith('.py'):
                continue
            if name.startswith('.'):
                continue
            ap = os.path.join(base, name)
            if os.path.isfile(ap):
                out.append(ap)
    except Exception:
        return []
    out.sort(key=lambda p: os.path.basename(p).lower())
    return out


def _install_custom_services_to_core_vm(
    ssh_client: Any,
    *,
    sudo_password: str | None,
    logger: logging.Logger,
) -> dict:
    """Copy repo-provided CORE custom services to the remote CORE VM.

    - Uploads python service modules under on_core_machine/custom_services.
    - Installs them into the remote core.services package directory.
    - Restarts (or starts) core-daemon.
    - Verifies the modules import successfully.
    """

    local_files = _local_custom_service_files()
    if not local_files:
        raise RuntimeError(f'No custom services found under {_local_custom_services_dir()}')
    module_names = [os.path.splitext(os.path.basename(p))[0] for p in local_files]

    def _exec(cmd: str, *, timeout: float = 25.0) -> tuple[int, str, str]:
        stdin = stdout = stderr = None
        try:
            stdin, stdout, stderr = ssh_client.exec_command(cmd, timeout=timeout, get_pty=True)
            out = stdout.read() if stdout else b''
            err = stderr.read() if stderr else b''
            try:
                exit_code = stdout.channel.recv_exit_status() if (stdout and hasattr(stdout, 'channel')) else 0
            except Exception:
                exit_code = 0
            out_text = out.decode('utf-8', 'ignore') if isinstance(out, (bytes, bytearray)) else str(out or '')
            err_text = err.decode('utf-8', 'ignore') if isinstance(err, (bytes, bytearray)) else str(err or '')
            return exit_code, out_text, err_text
        finally:
            try:
                if stdin:
                    stdin.close()
            except Exception:
                pass

    def _sudo(cmd: str, *, timeout: float = 45.0) -> tuple[int, str, str]:
        if not sudo_password:
            # Installing into site-packages requires root on typical CORE installs.
            raise RuntimeError('Installing custom services requires sudo; provide an SSH password in Step 2.')
        wrapped = f"sudo -S -p '' sh -c {shlex.quote(cmd)}"
        stdin = stdout = stderr = None
        try:
            stdin, stdout, stderr = ssh_client.exec_command(wrapped, timeout=timeout, get_pty=True)
            try:
                stdin.write(str(sudo_password) + '\n')
                stdin.flush()
            except Exception:
                pass
            out = stdout.read() if stdout else b''
            err = stderr.read() if stderr else b''
            try:
                exit_code = stdout.channel.recv_exit_status() if (stdout and hasattr(stdout, 'channel')) else 0
            except Exception:
                exit_code = 0
            out_text = out.decode('utf-8', 'ignore') if isinstance(out, (bytes, bytearray)) else str(out or '')
            err_text = err.decode('utf-8', 'ignore') if isinstance(err, (bytes, bytearray)) else str(err or '')
            return exit_code, out_text, err_text
        finally:
            try:
                if stdin:
                    stdin.close()
            except Exception:
                pass

    # Discover the remote core.services directory (where CORE loads service modules).
    probe = (
        "python3 -c \"import os, core.services; print(os.path.dirname(core.services.__file__))\" 2>/dev/null "
        "|| python -c \"import os, core.services; print(os.path.dirname(core.services.__file__))\" 2>/dev/null"
    )
    code, out, err = _exec(f"sh -c {shlex.quote(probe)}", timeout=20.0)
    services_dir = (out or '').strip().splitlines()[-1].strip() if (out or '').strip() else ''
    if not services_dir:
        raise RuntimeError(f'Failed to locate remote core.services directory (probe exit={code}): {(err or out or "").strip()}')

    logger.info('[core] Installing custom services into %s', services_dir)

    # Upload files to a temp directory first.
    tmp_dir = '/tmp/coretg_custom_services'
    _exec(f"sh -c {shlex.quote(f'mkdir -p {tmp_dir}')}", timeout=15.0)
    sftp = None
    try:
        sftp = ssh_client.open_sftp()
        for lp in local_files:
            rp = f"{tmp_dir}/{os.path.basename(lp)}"
            sftp.put(lp, rp)
    finally:
        try:
            if sftp:
                sftp.close()
        except Exception:
            pass

    # Install into core.services directory.
    install_cmd = f"install -m 0644 {tmp_dir}/*.py {shlex.quote(services_dir)}/"
    code, out, err = _sudo(install_cmd, timeout=45.0)
    if code != 0:
        raise RuntimeError(f'Failed installing custom services (exit={code}): {(err or out or "").strip()}')

    # Restart (or start) core-daemon.
    restart_cmd = "systemctl restart core-daemon || systemctl start core-daemon"
    code, out, err = _sudo(restart_cmd, timeout=45.0)
    if code != 0:
        raise RuntimeError(f'Failed restarting core-daemon after install (exit={code}): {(err or out or "").strip()}')

    # Verify CORE discovers these services (not just that files exist).
    # We import the installed modules, extract CoreService subclasses and their `name`s,
    # then ensure those names are present in the discoverable scan of `core.services`.
    module_list_literal = json.dumps(module_names)
    verify_script = textwrap.dedent(
        f"""
        import importlib
        import inspect
        import json
        import pkgutil
        import sys

        import core.services

        try:
            from core.services.base import CoreService  # type: ignore
        except Exception:  # pragma: no cover - remote execution
            from core.services.coreservices import CoreService  # type: ignore

        custom_modules = json.loads({module_list_literal!r})

        def service_names_from_module(mod):
            names = []
            for _name, obj in inspect.getmembers(mod, inspect.isclass):
                try:
                    is_service = issubclass(obj, CoreService) and obj is not CoreService
                except Exception:
                    continue
                if not is_service:
                    continue
                svc_name = getattr(obj, 'name', None)
                if isinstance(svc_name, str) and svc_name.strip():
                    names.append(svc_name.strip())
            # keep deterministic output
            return sorted(set(names))

        module_service_names = {{}}
        missing_modules = []
        modules_without_services = []
        custom_names = set()

        for mod_name in custom_modules:
            fq = f"core.services.{{mod_name}}"
            try:
                mod = importlib.import_module(fq)
            except Exception as exc:
                missing_modules.append({{'module': mod_name, 'error': repr(exc)}})
                continue
            names = service_names_from_module(mod)
            module_service_names[mod_name] = names
            if not names:
                modules_without_services.append(mod_name)
            custom_names.update(names)

        # Discoverable scan across core.services.
        all_names = set()
        for m in pkgutil.iter_modules(core.services.__path__):
            name = getattr(m, 'name', None)
            if not name:
                continue
            try:
                mod = importlib.import_module(f"core.services.{{name}}")
            except Exception:
                continue
            for svc_name in service_names_from_module(mod):
                all_names.add(svc_name)

        custom_names_missing_from_scan = sorted([n for n in custom_names if n not in all_names])

        result = {{
            'custom_modules': custom_modules,
            'module_service_names': module_service_names,
            'custom_service_names': sorted(custom_names),
            'missing_modules': missing_modules,
            'modules_without_services': modules_without_services,
            'all_service_names_count': len(all_names),
            'custom_names_missing_from_scan': custom_names_missing_from_scan,
        }}
        print('::SERVICESCHECK::' + json.dumps(result, sort_keys=True))

        if missing_modules or modules_without_services or custom_names_missing_from_scan:
            sys.exit(4)
        """
    ).strip()

    verify_cmd_py3 = textwrap.dedent(
        f"""
        cat <<'PY' | python3 -
        {verify_script}
        PY
        """
    ).strip()
    verify_cmd_py = textwrap.dedent(
        f"""
        cat <<'PY' | python -
        {verify_script}
        PY
        """
    ).strip()
    verify_cmd = f"{verify_cmd_py3} 2>/dev/null || {verify_cmd_py}"
    code, out, err = _exec(f"sh -c {shlex.quote(verify_cmd)}", timeout=35.0)
    marker = '::SERVICESCHECK::'
    payload_line = ''
    for line in (out or '').splitlines():
        if marker in line:
            payload_line = line.strip()
    if not payload_line:
        raise RuntimeError(f'Custom services verification did not produce expected output (exit={code}): {(err or out or "").strip()}')
    try:
        verify_payload = json.loads(payload_line.split(marker, 1)[1])
    except Exception as exc:
        raise RuntimeError(f'Custom services verification output could not be parsed: {exc}: {payload_line}')
    if code != 0:
        raise RuntimeError(f'Custom services failed CORE discovery verification: {json.dumps(verify_payload, indent=2)}')

    return {
        'services_dir': services_dir,
        'modules': module_names,
        'service_names': verify_payload.get('custom_service_names') if isinstance(verify_payload, dict) else None,
        'module_service_names': verify_payload.get('module_service_names') if isinstance(verify_payload, dict) else None,
    }


def _ensure_remote_core_daemon_ready(
    client: Any,
    *,
    core_cfg: Dict[str, Any],
    auto_start_allowed: bool,
    sudo_password: str | None,
    logger: logging.Logger,
    log_handle: Optional[TextIO] = None,
    log_prefix: str = '[remote] ',
) -> int:
    """Verify that exactly one core-daemon process is running, auto-starting if permitted."""

    def _log_daemon_probe(stage: str, attempt_idx: int, pids: List[int]) -> None:
        if not log_handle:
            return
        if pids:
            pid_text = ', '.join(str(pid) for pid in pids)
            summary = f'PID(s) {pid_text}'
        else:
            summary = 'no process found'
        try:
            log_handle.write(f"{log_prefix}{stage} probe {attempt_idx + 1}: {summary}\n")
            _write_sse_marker(
                log_handle,
                'phase',
                {
                    'stage': f'core-daemon.{stage}.probe',
                    'attempt': attempt_idx + 1,
                    'pids': pids,
                    'summary': summary,
                },
            )
        except Exception:
            pass

    def _poll_for_single_daemon(max_attempts: int, delay: float, stage: str) -> Optional[int]:
        last_pids: List[int] = []
        for attempt in range(max(1, max_attempts)):
            if log_handle:
                try:
                    log_handle.write(f"{log_prefix}{stage}: checking for core-daemon (attempt {attempt + 1}/{max(1, max_attempts)})\n")
                    _write_sse_marker(
                        log_handle,
                        'phase',
                        {
                            'stage': f'core-daemon.{stage}.poll',
                            'attempt': attempt + 1,
                            'max_attempts': max(1, max_attempts),
                        },
                    )
                except Exception:
                    pass
            poll_pids = _collect_remote_core_daemon_pids(
                client,
                log_handle=log_handle,
                log_prefix=log_prefix,
                stage=f"{stage}.pgrep",
            )
            _log_daemon_probe(stage, attempt, poll_pids)
            last_pids = poll_pids
            if len(poll_pids) > 1:
                raise CoreDaemonConflictError(
                    'Multiple core-daemon processes are running on the CORE VM.',
                    pids=poll_pids,
                )
            if len(poll_pids) == 1:
                if log_handle:
                    try:
                        _write_sse_marker(
                            log_handle,
                            'phase',
                            {
                                'stage': 'core-daemon.ready',
                                'pid': poll_pids[0],
                                'source': stage,
                            },
                        )
                    except Exception:
                        pass
                return poll_pids[0]
            if attempt < max_attempts - 1:
                try:
                    time.sleep(delay)
                except Exception:
                    pass
        return None

    pid = _poll_for_single_daemon(max_attempts=1, delay=0.5, stage='initial')
    if pid is not None:
        if log_handle:
            try:
                log_handle.write(f"{log_prefix}core-daemon already running (PID {pid})\n")
            except Exception:
                pass
        return pid
    if not auto_start_allowed:
        raise CoreDaemonMissingError(
            'core-daemon is not running on the CORE VM.',
            can_auto_start=True,
            start_command=CORE_DAEMON_START_COMMAND,
        )
    exit_code = -1
    stdout_text = ''
    stderr_text = ''
    try:
        try:
            logger.info('[core] Auto-starting core-daemon via `%s`', CORE_DAEMON_START_COMMAND)
        except Exception:
            pass
        if log_handle:
            try:
                log_handle.write(
                    f"{log_prefix}core-daemon not detected; attempting auto-start with command: {CORE_DAEMON_START_COMMAND}\n"
                )
                if not sudo_password:
                    log_handle.write(
                        f"{log_prefix}NOTE: No SSH sudo password provided; using non-interactive sudo (-n). If sudo requires a password, auto-start will fail quickly instead of hanging.\n"
                    )
            except Exception:
                pass
        try:
            logger.info('[core] auto-starting core-daemon via `%s`', CORE_DAEMON_START_COMMAND)
        except Exception:
            pass
        exit_code, stdout_text, stderr_text = _start_remote_core_daemon(client, sudo_password, logger)
        # If sudo cannot run non-interactively, fail fast with a clear message.
        if exit_code != 0 and not sudo_password:
            err_lower = (stderr_text or '').lower()
            if 'password' in err_lower or 'sudo' in err_lower:
                raise CoreDaemonMissingError(
                    'core-daemon auto-start failed: sudo requires a password (none provided).',
                    can_auto_start=False,
                    start_command=CORE_DAEMON_START_COMMAND,
                )
        if log_handle:
            try:
                log_handle.write(
                    f"{log_prefix}core-daemon auto-start command '{CORE_DAEMON_START_COMMAND}' exit={exit_code} stdout={stdout_text.strip()} stderr={stderr_text.strip()}\n"
                )
                _write_sse_marker(
                    log_handle,
                    'phase',
                    {
                        'stage': 'core-daemon.start',
                        'kind': 'ssh',
                        'command': CORE_DAEMON_START_COMMAND,
                        'exit': exit_code,
                        'stdout': (stdout_text or '').strip()[:1500],
                        'stderr': (stderr_text or '').strip()[:1500],
                    },
                )
            except Exception:
                pass
    except Exception as exc:
        raise CoreDaemonMissingError(
            f'core-daemon auto-start failed: {exc}',
            can_auto_start=False,
            start_command=CORE_DAEMON_START_COMMAND,
        ) from exc
    try:
        status_cmd = 'sh -c "timeout 10s systemctl is-active core-daemon"'
        status_code, status_out, status_err = _exec_ssh_command(client, status_cmd, timeout=15.0)
        if log_handle:
            try:
                log_handle.write(
                    f"{log_prefix}{status_cmd} -> exit={status_code} stdout={status_out.strip()} stderr={status_err.strip()}\n"
                )
                _write_sse_marker(
                    log_handle,
                    'phase',
                    {
                        'stage': 'core-daemon.systemctl.is-active',
                        'kind': 'ssh',
                        'command': status_cmd,
                        'exit': status_code,
                        'stdout': (status_out or '').strip()[:1500],
                        'stderr': (status_err or '').strip()[:1500],
                    },
                )
            except Exception:
                pass
        # Capture a small amount of extra detail for operator visibility.
        if log_handle:
            try:
                status_detail_cmd = "sh -c \"timeout 15s systemctl status core-daemon --no-pager -l | tail -n 40\""
                detail_code, detail_out, detail_err = _exec_ssh_command(client, status_detail_cmd, timeout=20.0)
                out_clean = (detail_out or '').rstrip()
                err_clean = (detail_err or '').rstrip()
                log_handle.write(
                    f"{log_prefix}{status_detail_cmd} -> exit={detail_code}\n"
                )
                if out_clean:
                    log_handle.write(f"{log_prefix}systemctl status (tail)\n{out_clean}\n")
                if err_clean:
                    log_handle.write(f"{log_prefix}systemctl status stderr\n{err_clean}\n")
                _write_sse_marker(
                    log_handle,
                    'phase',
                    {
                        'stage': 'core-daemon.systemctl.status',
                        'kind': 'ssh',
                        'command': status_detail_cmd,
                        'exit': detail_code,
                        'stdout': out_clean[:2500] if out_clean else '',
                        'stderr': err_clean[:1500] if err_clean else '',
                    },
                )
            except Exception:
                pass
        if exit_code != 0 and log_handle:
            try:
                journal_cmd = "sh -c \"timeout 15s journalctl -u core-daemon -n 40 --no-pager || true\""
                j_code, j_out, j_err = _exec_ssh_command(client, journal_cmd, timeout=20.0)
                out_clean = (j_out or '').rstrip()
                err_clean = (j_err or '').rstrip()
                log_handle.write(f"{log_prefix}{journal_cmd} -> exit={j_code}\n")
                if out_clean:
                    log_handle.write(f"{log_prefix}journalctl (tail)\n{out_clean}\n")
                if err_clean:
                    log_handle.write(f"{log_prefix}journalctl stderr\n{err_clean}\n")
                _write_sse_marker(
                    log_handle,
                    'phase',
                    {
                        'stage': 'core-daemon.journalctl',
                        'kind': 'ssh',
                        'command': journal_cmd,
                        'exit': j_code,
                        'stdout': out_clean[:2500] if out_clean else '',
                        'stderr': err_clean[:1500] if err_clean else '',
                    },
                )
            except Exception:
                pass
    except Exception:
        pass
    pid = _poll_for_single_daemon(max_attempts=10, delay=0.5, stage='post-start')
    if pid is not None:
        return pid
    raise CoreDaemonMissingError(
        'core-daemon auto-start did not stabilize within the expected time.',
        can_auto_start=False,
        start_command=CORE_DAEMON_START_COMMAND,
    )


def _check_remote_daemon_before_setup(
    client: Any,
    *,
    core_cfg: Dict[str, Any],
    auto_start_allowed: bool,
    log_handle: Optional[TextIO],
    log_prefix: str,
) -> int:
    logger = getattr(app, 'logger', logging.getLogger(__name__))
    pid = _ensure_remote_core_daemon_ready(
        client=client,
        core_cfg=core_cfg,
        auto_start_allowed=auto_start_allowed,
        sudo_password=core_cfg.get('ssh_password'),
        logger=logger,
        log_handle=log_handle,
        log_prefix=log_prefix,
    )
    if log_handle:
        try:
            log_handle.write(f"{log_prefix}core-daemon PID {pid} confirmed before run setup\n")
        except Exception:
            pass
    return pid

def _execute_remote_core_session_action(
    core_cfg: Dict[str, Any],
    action: str,
    session_id: int,
    *,
    logger: logging.Logger,
    meta: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    # This script runs on the SSH host. For docker-compose deployments, CORE is often
    # the *host machine* and the container uses host.docker.internal to reach it.
    # But host.docker.internal is not resolvable on the host itself, so we must
    # translate that to a loopback address for the remote execution.
    remote_target_host = str(cfg.get('host') or CORE_HOST or 'localhost').strip() or 'localhost'
    if remote_target_host in {'host.docker.internal', 'localhost', '127.0.0.1', '::1'}:
        remote_target_host = '127.0.0.1'
    address = f"{remote_target_host}:{cfg.get('port') or CORE_PORT}"
    script = _remote_core_session_action_script(address, action, session_id)
    command_desc = (
        f"remote ssh {ssh_user}@{ssh_host} -> CoreGrpcClient.{action}_session {address} (session={session_id})"
    )
    payload = _run_remote_python_json(
        cfg,
        script,
        logger=logger,
        label=f'core.{action}_session',
        meta=meta,
        command_desc=command_desc,
        timeout=120.0,
    )
    error_text = payload.get('error')
    if error_text:
        tb = payload.get('traceback')
        if tb:
            logger.debug('[core.%s_session] traceback: %s', action, tb)
        raise RuntimeError(error_text)
    return payload


def _list_active_core_sessions_via_remote_python(
    core_cfg: Dict[str, Any],
    *,
    errors: Optional[List[str]] = None,
    meta: Optional[Dict[str, Any]] = None,
    logger: Optional[logging.Logger] = None,
) -> list[dict]:
    cfg = _normalize_core_config(core_cfg, include_password=True)
    log = logger or getattr(app, 'logger', logging.getLogger(__name__))
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    target_host = str(cfg.get('host') or 'localhost').strip() or 'localhost'
    try:
        target_port = int(cfg.get('port') or CORE_PORT)
    except Exception:
        target_port = CORE_PORT

    # This remote script executes on ssh_host, so translate docker-only hostnames
    # to a loopback address that is correct from the SSH host's perspective.
    if target_host in {'host.docker.internal', 'localhost', '127.0.0.1', '::1'}:
        target_host = '127.0.0.1'
    if meta is not None:
        meta['grpc_command'] = (
            f"remote ssh {ssh_user}@{ssh_host} -> CoreGrpcClient.get_sessions {target_host}:{target_port}"
        )
    address = f"{target_host}:{target_port}"
    script = _remote_core_sessions_script(address)
    command_desc = meta['grpc_command'] if meta and 'grpc_command' in meta else f'get_sessions {address}'
    try:
        payload = _run_remote_python_json(
            cfg,
            script,
            logger=log,
            label='core.get_sessions',
            meta=meta,
            command_desc=command_desc,
            timeout=120.0,
        )
    except Exception as exc:
        msg = f'Remote CORE session fetch failed: {exc}'
        log.warning('[core.grpc] %s', msg)
        if errors is not None:
            errors.append(msg)
        return []
    error_text = payload.get('error')
    if error_text:
        log.warning('[core.grpc] remote CORE session fetch error: %s', error_text)
        if errors is not None:
            errors.append(f'Remote CORE session fetch failed: {error_text}')
        tb = payload.get('traceback')
        if tb:
            log.debug('[core.grpc] remote traceback: %s', tb)
        return []
    sessions = payload.get('sessions') or []
    log.info('[core.grpc] remote returned %d session(s)', len(sessions))
    return sessions


def _exec_ssh_python_probe(client: Any, command: str, *, timeout: float) -> Tuple[int, str, str]:
    stdin, stdout, stderr = client.exec_command(command, timeout=timeout)
    try:
        stdout_data = stdout.read()
        stderr_data = stderr.read()
    finally:
        try:
            stdin.close()
        except Exception:
            pass
    exit_code = stdout.channel.recv_exit_status() if hasattr(stdout, 'channel') else 0
    if isinstance(stdout_data, bytes):
        stdout_text = stdout_data.decode('utf-8', 'ignore')
    else:
        stdout_text = str(stdout_data)
    if isinstance(stderr_data, bytes):
        stderr_text = stderr_data.decode('utf-8', 'ignore')
    else:
        stderr_text = str(stderr_data)
    return exit_code, stdout_text, stderr_text


def _ensure_core_daemon_listening(core_cfg: Dict[str, Any], *, timeout: float = 5.0) -> None:
    cfg = _require_core_ssh_credentials(core_cfg)
    _ensure_paramiko_available()
    daemon_host = str(cfg.get('host') or CORE_HOST or 'localhost').strip() or 'localhost'
    if daemon_host in {'0.0.0.0', '::', '::0', '[::]'}:
        daemon_host = '127.0.0.1'
    daemon_port = int(cfg.get('port') or CORE_PORT)
    ssh_host = str(cfg.get('ssh_host') or cfg.get('host') or 'localhost')
    ssh_port = int(cfg.get('ssh_port') or 22)
    username = str(cfg.get('ssh_username') or '').strip()
    password = cfg.get('ssh_password') or ''
    logger = getattr(app, 'logger', logging.getLogger(__name__))
    client = paramiko.SSHClient()  # type: ignore[assignment]
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
    try:
        client.connect(
            hostname=ssh_host,
            port=ssh_port,
            username=username,
            password=password,
            look_for_keys=False,
            allow_agent=False,
            timeout=max(timeout, 5.0),
            banner_timeout=max(timeout, 5.0),
            auth_timeout=max(timeout, 5.0),
        )
        probe_errors: List[str] = []
        interpreter_candidates = _candidate_remote_python_interpreters(cfg)
        if not interpreter_candidates:
            interpreter_candidates = ['python3', 'python']
        for interpreter in interpreter_candidates:
            command = _build_python_probe_command(daemon_host, daemon_port, timeout, interpreter=interpreter)
            exit_code, stdout_text, stderr_text = _exec_ssh_python_probe(client, command, timeout=timeout + 2.0)
            stdout_clean = stdout_text.strip()
            stderr_clean = stderr_text.strip()
            combined = ' '.join(part for part in (stdout_clean, stderr_clean) if part).strip()
            if exit_code == 0 and 'OK' in stdout_clean:
                logger.info('[core] Verified core-daemon listening at %s:%s via %s', daemon_host, daemon_port, interpreter)
                return
            if not combined:
                combined = f'exit status {exit_code}'
            probe_errors.append(f'{interpreter}: {combined}')
            lower = combined.lower()
            if 'not found' in lower or 'command not found' in lower:
                continue
        error_detail = '; '.join(probe_errors) if probe_errors else 'probe failed'
        raise RuntimeError(f'core-daemon did not respond on {daemon_host}:{daemon_port} ({error_detail})')
    finally:
        try:
            client.close()
        except Exception:
            pass


def _run_core_connection_advanced_checks(
    cfg: Dict[str, Any],
    *,
    adv_fix_docker_daemon: bool = False,
    adv_run_core_cleanup: bool = False,
    adv_check_core_version: bool = False,
    adv_restart_core_daemon: bool = False,
    adv_start_core_daemon: bool = False,
    adv_auto_kill_sessions: bool = False,
) -> Dict[str, Dict[str, Any]]:
    """Run optional advanced checks against the CORE VM.

    These checks mirror the ones available in the Execute Scenario modal.
    Returns a dict keyed by adv_* flag names with shape:
      { enabled: bool, ok: bool|None, message: str }

    Notes:
    - This function is best-effort: when a check is enabled, it will attempt the action
      and report failure in its own result without raising (except for totally unexpected errors).
    - Some checks require sudo (SSH password). If missing, the check is marked failed.
    """

    results: Dict[str, Dict[str, Any]] = {}

    def _set(key: str, *, enabled: bool, ok: Optional[bool], message: str) -> None:
        results[key] = {
            'enabled': bool(enabled),
            'ok': ok,
            'message': str(message or '').strip(),
        }

    enabled_any = any(
        bool(v)
        for v in (
            adv_fix_docker_daemon,
            adv_run_core_cleanup,
            adv_check_core_version,
            adv_restart_core_daemon,
            adv_start_core_daemon,
            adv_auto_kill_sessions,
        )
    )
    if not enabled_any:
        # Keep keys present but disabled for stable UI.
        _set('adv_check_core_version', enabled=False, ok=None, message='')
        _set('adv_fix_docker_daemon', enabled=False, ok=None, message='')
        _set('adv_run_core_cleanup', enabled=False, ok=None, message='')
        _set('adv_restart_core_daemon', enabled=False, ok=None, message='')
        _set('adv_start_core_daemon', enabled=False, ok=None, message='')
        _set('adv_auto_kill_sessions', enabled=False, ok=None, message='')
        return results

    logger = getattr(app, 'logger', logging.getLogger(__name__))
    cfg = _normalize_core_config(cfg, include_password=True)
    if paramiko is None:
        # Can't run these without SSH.
        for key, enabled in (
            ('adv_check_core_version', adv_check_core_version),
            ('adv_fix_docker_daemon', adv_fix_docker_daemon),
            ('adv_run_core_cleanup', adv_run_core_cleanup),
            ('adv_restart_core_daemon', adv_restart_core_daemon),
            ('adv_start_core_daemon', adv_start_core_daemon),
            ('adv_auto_kill_sessions', adv_auto_kill_sessions),
        ):
            if enabled:
                _set(key, enabled=True, ok=False, message='Paramiko unavailable; cannot run this check.')
            else:
                _set(key, enabled=False, ok=None, message='')
        return results

    def _check_core_version(client: Any, required: str = '9.2.1') -> str:
        candidates = [
            "sh -c 'timeout 6s core-daemon --version 2>/dev/null || true'",
            "sh -c 'timeout 6s core-daemon -v 2>/dev/null || true'",
            "sh -c 'timeout 6s dpkg-query -W -f=\"${Version}\" core-daemon 2>/dev/null || true'",
            "sh -c 'timeout 6s rpm -q --qf \"%{VERSION}-%{RELEASE}\" core-daemon 2>/dev/null || true'",
            "sh -c 'timeout 6s core --version 2>/dev/null || true'",
        ]
        raw = ''
        for cmd in candidates:
            try:
                _code, out, err = _exec_ssh_command(client, cmd, timeout=12.0)
            except Exception:
                continue
            text = (out or '').strip() or (err or '').strip()
            if text:
                raw = text
                break
        if not raw:
            raise RuntimeError('Unable to determine CORE version on remote host')
        found = None
        try:
            m = re.search(r"(\d+\.\d+\.\d+)", raw)
            if m:
                found = m.group(1)
        except Exception:
            found = None
        if not found:
            raise RuntimeError(f'Unable to parse CORE version from: {raw}')
        if found != required:
            raise RuntimeError(f'CORE version mismatch: expected {required}, found {found}')
        return found

    def _sudo_exec(client: Any, cmd: str, *, timeout: float = 45.0) -> tuple[int, str, str]:
        sudo_password = cfg.get('ssh_password')
        # Wrap in timeout to avoid hanging.
        wrapped = f"sh -c 'timeout {int(max(5, timeout))}s {cmd}'"
        if sudo_password:
            sudo_cmd = f"sudo -S -p '' {wrapped}"
        else:
            sudo_cmd = f"sudo -n {wrapped}"
        stdin = stdout = stderr = None
        try:
            stdin, stdout, stderr = client.exec_command(sudo_cmd, timeout=timeout + 5.0, get_pty=True)
            if sudo_password:
                try:
                    stdin.write(str(sudo_password) + '\n')
                    stdin.flush()
                except Exception:
                    pass
            stdout_data = stdout.read() if stdout else b''
            stderr_data = stderr.read() if stderr else b''
            try:
                exit_code = stdout.channel.recv_exit_status() if (stdout and hasattr(stdout, 'channel')) else 0
            except Exception:
                exit_code = 0
            out_text = stdout_data.decode('utf-8', 'ignore') if isinstance(stdout_data, (bytes, bytearray)) else str(stdout_data or '')
            err_text = stderr_data.decode('utf-8', 'ignore') if isinstance(stderr_data, (bytes, bytearray)) else str(stderr_data or '')
            return exit_code, out_text, err_text
        finally:
            try:
                if stdin:
                    stdin.close()
            except Exception:
                pass

    def _maybe_fix_docker_daemon(client: Any) -> None:
        desired = {'bridge': 'none', 'iptables': False}
        existing: dict[str, Any] = {}
        try:
            _code, out, _err = _exec_ssh_command(client, "sh -c 'timeout 5s cat /etc/docker/daemon.json 2>/dev/null || true'", timeout=10.0)
            text = (out or '').strip()
            if text:
                try:
                    existing = json.loads(text)
                except Exception:
                    existing = {}
        except Exception:
            existing = {}
        merged = dict(existing) if isinstance(existing, dict) else {}
        merged.update(desired)
        payload = json.dumps(merged, indent=2, sort_keys=True) + "\n"

        tmp_local = None
        remote_tmp = None
        try:
            import tempfile

            with tempfile.NamedTemporaryFile('w', delete=False, encoding='utf-8') as tf:
                tf.write(payload)
                tmp_local = tf.name
            remote_tmp = _upload_file_to_core_host(cfg, tmp_local, remote_dir='/tmp/core-topo-gen/uploads')

            exit_code, _out, err = _sudo_exec(client, 'install -d -m 0755 /etc/docker', timeout=20.0)
            if exit_code != 0:
                err_lower = (err or '').lower()
                if (not cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                    raise RuntimeError('Fix docker daemon failed: sudo requires a password (none provided).')
                raise RuntimeError('Fix docker daemon failed: could not create /etc/docker')
            exit_code, _out, err = _sudo_exec(client, f"install -m 0644 {shlex.quote(remote_tmp)} /etc/docker/daemon.json", timeout=25.0)
            if exit_code != 0:
                err_lower = (err or '').lower()
                if (not cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                    raise RuntimeError('Fix docker daemon failed: sudo requires a password (none provided).')
                raise RuntimeError('Fix docker daemon failed: could not write /etc/docker/daemon.json')

            rc, _o, _e = _sudo_exec(client, 'systemctl restart docker || service docker restart || true', timeout=40.0)
            if rc != 0:
                raise RuntimeError('Fix docker daemon failed: docker restart did not succeed')
        finally:
            try:
                if tmp_local and os.path.exists(tmp_local):
                    os.remove(tmp_local)
            except Exception:
                pass
            try:
                if remote_tmp:
                    _remove_remote_file(cfg, remote_tmp)
            except Exception:
                pass

    def _maybe_core_cleanup(client: Any) -> None:
        # Prefer system-provided core-cleanup if available; otherwise do safe stale /tmp/pycore.* purge.
        try:
            _code, out, _err = _exec_ssh_command(client, "sh -c 'command -v core-cleanup >/dev/null 2>&1; echo $?'", timeout=10.0)
            has_core_cleanup = (out or '').strip() == '0'
        except Exception:
            has_core_cleanup = False
        if has_core_cleanup:
            exit_code, _out, err = _sudo_exec(client, 'core-cleanup', timeout=70.0)
            if exit_code != 0:
                err_lower = (err or '').lower()
                if (not cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                    raise RuntimeError('core-cleanup failed: sudo requires a password (none provided).')
                raise RuntimeError('core-cleanup failed')
            return

        # Fallback: remove stale pycore directories not in active session ids.
        try:
            target_host = str(cfg.get('host') or CORE_HOST)
            target_port = int(cfg.get('port') or CORE_PORT)
        except Exception:
            target_host = str(cfg.get('host') or CORE_HOST)
            target_port = CORE_PORT
        try:
            sessions = _list_active_core_sessions(target_host, int(target_port), cfg, errors=[], meta={})
        except Exception:
            sessions = []
        active_ids: set[int] = set()
        for entry in sessions:
            try:
                sid = entry.get('id')
                if sid is None:
                    continue
                active_ids.add(int(str(sid).strip()))
            except Exception:
                continue
        active_json = json.dumps(sorted(active_ids))
        cleanup_cmd = (
            "python3 - <<'PY'\n"
            "import os, json, glob, shutil, time\n"
            "active=set(json.loads(os.environ.get('ACTIVE_IDS','[]')))\n"
            "removed=[]\nkept=[]\nnow=time.time()\n"
            "for p in glob.glob('/tmp/pycore.*'):\n"
            "  base=os.path.basename(p)\n"
            "  try: sid=int(base.split('.')[-1])\n"
            "  except Exception: kept.append(p); continue\n"
            "  if sid in active: kept.append(p); continue\n"
            "  try: age=now-os.stat(p).st_mtime\n"
            "  except Exception: age=999\n"
            "  if age < 30: kept.append(p); continue\n"
            "  try: shutil.rmtree(p); removed.append(p)\n"
            "  except Exception: kept.append(p)\n"
            "print(json.dumps({'removed':removed,'kept':kept,'active_session_ids':sorted(active)}))\n"
            "PY"
        )
        shell_cmd = f"ACTIVE_IDS={shlex.quote(active_json)} {cleanup_cmd}"
        _exec_ssh_command(client, f"sh -c {shlex.quote(shell_cmd)}", timeout=30.0)

    def _maybe_restart_core_daemon(client: Any) -> None:
        exit_code, _out, err = _sudo_exec(client, 'systemctl restart core-daemon', timeout=35.0)
        if exit_code != 0:
            err_lower = (err or '').lower()
            if (not cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                raise RuntimeError('Restart core-daemon failed: sudo requires a password (none provided).')
            raise RuntimeError('Restart core-daemon failed')

    def _maybe_start_core_daemon(client: Any) -> str:
        """Try starting core-daemon if not running. Returns status message."""
        # First check if daemon is running
        exit_code, out, _err = _sudo_exec(client, 'systemctl is-active core-daemon', timeout=15.0)
        if exit_code == 0 and (out or '').strip().lower() == 'active':
            return 'already running'
        
        # Try to start it
        exit_code, _out, err = _sudo_exec(client, 'systemctl start core-daemon', timeout=35.0)
        if exit_code != 0:
            err_lower = (err or '').lower()
            if (not cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                raise RuntimeError('Start core-daemon failed: sudo requires a password (none provided).')
            raise RuntimeError(f'Start core-daemon failed: {err or "unknown error"}')
        
        # Verify it started
        exit_code, out, _err = _sudo_exec(client, 'systemctl is-active core-daemon', timeout=15.0)
        if exit_code == 0 and (out or '').strip().lower() == 'active':
            return 'started successfully'
        return 'start attempted, status unclear'

    def _maybe_kill_active_sessions() -> tuple[list[int], list[str]]:
        deleted: list[int] = []
        errors: list[str] = []
        try:
            target_host = str(cfg.get('host') or CORE_HOST)
            target_port = int(cfg.get('port') or CORE_PORT)
        except Exception:
            target_host = str(cfg.get('host') or CORE_HOST)
            target_port = CORE_PORT
        try:
            sessions = _list_active_core_sessions(target_host, int(target_port), cfg, errors=[], meta={})
        except Exception as exc:
            errors.append(f"Failed listing active CORE sessions: {exc}")
            return deleted, errors
        ids: list[int] = []
        for entry in sessions:
            sid = entry.get('id')
            if sid in (None, ''):
                continue
            try:
                ids.append(int(str(sid).strip()))
            except Exception:
                continue
        seen: set[int] = set()
        ordered: list[int] = []
        for sid in ids:
            if sid in seen:
                continue
            seen.add(sid)
            ordered.append(sid)
        for sid in ordered:
            try:
                _execute_remote_core_session_action(cfg, 'delete', sid, logger=logger)
                deleted.append(sid)
            except Exception as exc:
                errors.append(f"Failed deleting session {sid}: {exc}")
        return deleted, errors

    _ensure_paramiko_available()
    client = paramiko.SSHClient()  # type: ignore[assignment]
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
    try:
        client.connect(
            hostname=str(cfg.get('ssh_host') or cfg.get('host') or 'localhost'),
            port=int(cfg.get('ssh_port') or 22),
            username=str(cfg.get('ssh_username') or ''),
            password=cfg.get('ssh_password') or '',
            look_for_keys=False,
            allow_agent=False,
            timeout=15.0,
            banner_timeout=15.0,
            auth_timeout=15.0,
        )

        if adv_check_core_version:
            try:
                ver = _check_core_version(client, '9.2.1')
                _set('adv_check_core_version', enabled=True, ok=True, message=f'found {ver}')
            except Exception as exc:
                _set('adv_check_core_version', enabled=True, ok=False, message=str(exc))
        else:
            _set('adv_check_core_version', enabled=False, ok=None, message='')

        if adv_fix_docker_daemon:
            try:
                _maybe_fix_docker_daemon(client)
                _set('adv_fix_docker_daemon', enabled=True, ok=True, message='applied /etc/docker/daemon.json and restarted docker')
            except Exception as exc:
                _set('adv_fix_docker_daemon', enabled=True, ok=False, message=str(exc))
        else:
            _set('adv_fix_docker_daemon', enabled=False, ok=None, message='')

        if adv_run_core_cleanup:
            try:
                _maybe_core_cleanup(client)
                _set('adv_run_core_cleanup', enabled=True, ok=True, message='completed')
            except Exception as exc:
                _set('adv_run_core_cleanup', enabled=True, ok=False, message=str(exc))
        else:
            _set('adv_run_core_cleanup', enabled=False, ok=None, message='')

        if adv_restart_core_daemon:
            try:
                _maybe_restart_core_daemon(client)
                _set('adv_restart_core_daemon', enabled=True, ok=True, message='requested')
            except Exception as exc:
                _set('adv_restart_core_daemon', enabled=True, ok=False, message=str(exc))
        else:
            _set('adv_restart_core_daemon', enabled=False, ok=None, message='')

        if adv_start_core_daemon:
            try:
                status_msg = _maybe_start_core_daemon(client)
                _set('adv_start_core_daemon', enabled=True, ok=True, message=status_msg)
            except Exception as exc:
                _set('adv_start_core_daemon', enabled=True, ok=False, message=str(exc))
        else:
            _set('adv_start_core_daemon', enabled=False, ok=None, message='')

    finally:
        try:
            client.close()
        except Exception:
            pass

    # Auto-kill sessions is done via gRPC calls (remote python) and does not require an open SSHClient.
    if adv_auto_kill_sessions:
        try:
            deleted, errs = _maybe_kill_active_sessions()
            if errs:
                _set('adv_auto_kill_sessions', enabled=True, ok=False, message='; '.join(errs)[:500])
            else:
                detail = f"deleted {len(deleted)} session(s)" if deleted else 'no sessions deleted'
                _set('adv_auto_kill_sessions', enabled=True, ok=True, message=detail)
        except Exception as exc:
            _set('adv_auto_kill_sessions', enabled=True, ok=False, message=str(exc))
    else:
        _set('adv_auto_kill_sessions', enabled=False, ok=None, message='')

    return results


def _normalize_hitl_attachment(raw_value: Any) -> str:
    if isinstance(raw_value, str):
        candidate = raw_value.strip()
        if candidate in _HITL_ATTACHMENT_ALLOWED:
            return candidate
        normalized = candidate.lower().replace('-', '_').replace(' ', '_')
        if normalized in _HITL_ATTACHMENT_ALLOWED:
            return normalized
        synonyms = {
            "router": "existing_router",
            "existing": "existing_router",
            "existingrouter": "existing_router",
            "existing_router": "existing_router",
            "existing-switch": "existing_switch",
            "existing switch": "existing_switch",
            "existing_switch": "existing_switch",
            "switch": "existing_switch",
            "newrouter": "new_router",
            "new_router": "new_router",
            "new router": "new_router",
            "router_new": "new_router",
            "proxmox_vm": "proxmox_vm",
            "proxmoxvm": "proxmox_vm",
            "proxmox-vm": "proxmox_vm",
            "proxmox vm": "proxmox_vm",
            "vm": "proxmox_vm",
            "external_vm": "proxmox_vm",
            "externalvm": "proxmox_vm",
        }
        if normalized in synonyms:
            return synonyms[normalized]
    return _DEFAULT_HITL_ATTACHMENT


def _normalize_hitl_interface_name(raw_value: Any) -> str:
    """Strip any legacy hitl- prefix so CORE sees the raw device name."""
    if isinstance(raw_value, str):
        candidate = raw_value.strip()
    elif raw_value is None:
        candidate = ''
    else:
        candidate = str(raw_value).strip()
    if not candidate:
        return ''
    prefix = 'hitl-'
    if candidate.lower().startswith(prefix):
        return candidate[len(prefix):]
    return candidate


def _slugify_hitl_name(raw_value: Any, fallback: str) -> str:
    value = ''
    if isinstance(raw_value, str):
        value = raw_value.strip().lower()
    elif raw_value is not None:
        value = str(raw_value).strip().lower()
    if not value:
        value = fallback.lower()
    cleaned = []
    for ch in value:
        if ch.isalnum():
            cleaned.append(ch)
        elif ch in {'-', '_'}:
            cleaned.append(ch)
        else:
            cleaned.append('-')
    slug = ''.join(cleaned).strip('-_')
    if not slug:
        slug = fallback.lower().strip('-_') or 'iface'
    return slug[:48]


def _stable_hitl_preview_router_id(scenario_key: str, slug: str, idx: int) -> int:
    key = f"hitl-router|{scenario_key or '__default__'}|{slug}|{idx}"
    digest = hashlib.sha256(key.encode('utf-8', 'replace')).hexdigest()
    return 700_000 + (int(digest[:10], 16) % 200_000)


def _build_hitl_preview_router(
    scenario_key: str,
    iface: Dict[str, Any],
    slug: str,
    ordinal: int,
    ip_info: Dict[str, Any],
) -> Dict[str, Any]:
    node_id = _stable_hitl_preview_router_id(scenario_key, slug, ordinal)
    new_router_ip = ip_info.get('new_router_ip4')
    link_network = ip_info.get('network_cidr') or ip_info.get('network')
    prefix_len = ip_info.get('prefix_len')
    new_router_ip_cidr = None
    if new_router_ip:
        if prefix_len and '/' not in str(new_router_ip):
            new_router_ip_cidr = f"{new_router_ip}/{prefix_len}"
        else:
            new_router_ip_cidr = str(new_router_ip)
    r2r_interfaces: Dict[str, Any] = {}
    metadata = {
        'hitl_preview': True,
        'hitl_interface_name': iface.get('name'),
        'hitl_attachment': iface.get('attachment'),
        'hitl_slug': slug,
        'link_network': link_network,
        'rj45_ip4': ip_info.get('rj45_ip4'),
        'existing_router_ip4': ip_info.get('existing_router_ip4'),
        'new_router_ip4': new_router_ip,
        'prefix_len': ip_info.get('prefix_len'),
        'netmask': ip_info.get('netmask'),
    }
    preview_router = {
        'node_id': node_id,
        'name': f"hitl-router-{slug}",
        'role': 'router',
        'kind': 'router',
        'ip4': new_router_ip_cidr,
        'r2r_interfaces': r2r_interfaces,
        'vulnerabilities': [],
        'is_base_bridge': False,
        'metadata': metadata,
    }
    return preview_router


def _sanitize_hitl_config(hitl_config: Any, scenario_name: Optional[str], xml_basename: Optional[str]) -> Dict[str, Any]:
    def _normalize_list(value: Any) -> List[str]:
        if isinstance(value, list):
            return [str(v).strip() for v in value if str(v).strip()]
        if isinstance(value, str):
            return [part.strip() for part in value.split(',') if part.strip()]
        return []

    cfg = hitl_config if isinstance(hitl_config, dict) else {}
    enabled = bool(cfg.get('enabled'))
    raw_interfaces = cfg.get('interfaces')

    if isinstance(raw_interfaces, list):
        iterable = raw_interfaces
    elif isinstance(raw_interfaces, str) and raw_interfaces.strip():
        iterable = [{'name': raw_interfaces.strip()}]
    elif isinstance(raw_interfaces, dict) and raw_interfaces.get('name'):
        iterable = [raw_interfaces]
    else:
        iterable = []

    sanitized: List[Dict[str, Any]] = []
    for entry in iterable:
        if entry is None:
            continue
        if isinstance(entry, str):
            name = _normalize_hitl_interface_name(entry)
            if name:
                sanitized.append({'name': name, 'attachment': _DEFAULT_HITL_ATTACHMENT})
            continue
        if not isinstance(entry, dict):
            continue
        clone = dict(entry)
        name_candidate = clone.get('name') or clone.get('interface') or clone.get('iface')
        if not isinstance(name_candidate, str):
            name_candidate = str(name_candidate or '').strip()
        else:
            name_candidate = name_candidate.strip()
        normalized_name = _normalize_hitl_interface_name(name_candidate)
        if not normalized_name:
            continue
        clone['name'] = normalized_name
        alias_candidate = clone.get('alias') or clone.get('description') or clone.get('display') or clone.get('summary')
        if isinstance(alias_candidate, str) and alias_candidate.strip():
            clone['alias'] = alias_candidate.strip()
        if 'display' in clone and not clone.get('description'):
            disp_val = clone.get('display')
            if isinstance(disp_val, str) and disp_val.strip():
                clone['description'] = disp_val.strip()
        clone['attachment'] = _normalize_hitl_attachment(clone.get('attachment'))
        if 'ipv4' in clone:
            clone['ipv4'] = _normalize_list(clone.get('ipv4'))
        if 'ipv6' in clone:
            clone['ipv6'] = _normalize_list(clone.get('ipv6'))
        sanitized.append(clone)

    scenario_key = ''
    candidate = cfg.get('scenario_key')
    if isinstance(candidate, str) and candidate.strip():
        scenario_key = candidate.strip()
    elif isinstance(scenario_name, str) and scenario_name.strip():
        scenario_key = scenario_name.strip()
    elif isinstance(xml_basename, str) and xml_basename.strip():
        scenario_key = xml_basename.strip()
    else:
        scenario_key = '__default__'

    sanitized_cfg = {
        'enabled': enabled,
        'interfaces': sanitized,
        'scenario_key': scenario_key,
    }
    core_cfg = _extract_optional_core_config(cfg.get('core'), include_password=False)
    if core_cfg:
        sanitized_cfg['core'] = core_cfg
    _enrich_hitl_interfaces_with_ips(sanitized_cfg)
    return sanitized_cfg


def _enrich_hitl_interfaces_with_ips(hitl_cfg: Dict[str, Any]) -> None:
    interfaces = hitl_cfg.get('interfaces') or []
    scenario_key = hitl_cfg.get('scenario_key') or '__default__'
    preview_routers: List[Dict[str, Any]] = []
    used_hitl_link_networks: set[str] = set()
    total_interfaces = len(interfaces)
    for idx, iface in enumerate(list(interfaces)):
        if not isinstance(iface, dict):
            continue
        attachment = _normalize_hitl_attachment(iface.get('attachment'))
        iface['attachment'] = attachment
        slug = _slugify_hitl_name(iface.get('name'), f"iface-{idx+1}")
        iface['slug'] = slug
        iface['ordinal'] = idx
        iface['interface_count'] = total_interfaces
        ip_info: Optional[Dict[str, Any]] = None
        if attachment in {'new_router', 'existing_router'}:
            ip_info = predict_hitl_link_ips_unique(scenario_key, iface.get('name'), idx, used_hitl_link_networks)
        if attachment in {'new_router', 'existing_router'} and ip_info:
            iface['link_network'] = ip_info.get('network')
            iface['link_network_cidr'] = ip_info.get('network_cidr') or ip_info.get('network')
            iface['prefix_len'] = ip_info.get('prefix_len')
            iface['netmask'] = ip_info.get('netmask')
            iface['existing_router_ip4'] = ip_info.get('existing_router_ip4')
            iface['new_router_ip4'] = ip_info.get('new_router_ip4')
            iface['rj45_ip4'] = ip_info.get('rj45_ip4')
            ipv4_current = iface.get('ipv4') if isinstance(iface.get('ipv4'), list) else []
            rj45_ip = iface.get('rj45_ip4')
            # Keep any user-provided (real) interface IPs first; append predicted RJ45 IP as a fallback.
            # This makes downstream "first valid ipv4" selection match what users typically expect
            # (their actual interface IP), while still retaining the predicted link IP.
            if rj45_ip:
                if not ipv4_current:
                    iface['ipv4'] = [rj45_ip]
                else:
                    if rj45_ip not in ipv4_current:
                        iface['ipv4'] = list(ipv4_current) + [rj45_ip]
                    else:
                        iface['ipv4'] = ipv4_current
        if attachment != 'new_router':
            continue
        if not ip_info:
            continue
        preview_router = _build_hitl_preview_router(scenario_key, iface, slug, idx, ip_info)
        preview_metadata = preview_router.setdefault('metadata', {})
        preview_metadata['scenario_key'] = scenario_key
        preview_metadata['ordinal'] = idx
        preview_metadata['interface_count'] = total_interfaces
        iface['preview_router'] = preview_router
        preview_routers.append(preview_router)
    if preview_routers:
        hitl_cfg['preview_routers'] = preview_routers


def _deterministic_hitl_peer_index(
    scenario_key: str,
    iface_name: str,
    ordinal: int,
    total_ifaces: int,
    candidate_count: int,
) -> Optional[int]:
    if candidate_count <= 0:
        return None
    total = total_ifaces if total_ifaces and total_ifaces > 0 else candidate_count
    seed = f"{scenario_key or '__default__'}|{iface_name or ordinal}|{ordinal}|{total}"
    try:
        base_digest = hashlib.sha256(seed.encode('utf-8', 'replace')).digest()
        counter_bytes = (0).to_bytes(8, 'little', signed=False)
        digest = hashlib.sha256(base_digest + counter_bytes).digest()
        value = int.from_bytes(digest[:8], 'big') / float(1 << 64)
        index = int(value * candidate_count) % candidate_count
        return index
    except Exception:
        return 0


def _wire_hitl_preview_routers(full_preview: Dict[str, Any], hitl_cfg: Dict[str, Any]) -> None:
    routers_list = full_preview.get('routers')
    if not isinstance(routers_list, list) or not routers_list:
        return
    interfaces = hitl_cfg.get('interfaces') or []
    preview_interfaces = [iface for iface in interfaces if isinstance(iface, dict) and iface.get('preview_router')]
    if not preview_interfaces:
        return
    base_routers = [router for router in routers_list if not (router.get('metadata', {}) or {}).get('hitl_preview')]
    if not base_routers:
        return
    scenario_key = hitl_cfg.get('scenario_key') or '__default__'
    total_ifaces = len(preview_interfaces)
    edges_list = full_preview.setdefault('r2r_edges_preview', [])
    existing_edge_pairs: set[tuple[int, int]] = set()
    normalized_edges: List[tuple[int, int]] = []
    for edge in list(edges_list):
        try:
            a, b = edge
            pair = tuple(sorted((int(a), int(b))))
            normalized_edges.append(pair)
            existing_edge_pairs.add(pair)
        except Exception:
            continue
    if normalized_edges:
        edges_list[:] = normalized_edges
    else:
        edges_list.clear()
    links_list = full_preview.setdefault('r2r_links_preview', [])
    existing_edge_id = max(
        (
            detail.get('edge_id', 0)
            for detail in links_list
            if isinstance(detail, dict) and isinstance(detail.get('edge_id'), int)
        ),
        default=0,
    )
    next_edge_id = existing_edge_id + 1
    degree_map = full_preview.get('r2r_degree_preview')
    if not isinstance(degree_map, dict):
        degree_map = {}
        full_preview['r2r_degree_preview'] = degree_map
    policy_preview = full_preview.get('r2r_policy_preview')
    policy_degree = None
    if isinstance(policy_preview, dict):
        policy_degree = policy_preview.setdefault('degree_sequence', {})
    router_lookup = {router.get('node_id'): router for router in routers_list if isinstance(router, dict)}
    for iface in preview_interfaces:
        preview_router = iface.get('preview_router')
        if not isinstance(preview_router, dict):
            continue
        metadata = preview_router.setdefault('metadata', {})
        if metadata.get('hitl_peer_wired'):
            continue
        new_router_id = preview_router.get('node_id')
        if new_router_id is None:
            continue
        candidate_count = len(base_routers)
        if candidate_count <= 0:
            continue
        iface_name = iface.get('name') or metadata.get('hitl_interface_name') or iface.get('slug') or f"iface-{iface.get('ordinal', 0)}"
        ordinal = iface.get('ordinal') if isinstance(iface.get('ordinal'), int) else metadata.get('ordinal') or 0
        total_count = iface.get('interface_count') if isinstance(iface.get('interface_count'), int) else metadata.get('interface_count') or total_ifaces
        peer_index = _deterministic_hitl_peer_index(
            scenario_key,
            str(iface_name),
            int(ordinal),
            int(total_count or total_ifaces or 1),
            candidate_count,
        ) or 0
        peer_router = base_routers[peer_index % candidate_count]
        peer_id = peer_router.get('node_id')
        if peer_id is None:
            continue
        prefix_len = iface.get('prefix_len') or metadata.get('prefix_len')
        new_ip = iface.get('new_router_ip4') or metadata.get('new_router_ip4')
        existing_ip = iface.get('existing_router_ip4') or metadata.get('existing_router_ip4')
        subnet = (
            iface.get('link_network_cidr')
            or metadata.get('link_network')
            or iface.get('link_network')
        )
        if subnet and prefix_len and '/' not in str(subnet):
            subnet = f"{subnet}/{prefix_len}"

        def _fmt_ip(ip: Any) -> Optional[str]:
            if not ip:
                return None
            ip_str = str(ip)
            if '/' in ip_str:
                return ip_str
            if prefix_len:
                return f"{ip_str}/{prefix_len}"
            return ip_str

        new_ip_cidr = _fmt_ip(new_ip)
        existing_ip_cidr = _fmt_ip(existing_ip)
        preview_iface_map = preview_router.setdefault('r2r_interfaces', {})
        peer_iface_map = peer_router.setdefault('r2r_interfaces', {})
        if new_ip_cidr:
            preview_iface_map[str(peer_id)] = new_ip_cidr
        else:
            preview_iface_map.setdefault(str(peer_id), '')
        if existing_ip_cidr:
            peer_iface_map[str(new_router_id)] = existing_ip_cidr
        else:
            peer_iface_map.setdefault(str(new_router_id), '')
        metadata['peer_router_node_id'] = peer_id
        metadata['peer_router_name'] = peer_router.get('name')
        metadata['hitl_peer_wired'] = True
        iface['peer_router_node_id'] = peer_id
        iface['target_router_id'] = peer_id
        layout_positions = full_preview.get('layout_positions')
        if isinstance(layout_positions, dict):
            routers_positions = layout_positions.setdefault('routers', {})
            if isinstance(routers_positions, dict):
                peer_pos = routers_positions.get(str(peer_id)) or routers_positions.get(peer_id)
                offset_x = 90 + 15 * (int(metadata.get('ordinal') or 0))
                offset_y = 60 + 10 * (int(metadata.get('ordinal') or 0))
                if isinstance(peer_pos, dict):
                    base_x = peer_pos.get('x', 0)
                    base_y = peer_pos.get('y', 0)
                else:
                    base_x = 200 + 120 * (int(metadata.get('ordinal') or 0))
                    base_y = 200 + 90 * (int(metadata.get('ordinal') or 0))
                routers_positions[str(new_router_id)] = {
                    'x': int(base_x) + offset_x,
                    'y': int(base_y) + offset_y,
                }
        edge_pair = tuple(sorted((int(peer_id), int(new_router_id))))
        if edge_pair not in existing_edge_pairs:
            existing_edge_pairs.add(edge_pair)
            edges_list.append(edge_pair)
            link_detail = {
                'edge_id': next_edge_id,
                'routers': [
                    {'id': peer_id, 'ip': existing_ip_cidr},
                    {'id': new_router_id, 'ip': new_ip_cidr},
                ],
                'subnet': subnet,
                'hitl_preview': True,
            }
            links_list.append(link_detail)
            if subnet:
                subnets_list = full_preview.setdefault('r2r_subnets', [])
                if subnet not in subnets_list:
                    subnets_list.append(subnet)
            degree_map[peer_id] = degree_map.get(peer_id, 0) + 1
            degree_map[new_router_id] = degree_map.get(new_router_id, 0) + 1
            next_edge_id += 1
            metadata['peer_router_node_id'] = peer_id
            iface['peer_router_node_id'] = peer_id
        else:
            degree_map.setdefault(peer_id, degree_map.get(peer_id, 0))
            degree_map.setdefault(new_router_id, degree_map.get(new_router_id, 0))
        if policy_degree is not None:
            policy_degree[str(peer_id)] = degree_map.get(peer_id, 0)
            policy_degree[str(new_router_id)] = degree_map.get(new_router_id, 0)
    if degree_map:
        values = [int(v) for v in degree_map.values() if isinstance(v, int)]
        if values:
            full_preview['r2r_stats_preview'] = {
                'min': min(values),
                'max': max(values),
                'avg': round(sum(values) / len(values), 2),
            }


def _augment_hitl_existing_router_interfaces(full_preview: Dict[str, Any], hitl_cfg: Dict[str, Any]) -> None:
    if not isinstance(full_preview, dict) or not isinstance(hitl_cfg, dict):
        return
    routers_list = full_preview.get('routers')
    if not isinstance(routers_list, list) or not routers_list:
        return
    interfaces = hitl_cfg.get('interfaces') or []
    existing_router_ifaces = [
        iface for iface in interfaces
        if isinstance(iface, dict) and _normalize_hitl_attachment(iface.get('attachment')) == 'existing_router'
    ]
    if not existing_router_ifaces:
        return
    base_router_entries = [
        router for router in routers_list
        if isinstance(router, dict) and not (router.get('metadata', {}) or {}).get('hitl_preview')
    ]
    if not base_router_entries:
        return
    scenario_key = hitl_cfg.get('scenario_key') or '__default__'
    total_ifaces = len(existing_router_ifaces)
    router_lookup: Dict[Any, Dict[str, Any]] = {}
    for router in routers_list:
        if not isinstance(router, dict):
            continue
        node_id = router.get('node_id')
        if node_id is not None:
            router_lookup[node_id] = router
    links_list = full_preview.setdefault('r2r_links_preview', [])
    existing_edge_id = max(
        (
            detail.get('edge_id', 0)
            for detail in links_list
            if isinstance(detail, dict) and isinstance(detail.get('edge_id'), int)
        ),
        default=0,
    )
    next_edge_id = existing_edge_id + 1
    existing_link_keys: set[tuple[Any, Any]] = set()
    for link in list(links_list):
        if not isinstance(link, dict):
            continue
        routers = link.get('routers')
        if not isinstance(routers, list) or len(routers) < 2:
            continue
        ra = routers[0].get('id') if isinstance(routers[0], dict) else None
        rb = routers[1].get('id') if isinstance(routers[1], dict) else None
        if ra is None or rb is None:
            continue
        existing_link_keys.add((ra, rb))
        existing_link_keys.add((rb, ra))
    global_overlay = full_preview.setdefault('hitl_existing_router_interfaces', [])
    overlay_keys = {
        (entry.get('router_id'), entry.get('slug'))
        for entry in global_overlay
        if isinstance(entry, dict)
    }

    def _compose_ip_with_prefix(ip_val: Any, prefix_len: Any) -> Optional[str]:
        if not ip_val:
            return None
        ip_str = str(ip_val)
        if '/' in ip_str:
            return ip_str
        if prefix_len:
            try:
                return f"{ip_str}/{int(prefix_len)}"
            except Exception:
                return f"{ip_str}/{prefix_len}"
        return ip_str

    for iface in existing_router_ifaces:
        slug = iface.get('slug')
        if not isinstance(slug, str) or not slug:
            slug = _slugify_hitl_name(iface.get('name'), f"iface-{(iface.get('ordinal') or 0) + 1}")
            iface['slug'] = slug
        ordinal = iface.get('ordinal') if isinstance(iface.get('ordinal'), int) else existing_router_ifaces.index(iface)
        total_count = iface.get('interface_count') if isinstance(iface.get('interface_count'), int) else total_ifaces
        target_router_id = iface.get('target_router_id') if iface.get('target_router_id') in router_lookup else None
        if target_router_id is None:
            if not base_router_entries:
                continue
            iface_name = iface.get('name') or slug or f"iface-{ordinal}"
            peer_index = _deterministic_hitl_peer_index(
                scenario_key,
                str(iface_name),
                int(ordinal or 0),
                int(total_count or total_ifaces or 1),
                len(base_router_entries),
            ) or 0
            chosen_router = base_router_entries[peer_index % len(base_router_entries)]
            target_router_id = chosen_router.get('node_id')
        if target_router_id is None or target_router_id not in router_lookup:
            continue
        iface['target_router_id'] = target_router_id
        iface['peer_router_node_id'] = target_router_id
        router_entry = router_lookup[target_router_id]
        prefix_len = iface.get('prefix_len')
        existing_ip_cidr = _compose_ip_with_prefix(iface.get('existing_router_ip4'), prefix_len)
        rj45_ip_cidr = _compose_ip_with_prefix(iface.get('rj45_ip4'), prefix_len)
        iface['existing_router_ip4_cidr'] = existing_ip_cidr
        iface['rj45_ip4_cidr'] = rj45_ip_cidr
        peer_key = slug
        iface['hitl_peer_key'] = peer_key
        router_iface_map = router_entry.setdefault('r2r_interfaces', {})
        if existing_ip_cidr:
            router_iface_map[peer_key] = existing_ip_cidr
        else:
            router_iface_map.setdefault(peer_key, '')
        router_metadata = router_entry.setdefault('metadata', {})
        router_overlay_list = router_metadata.setdefault('hitl_existing_router_interfaces', [])
        router_overlay_keys = {entry.get('slug') for entry in router_overlay_list if isinstance(entry, dict)}
        overlay_entry = {
            'slug': slug,
            'interface_name': iface.get('name'),
            'router_id': target_router_id,
            'router_name': router_entry.get('name'),
            'ip': existing_ip_cidr,
            'rj45_ip': rj45_ip_cidr,
            'network': iface.get('link_network_cidr') or iface.get('link_network'),
            'hitl_preview': True,
            'hitl_attachment': 'existing_router',
            'hitl_peer_key': peer_key,
            'scenario_key': scenario_key,
        }
        if slug not in router_overlay_keys:
            router_overlay_list.append(dict(overlay_entry))
        global_key = (target_router_id, slug)
        if global_key not in overlay_keys:
            global_overlay.append(dict(overlay_entry))
            overlay_keys.add(global_key)
        link_key = (target_router_id, peer_key)
        if link_key not in existing_link_keys:
            link_detail = {
                'edge_id': next_edge_id,
                'routers': [
                    {'id': target_router_id, 'ip': existing_ip_cidr},
                    {'id': peer_key, 'ip': rj45_ip_cidr},
                ],
                'subnet': iface.get('link_network_cidr') or iface.get('link_network'),
                'hitl_preview': True,
                'hitl_attachment': 'existing_router',
                'hitl_interface_slug': slug,
            }
            links_list.append(link_detail)
            existing_link_keys.add(link_key)
            existing_link_keys.add((peer_key, target_router_id))
            next_edge_id += 1


def _merge_hitl_preview_with_full_preview(full_preview: Dict[str, Any], hitl_cfg: Dict[str, Any]) -> None:
    if not isinstance(full_preview, dict) or not isinstance(hitl_cfg, dict):
        return
    preview_routers = hitl_cfg.get('preview_routers') or []
    routers_list = full_preview.get('routers')
    if not isinstance(routers_list, list):
        routers_list = []
        full_preview['routers'] = routers_list
    existing_ids = set()
    for entry in routers_list:
        if isinstance(entry, dict):
            node_id = entry.get('node_id')
            if node_id is not None:
                existing_ids.add(node_id)
    appended_ids: List[Any] = []
    for router in preview_routers:
        if not isinstance(router, dict):
            continue
        node_id = router.get('node_id')
        if node_id in existing_ids:
            continue
        routers_list.append(router)
        existing_ids.add(node_id)
        appended_ids.append(node_id)
    if appended_ids:
        # Keep routers sorted by node_id for deterministic previews
        try:
            def _router_sort_key(entry: Any) -> tuple[int, int]:
                if not isinstance(entry, dict):
                    return (1, 0)
                node_id = entry.get('node_id')
                if isinstance(node_id, int):
                    return (0, node_id)
                sort_val = 0
                if node_id is not None:
                    try:
                        sort_val = int(str(node_id))
                    except Exception:
                        digest = hashlib.sha256(str(node_id).encode('utf-8', 'replace')).hexdigest()
                        sort_val = int(digest[:8], 16)
                return (0, sort_val)

            routers_list.sort(key=_router_sort_key)
        except Exception:
            pass
        hitl_router_ids = full_preview.get('hitl_router_ids')
        if not isinstance(hitl_router_ids, list):
            hitl_router_ids = []
            full_preview['hitl_router_ids'] = hitl_router_ids
        for node_id in appended_ids:
            hitl_router_ids.append(node_id)
        try:
            seen = set()
            deduped = []
            for nid in hitl_router_ids:
                if nid in seen:
                    continue
                seen.add(nid)
                deduped.append(nid)
            full_preview['hitl_router_ids'] = deduped
            hitl_router_ids = deduped
        except Exception:
            pass
        full_preview['hitl_router_count'] = len([nid for nid in hitl_router_ids if nid is not None])
    _wire_hitl_preview_routers(full_preview, hitl_cfg)
    _augment_hitl_existing_router_interfaces(full_preview, hitl_cfg)

"""Flask web backend for core-topo-gen.

Augmented to guarantee the in-repo version of core_topo_gen is imported
instead of any globally installed distribution so new planning modules
like planning.full_preview are always available.
"""

# Ensure repository root (parent directory) precedes any site-packages version & purge shadowed installs
try:
    _THIS_DIR = os.path.abspath(os.path.dirname(__file__))
    _REPO_ROOT = os.path.abspath(os.path.join(_THIS_DIR, '..'))
    if _REPO_ROOT not in sys.path:
        sys.path.insert(0, _REPO_ROOT)
    # Purge any pre-imported site-packages version of core_topo_gen so we always load in-repo
    import sys as _sys
    for k in list(_sys.modules.keys()):
        if k == 'core_topo_gen' or k.startswith('core_topo_gen.'):
            del _sys.modules[k]
except Exception:
    pass

try:
    from core_topo_gen.parsers.hitl import parse_hitl_info
except ModuleNotFoundError as exc:
    raise RuntimeError(
        "core_topo_gen package is not available from this context. "
        "Run webapp commands from the repository root so the in-repo package is importable."
    ) from exc

from core_topo_gen.utils.hitl import predict_hitl_link_ips, predict_hitl_link_ips_unique

# Proactively ensure the in-repo planning.full_preview module is available even if an
# older site-packages installation of core_topo_gen (without that module) is first on sys.path.
def _ensure_full_preview_module():  # safe no-op if already present
    try:
        import importlib, sys as _sys
        try:
            # Fast path: module already importable
            import core_topo_gen.planning.full_preview  # type: ignore
            try:
                app.logger.debug('[full_preview] already importable (fast path)')
            except Exception:
                pass
            return True
        except ModuleNotFoundError:
            # Force reload planning package from repo root then load file directly
            repo_root = _REPO_ROOT
            planning_dir = os.path.join(repo_root, 'core_topo_gen', 'planning')
            candidate = os.path.join(planning_dir, 'full_preview.py')
            if not os.path.exists(candidate):
                try:
                    app.logger.error('[full_preview] candidate missing at %s', candidate)
                except Exception:
                    pass
                return False
            import importlib.util
            spec = importlib.util.spec_from_file_location('core_topo_gen.planning.full_preview', candidate)
            if not spec or not spec.loader:
                try:
                    app.logger.error('[full_preview] spec/loader missing for %s', candidate)
                except Exception:
                    pass
                return False
            module = importlib.util.module_from_spec(spec)
            _sys.modules['core_topo_gen.planning.full_preview'] = module
            try:
                spec.loader.exec_module(module)  # type: ignore
            except Exception:
                try:
                    import traceback, io as _io
                    buf = _io.StringIO(); traceback.print_exc(file=buf)
                    app.logger.error('[full_preview] exec_module failed: %s', buf.getvalue())
                except Exception:
                    pass
                return False
            # Attach as attribute of planning package for attribute-based access patterns
            try:
                import core_topo_gen.planning as planning_pkg  # type: ignore
                setattr(planning_pkg, 'full_preview', module)
            except Exception:
                pass
            try:
                app.logger.info('[full_preview] dynamically loaded from %s', candidate)
            except Exception:
                pass
            return True
    except Exception:
        return False

# Attempt early so later endpoints succeed
try:
    if not _ensure_full_preview_module():
        # Will try again lazily in the endpoint if needed
        pass
except Exception:
    pass

app = Flask(__name__)
app.secret_key = os.environ.get('FLASK_SECRET', 'coretopogenweb')
_log_level_name = os.environ.get('WEBAPP_LOG_LEVEL', 'INFO').strip().upper()
try:
    app.logger.setLevel(getattr(logging, _log_level_name, logging.INFO))
except Exception:
    pass
class _SkipWebuiLogTailFilter(logging.Filter):
    def filter(self, record: logging.LogRecord) -> bool:
        try:
            msg = record.getMessage()
        except Exception:
            msg = ''
        return '/api/webui/log_tail' not in msg

try:
    logging.getLogger('werkzeug').addFilter(_SkipWebuiLogTailFilter())
except Exception:
    pass
try:
    port = str(os.environ.get('CORETG_PORT') or '').strip() or '9090'
except Exception:
    port = '9090'
try:
    logs_dir = os.path.abspath(os.path.join(_outputs_dir(), 'logs'))
except Exception:
    logs_dir = os.path.abspath(os.path.join(os.getcwd(), 'outputs', 'logs'))
try:
    os.makedirs(logs_dir, exist_ok=True)
except Exception:
    pass
try:
    log_path = os.path.join(logs_dir, f'webui-{port}.log')
    has_handler = False
    for h in list(app.logger.handlers):
        try:
            if isinstance(h, logging.FileHandler) and getattr(h, 'baseFilename', None) == log_path:
                has_handler = True
                break
        except Exception:
            continue
    if not has_handler:
        fh = logging.FileHandler(log_path)
        fh.setLevel(getattr(logging, _log_level_name, logging.INFO))
        fh.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(name)s: %(message)s'))
        app.logger.addHandler(fh)
        try:
            root = logging.getLogger()
            if fh not in root.handlers:
                root.addHandler(fh)
        except Exception:
            pass
except Exception:
    pass

def _enumerate_host_interfaces(include_down: bool = False) -> List[Dict[str, Any]]:
    """Return host network interfaces available for Hardware-in-the-Loop selection."""
    results: List[Dict[str, Any]] = []
    logger = getattr(app, 'logger', logging.getLogger(__name__))
    if HITL_DISABLE_HOST_ENUM:
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug('[hitl] host interface enumeration skipped (HITL_DISABLE_HOST_ENUM=1)')
        return results
    if psutil is None:
        logger.warning('[hitl] psutil not available; host interface enumeration skipped')
        try:
            logger.warning('[hitl] psutil import failed under interpreter: %s', sys.executable)
            logger.debug('[hitl] sys.path=%s', sys.path)
            logger.debug('[hitl] PATH=%s', os.environ.get('PATH'))
            logger.debug('[hitl] PYTHONPATH=%s', os.environ.get('PYTHONPATH'))
        except Exception:
            pass
        return results
    try:
        logger.debug('[hitl] enumerating host interfaces (include_down=%s)', include_down)
        stats = psutil.net_if_stats()
        addrs = psutil.net_if_addrs()
    except Exception as exc:
        logger.error('[hitl] interface enumeration failed while retrieving stats: %s', exc, exc_info=True)
        return results

    link_families = set()
    for attr in ('AF_LINK',):
        fam = getattr(psutil, attr, None)
        if fam is not None:
            link_families.add(fam)
    for attr in ('AF_PACKET', 'AF_LINK'):
        fam = getattr(socket, attr, None)
        if fam is not None:
            link_families.add(fam)

    total_seen = 0
    skipped_down = 0
    skipped_loopback = 0
    skipped_other = 0
    def _is_non_physical(ifname: str) -> Optional[str]:
        name_normalized = ifname.lower()
        if name_normalized in NON_PHYSICAL_INTERFACE_NAMES:
            return 'exact'
        for prefix in NON_PHYSICAL_INTERFACE_PREFIXES:
            if name_normalized.startswith(prefix):
                return f'prefix:{prefix}'
        for needle in NON_PHYSICAL_INTERFACE_SUBSTRINGS:
            if needle in name_normalized:
                return f'substr:{needle}'
        # Treat typical virtual adapters that expose no MAC and no IPv4 address as non-physical
        return None

    for name, addr_list in addrs.items():
        total_seen += 1
        stat = stats.get(name)
        is_up = bool(getattr(stat, 'isup', False)) if stat else False
        if not include_down and not is_up:
            skipped_down += 1
            logger.debug('[hitl] skipping interface %s: interface is down', name)
            continue

        ipv4: List[str] = []
        ipv6: List[str] = []
        mac_addr: Optional[str] = None
        is_loopback = False

        for addr in addr_list:
            fam = addr.family
            if fam == socket.AF_INET:
                if addr.address:
                    ipv4.append(addr.address)
                    if addr.address.startswith('127.'):
                        is_loopback = True
            elif fam == getattr(socket, 'AF_INET6', None):
                if addr.address:
                    address = addr.address.split('%')[0]
                    ipv6.append(address)
                    if address == '::1':
                        is_loopback = True
            elif fam in link_families:
                if addr.address and addr.address != '00:00:00:00:00:00':
                    mac_addr = addr.address

        name_lc = name.lower()
        if name_lc.startswith('lo') or name == 'lo0':
            is_loopback = True

        if is_loopback:
            skipped_loopback += 1
            logger.debug('[hitl] skipping interface %s: loopback detected', name)
            continue

        non_physical_reason = _is_non_physical(name)
        if non_physical_reason is not None:
            skipped_other += 1
            logger.debug('[hitl] skipping interface %s: marked non-physical (%s)', name, non_physical_reason)
            continue

        entry: Dict[str, Any] = {
            'name': name,
            'display': name,
            'mac': mac_addr,
            'ipv4': ipv4,
            'ipv6': ipv6,
            'mtu': getattr(stat, 'mtu', None) if stat else None,
            'speed': getattr(stat, 'speed', None) if stat else None,
            'is_up': is_up,
        }
        flags = getattr(stat, 'flags', None)
        if isinstance(flags, str):
            entry['flags'] = [flag for flag in flags.replace(',', ' ').split() if flag]
        elif isinstance(flags, (list, tuple, set)):
            entry['flags'] = list(flags)

        results.append(entry)
        logger.debug('[hitl] captured interface %s: mac=%s ipv4=%s ipv6=%s is_up=%s',
                     name, mac_addr, ','.join(ipv4) or '-', ','.join(ipv6) or '-', is_up)

    logger.debug(
        '[hitl] host interface enumeration complete: total_seen=%d exported=%d skipped_down=%d skipped_loopback=%d skipped_other=%d',
        total_seen,
        len(results),
        skipped_down,
        skipped_loopback,
        skipped_other,
    )


def _normalize_mac_value(value: Any) -> str:
    if not value:
        return ''
    text = str(value).strip().lower()
    return ''.join(ch for ch in text if ch.isalnum())


def _enumerate_core_vm_interfaces_via_ssh(
    *,
    ssh_host: str,
    ssh_port: int,
    username: str,
    password: str,
    prox_interfaces: Optional[List[Dict[str, Any]]] = None,
    include_down: bool = False,
    vm_context: Optional[Dict[str, Any]] = None,
    timeout: float = 10.0,
) -> List[Dict[str, Any]]:
    logger = getattr(app, 'logger', logging.getLogger(__name__))
    logger.info(f"[hitl-debug] entering _enumerate_core_vm_interfaces_via_ssh")
    logger.info(f"[hitl-debug] prox_interfaces type: {type(prox_interfaces)}")
    if prox_interfaces:
        logger.info(f"[hitl-debug] prox_interfaces len: {len(prox_interfaces)}")

    _ensure_paramiko_available()
    client = paramiko.SSHClient()  # type: ignore[assignment]
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
    try:
        client.connect(
            hostname=ssh_host,
            port=int(ssh_port),
            username=username,
            password=password,
            look_for_keys=False,
            allow_agent=False,
            timeout=timeout,
            banner_timeout=timeout,
            auth_timeout=timeout,
        )
    except Exception as exc:  # pragma: no cover - network dependent
        raise _SSHTunnelError(f'Failed to establish SSH session to {ssh_host}:{ssh_port}: {exc}') from exc
    try:
        physical_ifaces: Optional[set[str]] = None
        phys_cmd = 'for I in $(ls -1 /sys/class/net 2>/dev/null); do if [ -e "/sys/class/net/$I/device" ]; then printf "%s\n" "$I"; fi; done'
        try:
            _, phys_stdout, _ = client.exec_command(phys_cmd, timeout=timeout)
            phys_data = phys_stdout.read()
            phys_status = phys_stdout.channel.recv_exit_status() if hasattr(phys_stdout, 'channel') else 0
            if phys_status == 0:
                if isinstance(phys_data, bytes):
                    phys_text = phys_data.decode('utf-8', 'ignore')
                else:
                    phys_text = str(phys_data)
                physical_list = [line.strip() for line in phys_text.splitlines() if line.strip()]
                if physical_list:
                    physical_ifaces = {name for name in physical_list if name and not name.lower().startswith('lo')}
        except Exception:  # pragma: no cover - best effort only
            app.logger.debug('[hitl] unable to enumerate physical interface set from CORE VM', exc_info=True)

        command = 'LANG=C ip -json address show'
        try:
            stdin, stdout, stderr = client.exec_command(command, timeout=timeout)
            stdout_data = stdout.read()
            stderr_data = stderr.read()
            exit_status = stdout.channel.recv_exit_status() if hasattr(stdout, 'channel') else 0
        except Exception as exc:  # pragma: no cover - remote command failure
            raise RuntimeError(f'Failed to execute "{command}" on CORE VM: {exc}') from exc
        if isinstance(stdout_data, bytes):
            stdout_text = stdout_data.decode('utf-8', 'ignore')
        else:
            stdout_text = str(stdout_data)
        if exit_status != 0:
            err_text = stderr_data.decode('utf-8', 'ignore') if isinstance(stderr_data, bytes) else str(stderr_data)
            raise RuntimeError(f'CORE VM rejected interface query ({exit_status}): {err_text.strip() or "unknown error"}')
        try:
            parsed = json.loads(stdout_text.strip() or '[]')
        except json.JSONDecodeError as exc:
            raise RuntimeError('Unable to parse interface data from CORE VM (ip -json output invalid)') from exc
        if not isinstance(parsed, list):
            raise RuntimeError('Unexpected interface payload from CORE VM')

        prox_context = vm_context if isinstance(vm_context, dict) else {}
        prox_map: Dict[str, Dict[str, Any]] = {}
        if prox_interfaces and isinstance(prox_interfaces, list):
            logger.info(f"[hitl-debug] Enumerate SSH with {len(prox_interfaces)} prox interfaces")
            if len(prox_interfaces) > 0:
                 logger.info(f"[hitl-debug] First prox interface sample: {prox_interfaces[0]}")
            for entry in prox_interfaces:
                if not isinstance(entry, dict):
                    continue
                mac_norm = _normalize_mac_value(entry.get('macaddr') or entry.get('mac') or entry.get('hwaddr'))
                if not mac_norm:
                    continue
                prox_info: Dict[str, Any] = {
                    'id': entry.get('id') or entry.get('interface_id') or entry.get('name') or entry.get('label'),
                    'macaddr': entry.get('macaddr') or entry.get('mac') or entry.get('hwaddr'),
                    'bridge': entry.get('bridge'),
                    'model': entry.get('model'),
                    'raw': entry,
                }
                prox_info.update({
                    'vm_key': prox_context.get('vm_key'),
                    'vm_name': prox_context.get('vm_name'),
                    'vm_node': prox_context.get('vm_node'),
                    'vmid': prox_context.get('vmid'),
                })
                prox_map[mac_norm] = prox_info

        results: List[Dict[str, Any]] = []
        for item in parsed:
            if not isinstance(item, dict):
                continue
            ifname = item.get('ifname') or item.get('name')
            if not ifname:
                continue
            name_lc = str(ifname).lower()
            if name_lc.startswith('lo'):
                continue
            if physical_ifaces is not None:
                base_name = str(ifname).split('@', 1)[0]
                base_name = base_name.split(':', 1)[0]
                root_name = base_name.split('.', 1)[0]
                if base_name not in physical_ifaces and root_name not in physical_ifaces:
                    continue
            flags = item.get('flags') if isinstance(item.get('flags'), list) else []
            operstate = str(item.get('operstate') or '').strip().upper()
            is_up = operstate == 'UP' or 'UP' in [str(flag).upper() for flag in flags]
            if not include_down and not is_up:
                continue
            mac_addr = item.get('address') or item.get('mac')
            mac_norm = _normalize_mac_value(mac_addr)
            addr_info = item.get('addr_info') if isinstance(item.get('addr_info'), list) else []
            ipv4: List[str] = []
            ipv6: List[str] = []
            for addr in addr_info:
                if not isinstance(addr, dict):
                    continue
                family = str(addr.get('family') or '').lower()
                value = addr.get('local') or addr.get('address')
                if not value:
                    continue
                if family == 'inet':
                    ipv4.append(str(value))
                elif family == 'inet6':
                    ipv6.append(str(value).split('%')[0])
            entry: Dict[str, Any] = {
                'name': ifname,
                'display': ifname,
                'mac': mac_addr,
                'ipv4': ipv4,
                'ipv6': ipv6,
                'mtu': item.get('mtu'),
                'speed': item.get('link_speed') or None,
                'is_up': is_up,
                'flags': flags,
            }
            prox_entry = prox_map.get(mac_norm)
            if prox_entry:
                entry['proxmox'] = prox_entry
                bridge_val = prox_entry.get('bridge')
                if bridge_val:
                    entry['bridge'] = bridge_val
            results.append(entry)

        results.sort(key=lambda rec: rec.get('name') or '')
        logger.info('[hitl] enumerated %d interfaces via CORE VM SSH (host=%s)', len(results), ssh_host)
        return results
    finally:
        try:
            client.close()
        except Exception:
            logger.debug('SSH client close failed after interface enumeration', exc_info=True)


def _enumerate_core_vm_interfaces_from_secret(
    secret_id: str,
    *,
    prox_interfaces: Optional[List[Dict[str, Any]]] = None,
    include_down: bool = False,
    vm_context: Optional[Dict[str, Any]] = None,
) -> List[Dict[str, Any]]:
    if not secret_id:
        raise ValueError('CORE credential identifier is required')
    record = _load_core_credentials(secret_id)
    if not record:
        raise ValueError('Stored CORE credentials not found')
    password = record.get('ssh_password_plain') or record.get('password_plain') or ''
    if not password:
        raise ValueError('Stored CORE credentials are missing password material')
    username = str(record.get('ssh_username') or '').strip()
    if not username:
        raise ValueError('Stored CORE credentials are missing SSH username')
    ssh_host = str(record.get('ssh_host') or record.get('host') or '').strip()
    if not ssh_host:
        raise ValueError('Stored CORE credentials are missing SSH host')
    try:
        ssh_port = int(record.get('ssh_port') or 22)
    except Exception:
        ssh_port = 22
    vm_meta = vm_context if isinstance(vm_context, dict) else {
        'vm_key': record.get('vm_key'),
        'vm_name': record.get('vm_name'),
        'vm_node': record.get('vm_node'),
        'vmid': record.get('vmid'),
    }
    return _enumerate_core_vm_interfaces_via_ssh(
        ssh_host=ssh_host,
        ssh_port=ssh_port,
        username=username,
        password=password,
        prox_interfaces=prox_interfaces,
        include_down=include_down,
        vm_context=vm_meta,
    )

    results.sort(key=lambda item: item['name'])
    return results

# ----------------------- Basic Path Helpers (restored) -----------------------
def _get_repo_root() -> str:
    """Return absolute repository root (directory containing this webapp folder)."""
    try:
        return _REPO_ROOT
    except Exception:
        return os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

def _outputs_dir() -> str:
    d = os.path.join(_get_repo_root(), 'outputs')
    os.makedirs(d, exist_ok=True)
    return d

def _uploads_dir() -> str:
    d = os.path.join(_get_repo_root(), 'uploads')
    os.makedirs(d, exist_ok=True)
    return d

def _reports_dir() -> str:
    d = os.path.join(_get_repo_root(), 'reports')
    os.makedirs(d, exist_ok=True)
    return d

def _node_schema_authoring_path() -> str:
    return os.path.join(_get_repo_root(), 'new-schema', 'node_schema_authoring.yaml')


_NODE_SCHEMA_VALIDATION_CACHE: dict[str, Any] | None = None


def _node_schema_validation_path() -> str:
    return os.path.join(_get_repo_root(), 'new-schema', 'node_schema_validation.json')


def _load_node_schema_validation() -> dict[str, Any] | None:
    global _NODE_SCHEMA_VALIDATION_CACHE
    if isinstance(_NODE_SCHEMA_VALIDATION_CACHE, dict):
        return _NODE_SCHEMA_VALIDATION_CACHE
    path = _node_schema_validation_path()
    if not os.path.isfile(path):
        _NODE_SCHEMA_VALIDATION_CACHE = None
        return None
    try:
        with open(path, 'r', encoding='utf-8') as f:
            _NODE_SCHEMA_VALIDATION_CACHE = json.load(f)
        return _NODE_SCHEMA_VALIDATION_CACHE
    except Exception:
        _NODE_SCHEMA_VALIDATION_CACHE = None
        return None


def _try_resolve_latest_outputs_xml(xml_path: str) -> Optional[str]:
    """Best-effort recovery for stale saved XML paths.

    The web UI stores `result_path` pointing to an XML under `outputs/scenarios-<ts>/...`.
    Those folders can be deleted or moved (e.g., scenario deletion/purge), leaving a stale
    path in localStorage. When that happens, try to find the newest file with the same
    basename under `outputs/scenarios-*`.

    Returns an absolute path if found, else None.
    """
    try:
        if not xml_path:
            return None
        abs_path = os.path.abspath(xml_path)
        if os.path.exists(abs_path):
            return abs_path
        base = os.path.basename(abs_path)
        if not base.lower().endswith('.xml'):
            return None
        outputs_dir = os.path.abspath(_outputs_dir())
        import glob
        candidates = glob.glob(os.path.join(outputs_dir, 'scenarios-*', base))
        candidates = [p for p in candidates if p and os.path.exists(p)]
        if not candidates:
            return None
        candidates.sort(key=lambda p: os.path.getmtime(p), reverse=True)
        best = os.path.abspath(candidates[0])
        try:
            if os.path.commonpath([best, outputs_dir]) != outputs_dir:
                return None
        except Exception:
            return None
        return best
    except Exception:
        return None


def _derive_default_seed(xml_hash: str) -> int:
    try:
        seed_val = int(xml_hash[:12], 16)
        seed_val %= (2**31 - 1)
        if seed_val <= 0:
            seed_val = 97531
        return seed_val
    except Exception:
        return 1357911


def _derive_seed_for_scenario(xml_hash: str, scenario: str | None) -> int:
    """Derive a stable seed for a given scenario based on the XML hash.

    This is used for UI hints (e.g., sidebar) when no explicit seed was provided.
    It should be stable across refreshes and different clients.
    """
    scen = (scenario or '').strip()
    if not scen:
        return _derive_default_seed(xml_hash)
    try:
        digest = hashlib.sha256(f"{xml_hash}|{scen}".encode('utf-8', errors='ignore')).hexdigest()
        return _derive_default_seed(digest)
    except Exception:
        return _derive_default_seed(xml_hash)

# Additional helper dirs (stubs restored after accidental removal)
def _traffic_dir() -> str:
    d = os.path.join(_outputs_dir(), 'traffic')
    os.makedirs(d, exist_ok=True)
    return d

def _segmentation_dir() -> str:
    d = os.path.join(_outputs_dir(), 'segmentation')
    os.makedirs(d, exist_ok=True)
    return d

def _vuln_base_dir() -> str:
    d = os.path.join(_outputs_dir(), 'vulns')
    os.makedirs(d, exist_ok=True)
    return d

def _vuln_repo_subdir() -> str:
    return 'repo'


def _ensure_private_dir(path: str, mode: int = 0o700) -> str:
    os.makedirs(path, exist_ok=True)
    if os.name != 'nt':
        try:
            os.chmod(path, mode)
        except Exception:
            pass
    return path


def _ensure_private_file(path: str, mode: int = 0o600) -> None:
    if os.name != 'nt':
        try:
            os.chmod(path, mode)
        except Exception:
            pass


def _sanitize_secret_slug(raw: str, fallback: str = 'entry') -> str:
    cleaned = ''.join(ch.lower() if ch.isalnum() else '-' for ch in (raw or ''))
    cleaned = re.sub(r'-{2,}', '-', cleaned).strip('-')
    return cleaned[:48] or fallback


def _proxmox_secret_dir() -> str:
    base = _ensure_private_dir(os.path.join(_outputs_dir(), 'secrets'))
    prox_dir = os.path.join(base, 'proxmox')
    return _ensure_private_dir(prox_dir)


def _proxmox_secret_key_path() -> str:
    return os.path.join(_proxmox_secret_dir(), '.key')


def _load_or_create_proxmox_key() -> bytes:
    if Fernet is None:  # pragma: no cover - dependency missing
        raise RuntimeError('cryptography package is required for secure Proxmox credential storage')
    env_key = os.environ.get('PROXMOX_SECRET_KEY')
    if env_key:
        key_bytes = env_key.encode('utf-8')
        try:
            Fernet(key_bytes)
            return key_bytes
        except Exception as exc:  # pragma: no cover - misconfigured env
            logging.getLogger(__name__).warning('Invalid PROXMOX_SECRET_KEY provided: %s', exc)
    key_path = _proxmox_secret_key_path()
    if os.path.exists(key_path):
        try:
            with open(key_path, 'rb') as fh:
                key_bytes = fh.read().strip()
            if key_bytes:
                Fernet(key_bytes)  # validate
                return key_bytes
        except Exception:
            logging.getLogger(__name__).warning('Existing Proxmox secret key invalid; regenerating')
    key_bytes = Fernet.generate_key()
    tmp_path = key_path + '.tmp'
    with open(tmp_path, 'wb') as fh:
        fh.write(key_bytes)
    os.replace(tmp_path, key_path)
    _ensure_private_file(key_path)
    return key_bytes


def _get_proxmox_cipher():
    if Fernet is None:  # pragma: no cover - dependency missing
        raise RuntimeError('cryptography package is required for secure Proxmox credential storage')
    key = _load_or_create_proxmox_key()
    return Fernet(key)


def _sanitize_proxmox_slug(raw: str, fallback: str = 'scenario') -> str:
    return _sanitize_secret_slug(raw, fallback)


def _derive_proxmox_identifier(
    scenario_name: str,
    scenario_index: Optional[int],
    url: str,
    username: str,
) -> str:
    slug = _sanitize_proxmox_slug(scenario_name or '')
    index_part = f"{scenario_index:02d}-" if isinstance(scenario_index, int) and scenario_index >= 0 else ''
    fingerprint_src = f"{url}|{username}"
    fingerprint = hashlib.sha256(fingerprint_src.encode('utf-8', 'ignore')).hexdigest()[:12]
    return f"{index_part}{slug}-{fingerprint}"


def _proxmox_secret_path(identifier: str) -> str:
    safe = _sanitize_proxmox_slug(identifier, 'proxmox')
    return os.path.join(_proxmox_secret_dir(), f"{safe}.json")


def _save_proxmox_credentials(payload: Dict[str, Any]) -> Dict[str, Any]:
    cipher = _get_proxmox_cipher()
    scenario_name = str(payload.get('scenario_name') or '').strip()
    scenario_index = payload.get('scenario_index')
    url = str(payload.get('url') or '').strip()
    username = str(payload.get('username') or '').strip()
    password = payload.get('password') or ''
    port = int(payload.get('port') or 8006)
    verify_ssl = bool(payload.get('verify_ssl', False))
    identifier = _derive_proxmox_identifier(scenario_name, scenario_index if isinstance(scenario_index, int) else None, url, username)
    encrypted_password = cipher.encrypt(password.encode('utf-8')).decode('utf-8')
    record = {
        'identifier': identifier,
        'scenario_name': scenario_name,
        'scenario_index': scenario_index if isinstance(scenario_index, int) else None,
        'url': url,
        'port': port,
        'username': username,
        'password': encrypted_password,
        'verify_ssl': verify_ssl,
        'stored_at': datetime.datetime.now(datetime.timezone.utc).isoformat(),
    }
    path = _proxmox_secret_path(identifier)
    tmp_path = path + '.tmp'
    with open(tmp_path, 'w', encoding='utf-8') as fh:
        json.dump(record, fh, indent=2)
    os.replace(tmp_path, path)
    _ensure_private_file(path)
    return {
        'identifier': identifier,
        'url': url,
        'port': port,
        'username': username,
        'verify_ssl': verify_ssl,
        'stored_at': record['stored_at'],
    }


def _load_proxmox_credentials(identifier: str) -> Optional[Dict[str, Any]]:
    path = _proxmox_secret_path(identifier)
    if not os.path.exists(path):
        return None
    try:
        with open(path, 'r', encoding='utf-8') as fh:
            data = json.load(fh)
        cipher = _get_proxmox_cipher()
        enc = data.get('password')
        password = ''
        if isinstance(enc, str) and enc:
            try:
                password = cipher.decrypt(enc.encode('utf-8')).decode('utf-8')
            except InvalidToken:
                password = ''
        data['password_plain'] = password
        return data
    except Exception:
        logging.getLogger(__name__).exception('Failed to load Proxmox credentials for %s', identifier)
        return None


def _delete_proxmox_credentials(identifier: str) -> bool:
    if not isinstance(identifier, str) or not identifier.strip():
        return False
    path = _proxmox_secret_path(identifier)
    try:
        if os.path.exists(path):
            os.remove(path)
            return True
    except Exception:
        logging.getLogger(__name__).exception('Failed to delete Proxmox credentials for %s', identifier)
        raise
    return False


def _core_secret_dir() -> str:
    base = _ensure_private_dir(os.path.join(_outputs_dir(), 'secrets'))
    core_dir = os.path.join(base, 'core')
    return _ensure_private_dir(core_dir)


def _core_secret_key_path() -> str:
    return os.path.join(_core_secret_dir(), '.key')


def _load_or_create_core_key() -> bytes:
    if Fernet is None:  # pragma: no cover - dependency missing
        raise RuntimeError('cryptography package is required for secure CORE credential storage')
    env_key = os.environ.get('CORE_SECRET_KEY')
    if env_key:
        key_bytes = env_key.encode('utf-8')
        try:
            Fernet(key_bytes)
            return key_bytes
        except Exception as exc:  # pragma: no cover - misconfigured env
            logging.getLogger(__name__).warning('Invalid CORE_SECRET_KEY provided: %s', exc)
    key_path = _core_secret_key_path()
    if os.path.exists(key_path):
        try:
            with open(key_path, 'rb') as fh:
                key_bytes = fh.read().strip()
            if key_bytes:
                Fernet(key_bytes)
                return key_bytes
        except Exception:
            logging.getLogger(__name__).warning('Existing CORE secret key invalid; regenerating')
    key_bytes = Fernet.generate_key()
    tmp_path = key_path + '.tmp'
    with open(tmp_path, 'wb') as fh:
        fh.write(key_bytes)
    os.replace(tmp_path, key_path)
    _ensure_private_file(key_path)
    return key_bytes


def _get_core_cipher():
    if Fernet is None:  # pragma: no cover - dependency missing
        raise RuntimeError('cryptography package is required for secure CORE credential storage')
    key = _load_or_create_core_key()
    return Fernet(key)


def _derive_core_identifier(
    scenario_name: str,
    scenario_index: Optional[int],
    host: str,
    ssh_username: str,
) -> str:
    slug = _sanitize_secret_slug(scenario_name or '', 'core')
    index_part = f"{scenario_index:02d}-" if isinstance(scenario_index, int) and scenario_index >= 0 else ''
    fingerprint_src = f"{host}|{ssh_username}"
    fingerprint = hashlib.sha256(fingerprint_src.encode('utf-8', 'ignore')).hexdigest()[:12]
    return f"{index_part}{slug}-{fingerprint}"


def _save_core_credentials(payload: Dict[str, Any]) -> Dict[str, Any]:
    cipher = _get_core_cipher()
    scenario_name = str(payload.get('scenario_name') or '').strip()
    raw_index = payload.get('scenario_index')
    scenario_index: Optional[int]
    try:
        scenario_index = int(raw_index)
    except Exception:
        scenario_index = None
    grpc_host = str(payload.get('grpc_host') or payload.get('host') or '').strip()
    if not grpc_host:
        raise ValueError('gRPC host is required to persist CORE credentials')
    try:
        grpc_port = int(payload.get('grpc_port') or payload.get('port') or 50051)
    except Exception:
        grpc_port = 50051
    ssh_host = str(payload.get('ssh_host') or grpc_host).strip()
    try:
        ssh_port = int(payload.get('ssh_port') or 22)
    except Exception:
        ssh_port = 22
    ssh_username = str(payload.get('ssh_username') or '').strip()
    ssh_password_raw = payload.get('ssh_password') or ''
    if not isinstance(ssh_password_raw, str):
        ssh_password_raw = str(ssh_password_raw)
    if not ssh_username:
        raise ValueError('SSH username is required to persist CORE credentials')
    if not ssh_password_raw:
        raise ValueError('SSH password is required to persist CORE credentials')
    ssh_enabled = bool(payload.get('ssh_enabled', True))
    venv_bin_raw = payload.get('venv_bin')
    if venv_bin_raw in (None, ''):
        venv_bin = DEFAULT_CORE_VENV_BIN
    else:
        venv_bin = str(venv_bin_raw).strip() or DEFAULT_CORE_VENV_BIN
    identifier = _derive_core_identifier(scenario_name, scenario_index, grpc_host or ssh_host, ssh_username)
    encrypted_password = cipher.encrypt(ssh_password_raw.encode('utf-8')).decode('utf-8')
    vm_key = str(payload.get('vm_key') or '').strip()
    vm_name = str(payload.get('vm_name') or '').strip()
    vm_node = str(payload.get('vm_node') or '').strip()
    vmid_raw = payload.get('vmid')
    vmid = ''
    if isinstance(vmid_raw, (str, int)):
        vmid = str(vmid_raw).strip()
    prox_secret_raw = payload.get('proxmox_secret_id') or payload.get('secret_id')
    prox_secret_id = str(prox_secret_raw).strip() if prox_secret_raw not in (None, '') else None
    prox_target = None
    raw_target = payload.get('proxmox_target')
    if isinstance(raw_target, dict):
        target_slim: Dict[str, Any] = {}
        for key in ('node', 'vmid', 'interface_id', 'macaddr', 'bridge', 'model', 'vm_name', 'label'):
            if key in raw_target:
                target_slim[key] = raw_target.get(key)
        prox_target = target_slim or None
    record = {
        'identifier': identifier,
        'scenario_name': scenario_name,
        'scenario_index': scenario_index,
        'host': grpc_host,
        'port': grpc_port,
        'grpc_host': grpc_host,
        'grpc_port': grpc_port,
        'ssh_host': ssh_host,
        'ssh_port': ssh_port,
        'ssh_username': ssh_username,
        'ssh_enabled': ssh_enabled,
        'venv_bin': venv_bin,
        'password': encrypted_password,
        'vm_key': vm_key,
        'vm_name': vm_name,
        'vm_node': vm_node,
    'vmid': vmid if vmid else None,
        'proxmox_secret_id': prox_secret_id,
        'proxmox_target': prox_target,
        'stored_at': datetime.datetime.now(datetime.timezone.utc).isoformat(),
    }
    path = os.path.join(_core_secret_dir(), f"{identifier}.json")
    tmp_path = path + '.tmp'
    with open(tmp_path, 'w', encoding='utf-8') as fh:
        json.dump(record, fh, indent=2)
    os.replace(tmp_path, path)
    _ensure_private_file(path)
    return {
        'identifier': identifier,
        'scenario_name': scenario_name,
        'scenario_index': scenario_index,
        'host': grpc_host,
        'port': grpc_port,
        'grpc_host': grpc_host,
        'grpc_port': grpc_port,
        'ssh_host': ssh_host,
        'ssh_port': ssh_port,
        'ssh_username': ssh_username,
        'ssh_enabled': ssh_enabled,
        'venv_bin': venv_bin,
        'vm_key': vm_key,
        'vm_name': vm_name,
        'vm_node': vm_node,
    'vmid': vmid if vmid else None,
        'proxmox_secret_id': prox_secret_id,
        'proxmox_target': prox_target,
        'stored_at': record['stored_at'],
    }


def _load_core_credentials(identifier: str) -> Optional[Dict[str, Any]]:
    path = os.path.join(_core_secret_dir(), f"{identifier}.json")
    if not os.path.exists(path):
        return None
    try:
        with open(path, 'r', encoding='utf-8') as fh:
            data = json.load(fh)
        cipher = _get_core_cipher()
        encrypted = data.get('password')
        password_plain = ''
        if isinstance(encrypted, str) and encrypted:
            try:
                password_plain = cipher.decrypt(encrypted.encode('utf-8')).decode('utf-8')
            except InvalidToken:
                password_plain = ''
        data['ssh_password_plain'] = password_plain
        return data
    except Exception:
        logging.getLogger(__name__).exception('Failed to load CORE credentials for %s', identifier)
        return None


def _delete_core_credentials(identifier: str) -> bool:
    if not isinstance(identifier, str) or not identifier.strip():
        return False
    path = os.path.join(_core_secret_dir(), f"{identifier}.json")
    try:
        if os.path.exists(path):
            os.remove(path)
            return True
    except Exception:
        logging.getLogger(__name__).exception('Failed to delete CORE credentials for %s', identifier)
        raise
    return False


def _connect_proxmox_from_secret(identifier: str, *, timeout: float = 8.0) -> tuple[Any, Dict[str, Any]]:
    if ProxmoxAPI is None:  # pragma: no cover - dependency missing
        raise RuntimeError('Proxmox integration unavailable: install proxmoxer package')
    if not isinstance(identifier, str) or not identifier.strip():
        raise ValueError('Proxmox secret identifier is required')
    record = _load_proxmox_credentials(identifier)
    if not record:
        raise ValueError('Stored Proxmox credentials not found')
    password = record.get('password_plain') or ''
    if not password:
        raise ValueError('Stored Proxmox credentials are missing password material')
    url_raw = str(record.get('url') or '').strip()
    parsed = urlparse(url_raw) if url_raw else None
    host = parsed.hostname if parsed and parsed.hostname else url_raw
    if not host:
        raise ValueError('Stored Proxmox URL is invalid')
    try:
        port = int(record.get('port') or (parsed.port if parsed else 8006) or 8006)
    except Exception:
        port = 8006
    verify_ssl = bool(record.get('verify_ssl', True))
    backend = 'https'
    if parsed and parsed.scheme:
        backend = 'https' if parsed.scheme.lower() == 'https' else 'http'
    client = ProxmoxAPI(  # type: ignore[call-arg]
        host=host,
        user=record.get('username'),
        password=password,
        port=port,
        verify_ssl=verify_ssl,
        timeout=max(2.0, min(float(timeout), 30.0)),
        backend=backend,
    )
    return client, record


def _parse_proxmox_net_config(raw: Any) -> Dict[str, Any]:
    result: Dict[str, Any] = {}
    if not isinstance(raw, str) or not raw:
        return result
    tokens = [tok.strip() for tok in raw.split(',') if tok.strip()]
    for idx, token in enumerate(tokens):
        if '=' not in token:
            continue
        key, value = token.split('=', 1)
        key = key.strip().lower()
        value = value.strip()
        if idx == 0 and key in {'virtio', 'e1000', 'rtl8139', 'vmxnet3', 'ne2k_isa', 'i82551'}:
            result['model'] = key
            result['macaddr'] = value
            continue
        if key == 'macaddr' and 'macaddr' in result:
            # Keep first mac address discovered from model token
            continue
        result[key] = value
    
    # Debug logging for parsed result
    if 'macaddr' in result:
        print(f"DEBUG: Parsed Proxmox net config: {raw} -> {result}", flush=True)
    return result


_BRIDGE_NAME_SANITIZER = re.compile(r'[^a-z0-9_-]+')


def _normalize_internal_bridge_name(raw: Any) -> str:
    if raw is None:
        raise ValueError('Internal bridge name is required')
    candidate = str(raw).strip().lower()
    if not candidate:
        raise ValueError('Internal bridge name is required')
    candidate = _BRIDGE_NAME_SANITIZER.sub('-', candidate).strip('-_')
    candidate = re.sub(r'[-_]{2,}', '-', candidate)
    if not candidate:
        raise ValueError('Internal bridge name is invalid')
    if len(candidate) > 10:
        candidate = candidate[:10].rstrip('-_')
        if not candidate:
            raise ValueError('Internal bridge name is invalid')
    if not re.fullmatch(r'[a-z0-9][a-z0-9_-]*', candidate):
        raise ValueError('Internal bridge name may contain only lowercase letters, numbers, hyphen, and underscore, and must start with an alphanumeric character')
    return candidate


def _rewrite_bridge_in_net_config(config_value: str, bridge_name: str) -> tuple[str, bool, Optional[str]]:
    if not isinstance(config_value, str) or not config_value.strip():
        raise ValueError('Proxmox network configuration string is required')
    tokens = [tok.strip() for tok in config_value.split(',') if tok.strip()]
    previous_bridge: Optional[str] = None
    updated = False
    for idx, token in enumerate(tokens):
        if '=' not in token:
            continue
        key, value = token.split('=', 1)
        if key.strip().lower() == 'bridge':
            previous_bridge = value.strip()
            if previous_bridge == bridge_name:
                return config_value, False, previous_bridge
            tokens[idx] = f'bridge={bridge_name}'
            updated = True
            break
    if not updated:
        tokens.append(f'bridge={bridge_name}')
        updated = True
    new_config = ','.join(tokens)
    return new_config, updated, previous_bridge


def _parse_proxmox_vm_key(vm_key: str) -> tuple[str, int]:
    if not isinstance(vm_key, str):
        raise ValueError('VM key must be a string')
    parts = vm_key.split('::', 1)
    if len(parts) != 2:
        raise ValueError('VM key must be in the format "node::vmid"')
    node = parts[0].strip()
    vmid_raw = parts[1].strip()
    if not node or not vmid_raw:
        raise ValueError('VM key is missing node or VM ID information')
    try:
        vmid = int(vmid_raw)
    except Exception as exc:  # pragma: no cover - defensive
        raise ValueError(f'VM ID must be an integer (received {vmid_raw!r})') from exc
    return node, vmid


def _ensure_proxmox_bridge(client: Any, node: str, bridge_name: str, *, comment: Optional[str] = None) -> Dict[str, Any]:
    """Validate that the requested bridge already exists on the target node.

    The HITL workflow now assumes operators have pre-created the internal
    bridge (for example, ``<username>`` truncated to 10 characters) outside of apply time. We still
    enumerate the node's network devices so we can return a consistent
    metadata structure, but we no longer attempt to create or reload the
    bridge automatically. If the bridge is missing, surface a clear error so
    users can provision it manually before retrying.
    """

    try:
        networks = client.nodes(node).network.get()
    except Exception as exc:  # pragma: no cover - network enumeration failure
        raise RuntimeError(f'Failed to enumerate network devices on node {node}: {exc}') from exc

    for entry in networks or []:
        if not isinstance(entry, dict):
            continue
        iface = str(entry.get('iface') or '').strip()
        if iface == bridge_name:
            return {
                'created': False,
                'already_exists': True,
                'reload_invoked': False,
                'reload_ok': True,
                'reload_error': None,
            }

    raise RuntimeError(
        f'Bridge {bridge_name} not found on node {node}. Create the bridge manually before applying HITL mappings.'
    )


def _enumerate_proxmox_vms(identifier: str) -> Dict[str, Any]:
    client, record = _connect_proxmox_from_secret(identifier)
    inventory: List[Dict[str, Any]] = []
    nodes = []
    try:
        nodes = client.nodes.get()  # type: ignore[assignment]
    except Exception as exc:
        raise RuntimeError(f'Failed to list Proxmox nodes: {exc}') from exc
    for node in nodes or []:
        node_name = node.get('node') if isinstance(node, dict) else None
        if not node_name:
            continue
        try:
            vms = client.nodes(node_name).qemu.get()
        except Exception as exc:
            logging.getLogger(__name__).warning('Failed to enumerate VMs for node %s: %s', node_name, exc)
            continue
        for vm in vms or []:
            if not isinstance(vm, dict):
                continue
            vmid = vm.get('vmid')
            if vmid is None:
                continue
            template_flag = vm.get('template')
            if template_flag in (True, 1, '1', 'true', 'TRUE'):
                continue
            try:
                vmid_int = int(vmid)
            except Exception:
                vmid_int = vmid
            vm_name = vm.get('name') or f'VM {vmid_int}'
            vm_status = vm.get('status')
            config: Dict[str, Any] = {}
            try:
                config = client.nodes(node_name).qemu(vmid_int).config.get()
            except Exception as exc:
                logging.getLogger(__name__).warning('Failed to fetch config for VM %s on node %s: %s', vmid_int, node_name, exc)
            interfaces: List[Dict[str, Any]] = []
            for key, value in (config or {}).items():
                if not isinstance(key, str) or not key.lower().startswith('net'):
                    continue
                parsed = _parse_proxmox_net_config(value)
                iface_entry = {
                    'id': key,
                    'macaddr': parsed.get('macaddr') or parsed.get('hwaddr') or '',
                    'bridge': parsed.get('bridge') or '',
                    'model': parsed.get('model') or parsed.get('modeltype') or '',
                    'tag': parsed.get('tag') or parsed.get('vlan') or '',
                    'firewall': parsed.get('firewall') or '',
                    'raw': value,
                }
                interfaces.append(iface_entry)
            inventory.append({
                'node': node_name,
                'vmid': vmid_int,
                'name': vm_name,
                'status': vm_status,
                'interfaces': interfaces,
            })
    return {
        'fetched_at': datetime.datetime.now(datetime.timezone.utc).isoformat(),
        'url': record.get('url'),
        'username': record.get('username'),
        'verify_ssl': record.get('verify_ssl', True),
        'vms': inventory,
    }


def _find_proxmox_vm_config(target_node: Optional[str], target_vmid: Any) -> Optional[List[Dict[str, Any]]]:
    """
    Search all stored Proxmox credentials to find the VM and return its current interface config.
    Useful when the frontend has stale data.
    """
    if not target_vmid:
        return None
    
    try:
        target_vmid_int = int(target_vmid)
    except Exception:
        return None

    # List all secret files
    try:
        secret_dir = _proxmox_secret_dir()
        if not os.path.exists(secret_dir):
            return None
        files = [f for f in os.listdir(secret_dir) if f.endswith('.json')]
    except Exception:
        return None

    for filename in files:
        identifier = filename[:-5] # remove .json
        try:
            # Short timeout to avoid hanging
            client, _ = _connect_proxmox_from_secret(identifier, timeout=3.0)
            
            # If target_node provided, check if it exists
            nodes = []
            if target_node:
                try:
                    # Quick check if node exists
                    nodes = [n for n in client.nodes.get() if n.get('node') == target_node]
                except Exception:
                    pass
            
            if not nodes:
                # If node not specified or not found, try all nodes
                try:
                    nodes = client.nodes.get()
                except Exception:
                    continue
            
            for node in nodes or []:
                node_name = node.get('node')
                if not node_name:
                    continue
                
                # Check if VM exists on this node
                try:
                    # Get specific VM config directly
                    config = client.nodes(node_name).qemu(target_vmid_int).config.get()
                    
                    # If we got config, parse interfaces!
                    interfaces: List[Dict[str, Any]] = []
                    for key, value in (config or {}).items():
                        if not isinstance(key, str) or not key.lower().startswith('net'):
                            continue
                        parsed = _parse_proxmox_net_config(value)
                        iface_entry = {
                            'id': key,
                            'macaddr': parsed.get('macaddr') or parsed.get('hwaddr') or '',
                            'bridge': parsed.get('bridge') or '',
                            'model': parsed.get('model') or parsed.get('modeltype') or '',
                            'tag': parsed.get('tag') or parsed.get('vlan') or '',
                            'firewall': parsed.get('firewall') or '',
                            'raw': value,
                        }
                        interfaces.append(iface_entry)
                    return interfaces

                except Exception:
                    # VM not found on this node or credential permission issue
                    continue

        except Exception:
            continue
            
    return None

# ---------------- User persistence helpers (restored) ----------------
def _users_db_path() -> str:
    override = os.environ.get('CORE_TOPO_GEN_USERS_DB_PATH')
    if override:
        return os.path.abspath(override)
    # During pytest, isolate persisted state so local dev credentials don't break tests.
    if os.environ.get('PYTEST_CURRENT_TEST') or ('pytest' in sys.modules):
        base = os.path.join(tempfile.gettempdir(), 'core_topo_gen_test_users')
        os.makedirs(base, exist_ok=True)
        return os.path.join(base, 'users.json')
    base = os.path.join(_outputs_dir(), 'users')
    os.makedirs(base, exist_ok=True)
    return os.path.join(base, 'users.json')


def _base_upload_state_path() -> str:
    return os.path.join(_outputs_dir(), 'base_upload.json')


def _sanitize_base_upload_meta(meta: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    if not isinstance(meta, dict):
        return None
    out: Dict[str, Any] = {}
    path = meta.get('path') or meta.get('filepath')
    if isinstance(path, str) and path:
        out['path'] = path
    display = meta.get('display_name') or meta.get('name')
    if isinstance(display, str) and display:
        out['display_name'] = display
    if 'valid' in meta:
        out['valid'] = bool(meta.get('valid'))
    if 'exists' in meta:
        out['exists'] = bool(meta.get('exists'))
    if not out.get('path'):
        return None
    return out


def _load_base_upload_state() -> Optional[Dict[str, Any]]:
    path = _base_upload_state_path()
    if not os.path.exists(path):
        return None
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return _sanitize_base_upload_meta(data)
    except Exception:
        return None


def _save_base_upload_state(meta: Dict[str, Any]) -> None:
    clean = _sanitize_base_upload_meta(meta)
    if not clean:
        return
    clean = dict(clean)
    clean['updated_at'] = datetime.datetime.now(datetime.timezone.utc).isoformat()
    try:
        with open(_base_upload_state_path(), 'w', encoding='utf-8') as f:
            json.dump(clean, f, indent=2)
    except Exception:
        pass


def _clear_base_upload_state() -> None:
    path = _base_upload_state_path()
    try:
        if os.path.exists(path):
            os.remove(path)
    except Exception:
        pass


def _hydrate_base_upload_from_disk(payload: Dict[str, Any]) -> None:
    if payload.get('base_upload'):
        return
    meta = _load_base_upload_state()
    if not meta:
        return
    meta = dict(meta)
    path = meta.get('path') or ''
    exists = bool(path) and os.path.exists(path)
    meta['exists'] = exists
    if path and exists:
        ok, _errs = _validate_core_xml(path)
        meta['valid'] = bool(ok)
        if 'display_name' not in meta or not meta['display_name']:
            meta['display_name'] = os.path.basename(path)
    payload['base_upload'] = meta
    scen_list = payload.get('scenarios') or []
    if scen_list and isinstance(scen_list[0], dict):
        base_section = scen_list[0].setdefault('base', {})
        if path and not base_section.get('filepath'):
            base_section['filepath'] = path
        display = meta.get('display_name')
        if display and not base_section.get('display_name'):
            base_section['display_name'] = display

def _load_users() -> dict:
    p = _users_db_path()
    if not os.path.exists(p):
        return { 'users': [] }
    try:
        with open(p, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, dict) and isinstance(data.get('users'), list):
                return data
    except Exception:
        pass
    return { 'users': [] }

def _save_users(data: dict) -> None:
    p = _users_db_path(); tmp = p + '.tmp'
    try:
        os.makedirs(os.path.dirname(p), exist_ok=True)
        with open(tmp, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2)
        os.replace(tmp, p)
    except Exception:
        try:
            if os.path.exists(tmp): os.remove(tmp)
        except Exception: pass

def _ensure_admin_user() -> None:
    db = _load_users(); users = db.get('users', [])
    if not users:
        users = [{ 'username': 'coreadmin', 'password_hash': generate_password_hash('coreadmin'), 'role': 'admin' }]
        db['users'] = users; _save_users(db)
        try: app.logger.warning("Seeded default admin user 'coreadmin' / 'coreadmin'. Change immediately.")
        except Exception: pass
        return
    if not any(u.get('role') == 'admin' for u in users):
        import secrets as _secrets
        pwd = os.environ.get('ADMIN_PASSWORD') or _secrets.token_urlsafe(10)
        users.append({ 'username': 'admin', 'password_hash': generate_password_hash(pwd), 'role': 'admin' })
        db['users'] = users; _save_users(db)
        try: app.logger.warning("No admin found; created 'admin' user with generated password: %s", pwd)
        except Exception: pass

_ensure_admin_user()

def _current_user_record() -> Optional[dict]:
    if not has_request_context():
        return None
    cached = getattr(g, '_current_user_record', _G_USER_RECORD_SENTINEL)
    if cached is not _G_USER_RECORD_SENTINEL:
        return cached
    record: Optional[dict] = None
    user = _current_user()
    if user and user.get('username'):
        username = user.get('username')
        db = _load_users()
        for entry in db.get('users', []):
            if entry.get('username') != username:
                continue
            record = dict(entry)
            record.pop('password_hash', None)
            record['role'] = _normalize_role_value(entry.get('role'))
            record['scenarios'] = _normalize_scenario_assignments(entry.get('scenarios'))
            break
    setattr(g, '_current_user_record', record)
    return record


def _current_user_assigned_scenarios() -> list[str]:
    record = _current_user_record()
    if not record:
        return []
    scenarios = record.get('scenarios')
    if isinstance(scenarios, list):
        return list(scenarios)
    return []


def _assigned_scenarios_for_user(user: Optional[dict]) -> list[str]:
    if not user or not isinstance(user, dict):
        return []
    username = user.get('username') or ''
    if not isinstance(username, str):
        try:
            username = str(username)
        except Exception:
            username = ''
    username = username.strip()
    if not username:
        return []
    if has_request_context():
        current = _current_user()
        if current and current.get('username') == username:
            return _current_user_assigned_scenarios()
    db = _load_users()
    users = db.get('users', []) if isinstance(db, dict) else []
    for entry in users:
        if not isinstance(entry, dict):
            continue
        if entry.get('username') != username:
            continue
        return _normalize_scenario_assignments(entry.get('scenarios'))
    return []


def _builder_allowed_norms(user: Optional[dict] = None) -> Optional[set[str]]:
    """Return assigned scenario norms for roles that must be restricted.

    Historically this function applied only to the builder role.
    Participants are also restricted to their assigned scenarios across the UI,
    so we include them here to keep scenario visibility consistent.
    """
    effective_user = user
    if effective_user is None and has_request_context():
        effective_user = _current_user()
    if not effective_user:
        return None
    role = _normalize_role_value(effective_user.get('role'))
    if role not in {'builder', 'participant'}:
        return None
    assigned = _assigned_scenarios_for_user(effective_user)
    return {norm for norm in assigned if norm}


def _builder_placeholder_scenario(name: str, participant_url: str = '') -> Dict[str, Any]:
    """Return a minimal scenario structure for builder-only views."""
    template = copy.deepcopy(_default_scenarios_payload()['scenarios'][0])
    template['name'] = name or template.get('name') or 'Scenario'
    hitl_meta = template.get('hitl') if isinstance(template.get('hitl'), dict) else {}
    hitl_meta = dict(hitl_meta)
    if participant_url:
        for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
            hitl_meta[key] = participant_url
    else:
        for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
            hitl_meta.pop(key, None)
    template['hitl'] = hitl_meta
    return template


def _parse_iso_datetime(raw: Any) -> Optional[datetime.datetime]:
    if not raw:
        return None
    try:
        text = str(raw).strip()
    except Exception:
        return None
    if not text:
        return None
    try:
        # Accept trailing Z
        if text.endswith('Z'):
            text = text[:-1] + '+00:00'
        dt = datetime.datetime.fromisoformat(text)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=datetime.timezone.utc)
        return dt
    except Exception:
        return None


def _select_builder_hitl_fallback(hints: dict[str, Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    """Pick a best-effort admin HITL validation hint for builder scenarios.

    Builder users (and admin users previewing builder mode) cannot validate or select
    CORE VMs themselves, but they still need to see admin-managed connectivity.
    """
    best: Optional[Dict[str, Any]] = None
    best_dt: Optional[datetime.datetime] = None
    for value in (hints or {}).values():
        if not isinstance(value, dict):
            continue
        prox = value.get('proxmox') if isinstance(value.get('proxmox'), dict) else {}
        core = value.get('core') if isinstance(value.get('core'), dict) else {}
        has_any = bool(
            (prox.get('secret_id') or prox.get('validated'))
            or (core.get('core_secret_id') or core.get('vm_key') or core.get('validated'))
        )
        if not has_any:
            continue
        dt = (
            _parse_iso_datetime(core.get('stored_at'))
            or _parse_iso_datetime(core.get('last_validated_at'))
            or _parse_iso_datetime(prox.get('stored_at'))
            or _parse_iso_datetime(prox.get('last_validated_at'))
        )
        if best is None:
            best = value
            best_dt = dt
            continue
        if best_dt is None and dt is not None:
            best = value
            best_dt = dt
            continue
        if best_dt is not None and dt is not None and dt > best_dt:
            best = value
            best_dt = dt
    return best


def _builder_catalog_seed_scenarios(
    allowed_norms: set[str],
    assignment_order: Optional[Iterable[str]] = None,
    *,
    user: Optional[dict] = None,
) -> list[Dict[str, Any]]:
    """Hydrate scenario payloads for builders from catalog + assignments."""
    if not allowed_norms:
        return []
    names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(None, user=user)
    hitl_validation_hints = _load_scenario_hitl_validation_from_disk()
    hitl_config_hints = _load_scenario_hitl_config_from_disk()
    builder_hitl_fallback = _select_builder_hitl_fallback(hitl_validation_hints)
    builder_hitl_config_fallback = _select_builder_hitl_fallback(hitl_config_hints) if hitl_config_hints else None
    # Compare permissions using match keys so minor punctuation differences
    # (e.g., "Scenario_1b" vs "Scenario 1b") don't hide scenarios.
    allowed_keys = {k for k in (_scenario_match_key(v) for v in allowed_norms) if k}
    if not allowed_keys:
        return []

    display_by_norm: dict[str, str] = {}
    catalog_norm_by_key: dict[str, str] = {}
    for display in names:
        norm = _normalize_scenario_label(display)
        key = _scenario_match_key(display)
        if norm:
            display_by_norm.setdefault(norm, display)
        if key and norm and key not in catalog_norm_by_key:
            catalog_norm_by_key[key] = norm

    def _humanize(norm_value: str) -> str:
        text = (norm_value or '').replace('_', ' ').strip()
        text = re.sub(r'\s+', ' ', text)
        return text.title() if text else norm_value

    ordered_keys: list[str] = []
    if assignment_order:
        for entry in assignment_order:
            k = _scenario_match_key(entry)
            if k and k in allowed_keys and k not in ordered_keys:
                ordered_keys.append(k)
    for display in names:
        k = _scenario_match_key(display)
        if k and k in allowed_keys and k not in ordered_keys:
            ordered_keys.append(k)
    # Any remaining allowed scenarios (not in catalog/assignment order)
    for k in sorted(allowed_keys):
        if k not in ordered_keys:
            ordered_keys.append(k)

    parsed_cache: dict[str, list[Dict[str, Any]]] = {}
    hydrated: list[Dict[str, Any]] = []

    # Map allowed keys back to a catalog norm if available (preserves exact display names).
    allowed_norm_by_key: dict[str, str] = {}
    for norm in allowed_norms:
        k = _scenario_match_key(norm)
        if k and k not in allowed_norm_by_key:
            allowed_norm_by_key[k] = norm

    for key in ordered_keys:
        norm = catalog_norm_by_key.get(key) or allowed_norm_by_key.get(key) or ''
        display_name = display_by_norm.get(norm) or _humanize(norm or key)
        participant_hint = scenario_url_hints.get(norm, '') if scenario_url_hints else ''
        validation_hint = hitl_validation_hints.get(norm) if hitl_validation_hints else None
        config_hint = hitl_config_hints.get(norm) if hitl_config_hints else None
        scenario_copy: Optional[Dict[str, Any]] = None
        candidate_paths = scenario_paths.get(norm) if scenario_paths else None
        best_path = _select_existing_path(candidate_paths) if candidate_paths else None
        if best_path:
            cached = parsed_cache.get(best_path)
            if cached is None:
                try:
                    parsed = _parse_scenarios_xml(best_path)
                    cached = parsed.get('scenarios', []) if isinstance(parsed, dict) else []
                except Exception:
                    cached = []
                parsed_cache[best_path] = cached
            for scen in cached or []:
                if not isinstance(scen, dict):
                    continue
                scen_norm = _normalize_scenario_label(scen.get('name'))
                if scen_norm == norm:
                    scenario_copy = copy.deepcopy(scen)
                    break
        if not scenario_copy:
            scenario_copy = _builder_placeholder_scenario(display_name, participant_hint)
        else:
            scenario_copy['name'] = display_name
            if participant_hint:
                hitl_meta = scenario_copy.get('hitl') if isinstance(scenario_copy.get('hitl'), dict) else {}
                hitl_meta = dict(hitl_meta)
                for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                    hitl_meta[key] = participant_hint
                scenario_copy['hitl'] = hitl_meta

        # Merge admin-validated HITL hints (safe subset) into builder scenarios.
        if isinstance(validation_hint, dict) and validation_hint:
            hitl_meta = scenario_copy.get('hitl') if isinstance(scenario_copy.get('hitl'), dict) else {}
            hitl_meta = dict(hitl_meta)
            prox_hint = validation_hint.get('proxmox')
            if isinstance(prox_hint, dict) and prox_hint:
                prox_state = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else {}
                prox_state = dict(prox_state)
                prox_state.update(prox_hint)
                hitl_meta['proxmox'] = prox_state
            core_hint = validation_hint.get('core')
            if isinstance(core_hint, dict) and core_hint:
                core_state = hitl_meta.get('core') if isinstance(hitl_meta.get('core'), dict) else {}
                core_state = dict(core_state)
                core_state.update(core_hint)
                hitl_meta['core'] = core_state
            scenario_copy['hitl'] = hitl_meta

        # Builder-only fallback: builders can't validate/select CORE VMs, so fill any missing
        # hint fields (e.g., vm_key) from the best available admin validation hint.
        if builder_hitl_fallback:
            hitl_meta = scenario_copy.get('hitl') if isinstance(scenario_copy.get('hitl'), dict) else {}
            hitl_meta = dict(hitl_meta)
            prox_hint = builder_hitl_fallback.get('proxmox') if isinstance(builder_hitl_fallback.get('proxmox'), dict) else None
            core_hint = builder_hitl_fallback.get('core') if isinstance(builder_hitl_fallback.get('core'), dict) else None

            if isinstance(prox_hint, dict) and prox_hint:
                prox_state = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else {}
                prox_state = dict(prox_state)
                for k, v in prox_hint.items():
                    # Only fill missing values; don't override scenario-specific settings.
                    if k not in prox_state or prox_state.get(k) in (None, '', False):
                        prox_state[k] = v
                hitl_meta['proxmox'] = prox_state

            if isinstance(core_hint, dict) and core_hint:
                core_state = hitl_meta.get('core') if isinstance(hitl_meta.get('core'), dict) else {}
                core_state = dict(core_state)
                for k, v in core_hint.items():
                    if k not in core_state or core_state.get(k) in (None, '', False):
                        core_state[k] = v
                hitl_meta['core'] = core_state

            scenario_copy['hitl'] = hitl_meta

        # Merge admin-managed HITL configuration (enabled/interfaces/mappings) into builder scenarios.
        effective_cfg = config_hint if (isinstance(config_hint, dict) and config_hint) else builder_hitl_config_fallback
        if isinstance(effective_cfg, dict) and effective_cfg:
            hitl_meta = scenario_copy.get('hitl') if isinstance(scenario_copy.get('hitl'), dict) else {}
            hitl_meta = dict(hitl_meta)
            # Config hints are authoritative for builder views.
            try:
                participant_cfg = _normalize_participant_proxmox_url(effective_cfg.get('participant_proxmox_url'))
                if participant_cfg:
                    for k in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                        if k not in hitl_meta or hitl_meta.get(k) in (None, ''):
                            hitl_meta[k] = participant_cfg
            except Exception:
                pass
            if 'enabled' in effective_cfg:
                hitl_meta['enabled'] = bool(effective_cfg.get('enabled'))
            if isinstance(effective_cfg.get('interfaces'), list):
                hitl_meta['interfaces'] = effective_cfg.get('interfaces')
            if isinstance(effective_cfg.get('core'), dict):
                core_state = hitl_meta.get('core') if isinstance(hitl_meta.get('core'), dict) else {}
                core_state = dict(core_state)
                core_state.update(effective_cfg.get('core'))
                hitl_meta['core'] = core_state
            if isinstance(effective_cfg.get('proxmox'), dict):
                prox_state = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else {}
                prox_state = dict(prox_state)
                prox_state.update(effective_cfg.get('proxmox'))
                hitl_meta['proxmox'] = prox_state
            scenario_copy['hitl'] = hitl_meta
        hydrated.append(scenario_copy)
    return hydrated


def _builder_filter_report_scenarios(
    scenario_names: list[str],
    scenario_norm: str,
    *,
    user: Optional[dict] = None,
) -> tuple[list[str], str, Optional[set[str]]]:
    allowed_norms = _builder_allowed_norms(user)
    if allowed_norms is None:
        return scenario_names, scenario_norm, None
    allowed_keys = {key for key in (_scenario_match_key(v) for v in allowed_norms) if key}
    filtered = [
        name for name in scenario_names or []
        if _scenario_match_key(name) in allowed_keys
    ]
    normalized_selection = scenario_norm if (not scenario_norm or _scenario_match_key(scenario_norm) in allowed_keys) else ''
    return filtered, normalized_selection, allowed_norms

# Diagnostic endpoint for environment/module troubleshooting
@app.route('/diag/modules')
def diag_modules():
    out = {}
    # core_topo_gen package file
    try:
        import core_topo_gen as ctg  # type: ignore
        out['core_topo_gen.__file__'] = getattr(ctg, '__file__', None)
    except Exception as e:
        out['core_topo_gen_error'] = str(e)
    # planning package
    try:
        import core_topo_gen.planning as plan_pkg  # type: ignore
        planning_file = getattr(plan_pkg, '__file__', None)
        out['planning_dir'] = os.path.dirname(planning_file) if planning_file else None
        if not planning_file:
            out['planning_file_is_none'] = True
    except Exception as e:
        out['planning_import_error'] = str(e)

def _current_user() -> dict | None:
    user = session.get('user')
    if isinstance(user, dict) and user.get('username'):
        return user
    return None


def _set_current_user(user: dict | None) -> None:
    if user:
        session['user'] = {
            'username': user.get('username'),
            'role': _normalize_role_value(user.get('role'))
        }
    else:
        session.pop('user', None)


@app.before_request
def _inject_current_user() -> None:
    try:
        g.current_user = _current_user()
    except Exception:
        g.current_user = None


@app.before_request
def _bind_ui_view_mode() -> None:
    try:
        g.ui_view_mode = _current_ui_view_mode()
    except Exception:
        g.ui_view_mode = _UI_VIEW_DEFAULT


@app.context_processor
def _inject_template_user() -> dict:
    try:
        user = _current_user()
        if user:
            return {
                'current_user': SimpleNamespace(
                    username=user.get('username'),
                    role=_normalize_role_value(user.get('role')),
                    is_authenticated=True,
                )
            }
    except Exception:
        pass
    return {
        'current_user': SimpleNamespace(
            username=None,
            role=None,
            is_authenticated=False,
        )
    }


@app.context_processor
def _inject_nav_participant_link() -> dict:
    try:
        # If the current page is scoped to a scenario, the navbar Participant UI link
        # should reflect THAT scenario only. If that scenario has no participant URL,
        # hide the nav item instead of falling back to some other scenario that does.
        scenario_norm = ''
        scenario_label = ''
        try:
            scenario_label = (request.args.get('scenario') or '').strip() if has_request_context() else ''
            scenario_norm = _normalize_scenario_label(scenario_label)
        except Exception:
            scenario_norm = ''
            scenario_label = ''

        if scenario_norm:
            user = _current_user()
            scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(None, user=user)
            mapping = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
            url_value = mapping.get(scenario_norm, '')
            return {
                'nav_participant_url': url_value or '',
                'nav_participant_scenario': scenario_label or '',
                'nav_participant_enabled': bool(url_value),
            }

        # If the page is not scoped to a scenario (no `?scenario=`), avoid "guessing" a participant URL
        # for admin/builder views. Otherwise CORE/Reports can show Participant UI for an unrelated scenario.
        view_mode = getattr(g, 'ui_view_mode', _UI_VIEW_DEFAULT)
        if view_mode != 'participant':
            user = _current_user()
            scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(None, user=user)
            mapping = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
            any_enabled = any(bool(v) for v in (mapping or {}).values())
            return {
                'nav_participant_url': '',
                'nav_participant_scenario': '',
                'nav_participant_enabled': bool(any_enabled),
            }

        url_value, scenario_label = _resolve_participant_ui_target()
        return {
            'nav_participant_url': url_value,
            'nav_participant_scenario': scenario_label,
            'nav_participant_enabled': bool(url_value),
        }
    except Exception:
        return {'nav_participant_url': '', 'nav_participant_scenario': '', 'nav_participant_enabled': False}


@app.context_processor
def _inject_ui_view_state() -> dict:
    return {'ui_view_mode': getattr(g, 'ui_view_mode', _UI_VIEW_DEFAULT)}


@app.route('/ui-view', methods=['POST'])
def set_ui_view_mode():
    user = _current_user()
    if not user or not _is_admin_view_role(user.get('role')):
        abort(403)
    requested = (request.form.get('mode') or '').strip().lower()
    if requested not in _UI_VIEW_ALLOWED:
        requested = _UI_VIEW_DEFAULT
    role = _normalize_role_value(user.get('role'))
    if role == 'builder' and requested == 'admin':
        requested = 'builder'
    if role not in _ADMIN_VIEW_ROLES:
        requested = _UI_VIEW_DEFAULT
    session[_UI_VIEW_SESSION_KEY] = requested
    target = request.form.get('next') or request.referrer
    scenario_hint = ''
    if target:
        try:
            parsed_target = urlparse(target)
            query_params = parse_qs(parsed_target.query or '')
            scenario_hint = (query_params.get('scenario', [''])[0] or '').strip()
        except Exception:
            scenario_hint = ''
    if not scenario_hint:
        try:
            scenario_hint = (request.form.get('scenario') or request.args.get('scenario') or '').strip()
        except Exception:
            scenario_hint = ''
    if requested == 'participant':
        redirect_target = url_for('participant_ui_page', scenario=scenario_hint) if scenario_hint else url_for('participant_ui_page')
    else:
        redirect_target = _resolve_ui_view_redirect_target(target)
    return redirect(redirect_target)


@app.route('/participant-ui')
def participant_ui_page():
    state = _participant_ui_state()
    url_value = state.get('selected_url', '')
    scenario_label = state.get('selected_label', '')
    nearest_gateway = state.get('selected_nearest_gateway', '')
    override = _normalize_participant_proxmox_url(request.args.get('url')) if request.args.get('url') else ''
    participant_url = override or url_value
    return render_template(
        'participant_ui.html',
        participant_url=participant_url,
        participant_scenario_label=scenario_label,
        participant_nearest_gateway=nearest_gateway,
        participant_scenarios=state.get('listing', []),
        participant_scenarios_heading=state.get('listing_heading'),
        participant_scenarios_hint=state.get('listing_hint'),
        participant_scenarios_empty=state.get('listing_empty_message'),
        participant_restricted=state.get('restrict_to_assigned', False),
        participant_active_norm=state.get('selected_norm', ''),
        participant_has_assignments=state.get('has_assignments', False),
    )


@app.route('/participant-ui/gateway')
def participant_ui_gateway_api():
    state = _participant_ui_state()
    scenario_norm = _normalize_scenario_label(request.args.get('scenario') or '')
    if not scenario_norm:
        scenario_norm = state.get('selected_norm', '')

    # Enforce assignment-based access: do not allow restricted users to query
    # gateway details for scenarios outside their assigned list.
    try:
        if state.get('restrict_to_assigned'):
            allowed_norms = {row.get('norm') for row in (state.get('listing') or []) if isinstance(row, dict) and row.get('norm')}
            if scenario_norm and scenario_norm not in allowed_norms:
                return jsonify({'ok': False, 'error': 'Scenario not assigned.'}), 403
    except Exception:
        pass

    gateway = ''

    # Prefer the most recent session XML for this scenario (matches core.html HITL gateway logic).
    try:
        history = _load_run_history()
        last_run = _latest_run_history_for_scenario(scenario_norm, history)
    except Exception:
        last_run = None
    session_xml_path = None
    if isinstance(last_run, dict):
        session_xml_path = last_run.get('session_xml_path') or last_run.get('post_xml_path')
    if session_xml_path:
        try:
            hitl = _hitl_details_from_path(str(session_xml_path))
            first = hitl[0] if isinstance(hitl, list) and hitl else None
            ips = first.get('ips') if isinstance(first, dict) else None
            if isinstance(ips, list) and ips:
                gateway = str(ips[0]).split('/', 1)[0]
        except Exception:
            gateway = ''

    # Fallback to participant state / saved XML mapping if we couldn't derive it from the last session XML.
    if not gateway:
        if scenario_norm and scenario_norm == state.get('selected_norm', ''):
            gateway = state.get('selected_nearest_gateway', '')
        else:
            try:
                _names, scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(None, user=_current_user())
            except Exception:
                scenario_paths = {}
            gateway = _nearest_gateway_address_for_scenario(scenario_norm, scenario_paths=scenario_paths)

    return jsonify({'ok': True, 'scenario_norm': scenario_norm, 'nearest_gateway': gateway or ''})


def _parse_iso_ts(ts: Any) -> float:
    if not ts:
        return 0.0
    if not isinstance(ts, str):
        ts = str(ts)
    text = ts.strip()
    if not text:
        return 0.0
    try:
        return datetime.datetime.fromisoformat(text.replace('Z', '+00:00')).timestamp()
    except Exception:
        return 0.0


def _latest_run_history_for_scenario(scenario_norm: str, history: Optional[list[dict]] = None) -> Optional[dict]:
    scenario_norm = _normalize_scenario_label(scenario_norm)
    if not scenario_norm:
        return None
    if history is None:
        history = _load_run_history()
    filtered = _filter_history_by_scenario(history, scenario_norm)
    if not filtered:
        return None
    best: Optional[dict] = None
    best_ts = -1.0
    for entry in filtered:
        if not isinstance(entry, dict):
            continue
        ts_val = _parse_iso_ts(entry.get('timestamp'))
        if ts_val > best_ts:
            best_ts = ts_val
            best = entry
    return best


def _load_summary_counts(summary_path: Optional[str]) -> dict:
    if not summary_path:
        return {}
    try:
        ap = os.path.abspath(str(summary_path))
    except Exception:
        ap = str(summary_path)
    if not ap or not os.path.exists(ap):
        return {}
    try:
        with open(ap, 'r', encoding='utf-8') as f:
            payload = json.load(f)
        counts = payload.get('counts') if isinstance(payload, dict) else None
        return counts if isinstance(counts, dict) else {}
    except Exception:
        return {}


def _load_summary_metadata(summary_path: Optional[str]) -> dict:
    if not summary_path:
        return {}
    try:
        ap = os.path.abspath(str(summary_path))
    except Exception:
        ap = str(summary_path)
    if not ap or not os.path.exists(ap):
        return {}
    try:
        with open(ap, 'r', encoding='utf-8') as f:
            payload = json.load(f)
        meta = payload.get('metadata') if isinstance(payload, dict) else None
        return meta if isinstance(meta, dict) else {}
    except Exception:
        return {}


def _summary_text_from_counts(counts: dict) -> str:
    if not isinstance(counts, dict) or not counts:
        return ''
    def _int(key: str) -> int:
        try:
            return int(counts.get(key) or 0)
        except Exception:
            return 0
    total_nodes = _int('total_nodes')
    hosts = _int('hosts')
    routers = _int('routers')
    switches = _int('switches')
    seg_rules = _int('segmentation_rules')
    flows = _int('traffic_flows')
    parts = []
    if total_nodes:
        parts.append(f"Nodes {total_nodes}")
    if hosts or routers or switches:
        sub = []
        if hosts:
            sub.append(f"hosts {hosts}")
        if routers:
            sub.append(f"routers {routers}")
        if switches:
            sub.append(f"switches {switches}")
        if sub:
            parts.append(f"({', '.join(sub)})")
    if seg_rules:
        parts.append(f"seg {seg_rules}")
    if flows:
        parts.append(f"flows {flows}")
    return ' '.join(parts)


def _seed_from_preview_plan(preview_plan_path: Optional[str]) -> Optional[int]:
    if not preview_plan_path:
        return None
    try:
        ap = os.path.abspath(str(preview_plan_path))
    except Exception:
        ap = str(preview_plan_path)
    if not ap or not os.path.exists(ap):
        return None
    try:
        with open(ap, 'r', encoding='utf-8') as f:
            payload = json.load(f)
        if not isinstance(payload, dict):
            return None
        meta = payload.get('metadata') if isinstance(payload.get('metadata'), dict) else {}
        seed = meta.get('seed')
        if seed is None and isinstance(payload.get('full_preview'), dict):
            seed = payload['full_preview'].get('seed')
        if seed is None:
            return None
        return int(seed)
    except Exception:
        return None


def _switch_names_from_session_xml(session_xml_path: Optional[str]) -> list[str]:
    if not session_xml_path:
        return []
    try:
        ap = os.path.abspath(str(session_xml_path))
    except Exception:
        ap = str(session_xml_path)
    if not ap or not os.path.exists(ap):
        return []
    try:
        summary = _analyze_core_xml(ap)
    except Exception:
        summary = {}
    switches = summary.get('switches') if isinstance(summary, dict) else None
    if not isinstance(switches, list):
        return []
    out: list[str] = []
    for s in switches:
        name = str(s).strip() if s is not None else ''
        if name and name not in out:
            out.append(name)
    return out


def _subnet_cidrs_from_session_xml(session_xml_path: Optional[str]) -> list[str]:
    if not session_xml_path:
        return []
    try:
        ap = os.path.abspath(str(session_xml_path))
    except Exception:
        ap = str(session_xml_path)
    if not ap or not os.path.exists(ap):
        return []

    try:
        import ipaddress
    except Exception:
        return []

    try:
        summary = _analyze_core_xml(ap)
    except Exception:
        summary = {}
    nodes = summary.get('nodes') if isinstance(summary, dict) else None
    if not isinstance(nodes, list):
        return []

    networks: set[str] = set()

    def _add_ipv4(ipv4: Any, ipv4_mask: Any = None) -> None:
        if not ipv4:
            return
        ip_text = str(ipv4).strip()
        if not ip_text:
            return
        try:
            if '/' in ip_text:
                iface = ipaddress.ip_interface(ip_text)
            else:
                mask_text = str(ipv4_mask).strip() if ipv4_mask else ''
                if not mask_text:
                    return
                iface = ipaddress.ip_interface(f"{ip_text}/{mask_text}")
            net = iface.network
            # Filter out host routes; we only want meaningful subnetworks.
            if getattr(net, 'prefixlen', 32) >= 32:
                return
            networks.add(str(net))
        except Exception:
            return

    for node in nodes:
        if not isinstance(node, dict):
            continue
        ifaces = node.get('interfaces')
        if not isinstance(ifaces, list):
            continue
        for iface in ifaces:
            if not isinstance(iface, dict):
                continue
            _add_ipv4(iface.get('ipv4'), iface.get('ipv4_mask'))

    return sorted(networks, key=lambda s: (s.split('/', 1)[0], int(s.split('/', 1)[1]) if '/' in s else 999))


def _vulnerability_ipv4s_from_session_xml(session_xml_path: Optional[str]) -> list[str]:
    """Best-effort vulnerability IPv4 addresses from a CORE session XML.

    We treat Docker-backed nodes as "vulnerabilities" for Participant UI purposes.
    """

    if not session_xml_path:
        return []
    try:
        ap = os.path.abspath(str(session_xml_path))
    except Exception:
        ap = str(session_xml_path)
    if not ap or not os.path.exists(ap):
        return []

    try:
        import ipaddress
    except Exception:
        return []

    # NOTE: We intentionally parse the XML directly instead of relying on
    # _analyze_core_xml() because its node list is filtered for UI convenience
    # and can drop docker/compose nodes when interface/link info is missing.
    try:
        root = LET.parse(ap).getroot()
    except Exception:
        return []

    def _local(tag: str) -> str:
        if not tag:
            return ''
        if '}' in tag:
            return tag.split('}', 1)[1]
        return tag

    def _iter_local(el, lname: str):
        lname = lname.lower()
        for e in el.iter():
            if _local(getattr(e, 'tag', '')).lower() == lname:
                yield e

    def _looks_like_vuln(dev) -> bool:
        try:
            t = str(dev.get('type') or '').strip().lower()
        except Exception:
            t = ''
        try:
            cls = str(dev.get('class') or '').strip().lower()
        except Exception:
            cls = ''
        try:
            comp = str(dev.get('compose') or '').strip()
        except Exception:
            comp = ''
        try:
            comp_name = str(dev.get('compose_name') or '').strip()
        except Exception:
            comp_name = ''
        if 'docker' in t or 'docker' in cls:
            return True
        if comp or comp_name:
            return True
        # fall back to services, if present
        try:
            for svc in dev.findall('.//service'):
                nm = (svc.get('name') or (svc.text or '')).strip().lower()
                if 'docker' in nm:
                    return True
        except Exception:
            pass
        return False

    vuln_ids: set[str] = set()
    # session exports sometimes use <node> instead of <device>
    for dev in list(_iter_local(root, 'device')) + list(_iter_local(root, 'node')):
        if not _looks_like_vuln(dev):
            continue
        did = str(dev.get('id') or '').strip()
        if did:
            vuln_ids.add(did)

    if not vuln_ids:
        return []

    ips: set[str] = set()

    def _add_ipv4(ip_value: Any) -> None:
        if not ip_value:
            return
        ip_raw = str(ip_value).strip()
        if not ip_raw:
            return
        ip_text = ip_raw.split('/', 1)[0].strip()
        if not ip_text:
            return
        try:
            ip_obj = ipaddress.ip_address(ip_text)
        except Exception:
            return
        if getattr(ip_obj, 'version', None) != 4:
            return
        if ip_obj.is_loopback or ip_obj.is_unspecified or ip_obj.is_link_local:
            return
        ips.add(str(ip_obj))

    def _maybe_add_ipv4_from_attrib(attrib: dict) -> None:
        if not attrib:
            return
        for k, v in attrib.items():
            key = str(k or '').strip().lower().replace('-', '_')
            if not key:
                continue
            # Common encodings seen across CORE XML exports.
            if key in {'ip4', 'ipv4', 'ip', 'addr', 'address'}:
                _add_ipv4(v)
                continue
            if key.endswith('_ip4') or key.endswith('_ipv4') or key.endswith('_ip'):
                _add_ipv4(v)
                continue

    # Collect addresses from link endpoints.
    for link in _iter_local(root, 'link'):
        try:
            n1 = str(link.get('node1') or link.get('node1_id') or '').strip()
            n2 = str(link.get('node2') or link.get('node2_id') or '').strip()
        except Exception:
            n1, n2 = '', ''

        # Common CORE format: iface1 belongs to node1; iface2 belongs to node2.
        try:
            if n1 and n1 in vuln_ids:
                iface1 = next(_iter_local(link, 'iface1'), None)
                if iface1 is not None:
                    _maybe_add_ipv4_from_attrib(getattr(iface1, 'attrib', {}) or {})
            if n2 and n2 in vuln_ids:
                iface2 = next(_iter_local(link, 'iface2'), None)
                if iface2 is not None:
                    _maybe_add_ipv4_from_attrib(getattr(iface2, 'attrib', {}) or {})
        except Exception:
            pass

        # Additional address encodings
        try:
            for child in list(link):
                tag = _local(getattr(child, 'tag', '')).lower()
                if tag not in ('iface', 'interface', 'addr', 'address'):
                    continue
                target = str(child.get('node') or child.get('node_id') or child.get('device') or '').strip()
                if target and target not in vuln_ids:
                    continue
                _maybe_add_ipv4_from_attrib(getattr(child, 'attrib', {}) or {})
                _add_ipv4(child.get('value') or (child.text or '').strip())
        except Exception:
            pass

    # Also attempt to read any interfaces nested under the vuln node itself.
    for dev in list(_iter_local(root, 'device')) + list(_iter_local(root, 'node')):
        did = str(dev.get('id') or '').strip()
        if not did or did not in vuln_ids:
            continue
        try:
            _maybe_add_ipv4_from_attrib(getattr(dev, 'attrib', {}) or {})
            for iface in list(_iter_local(dev, 'interface')) + list(_iter_local(dev, 'iface')):
                _maybe_add_ipv4_from_attrib(getattr(iface, 'attrib', {}) or {})
        except Exception:
            pass

        # Fallback: scan any descendant elements for common address attributes.
        try:
            for child in dev.iter():
                _maybe_add_ipv4_from_attrib(getattr(child, 'attrib', {}) or {})
                tag = _local(getattr(child, 'tag', '')).lower()
                if tag in {'ip', 'ip4', 'ipv4', 'addr', 'address'}:
                    _add_ipv4((child.text or '').strip())
        except Exception:
            pass

    try:
        return sorted(ips, key=lambda s: int(ipaddress.ip_address(s)))
    except Exception:
        return sorted(ips)


def _counts_from_session_xml(session_xml_path: Optional[str]) -> dict:
    """Best-effort {nodes, routers, switches} from a CORE session XML.

    This intentionally parses the XML directly (instead of _analyze_core_xml()) so
    that counts are not affected by any UI-oriented filtering.
    """
    if not session_xml_path:
        return {}
    try:
        ap = os.path.abspath(str(session_xml_path))
    except Exception:
        ap = str(session_xml_path)
    if not ap or not os.path.exists(ap):
        return {}
    try:
        root = LET.parse(ap).getroot()
    except Exception:
        return {}

    def _local(tag: str) -> str:
        if not tag:
            return ''
        if '}' in tag:
            return tag.split('}', 1)[1]
        return tag

    def _iter_local(el, lname: str):
        lname = lname.lower()
        for e in el.iter():
            if _local(getattr(e, 'tag', '')).lower() == lname:
                yield e

    candidates = list(_iter_local(root, 'device')) + list(_iter_local(root, 'node'))
    devices: list[Any] = []
    seen_ids: set[str] = set()
    for cand in candidates:
        ident = cand.get('id') or cand.get('name')
        key = str(ident).strip() if ident is not None else ''
        if key and key in seen_ids:
            continue
        if key:
            seen_ids.add(key)
        devices.append(cand)

    def _dev_type(dev) -> str:
        try:
            t = str(dev.get('type') or '').strip()
        except Exception:
            t = ''
        if not t and hasattr(dev, 'find'):
            try:
                type_el = dev.find('./type') or dev.find('./model') or dev.find('./icon')
                if type_el is not None and getattr(type_el, 'text', None):
                    t = type_el.text.strip()
            except Exception:
                t = ''
        return t

    routers = 0
    switches_device = 0
    switch_ids: set[str] = set()
    for idx, dev in enumerate(devices, start=1):
        try:
            did = str((dev.get('id') or dev.get('name') or f"device_{idx}") or '').strip() or f"device_{idx}"
        except Exception:
            did = f"device_{idx}"
        t = _dev_type(dev).lower()
        if 'router' in t:
            routers += 1
        if t == 'switch':
            switches_device += 1
            if did:
                switch_ids.add(did)

    # Some session exports represent switches as <network type="switch"> entries.
    extra_switches = 0
    try:
        for net in _iter_local(root, 'network'):
            ntype = str(net.get('type') or '').strip().lower()
            if 'switch' not in ntype:
                continue
            nid = str(net.get('id') or net.get('name') or '').strip()
            if nid and nid in switch_ids:
                continue
            extra_switches += 1
    except Exception:
        extra_switches = 0

    out: dict[str, Any] = {}
    out['nodes'] = len(devices)
    out['routers'] = routers
    out['switches'] = switches_device + extra_switches
    return out


def _recent_session_id_for_scenario(
    scenario_norm: str,
    *,
    scenario_paths: dict[str, set[str]],
) -> tuple[Optional[int], float]:
    """Return (session_id, last_seen_epoch) from the newest known CORE session XML mapping."""
    scenario_norm = _normalize_scenario_label(scenario_norm)
    if not scenario_norm:
        return None, 0.0
    try:
        store = _load_core_sessions_store()
    except Exception:
        store = {}
    best_sid: Optional[int] = None
    best_mtime: float = 0.0
    for path, entry in (store or {}).items():
        if not path:
            continue
        stored_norm = _session_store_entry_scenario_norm(entry)
        if stored_norm and stored_norm != scenario_norm:
            continue
        if not stored_norm and not _path_matches_scenario(path, scenario_norm, scenario_paths):
            continue
        try:
            ap = os.path.abspath(str(path))
        except Exception:
            ap = str(path)
        if not ap or not os.path.exists(ap):
            continue
        try:
            mtime = os.path.getmtime(ap)
        except Exception:
            mtime = 0.0
        sid = _session_store_entry_session_id(entry)
        if sid is None:
            continue
        if best_sid is None or mtime > best_mtime:
            best_sid = sid
            best_mtime = mtime
    return best_sid, best_mtime


def _live_core_session_status_for_scenario(
    scenario_norm: str,
    *,
    history: list[dict],
    scenario_names: list[str],
    scenario_paths: dict[str, set[str]],
) -> Optional[dict]:
    """Best-effort live CORE session status for a scenario.

    Returns None if CORE cannot be queried (no credentials / remote failure).
        Otherwise returns: {running: bool|None, session_id: int|None, state: str}.

        Notes:
        - running=True means we could positively associate an active CORE session
            with the selected scenario.
        - running=False means we could query CORE and there are no active sessions.
        - running=None means we could query CORE and there are active sessions, but
            we could not confidently associate any of them with the selected scenario.

    This does not expose host/port; it only uses them server-side.
    """

    scenario_norm = _normalize_scenario_label(scenario_norm)
    if not scenario_norm:
        return None

    try:
        mapping = _load_core_sessions_store()
    except Exception:
        mapping = {}

    scenario_session_ids = _session_ids_for_scenario(mapping, scenario_norm, scenario_paths)

    try:
        core_cfg = _select_core_config_for_page(scenario_norm, history, include_password=True)
        host = core_cfg.get('host', CORE_HOST)
        port = int(core_cfg.get('port', CORE_PORT))
    except Exception:
        return None

    errors: list[str] = []
    meta: dict[str, Any] = {}
    try:
        sessions = _list_active_core_sessions(host, port, core_cfg, errors=errors, meta=meta)
    except Exception:
        return None
    if not isinstance(sessions, list):
        return None

    def _is_active(sess: dict) -> bool:
        state_raw = str(sess.get('state') or '').strip().lower()
        return state_raw not in {'shutdown'}

    def _session_id_int(sess: dict) -> Optional[int]:
        sid = sess.get('id')
        try:
            return int(sid) if sid is not None else None
        except Exception:
            return None

    def _matches(sess: dict) -> bool:
        # 1) session_id linkage via stored XML mapping
        sid_int = _session_id_int(sess)
        if sid_int is not None and sid_int in scenario_session_ids:
            return True
        # 2) CORE-provided scenario_name match (if present)
        label = _normalize_scenario_label(sess.get('scenario_name')) if sess.get('scenario_name') else ''
        if label and label == scenario_norm:
            return True
        # 3) file/dir path match
        if _path_matches_scenario(sess.get('file'), scenario_norm, scenario_paths):
            return True
        if _path_matches_scenario(sess.get('dir'), scenario_norm, scenario_paths):
            return True
        return False

    active_sessions = [s for s in sessions if isinstance(s, dict) and _is_active(s)]
    matching_active = [s for s in active_sessions if _matches(s)]

    for sess in matching_active:
        if not isinstance(sess, dict):
            continue
        state_raw = str(sess.get('state') or '').strip().lower()
        sid_int = _session_id_int(sess)
        return {
            'running': True,
            'session_id': sid_int,
            'state': state_raw,
        }

    # If there is exactly one active CORE session and the catalog only contains
    # one scenario (common in small deployments/tests), assume that session
    # belongs to the selected scenario.
    if len(active_sessions) == 1:
        try:
            only_catalog = [_normalize_scenario_label(n) for n in (scenario_names or [])]
            only_catalog = [n for n in only_catalog if n]
        except Exception:
            only_catalog = []
        if len(only_catalog) == 1 and only_catalog[0] == scenario_norm:
            sess = active_sessions[0]
            state_raw = str(sess.get('state') or '').strip().lower()
            sid_int = _session_id_int(sess)
            return {
                'running': True,
                'session_id': sid_int,
                'state': state_raw,
            }

    # Queried successfully but no matching active sessions.
    # If there are active sessions but none match the selected scenario, prefer
    # Unknown (None) over incorrectly marking the selected scenario as Running.
    if active_sessions:
        # Conservative fallback: if there is exactly one scenario in scope and
        # there are active CORE sessions, treat the first active session as the
        # selected scenario's session. This avoids reporting Unknown in fresh
        # runs where session->scenario linkage isn't available yet.
        try:
            if isinstance(scenario_names, list) and len(scenario_names) == 1:
                only_norm = _normalize_scenario_label(scenario_names[0])
                if only_norm and only_norm == scenario_norm:
                    first = active_sessions[0] if active_sessions else None
                    if isinstance(first, dict):
                        return {
                            'running': True,
                            'session_id': _session_id_int(first),
                            'state': str(first.get('state') or '').strip().lower(),
                        }
        except Exception:
            pass
        return {
            'running': None,
            'session_id': None,
            'state': '',
        }

    return {
        'running': False,
        'session_id': None,
        'state': '',
    }


@app.route('/participant-ui/details')
def participant_ui_details_api():
    state = _participant_ui_state()
    scenario_norm = _normalize_scenario_label(request.args.get('scenario') or '')
    if not scenario_norm:
        scenario_norm = _normalize_scenario_label(state.get('selected_norm', ''))

    listing = state.get('listing', [])
    listing_entry: Optional[dict] = None
    if isinstance(listing, list) and scenario_norm:
        for entry in listing:
            if isinstance(entry, dict) and (entry.get('norm') or '') == scenario_norm:
                listing_entry = entry
                break

    display = ''
    has_url = False
    assigned = False
    placeholder = False
    if isinstance(listing_entry, dict):
        display = str(listing_entry.get('display') or '')
        has_url = bool(listing_entry.get('has_url'))
        assigned = bool(listing_entry.get('assigned'))
        placeholder = bool(listing_entry.get('placeholder'))

    # Global open stats
    stats = _load_participant_ui_stats()
    scenarios_stats = stats.get('scenarios') if isinstance(stats.get('scenarios'), dict) else {}
    scenario_stats = scenarios_stats.get(scenario_norm, {}) if scenario_norm else {}
    if not isinstance(scenario_stats, dict):
        scenario_stats = {}

    # Execute history
    history = _load_run_history()
    last_run = _latest_run_history_for_scenario(scenario_norm, history)
    last_execute_ts = (last_run or {}).get('timestamp') if isinstance(last_run, dict) else ''
    returncode = (last_run or {}).get('returncode') if isinstance(last_run, dict) else None
    try:
        returncode_int = int(returncode) if returncode is not None else None
    except Exception:
        returncode_int = None
    last_execute_ok = (returncode_int == 0) if returncode_int is not None else None

    summary_path = (last_run or {}).get('summary_path') if isinstance(last_run, dict) else None
    summary_counts = _load_summary_counts(summary_path)
    summary_meta = _load_summary_metadata(summary_path)

    # Counts
    nodes_total = summary_counts.get('total_nodes')
    routers_total = summary_counts.get('routers')
    switches_total = summary_counts.get('switches')
    try:
        nodes_total = int(nodes_total) if nodes_total is not None else None
    except Exception:
        nodes_total = None
    try:
        routers_total = int(routers_total) if routers_total is not None else None
    except Exception:
        routers_total = None
    try:
        switches_total = int(switches_total) if switches_total is not None else None
    except Exception:
        switches_total = None

    # Subnetworks (best-effort: CIDR networks from session XML interface addresses)
    session_xml_path = None
    if isinstance(last_run, dict):
        session_xml_path = last_run.get('session_xml_path') or last_run.get('post_xml_path')
    session_xml_exists = bool(session_xml_path and os.path.exists(str(session_xml_path)))
    subnetworks = _subnet_cidrs_from_session_xml(session_xml_path) if session_xml_exists else []

    vulnerability_ips = _vulnerability_ipv4s_from_session_xml(session_xml_path) if session_xml_exists else []

    # Vulnerabilities count:
    # - Prefer actual session XML-derived value when we can parse a real file.
    # - Otherwise fall back to the planned additive count from the summary.
    vuln_total: Optional[int] = None
    xml_exists = False
    try:
        xml_exists = bool(session_xml_path and os.path.exists(str(session_xml_path)))
    except Exception:
        xml_exists = False
    if xml_exists:
        vuln_total = len(vulnerability_ips)
    else:
        planned = summary_meta.get('vuln_total_planned_additive') if isinstance(summary_meta, dict) else None
        try:
            vuln_total = int(planned) if planned is not None else None
        except Exception:
            vuln_total = None

    # Gateway: prefer the scenario's last session XML (matches core.html HITL gateway logic)
    gateway = ''
    if session_xml_path:
        try:
            hitl = _hitl_details_from_path(str(session_xml_path))
            first = hitl[0] if isinstance(hitl, list) and hitl else None
            ips = first.get('ips') if isinstance(first, dict) else None
            if isinstance(ips, list) and ips:
                gateway = str(ips[0]).split('/', 1)[0]
        except Exception:
            gateway = ''

    # Fallback to participant state / saved XML mapping if session XML doesn't yield a gateway.
    if not gateway:
        if scenario_norm and scenario_norm == _normalize_scenario_label(state.get('selected_norm', '')):
            gateway = str(state.get('selected_nearest_gateway') or '')
        else:
            try:
                _names, scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(None, user=_current_user())
            except Exception:
                scenario_paths = {}
            gateway = _nearest_gateway_address_for_scenario(scenario_norm, scenario_paths=scenario_paths) if scenario_norm else ''

    # Prefer counts from session XML when available (authoritative for Participant UI).
    xml_counts = _counts_from_session_xml(session_xml_path)
    if isinstance(xml_counts.get('nodes'), int):
        nodes_total = xml_counts.get('nodes')
    if isinstance(xml_counts.get('routers'), int):
        routers_total = xml_counts.get('routers')
    if isinstance(xml_counts.get('switches'), int):
        switches_total = xml_counts.get('switches')

    # Session status: prefer live CORE query; fall back to unknown instead of guessing.
    session_running: Optional[bool] = None
    session_state = ''
    session_id: Optional[int] = None
    # Catalog paths/names for live lookup.
    scenario_names_live: list[str]
    scenario_paths_live: dict[str, set[str]]
    try:
        scenario_names_live, scenario_paths_live, _scenario_url_hints_live = _scenario_catalog_for_user(history, user=_current_user())
    except Exception:
        scenario_names_live, scenario_paths_live = [], {}

    live = _live_core_session_status_for_scenario(
        scenario_norm,
        history=history,
        scenario_names=scenario_names_live,
        scenario_paths=scenario_paths_live,
    )

    if isinstance(live, dict):
        running_val = live.get('running')
        session_running = running_val if isinstance(running_val, bool) else None
        session_state = str(live.get('state') or '')
        sid_val = live.get('session_id')
        try:
            session_id = int(sid_val) if sid_val is not None else None
        except Exception:
            session_id = None
    else:
        session_running = None

    if session_id is None:
        # Best-effort show the most recent known session id from saved XML mapping.
        try:
            session_id, _last_seen = _recent_session_id_for_scenario(scenario_norm, scenario_paths=scenario_paths_live)
        except Exception:
            session_id = None

    try:
        app.logger.info(
            '[flow.prepare_preview_for_execute] done scenario=%s chain_len=%s flow_valid=%s flow_errors=%s',
            scenario_norm,
            len(chain_nodes or []),
            bool(flow_valid),
            (flow_errors or []),
        )
    except Exception:
        pass

    try:
        realized_flags: list[str] = []
        for fa in (flag_assignments or []):
            if not isinstance(fa, dict):
                continue
            ro = fa.get('resolved_outputs') if isinstance(fa.get('resolved_outputs'), dict) else {}
            flag_val = None
            if isinstance(ro, dict):
                flag_val = ro.get('Flag(flag_id)') or ro.get('flag')
            if not flag_val:
                flag_val = fa.get('flag_value')
            if isinstance(flag_val, str) and flag_val.strip():
                realized_flags.append(flag_val.strip())
        if realized_flags and len(set(realized_flags)) != len(realized_flags):
            return jsonify({
                'ok': False,
                'error': 'Duplicate flag value detected during resolve; retry with a different chain.',
                'scenario': scenario_label or scenario_norm,
                'length': length,
                'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or '')} for n in chain_nodes],
                'flag_assignments': flag_assignments,
            }), 422
    except Exception:
        pass

    return jsonify({
        'ok': True,
        'scenario_norm': scenario_norm,
        'scenario': {
            'display': display,
            'assigned': assigned,
            'placeholder': placeholder,
            'participant_link_configured': bool(has_url),
        },
        'gateway': gateway or '',
        'open_stats': {
            'open_count': int(scenario_stats.get('open_count') or 0),
            'last_open_ts': str(scenario_stats.get('last_open_ts') or ''),
        },
        'execute': {
            'last_execute_ts': str(last_execute_ts or ''),
            'returncode': returncode_int,
            'ok': last_execute_ok,
        },
        'session': {
            'session_id': session_id,
            'running': session_running,
            'state': session_state,
        },
        'counts': {
            'nodes': nodes_total,
            'routers': routers_total,
            'switches': switches_total,
            'vulnerabilities': vuln_total,
        },
        'subnetworks': subnetworks,
        'vulnerability_ips': vulnerability_ips,
    })


def _core_xml_device_summaries(xml_path: str) -> list[dict[str, str]]:
    """Return a best-effort list of {id,name,type} from CORE XML."""
    try:
        root = LET.parse(xml_path).getroot()
    except Exception:
        return []

    def _local(tag: str) -> str:
        if not tag:
            return ''
        if '}' in tag:
            return tag.split('}', 1)[1]
        return tag

    out: list[dict[str, str]] = []
    seen: set[str] = set()
    for dev in list(root.iter()):
        try:
            lname = _local(getattr(dev, 'tag', '')).lower()
        except Exception:
            lname = ''
        if lname not in {'device', 'node'}:
            continue
        did = (dev.get('id') or dev.get('name') or '').strip()
        if not did:
            continue
        if did in seen:
            continue
        seen.add(did)
        name_val = (dev.get('name') or '').strip()
        if not name_val:
            try:
                name_el = dev.find('./name')
                if name_el is not None and getattr(name_el, 'text', None):
                    name_val = str(name_el.text).strip()
            except Exception:
                name_val = ''
        type_val = (dev.get('type') or '').strip()
        if not type_val:
            try:
                type_el = dev.find('./type') or dev.find('./model')
                if type_el is not None and getattr(type_el, 'text', None):
                    type_val = str(type_el.text).strip()
            except Exception:
                type_val = ''

        try:
            compose_val = str(dev.get('compose') or '').strip()
        except Exception:
            compose_val = ''
        try:
            compose_name_val = str(dev.get('compose_name') or '').strip()
        except Exception:
            compose_name_val = ''

        out.append({
            'id': did,
            'name': name_val or did,
            'type': type_val or '',
            'compose': compose_val,
            'compose_name': compose_name_val,
        })
    return out


def _core_xml_network_summaries(xml_path: str) -> list[dict[str, str]]:
    """Return a best-effort list of {id,name,type} networks from CORE XML."""
    try:
        root = LET.parse(xml_path).getroot()
    except Exception:
        return []

    def _local(tag: str) -> str:
        if not tag:
            return ''
        if '}' in tag:
            return tag.split('}', 1)[1]
        return tag

    out: list[dict[str, str]] = []
    seen: set[str] = set()
    for net in list(root.iter()):
        try:
            lname = _local(getattr(net, 'tag', '')).lower()
        except Exception:
            lname = ''
        if lname != 'network':
            continue
        nid = (net.get('id') or net.get('name') or '').strip()
        if not nid or nid in seen:
            continue
        seen.add(nid)
        name_val = (net.get('name') or '').strip()
        if not name_val:
            try:
                name_el = net.find('./name')
                if name_el is not None and getattr(name_el, 'text', None):
                    name_val = str(name_el.text).strip()
            except Exception:
                name_val = ''
        type_val = (net.get('type') or '').strip()
        if not type_val:
            try:
                type_el = net.find('./type') or net.find('./model')
                if type_el is not None and getattr(type_el, 'text', None):
                    type_val = str(type_el.text).strip()
            except Exception:
                type_val = ''
        out.append({'id': nid, 'name': name_val or nid, 'type': type_val or ''})
    return out


def _core_xml_link_summaries(xml_path: str, *, id_to_name: dict[str, str] | None = None) -> list[dict[str, str]]:
    """Return best-effort link endpoints from CORE XML.

    Expected CORE session shape:
      <links><link node1="1" node2="51">...</link></links>
    """
    id_to_name = id_to_name or {}
    try:
        root = LET.parse(xml_path).getroot()
    except Exception:
        return []

    def _local(tag: str) -> str:
        if not tag:
            return ''
        if '}' in tag:
            return tag.split('}', 1)[1]
        return tag

    out: list[dict[str, str]] = []
    seen_pairs: set[tuple[str, str]] = set()
    for el in list(root.iter()):
        try:
            lname = _local(getattr(el, 'tag', '')).lower()
        except Exception:
            lname = ''
        if lname != 'link':
            continue
        a = str(el.get('node1') or '').strip()
        b = str(el.get('node2') or '').strip()
        if not a or not b or a == b:
            continue
        ordered = tuple(sorted((a, b)))
        if ordered in seen_pairs:
            continue
        seen_pairs.add(ordered)
        out.append({
            'node1': ordered[0],
            'node2': ordered[1],
            'node1_name': id_to_name.get(ordered[0]) or ordered[0],
            'node2_name': id_to_name.get(ordered[1]) or ordered[1],
        })
    return out


@app.route('/participant-ui/topology')
def participant_ui_topology_api():
    """Return a graph-friendly topology summary for the Participant UI."""
    state = _participant_ui_state()
    scenario_norm = _normalize_scenario_label(request.args.get('scenario') or '')
    if not scenario_norm:
        scenario_norm = _normalize_scenario_label(state.get('selected_norm', ''))

    # Enforce assignment-based access for restricted users.
    try:
        if state.get('restrict_to_assigned'):
            allowed_norms = {
                row.get('norm')
                for row in (state.get('listing') or [])
                if isinstance(row, dict) and row.get('norm')
            }
            if scenario_norm and scenario_norm not in allowed_norms:
                return jsonify({'ok': False, 'error': 'Scenario not assigned.'}), 403
    except Exception:
        pass

    flow_meta: dict[str, Any] | None = None
    try:
        if scenario_norm:
            flow_meta = _flow_state_from_latest_xml(scenario_norm)
    except Exception:
        flow_meta = None

    xml_path = None
    try:
        if scenario_norm:
            xml_path = _latest_session_xml_for_scenario_norm(scenario_norm)
    except Exception:
        xml_path = None

    if not xml_path or not os.path.exists(str(xml_path)):
        out = {
            'ok': True,
            'scenario_norm': scenario_norm,
            'status': 'No session XML found',
            'nodes': [],
            'links': [],
            'subnets': [],
            'vulnerability_ips': [],
        }
        if isinstance(flow_meta, dict) and flow_meta:
            out['flow'] = flow_meta
        return jsonify(out)

    nodes, links, _adj = _build_topology_graph_from_session_xml(str(xml_path))
    subnets = _subnet_cidrs_from_session_xml(str(xml_path))
    vuln_ips = _vulnerability_ipv4s_from_session_xml(str(xml_path))

    out = {
        'ok': True,
        'scenario_norm': scenario_norm,
        'status': '',
        'nodes': nodes,
        'links': links,
        'subnets': subnets,
        'vulnerability_ips': vuln_ips,
    }
    if isinstance(flow_meta, dict) and flow_meta:
        out['flow'] = flow_meta
    return jsonify(out)


@app.route('/api/flag-sequencing/save_flow_state_to_xml', methods=['POST'])
def api_flow_save_flow_state_to_xml():
    j = request.get_json(silent=True) or {}
    xml_path = str(j.get('xml_path') or '').strip()
    scenario_label = str(j.get('scenario') or '').strip()
    flow_state = j.get('flow_state') if isinstance(j.get('flow_state'), dict) else None
    clear_state = _coerce_bool(j.get('clear'))

    if not xml_path:
        return jsonify({'ok': False, 'error': 'xml_path required'}), 400
    if (not flow_state) and (not clear_state):
        return jsonify({'ok': False, 'error': 'flow_state required'}), 400

    try:
        xml_path = os.path.abspath(xml_path)
    except Exception:
        return jsonify({'ok': False, 'error': 'invalid xml_path'}), 400

    if clear_state:
        ok, msg = _clear_flow_state_in_xml(xml_path, scenario_label)
    else:
        flow_state = _enrich_flow_state_with_artifacts(flow_state)
        ok, msg = _update_flow_state_in_xml(xml_path, scenario_label, flow_state)
    if not ok:
        return jsonify({'ok': False, 'error': msg}), 422
    return jsonify({'ok': True, 'xml_path': xml_path})


@app.route('/api/planner/ensure_plan', methods=['POST'])
def api_planner_ensure_plan():
    j = request.get_json(silent=True) or {}
    xml_path = str(j.get('xml_path') or '').strip()
    scenario = str(j.get('scenario') or '').strip() or None
    seed = j.get('seed')
    try:
        if seed is not None:
            seed = int(seed)
    except Exception:
        seed = None

    if not xml_path:
        return jsonify({'ok': False, 'error': 'xml_path required'}), 400

    try:
        result = _planner_persist_flow_plan(xml_path=xml_path, scenario=scenario, seed=seed, persist_plan_file=False)
        return jsonify({
            'ok': True,
            'xml_path': result.get('xml_path'),
            'scenario': result.get('scenario'),
            'seed': result.get('seed'),
            'preview_plan_path': result.get('preview_plan_path'),
        })
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)}), 500


@app.route('/api/planner/latest_plan', methods=['GET'])
def api_planner_latest_plan():
    scenario = str(request.args.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'scenario required'}), 400
    xml_path = _latest_xml_path_for_scenario(scenario_norm)
    if xml_path:
        return jsonify({'ok': True, 'preview_plan_path': xml_path, 'xml_path': xml_path})
    return jsonify({'ok': False, 'error': 'No XML found for scenario.'}), 404


_PARTICIPANT_UI_STATS_PATH = os.path.join(_outputs_dir(), 'participant_ui_stats.json')


def _load_participant_ui_stats() -> dict:
    try:
        if not os.path.exists(_PARTICIPANT_UI_STATS_PATH):
            return {
                'version': 1,
                'totals': {'open_count': 0, 'last_open_ts': ''},
                'scenarios': {},
            }
        with open(_PARTICIPANT_UI_STATS_PATH, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            raise ValueError('participant_ui_stats payload not a dict')
        payload.setdefault('version', 1)
        totals = payload.get('totals')
        if not isinstance(totals, dict):
            payload['totals'] = {'open_count': 0, 'last_open_ts': ''}
        payload.setdefault('scenarios', {})
        if not isinstance(payload.get('scenarios'), dict):
            payload['scenarios'] = {}
        return payload
    except Exception:
        return {
            'version': 1,
            'totals': {'open_count': 0, 'last_open_ts': ''},
            'scenarios': {},
        }


def _save_participant_ui_stats(payload: dict) -> None:
    os.makedirs(os.path.dirname(_PARTICIPANT_UI_STATS_PATH), exist_ok=True)
    tmp_path = _PARTICIPANT_UI_STATS_PATH + '.tmp'
    with open(tmp_path, 'w', encoding='utf-8') as fh:
        json.dump(payload, fh, indent=2)
    try:
        _ensure_private_file(tmp_path)
    except Exception:
        pass
    os.replace(tmp_path, _PARTICIPANT_UI_STATS_PATH)
    try:
        _ensure_private_file(_PARTICIPANT_UI_STATS_PATH)
    except Exception:
        pass


@app.route('/participant-ui/stats')
def participant_ui_stats_api():
    # Returns global stats (all users) for the selected scenario.
    scenario_norm = _normalize_scenario_label(request.args.get('scenario') or '')
    if not scenario_norm:
        try:
            scenario_norm = _normalize_scenario_label(_participant_ui_state().get('selected_norm', ''))
        except Exception:
            scenario_norm = ''
    stats = _load_participant_ui_stats()
    scenarios = stats.get('scenarios') if isinstance(stats.get('scenarios'), dict) else {}
    scenario_stats = scenarios.get(scenario_norm, {}) if scenario_norm else {}
    if not isinstance(scenario_stats, dict):
        scenario_stats = {}
    totals = stats.get('totals') if isinstance(stats.get('totals'), dict) else {}
    return jsonify({
        'ok': True,
        'scenario_norm': scenario_norm,
        'scenario': {
            'open_count': int(scenario_stats.get('open_count') or 0),
            'last_open_ts': scenario_stats.get('last_open_ts') or '',
        },
        'totals': {
            'open_count': int(totals.get('open_count') or 0),
            'last_open_ts': totals.get('last_open_ts') or '',
        },
    })


@app.route('/participant-ui/record-open', methods=['POST'])
def participant_ui_record_open_api():
    # Records a user pressing the Open button (used for global stats).
    payload = request.get_json(silent=True) or {}
    if not isinstance(payload, dict):
        payload = {}
    scenario_norm = _normalize_scenario_label(payload.get('scenario_norm') or '')
    href_raw = (payload.get('href') or '').strip()
    href = _normalize_participant_proxmox_url(href_raw)
    # Allow recording same-origin redirect endpoints (used to avoid exposing participant URLs in HTML).
    if not href and href_raw.startswith('/participant-ui/'):
        href = href_raw
    if not href:
        # Do not record meaningless events.
        return jsonify({'ok': False, 'error': 'missing href'}), 400
    now = datetime.datetime.now(datetime.timezone.utc).replace(microsecond=0).isoformat().replace('+00:00', 'Z')
    stats = _load_participant_ui_stats()
    totals = stats.get('totals') if isinstance(stats.get('totals'), dict) else {}
    totals['open_count'] = int(totals.get('open_count') or 0) + 1
    totals['last_open_ts'] = now
    stats['totals'] = totals
    if scenario_norm:
        scenarios = stats.get('scenarios') if isinstance(stats.get('scenarios'), dict) else {}
        entry = scenarios.get(scenario_norm)
        if not isinstance(entry, dict):
            entry = {}
        entry['open_count'] = int(entry.get('open_count') or 0) + 1
        entry['last_open_ts'] = now
        scenarios[scenario_norm] = entry
        stats['scenarios'] = scenarios
    _save_participant_ui_stats(stats)
    return jsonify({
        'ok': True,
        'scenario_norm': scenario_norm,
        'last_open_ts': now,
        'totals': stats.get('totals', {}),
        'scenario': (stats.get('scenarios', {}) or {}).get(scenario_norm, {}) if scenario_norm else {},
    })


@app.route('/participant-ui/open')
def participant_ui_open_redirect():
    """Open the selected participant console as a top-level navigation.

    This endpoint intentionally redirects to the configured participant URL so the UI can avoid
    embedding or exposing raw participant URLs in HTML for restricted roles.
    """
    scenario_norm = _normalize_scenario_label(request.args.get('scenario') or '')
    state = _participant_ui_state()
    resolved = ''
    try:
        listing = state.get('listing', [])
        if isinstance(listing, list) and scenario_norm:
            for entry in listing:
                if not isinstance(entry, dict):
                    continue
                if (entry.get('norm') or '') == scenario_norm and entry.get('url'):
                    resolved = str(entry.get('url') or '')
                    break
    except Exception:
        resolved = ''
    if not resolved:
        try:
            resolved = str(state.get('selected_url') or '')
        except Exception:
            resolved = ''

    resolved = _normalize_participant_proxmox_url(resolved)
    if not resolved:
        abort(404)
    return redirect(resolved)


_LOGIN_EXEMPT_ENDPOINTS = {
    'login',
    'static',
    'healthz',
}


# ---- Attack Flow / Flag Chain (Flow page) ----


ATTACK_FLOW_EXTENSION_DEFINITION_ID = "extension-definition--fb9c968a-745b-4ade-9b25-c324172197f4"
# NOTE: Some tools (including Attack Flow Builder) try to fetch `extension-definition.schema`.
# The GitHub *web* URL returns HTML; use raw content so the URL is directly fetchable as JSON.
# (The GitHub Pages URL referenced in the schema `$id` has been observed to 404.)
ATTACK_FLOW_SCHEMA_URL = "https://raw.githubusercontent.com/center-for-threat-informed-defense/attack-flow/main/stix/attack-flow-schema-2.0.0.json"
ATTACK_FLOW_SCHEMA_VERSION = "2.0.0"


def _iso_now() -> str:
    try:
        # Attack Flow / STIX 2.1 common properties require timestamps with at least millisecond precision.
        return datetime.datetime.now(datetime.UTC).isoformat(timespec='milliseconds').replace('+00:00', 'Z')
    except Exception:
        return "1970-01-01T00:00:00Z"


def _new_stix_id(stix_type: str) -> str:
    return f"{stix_type}--{uuid.uuid4()}"


def _new_uuid() -> str:
    return str(uuid.uuid4())


def _latest_session_xml_for_scenario_norm(scenario_norm: str) -> str | None:
    try:
        history = _load_run_history()
        last_run = _latest_run_history_for_scenario(scenario_norm, history)
    except Exception:
        last_run = None

    if isinstance(last_run, dict):
        session_xml_path = last_run.get('session_xml_path') or last_run.get('post_xml_path')
        if session_xml_path:
            try:
                ap = os.path.abspath(str(session_xml_path))
            except Exception:
                ap = str(session_xml_path)
            if ap and os.path.exists(ap):
                return ap

    # Fallback: run history can become stale if artifacts under outputs/ are purged.
    # Best-effort: pick the newest matching session XML under outputs/core-sessions/.
    try:
        base = os.path.join(_outputs_dir(), 'core-sessions')
        if os.path.isdir(base) and scenario_norm:
            prefix = str(scenario_norm).strip().lower() + '-'
            best_path: str | None = None
            best_mtime: float = -1.0
            for name in os.listdir(base):
                if not isinstance(name, str):
                    continue
                low = name.lower()
                if not (low.startswith(prefix) and low.endswith('.xml')):
                    continue
                p = os.path.join(base, name)
                try:
                    m = os.path.getmtime(p)
                except Exception:
                    continue
                if m > best_mtime:
                    best_mtime = m
                    best_path = p
            if best_path and os.path.exists(best_path):
                return os.path.abspath(best_path)
    except Exception:
        pass

    return None


def _build_topology_graph_from_session_xml(xml_path: str) -> tuple[list[dict[str, Any]], list[dict[str, str]], dict[str, set[str]]]:
    """Return (nodes, links, adjacency) for a session XML.

    Intended to mirror Participant UI topology semantics:
    - includes network nodes (so hostLAN links are visible)
    - tags vulnerability nodes via ipv4 match
    - preserves HITL nodes for distinct styling
    """
    try:
        summary = _analyze_core_xml(xml_path)
    except Exception:
        summary = {}

    raw_nodes = summary.get('nodes') if isinstance(summary, dict) else None
    nodes_list: list[dict] = raw_nodes if isinstance(raw_nodes, list) else []

    all_devices = _core_xml_device_summaries(xml_path)
    all_networks = _core_xml_network_summaries(xml_path)

    name_map: dict[str, str] = {}
    device_compose_map: dict[str, dict[str, str]] = {}
    for row in (all_devices or []):
        if not isinstance(row, dict):
            continue
        rid = str(row.get('id') or '').strip()
        if not rid:
            continue
        name_map.setdefault(rid, str(row.get('name') or rid))
        try:
            comp = str(row.get('compose') or '').strip()
        except Exception:
            comp = ''
        try:
            comp_name = str(row.get('compose_name') or '').strip()
        except Exception:
            comp_name = ''
        if comp or comp_name:
            device_compose_map[rid] = {'compose': comp, 'compose_name': comp_name}
    for row in (all_networks or []):
        if not isinstance(row, dict):
            continue
        rid = str(row.get('id') or '').strip()
        if not rid:
            continue
        name_map.setdefault(rid, str(row.get('name') or rid))

    # IMPORTANT: do NOT use summary.links_detail here (it can be pruned).
    links_list: list[dict] = _core_xml_link_summaries(xml_path, id_to_name=name_map)

    # Start with nodes from analysis, then ensure all devices & networks exist.
    by_id: dict[str, dict[str, Any]] = {}
    for n in nodes_list:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if nid:
            by_id[nid] = n

    for dev in (all_devices or []):
        did = str(dev.get('id') or '').strip()
        if not did or did in by_id:
            continue
        by_id[did] = {
            'id': did,
            'name': dev.get('name') or did,
            'type': dev.get('type') or '',
            'compose': (device_compose_map.get(did) or {}).get('compose') or '',
            'compose_name': (device_compose_map.get(did) or {}).get('compose_name') or '',
            'services': [],
            'interfaces': [],
            'linked_nodes': [],
        }

    for net in (all_networks or []):
        nid = str(net.get('id') or '').strip()
        if not nid or nid in by_id:
            continue
        raw_type = str(net.get('type') or '').strip()
        type_hint = (raw_type or '').lower()
        name_hint = str(net.get('name') or '').strip().lower()
        is_hitl = False
        try:
            import re
            name_looks_like_iface = bool(re.match(r'^(ens|enp|eth)\d', name_hint))
        except Exception:
            name_looks_like_iface = False

        if name_looks_like_iface or 'rj45' in type_hint or 'rj-45' in type_hint or 'hitl' in type_hint or 'tap' in type_hint:
            coerced_type = 'hitl'
            is_hitl = True
        elif 'wlan' in type_hint or 'wireless' in type_hint:
            coerced_type = 'wlan'
        else:
            coerced_type = 'switch'

        by_id[nid] = {
            'id': nid,
            'name': net.get('name') or nid,
            'type': coerced_type,
            'services': [],
            'interfaces': [],
            'linked_nodes': [],
            'is_hitl': bool(is_hitl),
        }

    vuln_ips = _vulnerability_ipv4s_from_session_xml(xml_path)
    vuln_set = {str(ip).strip() for ip in (vuln_ips or []) if ip}
    subnets = _subnet_cidrs_from_session_xml(xml_path)

    try:
        import ipaddress
    except Exception:
        ipaddress = None

    out_nodes: list[dict[str, Any]] = []
    for nid, n in by_id.items():
        name_val = str(n.get('name') or nid)
        type_val = (str(n.get('type') or '').strip() or 'node')
        ifaces = n.get('interfaces') if isinstance(n.get('interfaces'), list) else []
        services_val = n.get('services') if isinstance(n.get('services'), list) else []

        ipv4s: list[str] = []
        subnet_hits: set[str] = set()
        for iface in ifaces:
            if not isinstance(iface, dict):
                continue
            ip4 = (iface.get('ipv4') or '').strip() if isinstance(iface.get('ipv4'), str) else ''
            mask = (iface.get('ipv4_mask') or '').strip() if isinstance(iface.get('ipv4_mask'), str) else ''
            if ip4:
                ip4_clean = ip4.split('/', 1)[0].strip()
                if ip4_clean:
                    ipv4s.append(ip4_clean)
            if ipaddress is not None and ip4:
                try:
                    if '/' in ip4:
                        net_obj = ipaddress.ip_interface(ip4).network
                    elif mask:
                        net_obj = ipaddress.ip_interface(f"{ip4}/{mask}").network
                    else:
                        net_obj = None
                    if net_obj is not None and getattr(net_obj, 'prefixlen', 32) < 32:
                        subnet_hits.add(str(net_obj))
                except Exception:
                    pass

        # stable unique ipv4s
        seen_ip: set[str] = set()
        ipv4s_u: list[str] = []
        for ip in ipv4s:
            if not ip or ip in seen_ip:
                continue
            seen_ip.add(ip)
            ipv4s_u.append(ip)
        is_vuln = any(ip in vuln_set for ip in ipv4s_u)

        out_nodes.append({
            'id': nid,
            'name': name_val,
            'type': type_val,
            'services': services_val,
            'interfaces': ifaces,
            'ipv4s': ipv4s_u,
            'subnets': sorted(subnet_hits) if subnet_hits else [],
            'is_vulnerability': bool(is_vuln),
            'is_hitl': bool(n.get('is_hitl')),
            'compose': str(n.get('compose') or (device_compose_map.get(nid) or {}).get('compose') or ''),
            'compose_name': str(n.get('compose_name') or (device_compose_map.get(nid) or {}).get('compose_name') or ''),
        })

    out_links: list[dict[str, str]] = []
    for l in links_list:
        if not isinstance(l, dict):
            continue
        a = str(l.get('node1') or '').strip()
        b = str(l.get('node2') or '').strip()
        if not a or not b or a == b:
            continue
        if a not in by_id or b not in by_id:
            continue
        out_links.append({
            'node1': a,
            'node2': b,
            'node1_name': str(l.get('node1_name') or name_map.get(a) or a),
            'node2_name': str(l.get('node2_name') or name_map.get(b) or b),
        })

    adj: dict[str, set[str]] = {nid: set() for nid in by_id.keys()}
    for l in out_links:
        a = str(l.get('node1') or '').strip()
        b = str(l.get('node2') or '').strip()
        if not a or not b:
            continue
        adj.setdefault(a, set()).add(b)
        adj.setdefault(b, set()).add(a)

    # Attach top-level subnets for any caller that wants them later.
    # (kept off the node objects to avoid bloating graph payload)
    try:
        for n in out_nodes:
            if isinstance(subnets, list) and subnets:
                n.setdefault('__subnets_all', subnets)
    except Exception:
        pass

    return out_nodes, out_links, adj


def _pick_flag_chain_nodes(nodes: list[dict[str, Any]], adj: dict[str, set[str]], *, length: int) -> list[dict[str, Any]]:
    """Pick an ordered list of nodes to place flags on.

    The chain is considered solvable if each consecutive pair is connected by
    at least one path in the topology graph. We build the chain by:
    - picking two far-apart endpoints (approx. diameter) in the full graph
    - extracting eligible nodes along that path
    """
    length = max(1, min(int(length or 1), 50))

    id_to_node: dict[str, dict[str, Any]] = {}
    allow_nonvuln_docker = False
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
        allow_nonvuln_docker = bool(node_gens)
    except Exception:
        allow_nonvuln_docker = False
    eligible_ids: list[str] = []
    for n in nodes:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if not nid:
            continue
        id_to_node[nid] = n
        t = (str(n.get('type') or '').strip().lower())
        # Flow placement eligibility:
        # - flag-generators may be placed on vulnerability nodes only
        # - flag-node-generators require non-vulnerability docker-role nodes (enforced elsewhere)
        is_docker = ('docker' in t) or (str(n.get('type') or '').strip().upper() == 'DOCKER')
        is_vuln = _flow_node_is_vuln(n)
        if is_vuln or (allow_nonvuln_docker and is_docker and (not is_vuln)):
            eligible_ids.append(nid)

    # De-dupe eligible ids to avoid accidental repeats.
    try:
        eligible_ids = list(dict.fromkeys(eligible_ids))
    except Exception:
        pass

    if not eligible_ids:
        return []

    # If any connected component has enough eligible nodes, pick within that component.
    try:
        comp_by_node: dict[str, int] = {}
        comps: list[list[str]] = []
        for nid in id_to_node.keys():
            if nid in comp_by_node:
                continue
            comp_nodes: list[str] = []
            q = deque([nid])
            comp_by_node[nid] = len(comps)
            while q:
                cur = q.popleft()
                comp_nodes.append(cur)
                for nb in adj.get(cur, set()):
                    if nb in comp_by_node:
                        continue
                    comp_by_node[nb] = len(comps)
                    q.append(nb)
            comps.append(comp_nodes)

        eligible_by_comp: dict[int, list[str]] = {}
        for nid in eligible_ids:
            cidx = comp_by_node.get(nid)
            if cidx is None:
                continue
            eligible_by_comp.setdefault(cidx, []).append(nid)
        if eligible_by_comp:
            best_comp = max(eligible_by_comp.items(), key=lambda kv: len(kv[1]))
            if len(best_comp[1]) >= length:
                try:
                    import random as _random
                    picks = list(best_comp[1])
                    _random.Random().shuffle(picks)
                except Exception:
                    picks = list(best_comp[1])
                chain_ids = picks[:length]
                try:
                    import random as _random
                    _random.Random().shuffle(chain_ids)
                except Exception:
                    pass
                return [id_to_node[nid] for nid in chain_ids if nid in id_to_node]
    except Exception:
        pass

    def bfs_farthest(start: str) -> tuple[str, dict[str, str | None]]:
        parent: dict[str, str | None] = {start: None}
        q = deque([start])
        last = start
        while q:
            cur = q.popleft()
            last = cur
            for nb in adj.get(cur, set()):
                if nb in parent:
                    continue
                parent[nb] = cur
                q.append(nb)
        return last, parent

    # Choose a start node that is likely connected.
    start = eligible_ids[0]
    far1, _p1 = bfs_farthest(start)
    far2, parents = bfs_farthest(far1)

    # Reconstruct full graph path far1 -> far2.
    path_ids: list[str] = []
    cur = far2
    while cur is not None:
        path_ids.append(cur)
        cur = parents.get(cur)
    path_ids.reverse()

    # Extract eligible nodes along the path.
    chain_ids: list[str] = [nid for nid in path_ids if nid in eligible_ids]

    # If not enough eligible nodes on that path, fall back to a BFS walk over eligible nodes.
    if len(chain_ids) < length:
        seen = set()
        q = deque([start])
        chain_ids = []
        while q and len(chain_ids) < length:
            cur = q.popleft()
            if cur in seen:
                continue
            seen.add(cur)
            if cur in eligible_ids:
                chain_ids.append(cur)
            for nb in sorted(adj.get(cur, set())):
                if nb not in seen:
                    q.append(nb)

    # If we still don't have enough, fill from remaining eligible nodes (random order).
    if len(chain_ids) < length and len(eligible_ids) >= length:
        try:
            import random as _random
            remaining = [nid for nid in eligible_ids if nid not in set(chain_ids)]
            _random.Random().shuffle(remaining)
        except Exception:
            remaining = [nid for nid in eligible_ids if nid not in set(chain_ids)]
        need = max(0, length - len(chain_ids))
        if need:
            chain_ids.extend(remaining[:need])

    chain_ids = chain_ids[:length]
    # Default ordering: randomize the sequence (user can tweak ordering in the UI).
    try:
        import random as _random
        _random.Random().shuffle(chain_ids)
    except Exception:
        pass
    return [id_to_node[nid] for nid in chain_ids if nid in id_to_node]


def _pick_flag_chain_nodes_allow_duplicates(
    nodes: list[dict[str, Any]],
    adj: dict[str, set[str]],
    *,
    length: int,
    seed: int = 0,
) -> list[dict[str, Any]]:
    """Pick an ordered list of eligible nodes, allowing repeats.

    Used only when the caller explicitly opts into duplicates.
    """
    length = max(1, min(int(length or 1), 50))

    id_to_node: dict[str, dict[str, Any]] = {}
    allow_nonvuln_docker = False
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
        allow_nonvuln_docker = bool(node_gens)
    except Exception:
        allow_nonvuln_docker = False
    eligible_ids: list[str] = []
    for n in nodes or []:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if not nid:
            continue
        id_to_node[nid] = n
        t = (str(n.get('type') or '').strip().lower())
        is_docker = ('docker' in t) or (str(n.get('type') or '').strip().upper() == 'DOCKER')
        is_vuln = _flow_node_is_vuln(n)
        if is_vuln or (allow_nonvuln_docker and is_docker and (not is_vuln)):
            eligible_ids.append(nid)

    # De-dupe eligible ids to avoid accidental repeats.
    try:
        eligible_ids = list(dict.fromkeys(eligible_ids))
    except Exception:
        pass

    if not eligible_ids:
        return []

    # Start with a best-effort unique chain (keeps prior behavior when possible).
    unique_target = min(length, len(list(dict.fromkeys(eligible_ids))))
    base = _pick_flag_chain_nodes(nodes, adj, length=unique_target)
    chain_ids: list[str] = [
        str(n.get('id') or '').strip()
        for n in (base or [])
        if isinstance(n, dict) and str(n.get('id') or '').strip()
    ]

    if len(chain_ids) < length:
        # Extend with repeats, deterministically when a seed is available.
        try:
            import random as _random
            rnd = _random.Random(int(seed or 0) ^ 0xD00DCAFE)
        except Exception:
            rnd = None

        pool = list(dict.fromkeys([x for x in eligible_ids if x]))
        if not pool:
            return []

        while len(chain_ids) < length:
            try:
                pick = (rnd.choice(pool) if rnd is not None else pool[0])
            except Exception:
                pick = pool[0]
            chain_ids.append(str(pick))

        # Shuffle final order.
        try:
            if rnd is not None:
                rnd.shuffle(chain_ids)
            else:
                import random as _random
                _random.Random().shuffle(chain_ids)
        except Exception:
            pass

    return [id_to_node[nid] for nid in chain_ids if nid in id_to_node]


def _pick_flag_chain_nodes_for_preset(
    nodes: list[dict[str, Any]],
    adj: dict[str, set[str]],
    *,
    steps: list[dict[str, str]],
) -> list[dict[str, Any]]:
    """Pick a chain that satisfies preset step constraints.

    Currently enforced:
    - flag-generator steps: must be placed on vulnerability nodes
    - flag-node-generator steps: must be placed on a non-vulnerability docker-role node
    """
    try:
        length = len(steps or [])
    except Exception:
        length = 0
    length = max(1, min(int(length or 1), 50))

    id_to_node: dict[str, dict[str, Any]] = {}
    eligible_ids: list[str] = []
    docker_ids: set[str] = set()
    vuln_ids: set[str] = set()
    for n in nodes or []:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if not nid:
            continue
        id_to_node[nid] = n
        t_raw = str(n.get('type') or '')
        t = t_raw.strip().lower()
        is_docker = ('docker' in t) or (t_raw.strip().upper() == 'DOCKER')
        is_vuln = bool(n.get('is_vuln')) or bool(n.get('vulnerabilities'))
        if is_vuln:
            vuln_ids.add(nid)
            eligible_ids.append(nid)
        if is_docker and not is_vuln:
            docker_ids.add(nid)
            eligible_ids.append(nid)

    if not eligible_ids:
        return []
    if length > len(set(eligible_ids)):
        return []

    # Try to keep the chain in a single connected component, starting from a docker node if possible.
    start = None
    if docker_ids:
        start = next(iter(sorted(docker_ids)))
    else:
        start = eligible_ids[0]

    visited: list[str] = []
    try:
        seen: set[str] = set()
        q = deque([start])
        while q:
            cur = q.popleft()
            if cur in seen:
                continue
            seen.add(cur)
            visited.append(cur)
            for nb in sorted(adj.get(cur, set())):
                if nb not in seen:
                    q.append(nb)
    except Exception:
        visited = list(dict.fromkeys(eligible_ids))

    comp_eligible = [nid for nid in visited if nid in set(eligible_ids)]
    if len(comp_eligible) < length:
        # Fallback: allow selecting across components rather than erroring.
        comp_eligible = list(dict.fromkeys(eligible_ids))

    used: set[str] = set()
    chosen: list[str] = []
    for step in (steps or [])[:length]:
        kind = str((step or {}).get('kind') or '').strip()
        need_docker = (kind == 'flag-node-generator')
        if need_docker:
            pool = [nid for nid in comp_eligible if nid not in used and nid in docker_ids]
        else:
            pool = [nid for nid in comp_eligible if nid not in used and nid in vuln_ids]
        if not pool:
            return []
        pick = pool[0]
        used.add(pick)
        chosen.append(pick)

    if len(chosen) < length:
        return []
    return [id_to_node[nid] for nid in chosen if nid in id_to_node]


def _flow_compose_docker_stats(nodes: list[dict[str, Any]]) -> dict[str, int]:
    """Return counts for debugging Flow eligibility.

    - docker_total: nodes that look like docker nodes
    - compose_backed_total: docker nodes that carry compose metadata
    - vuln_total: nodes that are vulnerability candidates
    - eligible_total: nodes eligible for chain placement (non-vuln docker + vuln nodes)

    Additional explicit metrics (new; kept alongside legacy keys):
    - docker_nonvuln_total: docker nodes that are NOT vulnerability candidates
    - flag_generator_eligible_total: nodes eligible for flag-generator steps (vuln nodes only)
    - flag_node_generator_eligible_total: nodes eligible for flag-node-generator steps (non-vuln docker role only)
    """
    docker_total = 0
    docker_nonvuln_total = 0
    vuln_total = 0
    compose_backed_total = 0
    eligible_total = 0
    for n in nodes or []:
        if not isinstance(n, dict):
            continue
        t_raw = str(n.get('type') or '')
        t = t_raw.strip().lower()
        is_docker = ('docker' in t) or (t_raw.strip().upper() == 'DOCKER')
        is_vuln = bool(n.get('is_vuln')) or bool(n.get('vulnerabilities'))
        if is_docker:
            docker_total += 1
            comp = str(n.get('compose') or '').strip()
            comp_name = str(n.get('compose_name') or '').strip()
            if comp or comp_name:
                compose_backed_total += 1
        if is_vuln:
            vuln_total += 1
        if is_docker and (not is_vuln):
            docker_nonvuln_total += 1
        if is_vuln or (is_docker and (not is_vuln)):
            eligible_total += 1
    return {
        'docker_total': docker_total,
        'docker_nonvuln_total': docker_nonvuln_total,
        'vuln_total': vuln_total,
        'compose_backed_total': compose_backed_total,
        'eligible_total': eligible_total,
        'flag_generator_eligible_total': vuln_total,
        'flag_node_generator_eligible_total': docker_nonvuln_total,
    }


def _attack_flow_builder_afb_for_chain(
    *,
    chain_nodes: list[dict[str, Any]],
    scenario_label: str,
    flag_assignments: list[dict[str, Any]] | None = None,
) -> dict[str, Any]:
    """Build an Attack Flow Builder .afb document for a linear chain.

    Attack Flow Builder's native format is a UI graph JSON (".afb") with explicit
    edge objects (e.g., dynamic_line source/target). This avoids tool warnings like
    "edges must connect on both sides" when importing a STIX bundle.
    """
    now = _iso_now()

    assignment_by_node_id: dict[str, dict[str, Any]] = {}
    try:
        for fa in (flag_assignments or []):
            if not isinstance(fa, dict):
                continue
            nid = str(fa.get('node_id') or '').strip()
            if nid and nid not in assignment_by_node_id:
                assignment_by_node_id[nid] = fa
    except Exception:
        assignment_by_node_id = {}

    # Attack Flow Builder v3 expects an OpenChart DiagramViewExport:
    # - root {schema, objects, (optional) theme/layout/camera}
    # - a single root canvas/group export (id="flow") with an `objects` list of child instances
    # - actions are blocks (id="action") and MUST include an `anchors` map (can be empty)
    flow_instance = _new_uuid()
    objects: list[dict[str, Any]] = []
    layout: dict[str, list[int]] = {}

    flow_name = f"Flag Chain" + (f" - {scenario_label}" if scenario_label else '')
    flow_children: list[str] = []
    action_instances: list[str] = []

    # Layout constants (tuned for non-overlap in Builder).
    action_x = 600
    action_y_cursor = 220
    action_step_min = 260
    side_x_offset = 340
    side_y_offset = 150
    side_row_gap = 110

    # Track per-action anchors so we can wire lines between actions.
    action_left_anchor: dict[str, str] = {}
    action_right_anchor: dict[str, str] = {}
    # Keep references to anchor export dicts so we can append latch instances.
    anchor_obj_by_instance: dict[str, dict[str, Any]] = {}

    # Active vulnerability catalog label (used as Source in AFB vuln nodes).
    try:
        active_vuln_pack_label = _active_vuln_catalog_label()
    except Exception:
        active_vuln_pack_label = ''

    # Create one action node per chain step.
    for idx, node in enumerate(chain_nodes, start=1):
        node_id = str(node.get('id') or '').strip()
        node_name = str(node.get('name') or node_id)

        node_ipv4 = _first_valid_ipv4((node or {}).get('ipv4'))

        fa = assignment_by_node_id.get(node_id)
        gen_id = str((fa or {}).get('id') or '').strip()
        gen_name = str((fa or {}).get('name') or '').strip()
        gen_kind = str((fa or {}).get('type') or '').strip()
        gen_source = str((fa or {}).get('flag_generator') or '').strip()
        gen_catalog = str((fa or {}).get('generator_catalog') or '').strip()

        output_files: list[str] = ['outputs.json', 'hint.txt']
        try:
            outs = (fa or {}).get('outputs')
            if isinstance(outs, list) and any(str(x).strip() in {'flag', 'Flag(flag_id)'} for x in outs):
                output_files.append('flag.txt')
        except Exception:
            pass
        try:
            actual = (fa or {}).get('actual_outputs')
            if isinstance(actual, list) and any(str(x).strip() in {'flag', 'Flag(flag_id)'} for x in actual):
                if 'flag.txt' not in output_files:
                    output_files.append('flag.txt')
        except Exception:
            pass

        # Keep action description mostly human text; outputs are represented as asset nodes.
        # Exception: when a realized flag value exists, include it here for convenience.
        desc_lines: list[str] = []
        artifacts_dir = ''
        if fa:
            try:
                gen_desc = str((fa or {}).get('description') or '').strip()
            except Exception:
                gen_desc = ''
            if gen_desc:
                desc_lines.append(gen_desc)

            # If the generator has already been executed (Flow runtime), embed the
            # realized flag value (when present) directly in the action description.
            try:
                artifacts_dir = str((fa or {}).get('artifacts_dir') or (fa or {}).get('run_dir') or '').strip()
            except Exception:
                artifacts_dir = ''
            try:
                flag_val = _flow_read_flag_value_from_artifacts_dir(artifacts_dir) if artifacts_dir else ''
            except Exception:
                flag_val = ''
            if flag_val:
                if desc_lines:
                    desc_lines.append('')
                desc_lines.append(f"Flag: {flag_val}")

            try:
                rin = (fa or {}).get('resolved_inputs')
                if isinstance(rin, dict) and rin:
                    items = []
                    for k, v in list(rin.items())[:6]:
                        try:
                            vs = v.strip() if isinstance(v, str) else json.dumps(v, ensure_ascii=False)
                        except Exception:
                            vs = str(v)
                        kk = str(k or '').strip()
                        if kk:
                            items.append(f"{kk}={vs}" if vs else kk)
                    if items:
                        if desc_lines:
                            desc_lines.append('')
                        desc_lines.append("Resolved inputs: " + ", ".join(items))
            except Exception:
                pass
            try:
                rout = (fa or {}).get('resolved_outputs')
                if isinstance(rout, dict) and rout:
                    items = []
                    for k, v in list(rout.items())[:6]:
                        try:
                            vs = v.strip() if isinstance(v, str) else json.dumps(v, ensure_ascii=False)
                        except Exception:
                            vs = str(v)
                        kk = str(k or '').strip()
                        if kk:
                            items.append(f"{kk}={vs}" if vs else kk)
                    if items:
                        if desc_lines:
                            desc_lines.append('')
                        desc_lines.append("Resolved outputs: " + ", ".join(items))
            except Exception:
                pass

        # Preload resolved output values for asset descriptions (best-effort).
        try:
            outputs_map = _flow_read_outputs_map_from_artifacts_dir(artifacts_dir) if artifacts_dir else {}
        except Exception:
            outputs_map = {}
        def _asset_description_for_key(key: str) -> str:
            k = str(key or '').strip()
            if not k:
                return ''
            resolved_val = None
            try:
                if isinstance(outputs_map, dict):
                    if k in outputs_map:
                        resolved_val = outputs_map.get(k)
                    elif k == 'artifact.flag' and ('Flag(flag_id)' in outputs_map or 'flag' in outputs_map):
                        resolved_val = outputs_map.get('Flag(flag_id)') or outputs_map.get('flag')
            except Exception:
                resolved_val = None
            if resolved_val is None:
                try:
                    if isinstance(fa, dict):
                        ro = fa.get('resolved_outputs') if isinstance(fa.get('resolved_outputs'), dict) else None
                        if isinstance(ro, dict):
                            if k in ro:
                                resolved_val = ro.get(k)
                            elif k == 'artifact.flag' and ('Flag(flag_id)' in ro or 'flag' in ro):
                                resolved_val = ro.get('Flag(flag_id)') or ro.get('flag')
                except Exception:
                    resolved_val = None
            if resolved_val is not None:
                try:
                    if isinstance(resolved_val, str):
                        txt = resolved_val.strip()
                    else:
                        txt = json.dumps(resolved_val, ensure_ascii=False)
                except Exception:
                    txt = str(resolved_val)
                return txt[:4096] if txt else ''
            return ''
        if not desc_lines:
            desc_lines = [f"Capture the flag on node '{node_name}'."]

        description = "\n".join([x for x in desc_lines if x is not None])

        # Action title: "Find Flag -- <flag-*generator name>: <node-name>"
        gen_label = (gen_name or gen_id or '').strip()
        if not gen_label:
            gen_label = 'Unassigned generator'
        action_title = f"Find Flag -- {gen_label}: {node_name}"

        action_instance = _new_uuid()
        flow_children.append(action_instance)
        action_instances.append(action_instance)

        # Create two anchors for this action, used for incoming/outgoing edges.
        left_anchor_instance = _new_uuid()
        right_anchor_instance = _new_uuid()
        action_left_anchor[action_instance] = left_anchor_instance
        action_right_anchor[action_instance] = right_anchor_instance

        # Builder uses angle-keyed anchors (e.g., "0", "180") that refer to
        # template IDs like horizontal_anchor/vertical_anchor.
        left_anchor_obj = {
            'id': 'horizontal_anchor',
            'instance': left_anchor_instance,
            'latches': [],
        }
        right_anchor_obj = {
            'id': 'horizontal_anchor',
            'instance': right_anchor_instance,
            'latches': [],
        }
        anchor_obj_by_instance[left_anchor_instance] = left_anchor_obj
        anchor_obj_by_instance[right_anchor_instance] = right_anchor_obj
        objects.append(left_anchor_obj)
        objects.append(right_anchor_obj)

        # Lay actions top-to-bottom.
        action_y = int(action_y_cursor)

        action_node_id = 'action'
        objects.append({
            'id': action_node_id,
            'instance': action_instance,
            # NOTE: OpenChart uses ordered entries (JsonEntries) not dicts.
            'properties': [
                ['name', action_title],
                # The builder template exposes a nested "ttp" mapping. Keep it present
                # but empty so the file loads without requiring ATT&CK mappings.
                ['ttp', [['tactic', None], ['technique', None]]],
                ['description', description],
            ],
            # Required for BlockExport; an empty map lets the importer create
            # default anchors from the template.
            # We provide anchors for the positions we actually use so the semantic
            # analyzer can discover edges (via anchor -> latch -> line).
            'anchors': {
                # Keys must match Builder's expected anchor keys (angle degrees).
                # "180"  left, "0"  right.
                '180': left_anchor_instance,
                '0': right_anchor_instance,
            },
        })

        # If this is a vulnerability node, attach an explicit Vulnerability node
        # with the vuln name and source details.
        is_vuln_node = False
        try:
            is_vuln_node = _flow_node_is_vuln(node)
        except Exception:
            is_vuln_node = bool(node.get('is_vuln')) or bool(node.get('vulnerabilities'))
        if is_vuln_node:
            vuln_name = ''
            vuln_source = ''
            try:
                vulns = node.get('vulnerabilities')
                if isinstance(vulns, list) and vulns:
                    for v in vulns:
                        if isinstance(v, dict):
                            if not vuln_name:
                                for key in ('name', 'title', 'id', 'vuln', 'cve', 'cve_id', 'slug'):
                                    vv = str(v.get(key) or '').strip()
                                    if vv:
                                        vuln_name = vv
                                        break
                            if not vuln_source:
                                for key in ('source', 'provider', 'catalog', 'origin', 'repo', 'repository', 'dataset', 'url'):
                                    ss = str(v.get(key) or '').strip()
                                    if ss:
                                        vuln_source = ss
                                        break
                        elif isinstance(v, str) and v.strip() and not vuln_name:
                            vuln_name = v.strip()
                        if vuln_name and vuln_source:
                            break
            except Exception:
                pass
            if not vuln_name:
                try:
                    for key in ('vulnerability_name', 'vuln_name', 'vulnerability', 'vuln', 'cve', 'cve_id'):
                        vv = str(node.get(key) or '').strip()
                        if vv:
                            vuln_name = vv
                            break
                except Exception:
                    pass
            if not vuln_source:
                try:
                    for key in ('vulnerability_source', 'vuln_source', 'source', 'provider', 'catalog'):
                        ss = str(node.get(key) or '').strip()
                        if ss:
                            vuln_source = ss
                            break
                except Exception:
                    pass
            if (not vuln_source) and active_vuln_pack_label:
                vuln_source = active_vuln_pack_label
            if not vuln_name:
                vuln_name = 'Vulnerability'
            if not vuln_source:
                vuln_source = 'unknown'

            vuln_instance = _new_uuid()
            flow_children.append(vuln_instance)

            vuln_left_anchor_instance = _new_uuid()
            vuln_right_anchor_instance = _new_uuid()
            vuln_left_anchor_obj = {
                'id': 'horizontal_anchor',
                'instance': vuln_left_anchor_instance,
                'latches': [],
            }
            vuln_right_anchor_obj = {
                'id': 'horizontal_anchor',
                'instance': vuln_right_anchor_instance,
                'latches': [],
            }
            anchor_obj_by_instance[vuln_left_anchor_instance] = vuln_left_anchor_obj
            anchor_obj_by_instance[vuln_right_anchor_instance] = vuln_right_anchor_obj
            objects.append(vuln_left_anchor_obj)
            objects.append(vuln_right_anchor_obj)

            objects.append({
                'id': 'vulnerability',
                'instance': vuln_instance,
                'properties': [
                    ['name', vuln_name],
                    ['description', f"Source: {vuln_source}"],
                ],
                'anchors': {
                    '180': vuln_left_anchor_instance,
                    '0': vuln_right_anchor_instance,
                },
            })

            # Place to the left of the action, slightly above.
            layout[vuln_instance] = [int(action_x) - side_x_offset, int(action_y) - 60]

            # Connect vulnerability -> action.
            src_anchor = vuln_right_anchor_instance
            trg_anchor = action_left_anchor.get(action_instance)
            if src_anchor and trg_anchor:
                line_instance = _new_uuid()
                src_latch_instance = _new_uuid()
                trg_latch_instance = _new_uuid()
                handle_instance = _new_uuid()

                try:
                    anchor_obj_by_instance[src_anchor]['latches'].append(src_latch_instance)
                except Exception:
                    pass
                try:
                    anchor_obj_by_instance[trg_anchor]['latches'].append(trg_latch_instance)
                except Exception:
                    pass

                objects.append({'id': 'generic_latch', 'instance': src_latch_instance})
                objects.append({'id': 'generic_latch', 'instance': trg_latch_instance})
                objects.append({'id': 'generic_handle', 'instance': handle_instance})

                try:
                    src_pos = layout.get(vuln_instance)
                    trg_pos = layout.get(action_instance)
                    if isinstance(src_pos, list) and len(src_pos) == 2 and isinstance(trg_pos, list) and len(trg_pos) == 2:
                        src_x, src_y = int(src_pos[0]), int(src_pos[1])
                        trg_x, trg_y = int(trg_pos[0]), int(trg_pos[1])
                        layout[src_latch_instance] = [src_x + 140, src_y]
                        layout[trg_latch_instance] = [trg_x - 140, trg_y]
                except Exception:
                    pass

                objects.append({
                    'id': 'dynamic_line',
                    'instance': line_instance,
                    'source': src_latch_instance,
                    'target': trg_latch_instance,
                    'handles': [handle_instance],
                })
                flow_children.append(line_instance)

        layout[action_instance] = [int(action_x), action_y]

        # Add an IPV4_ADDR node (if available) and connect action -> IPV4_ADDR.
        if node_ipv4:
            ipv4_instance = _new_uuid()
            flow_children.append(ipv4_instance)

            ipv4_left_anchor_instance = _new_uuid()
            ipv4_right_anchor_instance = _new_uuid()
            ipv4_left_anchor_obj = {
                'id': 'horizontal_anchor',
                'instance': ipv4_left_anchor_instance,
                'latches': [],
            }
            ipv4_right_anchor_obj = {
                'id': 'horizontal_anchor',
                'instance': ipv4_right_anchor_instance,
                'latches': [],
            }
            anchor_obj_by_instance[ipv4_left_anchor_instance] = ipv4_left_anchor_obj
            anchor_obj_by_instance[ipv4_right_anchor_instance] = ipv4_right_anchor_obj
            objects.append(ipv4_left_anchor_obj)
            objects.append(ipv4_right_anchor_obj)

            objects.append({
                'id': 'ipv4_addr',
                'instance': ipv4_instance,
                'properties': [
                    # Use the actual IP in both name + description to ensure it is visible
                    # regardless of which fields the Builder UI chooses to render.
                    ['name', node_ipv4],
                    ['description', node_ipv4],
                    ['value', node_ipv4],
                    ['address', node_ipv4],
                ],
                'anchors': {
                    '180': ipv4_left_anchor_instance,
                    '0': ipv4_right_anchor_instance,
                },
            })

            # Place to the right of the action (slightly above the asset rows).
            layout[ipv4_instance] = [int(action_x) + side_x_offset, int(action_y) - 60]

            src_anchor = action_right_anchor.get(action_instance)
            trg_anchor = ipv4_left_anchor_instance
            if src_anchor and trg_anchor:
                line_instance = _new_uuid()
                src_latch_instance = _new_uuid()
                trg_latch_instance = _new_uuid()
                handle_instance = _new_uuid()

                try:
                    anchor_obj_by_instance[src_anchor]['latches'].append(src_latch_instance)
                except Exception:
                    pass
                try:
                    anchor_obj_by_instance[trg_anchor]['latches'].append(trg_latch_instance)
                except Exception:
                    pass

                objects.append({'id': 'generic_latch', 'instance': src_latch_instance})
                objects.append({'id': 'generic_latch', 'instance': trg_latch_instance})
                objects.append({'id': 'generic_handle', 'instance': handle_instance})

                try:
                    src_pos = layout.get(action_instance)
                    trg_pos = layout.get(ipv4_instance)
                    if isinstance(src_pos, list) and len(src_pos) == 2 and isinstance(trg_pos, list) and len(trg_pos) == 2:
                        src_x, src_y = int(src_pos[0]), int(src_pos[1])
                        trg_x, trg_y = int(trg_pos[0]), int(trg_pos[1])
                        layout[src_latch_instance] = [src_x + 140, src_y]
                        layout[trg_latch_instance] = [trg_x - 140, trg_y]
                except Exception:
                    pass

                objects.append({
                    'id': 'dynamic_line',
                    'instance': line_instance,
                    'source': src_latch_instance,
                    'target': trg_latch_instance,
                    'handles': [handle_instance],
                })
                flow_children.append(line_instance)

        # Add asset nodes for outputs that are provided to the chain.
        # These correspond to the Flow UI "Outputs -> Provided to Chain".
        provided: list[str] = []
        try:
            prod = (fa or {}).get('produces')
            if isinstance(prod, list):
                provided.extend([str(x or '').strip() for x in prod if str(x or '').strip()])
        except Exception:
            pass
        try:
            out_fields = (fa or {}).get('output_fields')
            if isinstance(out_fields, list):
                provided.extend([str(x or '').strip() for x in out_fields if str(x or '').strip()])
        except Exception:
            pass
        # De-dupe while preserving order.
        seen_out: set[str] = set()
        provided2: list[str] = []
        for k in provided:
            if not k or k in seen_out:
                continue
            seen_out.add(k)
            provided2.append(k)

        # Injected files (inject_files allowlist) are represented as explicit artifact nodes.
        injected2: list[str] = []
        try:
            inj = (fa or {}).get('inject_files')
            if isinstance(inj, list):
                seen_inj: set[str] = set()
                for raw in inj:
                    s = str(raw or '').strip()
                    if not s or s in seen_inj:
                        continue
                    seen_inj.add(s)
                    injected2.append(s)
        except Exception:
            injected2 = []

        left_rows = 0
        right_rows = 0

        # Place injected artifacts to the left of the action.
        if injected2:
            for inj_idx, inj_path in enumerate(injected2):
                art_instance = _new_uuid()
                flow_children.append(art_instance)

                # Anchors for the artifact.
                art_left_anchor_instance = _new_uuid()
                art_right_anchor_instance = _new_uuid()
                art_left_anchor_obj = {
                    'id': 'horizontal_anchor',
                    'instance': art_left_anchor_instance,
                    'latches': [],
                }
                art_right_anchor_obj = {
                    'id': 'horizontal_anchor',
                    'instance': art_right_anchor_instance,
                    'latches': [],
                }
                anchor_obj_by_instance[art_left_anchor_instance] = art_left_anchor_obj
                anchor_obj_by_instance[art_right_anchor_instance] = art_right_anchor_obj
                objects.append(art_left_anchor_obj)
                objects.append(art_right_anchor_obj)

                resolved_inject_desc = _asset_description_for_key(inj_path)
                objects.append({
                    # Represent injected files as key artifact notes.
                    'id': 'note',
                    'instance': art_instance,
                    'properties': [
                        ['name', f"Key Artifact: {inj_path}"],
                        ['abstract', f"Key Artifact: {inj_path}"],
                        ['content', resolved_inject_desc or inj_path],
                        ['injects', [inj_path]],
                    ],
                    'anchors': {
                        '180': art_left_anchor_instance,
                        '0': art_right_anchor_instance,
                    },
                })

                # Place in rows on the left side.
                art_x = int(action_x) - side_x_offset - 220
                art_y = int(action_y) + 60 + (inj_idx * side_row_gap)
                layout[art_instance] = [art_x, art_y]
                left_rows += 1

                # Connect artifact -> action (inputs).
                src_anchor = art_right_anchor_instance
                trg_anchor = action_left_anchor.get(action_instance)
                if not src_anchor or not trg_anchor:
                    continue

                line_instance = _new_uuid()
                src_latch_instance = _new_uuid()
                trg_latch_instance = _new_uuid()
                handle_instance = _new_uuid()

                try:
                    anchor_obj_by_instance[src_anchor]['latches'].append(src_latch_instance)
                except Exception:
                    pass
                try:
                    anchor_obj_by_instance[trg_anchor]['latches'].append(trg_latch_instance)
                except Exception:
                    pass

                objects.append({'id': 'generic_latch', 'instance': src_latch_instance})
                objects.append({'id': 'generic_latch', 'instance': trg_latch_instance})
                objects.append({'id': 'generic_handle', 'instance': handle_instance})

                try:
                    src_pos = layout.get(art_instance)
                    trg_pos = layout.get(action_instance)
                    if isinstance(src_pos, list) and len(src_pos) == 2 and isinstance(trg_pos, list) and len(trg_pos) == 2:
                        src_x, src_y = int(src_pos[0]), int(src_pos[1])
                        trg_x, trg_y = int(trg_pos[0]), int(trg_pos[1])
                        layout[src_latch_instance] = [src_x + 140, src_y]
                        layout[trg_latch_instance] = [trg_x - 140, trg_y]
                except Exception:
                    pass

                objects.append({
                    'id': 'dynamic_line',
                    'instance': line_instance,
                    'source': src_latch_instance,
                    'target': trg_latch_instance,
                    'handles': [handle_instance],
                })
                flow_children.append(line_instance)

        if provided2:
            for out_idx, out_key in enumerate(provided2):
                asset_instance = _new_uuid()
                flow_children.append(asset_instance)

                # Anchors for the asset.
                asset_left_anchor_instance = _new_uuid()
                asset_right_anchor_instance = _new_uuid()

                asset_left_anchor_obj = {
                    'id': 'horizontal_anchor',
                    'instance': asset_left_anchor_instance,
                    'latches': [],
                }
                asset_right_anchor_obj = {
                    'id': 'horizontal_anchor',
                    'instance': asset_right_anchor_instance,
                    'latches': [],
                }
                anchor_obj_by_instance[asset_left_anchor_instance] = asset_left_anchor_obj
                anchor_obj_by_instance[asset_right_anchor_instance] = asset_right_anchor_obj
                objects.append(asset_left_anchor_obj)
                objects.append(asset_right_anchor_obj)

                objects.append({
                    'id': 'asset',
                    'instance': asset_instance,
                    'properties': [
                        ['name', out_key],
                        ['description', _asset_description_for_key(out_key)],
                    ],
                    'anchors': {
                        '180': asset_left_anchor_instance,
                        '0': asset_right_anchor_instance,
                    },
                })

                # Place assets alternating left/right, in rows, below the action.
                side = 'left' if (out_idx % 2 == 0) else 'right'
                if side == 'left':
                    row = left_rows
                    left_rows += 1
                    asset_x = int(action_x) - side_x_offset
                else:
                    row = right_rows
                    right_rows += 1
                    asset_x = int(action_x) + side_x_offset

                asset_y = int(action_y) + side_y_offset + (row * side_row_gap)
                layout[asset_instance] = [asset_x, asset_y]

                # Create an explicit edge (dynamic_line) from action -> asset.
                if side == 'left':
                    src_anchor = action_left_anchor.get(action_instance)
                    trg_anchor = asset_right_anchor_instance
                else:
                    src_anchor = action_right_anchor.get(action_instance)
                    trg_anchor = asset_left_anchor_instance
                if not src_anchor or not trg_anchor:
                    continue

                line_instance = _new_uuid()
                src_latch_instance = _new_uuid()
                trg_latch_instance = _new_uuid()
                handle_instance = _new_uuid()

                try:
                    anchor_obj_by_instance[src_anchor]['latches'].append(src_latch_instance)
                except Exception:
                    pass
                try:
                    anchor_obj_by_instance[trg_anchor]['latches'].append(trg_latch_instance)
                except Exception:
                    pass

                objects.append({'id': 'generic_latch', 'instance': src_latch_instance})
                objects.append({'id': 'generic_latch', 'instance': trg_latch_instance})
                objects.append({'id': 'generic_handle', 'instance': handle_instance})

                try:
                    src_pos = layout.get(action_instance)
                    trg_pos = layout.get(asset_instance)
                    if isinstance(src_pos, list) and len(src_pos) == 2 and isinstance(trg_pos, list) and len(trg_pos) == 2:
                        src_x, src_y = int(src_pos[0]), int(src_pos[1])
                        trg_x, trg_y = int(trg_pos[0]), int(trg_pos[1])
                        if side == 'left':
                            layout[src_latch_instance] = [src_x - 140, src_y]
                            layout[trg_latch_instance] = [trg_x + 140, trg_y]
                        else:
                            layout[src_latch_instance] = [src_x + 140, src_y]
                            layout[trg_latch_instance] = [trg_x - 140, trg_y]
                except Exception:
                    pass

                objects.append({
                    'id': 'dynamic_line',
                    'instance': line_instance,
                    'source': src_latch_instance,
                    'target': trg_latch_instance,
                    'handles': [handle_instance],
                })
                flow_children.append(line_instance)

        # Advance cursor by enough to avoid overlap with the widest side.
        max_rows = max(left_rows, right_rows)
        span = action_step_min
        if max_rows:
            span = max(span, side_y_offset + (max_rows * side_row_gap) + 80)
        action_y_cursor += span

    # Create explicit edges between consecutive actions.
    # In OpenChart exports, a line references its source/target latches, and an
    # anchor references linked latches. This is how Builder derives the graph.
    for i in range(len(action_instances) - 1):
        src_action = action_instances[i]
        trg_action = action_instances[i + 1]

        src_anchor = action_right_anchor.get(src_action)
        trg_anchor = action_left_anchor.get(trg_action)
        if not src_anchor or not trg_anchor:
            continue

        line_instance = _new_uuid()
        src_latch_instance = _new_uuid()
        trg_latch_instance = _new_uuid()
        handle_instance = _new_uuid()

        # Link latches to anchors.
        try:
            anchor_obj_by_instance[src_anchor]['latches'].append(src_latch_instance)
        except Exception:
            pass
        try:
            anchor_obj_by_instance[trg_anchor]['latches'].append(trg_latch_instance)
        except Exception:
            pass

        # Create latches and handle.
        objects.append({'id': 'generic_latch', 'instance': src_latch_instance})
        objects.append({'id': 'generic_latch', 'instance': trg_latch_instance})
        objects.append({'id': 'generic_handle', 'instance': handle_instance})

        # Builder's exported layout includes latch positions (but not anchors/lines).
        # Place latches near their respective actions.
        try:
            src_pos = layout.get(src_action)
            trg_pos = layout.get(trg_action)
            if isinstance(src_pos, list) and len(src_pos) == 2 and isinstance(trg_pos, list) and len(trg_pos) == 2:
                src_x, src_y = int(src_pos[0]), int(src_pos[1])
                trg_x, trg_y = int(trg_pos[0]), int(trg_pos[1])
                layout[src_latch_instance] = [src_x + 140, src_y]
                layout[trg_latch_instance] = [trg_x - 140, trg_y]
        except Exception:
            pass

        # Create line connecting the two latches.
        objects.append({
            'id': 'dynamic_line',
            'instance': line_instance,
            'source': src_latch_instance,
            'target': trg_latch_instance,
            'handles': [handle_instance],
        })
        flow_children.append(line_instance)

    # Root flow canvas/group export. Must reference children via `objects`.
    objects.append({
        'id': 'flow',
        'instance': flow_instance,
        'properties': [
            ['name', flow_name],
            ['description', 'A linear chain of flags placed on topology nodes.'],
            ['author', [
                ['name', 'CORE TopoGen'],
                ['identity_class', 'organization'],
                ['contact_information', ''],
            ]],
            ['scope', 'incident'],
            ['external_references', []],
            ['created', now],
        ],
        'objects': flow_children,
    })
    layout[flow_instance] = [160, 120]

    return {
        'schema': 'attack_flow_v2',
        'theme': 'dark_theme',
        'objects': objects,
        'layout': layout,
    }


def _flow_strip_ids_from_hint(text: str) -> str:
    """Strip any node id fragments from a rendered hint.

    Historically, hints included "(id=...)" to help debugging. For the user-facing
    experience, we now remove ids from the hint text while still keeping internal
    next_node_id/this_node_id fields for chaining.
    """
    try:
        s = str(text or '')
    except Exception:
        return ''
    if not s.strip():
        return ''
    try:
        # Remove patterns like " (id=abc123)" (case-insensitive, whitespace tolerant).
        s = re.sub(r"\s*\(\s*id\s*=\s*[^)]*\)", "", s, flags=re.IGNORECASE)
    except Exception:
        pass
    # If templates include optional fields (e.g., "({{NEXT_NODE_IP}})"),
    # the placeholder may resolve to an empty string. Strip empty parens.
    try:
        s = re.sub(r"\s*\(\s*\)", "", s)
    except Exception:
        pass
    # If templates include optional IP (e.g., "@ {{NEXT_NODE_IP}}") and the IP is
    # unavailable, the placeholder resolves to empty and leaves a dangling '@'.
    try:
        s = re.sub(r"\s*@\s*$", "", s)
    except Exception:
        pass
    # Remove any leftover unexpanded id placeholders.
    for token in ('{{NEXT_NODE_ID}}', '{{THIS_NODE_ID}}'):
        try:
            s = s.replace(token, '')
        except Exception:
            continue
    try:
        s = re.sub(r"[\t ]{2,}", " ", s)
    except Exception:
        pass
    return s.strip()


def _flow_render_hint_template(
    tpl: str,
    *,
    scenario_label: str,
    id_to_name: dict[str, str],
    id_to_ip: dict[str, str] | None = None,
    this_id: str,
    next_id: str,
) -> str:
    try:
        raw_tpl = str(tpl or '').strip() or 'Next: {{NEXT_NODE_NAME}}'
        text = raw_tpl
        next_id_val = str(next_id or '').strip()
        next_name_val = (id_to_name.get(next_id_val) or '').strip() if next_id_val else ''
        next_ip_val = ''
        this_ip_val = ''
        try:
            if isinstance(id_to_ip, dict):
                next_ip_val = str(id_to_ip.get(next_id_val) or '').strip()
                this_ip_val = str(id_to_ip.get(str(this_id or '').strip()) or '').strip()
        except Exception:
            next_ip_val = ''
            this_ip_val = ''
        if not next_id_val:
            return "You've completed this sequence of challenges!"
        if not next_name_val:
            next_name_val = next_id_val

        # If the template references NEXT_NODE_NAME but not NEXT_NODE_IP,
        # append the IP address to the displayed name when available.
        if ('{{NEXT_NODE_NAME}}' in raw_tpl) and ('{{NEXT_NODE_IP}}' not in raw_tpl) and next_ip_val:
            next_name_val = f"{next_name_val} ({next_ip_val})"

        repl = {
            '{{SCENARIO}}': str(scenario_label or ''),
            '{{THIS_NODE_ID}}': str(this_id or ''),
            '{{THIS_NODE_NAME}}': id_to_name.get(str(this_id or '')) or str(this_id or ''),
            '{{THIS_NODE_IP}}': this_ip_val,
            '{{NEXT_NODE_ID}}': next_id_val,
            '{{NEXT_NODE_NAME}}': next_name_val,
            '{{NEXT_NODE_IP}}': next_ip_val,
        }
        for k, v in repl.items():
            try:
                text = text.replace(k, str(v))
            except Exception:
                continue
        return _flow_strip_ids_from_hint(text)
    except Exception:
        return _flow_strip_ids_from_hint(str(tpl or '').strip() or 'Next: {{NEXT_NODE_NAME}}')


def _flow_hint_templates_from_generator(gen: dict[str, Any]) -> list[str]:
    """Return ordered hint templates from a generator view.

    Accepts:
    - gen['hint_templates']: list[str]
    - gen['hint_template']: str or list[str] (legacy / permissive)
    """
    out: list[str] = []
    try:
        ht = gen.get('hint_templates')
        if isinstance(ht, list):
            for x in ht:
                s = str(x or '').strip()
                if s:
                    s2 = _flow_strip_ids_from_hint(s)
                    if s2:
                        out.append(s2)
    except Exception:
        pass
    if out:
        return out
    try:
        ht1 = gen.get('hint_template')
        if isinstance(ht1, list):
            for x in ht1:
                s = str(x or '').strip()
                if s:
                    s2 = _flow_strip_ids_from_hint(s)
                    if s2:
                        out.append(s2)
        else:
            s = str(ht1 or '').strip()
            if s:
                s2 = _flow_strip_ids_from_hint(s)
                if s2:
                    out.append(s2)
    except Exception:
        pass
    if not out:
        out = ['Next: {{NEXT_NODE_NAME}}']
    return out


def _flow_preset_steps(preset: str) -> list[dict[str, str]]:
    p = str(preset or '').strip().lower()
    if p in {'sample_reverse_nfs_ssh', 'sample'}:
        return [
            {'id': 'binary_embed_text', 'kind': 'flag-generator', 'catalog': 'flag_generators'},
            {'id': 'nfs_sensitive_file', 'kind': 'flag-node-generator', 'catalog': 'flag_node_generators'},
            {'id': 'textfile_username_password', 'kind': 'flag-generator', 'catalog': 'flag_generators'},
        ]
    return []


def _flow_compute_flag_assignments_for_preset(
    preview: dict,
    chain_nodes: list[dict[str, Any]],
    scenario_label: str,
    preset: str,
) -> tuple[list[dict[str, Any]], str | None]:
    steps = _flow_preset_steps(preset)
    if not steps:
        return [], 'unknown preset'

    def _preset_stats() -> dict[str, int]:
        try:
            _nodes, _links, _adj = _build_topology_graph_from_preview_plan(preview)
            st = _flow_compose_docker_stats(_nodes)
            return st if isinstance(st, dict) else {}
        except Exception:
            return {}

    def _requirement_message(*, required_total: int, required_nonvuln_docker: int) -> str:
        st = _preset_stats()
        docker_total = int(st.get('docker_total') or 0)
        docker_nonvuln_total = int(st.get('docker_nonvuln_total') or 0)
        vuln_total = int(st.get('vuln_total') or 0)
        eligible_total = int(st.get('eligible_total') or 0)
        required_vuln = max(0, int(required_total) - int(required_nonvuln_docker))
        return (
            f"Requires {required_vuln} Vuln and {int(required_nonvuln_docker)} Non-Vuln Docker. "
            f"Current: Eligible: {eligible_total}, Docker: {docker_total} (Non-Vuln: {docker_nonvuln_total}), Vuln: {vuln_total}"
        )

    required_total = len(steps)
    required_nonvuln_docker = sum(1 for s in steps if str((s or {}).get('kind') or '').strip() == 'flag-node-generator')

    if len(chain_nodes) < required_total:
        return [], _requirement_message(required_total=required_total, required_nonvuln_docker=required_nonvuln_docker)

    try:
        gens, _ = _flag_generators_from_enabled_sources()
    except Exception:
        gens = []
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
    except Exception:
        node_gens = []

    by_id: dict[str, dict[str, Any]] = {}
    for g in (gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid and gid not in by_id:
            by_id[gid] = g
    for g in (node_gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid and gid not in by_id:
            by_id[gid] = g

    chain_ids: list[str] = [str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict) and str(n.get('id') or '').strip()]
    id_to_name: dict[str, str] = {}
    id_to_ip: dict[str, str] = {}
    vuln_names_by_id: dict[str, list[str]] = {}
    try:
        hosts = preview.get('hosts') if isinstance(preview, dict) else None
        if isinstance(hosts, list):
            for h in hosts:
                if not isinstance(h, dict):
                    continue
                hid = str(h.get('node_id') or '').strip()
                if not hid:
                    continue
                vulns = h.get('vulnerabilities') if isinstance(h.get('vulnerabilities'), list) else []
                names: list[str] = []
                for v in (vulns or []):
                    if isinstance(v, str):
                        s = v.strip()
                        if s:
                            names.append(s)
                        continue
                    if isinstance(v, dict):
                        for key in ('name', 'title', 'id', 'vuln', 'cve', 'cve_id', 'slug'):
                            val = v.get(key)
                            if isinstance(val, str) and val.strip():
                                names.append(val.strip())
                                break
                if names:
                    uniq = []
                    seen = set()
                    for n in names:
                        if n and n not in seen:
                            seen.add(n)
                            uniq.append(n)
                    vuln_names_by_id[hid] = uniq
    except Exception:
        vuln_names_by_id = {}
    def _extract_vuln_names(obj: dict[str, Any]) -> list[str]:
        names: list[str] = []
        try:
            raw = obj.get('vulnerabilities')
            if isinstance(raw, list):
                for v in raw:
                    if isinstance(v, str):
                        s = v.strip()
                        if s:
                            names.append(s)
                        continue
                    if isinstance(v, dict):
                        for key in ('name', 'title', 'id', 'vuln', 'cve', 'cve_id', 'slug'):
                            val = v.get(key)
                            if isinstance(val, str) and val.strip():
                                names.append(val.strip())
                                break
        except Exception:
            pass
        return names
    try:
        for n in (chain_nodes or []):
            if not isinstance(n, dict):
                continue
            nid = str(n.get('id') or '').strip()
            if not nid or nid in vuln_names_by_id:
                continue
            names = _extract_vuln_names(n)
            if names:
                uniq = []
                seen = set()
                for nm in names:
                    if nm and nm not in seen:
                        seen.add(nm)
                        uniq.append(nm)
                vuln_names_by_id[nid] = uniq
    except Exception:
        pass
    for n in chain_nodes:
        try:
            nid = str(n.get('id') or '').strip()
            nm = str(n.get('name') or '').strip()
            if nid:
                id_to_name[nid] = nm or nid
                ip = ''
                try:
                    ip = _first_valid_ipv4(n.get('ip4') or n.get('ipv4') or n.get('ip') or '')
                except Exception:
                    ip = ''
                if ip:
                    id_to_ip[nid] = ip
        except Exception:
            pass

    # Prefer v3 plugin contracts for artifact-level chaining semantics.
    try:
        plugins_by_id = _flow_enabled_plugin_contracts_by_id()
    except Exception:
        plugins_by_id = {}

    out: list[dict[str, Any]] = []
    for i, step in enumerate(steps):
        cid = chain_ids[i] if i < len(chain_ids) else ''
        next_id = chain_ids[i + 1] if (i + 1) < len(chain_ids) else ''
        gen_id = str(step.get('id') or '').strip()
        kind = str(step.get('kind') or '').strip() or 'flag-generator'
        catalog = str(step.get('catalog') or '').strip() or ('flag_node_generators' if kind == 'flag-node-generator' else 'flag_generators')

        # Mirror the existing rule: vuln nodes are only assigned flag-generators.
        try:
            node = chain_nodes[i] if i < len(chain_nodes) else {}
            if kind == 'flag-node-generator' and isinstance(node, dict) and bool(node.get('is_vuln')):
                return [], _requirement_message(required_total=required_total, required_nonvuln_docker=required_nonvuln_docker)
        except Exception:
            pass

        gen = by_id.get(gen_id)
        if not isinstance(gen, dict):
            return [], f'generator not found/enabled: {gen_id}'

        hint_templates = _flow_hint_templates_from_generator(gen)
        hint_tpl = hint_templates[0] if hint_templates else 'Next: {{NEXT_NODE_NAME}}'

        # Runtime IO (generator input/output fields).
        # Show required + optional separately (UI), but only required participates in feasibility.
        input_fields_all = sorted([
            str(x.get('name') or '').strip()
            for x in (gen.get('inputs') or [])
            if isinstance(x, dict) and str(x.get('name') or '').strip()
        ])
        input_fields_required = sorted([
            str(x.get('name') or '').strip()
            for x in (gen.get('inputs') or [])
            if isinstance(x, dict) and str(x.get('name') or '').strip() and x.get('required') is not False
        ])
        input_fields_optional = sorted([x for x in input_fields_all if x and x not in set(input_fields_required)])
        output_fields = sorted([
            str(x.get('name') or '').strip()
            for x in (gen.get('outputs') or [])
            if isinstance(x, dict) and str(x.get('name') or '').strip()
        ])

        # Artifact-level contracts (plugin requires/produces).
        requires_artifacts: list[str] = []
        produces_artifacts: list[str] = []
        try:
            plugin = plugins_by_id.get(gen_id)
            if isinstance(plugin, dict):
                req = plugin.get('requires')
                if isinstance(req, list):
                    requires_artifacts = sorted([str(x).strip() for x in req if str(x).strip()])
                prod = plugin.get('produces')
                if isinstance(prod, list):
                    produces_artifacts = sorted([
                        str(it.get('artifact') or '').strip()
                        for it in prod
                        if isinstance(it, dict) and str(it.get('artifact') or '').strip()
                    ])
        except Exception:
            requires_artifacts = []
            produces_artifacts = []

        # If an artifact "requires" token also appears as an optional input field,
        # treat it as optional (exclude from effective chaining requirements).
        try:
            optional_field_set = set(input_fields_optional)
            requires_effective = sorted([x for x in (requires_artifacts or []) if x and x not in optional_field_set])
        except Exception:
            requires_effective = list(requires_artifacts or [])

        # Effective chaining semantics used by ordering validation.
        inputs_effective = sorted(set(requires_effective) | set(input_fields_required))
        outputs_effective = sorted(set(produces_artifacts) | set(output_fields))
        output_fields = sorted(set(output_fields) | set(produces_artifacts))

        rendered_hints = [
            _flow_render_hint_template(
                t,
                scenario_label=scenario_label,
                id_to_name=id_to_name,
                id_to_ip=id_to_ip,
                this_id=str(cid),
                next_id=str(next_id),
            )
            for t in (hint_templates or [])
        ]
        out.append({
            'node_id': str(cid),
            'id': gen_id,
            'name': str(gen.get('name') or ''),
            'description': str(gen.get('description') or ''),
            'type': kind,
            'flag_generator': str(gen.get('_source_name') or '').strip() or 'unknown',
            'generator_catalog': catalog,
            'language': str(gen.get('language') or ''),
            'vulnerabilities': list(vuln_names_by_id.get(str(cid), []) or []),
            'description_hints': list(gen.get('description_hints') or []) if isinstance(gen.get('description_hints'), list) else [],
            'inject_files': list(gen.get('inject_files') or []) if isinstance(gen.get('inject_files'), list) else [],
            # Effective union (used for chaining feasibility / ordering validation).
            'inputs': inputs_effective,
            'outputs': outputs_effective,

            # Split-out views for UI transparency.
            'requires': requires_artifacts,
            'produces': produces_artifacts,
            'input_fields': input_fields_all,
            'input_fields_required': input_fields_required,
            'input_fields_optional': input_fields_optional,
            'output_fields': output_fields,
            'hint_template': hint_tpl,
            'hint_templates': hint_templates,
            'hint': rendered_hints[0] if rendered_hints else _flow_render_hint_template(
                hint_tpl,
                scenario_label=scenario_label,
                id_to_name=id_to_name,
                id_to_ip=id_to_ip,
                this_id=str(cid),
                next_id=str(next_id),
            ),
            'hints': rendered_hints,
            'next_node_id': str(next_id),
            'next_node_name': str(id_to_name.get(str(next_id)) or ''),
        })

    return out, None


@app.before_request
def _require_login_redirect() -> None | Response:
    try:
        endpoint = request.endpoint or ''
        if not endpoint:
            return None
        if endpoint.startswith('static'):
            return None
        if endpoint in _LOGIN_EXEMPT_ENDPOINTS:
            return None
        if _current_user() is None:
            # For API routes, return a JSON 401 instead of an HTML redirect.
            # This prevents frontend fetch() calls from failing with non-JSON responses.
            try:
                if (request.path or '').startswith('/api/'):
                    return jsonify({'ok': False, 'error': 'Login required'}), 401
            except Exception:
                pass
            return redirect(url_for('login'))
    except Exception:
        return None
    return None


def _require_admin() -> bool:
    user = _current_user()
    if user and (user.get('role') == 'admin'):
        return True
    flash('Admin privileges required')
    return False


@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'GET':
        return render_template('login.html')
    username = (request.form.get('username') or '').strip()
    password = request.form.get('password') or ''
    if not username or not password:
        flash('Username and password required')
        return render_template('login.html', error=True), 400
    db = _load_users()
    users = db.get('users', [])
    user = next((u for u in users if u.get('username') == username), None)
    if user and check_password_hash(user.get('password_hash', ''), password):
        role_value = _normalize_role_value(user.get('role'))
        _set_current_user({'username': user.get('username'), 'role': role_value})
        session.permanent = True
        try:
            session[_UI_VIEW_SESSION_KEY] = _default_ui_view_mode_for_role(role_value)
        except Exception:
            pass
        if _is_participant_role(role_value):
            return redirect(url_for('participant_ui_page'))
        return redirect(url_for('index'))
    flash('Invalid username or password')
    return render_template('login.html', error=True), 401


@app.route('/logout', methods=['POST', 'GET'])
def logout():
    _set_current_user(None)
    return redirect(url_for('login'))


@app.route('/scenarios/flag-sequencing')
def flow_page():
    history = _load_run_history()
    current_user = _current_user()
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
    scenario_participant_urls = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
    participant_url_flags = {
        norm: bool(url)
        for norm, url in scenario_participant_urls.items()
        if isinstance(norm, str) and norm
    }

    scenario_query = request.args.get('scenario', '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    # Enforce per-scenario view: default to the first available scenario when unset/invalid.
    if scenario_names:
        if not scenario_norm or not any(_normalize_scenario_label(n) == scenario_norm for n in scenario_names):
            scenario_norm = _normalize_scenario_label(scenario_names[0])
    active_scenario = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query)

    # Best-effort: provide the saved scenario XML path so the shared Preview button can work from this page.
    active_scenario_xml_path = ''
    xml_path = (request.args.get('xml_path') or '').strip()
    try:
        if xml_path:
            xml_path_abs = os.path.abspath(xml_path)
            if os.path.exists(xml_path_abs):
                active_scenario_xml_path = xml_path_abs
    except Exception:
        active_scenario_xml_path = ''
    if not active_scenario_xml_path:
        try:
            if scenario_norm and isinstance(scenario_paths, dict):
                p = scenario_paths.get(scenario_norm) or scenario_paths.get(active_scenario) or ''
                p2 = _select_existing_path(p)
                if p2:
                    active_scenario_xml_path = os.path.abspath(p2)
        except Exception:
            active_scenario_xml_path = ''

    xml_preview = ''
    flow_state_by_scenario: dict[str, Any] = {}
    try:
        if active_scenario_xml_path and os.path.isfile(active_scenario_xml_path):
            with open(active_scenario_xml_path, 'r', encoding='utf-8', errors='ignore') as f:
                xml_preview = f.read()
            try:
                parsed = _parse_scenarios_xml(active_scenario_xml_path)
                scen_list = parsed.get('scenarios') if isinstance(parsed, dict) else None
                if isinstance(scen_list, list):
                    for sc in scen_list:
                        if not isinstance(sc, dict):
                            continue
                        nm = str(sc.get('name') or '').strip()
                        fs = sc.get('flow_state')
                        if nm and isinstance(fs, dict) and fs:
                            flow_state_by_scenario[_normalize_scenario_label(nm)] = fs
            except Exception:
                flow_state_by_scenario = {}
    except Exception:
        xml_preview = ''

    return render_template(
        'flow.html',
        scenarios=scenario_names,
        active_scenario=active_scenario,
        participant_url_flags=participant_url_flags,
        preview_xml_path=active_scenario_xml_path,
        xml_preview=xml_preview,
        flow_state_by_scenario=flow_state_by_scenario,
        active_page='scenarios',
    )


@app.route('/scenarios/preview')
def scenarios_preview_page():
    """Scenarios sub-tab: Preview.

    This page renders a lightweight shell and loads the existing full preview page
    into an iframe while showing a loading modal.
    """
    history = _load_run_history()
    current_user = _current_user()
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)

    scenario_query = (request.args.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    if scenario_names:
        if not scenario_norm or not any(_normalize_scenario_label(n) == scenario_norm for n in scenario_names):
            scenario_norm = _normalize_scenario_label(scenario_names[0])
    active_scenario = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query)

    # Prefer explicit xml_path (e.g., coming from Topology editor save). Fallback to the catalog path for the active scenario.
    xml_path = (request.args.get('xml_path') or '').strip()
    xml_path_abs = ''
    try:
        if xml_path:
            xml_path_abs = os.path.abspath(xml_path)
            if not os.path.exists(xml_path_abs):
                xml_path_abs = ''
    except Exception:
        xml_path_abs = ''
    if not xml_path_abs:
        try:
            p = ''
            if scenario_norm and isinstance(scenario_paths, dict):
                p = scenario_paths.get(scenario_norm) or scenario_paths.get(active_scenario) or ''
            p2 = _select_existing_path(p)
            if p2:
                xml_path_abs = os.path.abspath(p2)
        except Exception:
            xml_path_abs = ''

    scenario_xml_by_name: dict[str, str] = {}
    try:
        if isinstance(scenario_names, list) and isinstance(scenario_paths, dict):
            for nm in scenario_names:
                try:
                    nm_norm = _normalize_scenario_label(nm)
                    raw_path = scenario_paths.get(nm_norm) or scenario_paths.get(nm) or ''
                    chosen = _select_existing_path(raw_path) or ''
                    scenario_xml_by_name[str(nm)] = os.path.abspath(chosen) if chosen else ''
                except Exception:
                    scenario_xml_by_name[str(nm)] = ''
    except Exception:
        scenario_xml_by_name = {}

    xml_preview = ''
    try:
        if xml_path_abs and os.path.exists(xml_path_abs):
            with open(xml_path_abs, 'r', encoding='utf-8', errors='ignore') as f:
                xml_preview = f.read()
    except Exception:
        xml_preview = ''

    return render_template(
        'scenarios_preview.html',
        scenarios=scenario_names,
        active_scenario=active_scenario,
        scenario_xml_by_name=scenario_xml_by_name,
        scenario_tab=active_scenario,
        preview_xml_path=xml_path_abs,
        xml_preview=xml_preview,
        active_page='scenarios',
    )


def _canonical_plan_path_for_scenario_norm(scenario_norm: str, *, create_dir: bool = False) -> str:
    scenario_norm = _normalize_scenario_label(scenario_norm or '')
    if not scenario_norm:
        scenario_norm = 'default'
    plans_dir = Path(_outputs_dir()) / 'plans'
    if create_dir:
        try:
            plans_dir.mkdir(parents=True, exist_ok=True)
        except Exception:
            pass
    return str(plans_dir / f"plan_{scenario_norm}.json")


def _canonical_plan_path_for_scenario(scenario: Optional[str], *, xml_path: Optional[str] = None, create_dir: bool = False) -> str:
    scenario_norm = _normalize_scenario_label(scenario or '')
    if not scenario_norm:
        try:
            base = os.path.splitext(os.path.basename(xml_path or ''))[0]
        except Exception:
            base = ''
        scenario_norm = _normalize_scenario_label(base or '') or 'default'
    plans_dir = Path(_outputs_dir()) / 'plans'
    if create_dir:
        try:
            plans_dir.mkdir(parents=True, exist_ok=True)
        except Exception:
            pass
    suffix = ''
    if xml_path:
        try:
            from core_topo_gen.planning.plan_cache import hash_xml_file
            xml_hash = hash_xml_file(xml_path)
            if xml_hash:
                suffix = f"__{xml_hash[:12]}"
        except Exception:
            suffix = ''
    return str(plans_dir / f"plan_{scenario_norm}{suffix}.json")


def _latest_plan_for_scenario_norm(scenario_norm: str) -> Optional[str]:
    """Locate the most recent plan file for a scenario (handles hashed variants)."""
    try:
        scenario_norm = _normalize_scenario_label(scenario_norm)
        if not scenario_norm:
            return None
        plans_dir = Path(_outputs_dir()) / 'plans'
        if not plans_dir.is_dir():
            return None
        candidates = []
        prefix = f"plan_{scenario_norm}"
        for p in plans_dir.glob(f"{prefix}*.json"):
            try:
                if not p.is_file():
                    continue
                candidates.append(p)
            except Exception:
                continue
        if not candidates:
            return None
        def _candidate_ts(path: Path) -> float:
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    payload = json.load(f) or {}
                meta = payload.get('metadata') if isinstance(payload, dict) else None
                if isinstance(meta, dict):
                    scenario_val = str(meta.get('scenario') or '').strip()
                    if _normalize_scenario_label(scenario_val) != scenario_norm:
                        return 0.0
                    updated = meta.get('updated_at') or meta.get('created_at')
                    if isinstance(updated, str) and updated:
                        return _parse_iso_ts(updated)
            except Exception:
                pass
            try:
                return path.stat().st_mtime
            except Exception:
                return 0.0
        best = max(candidates, key=_candidate_ts, default=None)
        return str(best) if best else None
    except Exception:
        return None


def _latest_preview_plan_for_scenario_norm(scenario_norm: str, *, prefer_flow: bool = True) -> Optional[str]:
    """Return absolute path to the canonical plan for a scenario."""
    try:
        scenario_norm = _normalize_scenario_label(scenario_norm)
        if not scenario_norm:
            return None
        xml_path = _latest_xml_path_for_scenario(scenario_norm)
        if not xml_path:
            return None
        payload = _load_plan_preview_from_xml(xml_path, scenario_norm)
        return xml_path if payload else None
    except Exception:
        return None


def _latest_preview_plan_for_scenario_norm_origin(scenario_norm: str, *, origin: str) -> Optional[str]:
    """Return canonical plan path for scenario with a specific metadata.origin."""
    try:
        scenario_norm = _normalize_scenario_label(scenario_norm)
        origin_norm = str(origin or '').strip().lower()
        if not scenario_norm or not origin_norm:
            return None
        xml_path = _latest_xml_path_for_scenario(scenario_norm)
        if not xml_path:
            return None
        payload = _load_plan_preview_from_xml(xml_path, scenario_norm)
        if not isinstance(payload, dict):
            return None
        meta = payload.get('metadata') if isinstance(payload.get('metadata'), dict) else {}
        scen = str(meta.get('scenario') or '').strip()
        if _normalize_scenario_label(scen) != scenario_norm:
            return None
        o = str(meta.get('origin') or '').strip().lower()
        if o != origin_norm:
            return None
        return xml_path
    except Exception:
        return None


def _latest_flow_plan_for_scenario_norm(scenario_norm: str) -> Optional[str]:
    """Return canonical plan path for scenario (single plan only)."""
    try:
        scenario_norm = _normalize_scenario_label(scenario_norm)
        if not scenario_norm:
            return None
        xml_path = _latest_xml_path_for_scenario(scenario_norm)
        if not xml_path:
            return None
        payload = _load_plan_preview_from_xml(xml_path, scenario_norm)
        return xml_path if payload else None
    except Exception:
        return None


def _planner_state_path() -> str:
    return os.path.join(_outputs_dir(), 'planner_state.json')


def _load_planner_state() -> dict[str, Any]:
    return {}


def _save_planner_state(state: dict[str, Any]) -> None:
    return


def _planner_set_plan(scenario_norm: str, *, plan_path: str, xml_path: str | None = None, seed: int | None = None) -> None:
    try:
        key = _normalize_scenario_label(scenario_norm)
        if not key:
            return
        state = _load_planner_state()
        entry = {
            'plan_path': plan_path,
            'xml_path': xml_path or '',
            'seed': seed,
            'updated_at': int(time.time()),
        }
        state[key] = entry
        _save_planner_state(state)
    except Exception:
        pass


def _planner_get_plan(scenario_norm: str) -> dict[str, Any] | None:
    try:
        key = _normalize_scenario_label(scenario_norm)
        if not key:
            return None
        state = _load_planner_state()
        entry = state.get(key) if isinstance(state, dict) else None
        if not isinstance(entry, dict):
            entry = None
        xml_path = str((entry or {}).get('xml_path') or '').strip()
        if not xml_path:
            xml_path = _latest_xml_path_for_scenario(key) or ''
        if xml_path and os.path.exists(xml_path):
            return {
                'plan_path': xml_path,
                'xml_path': xml_path,
                'seed': (entry or {}).get('seed'),
                'updated_at': int(time.time()),
            }
        return None
    except Exception:
        return None


def _purge_planner_state_for_scenarios(names: list[str]) -> int:
    try:
        norms = {_normalize_scenario_label(n) for n in (names or []) if _normalize_scenario_label(n)}
        if not norms:
            return 0
        state = _load_planner_state()
        if not isinstance(state, dict) or not state:
            return 0
        removed = 0
        for key in list(state.keys()):
            if _normalize_scenario_label(key) in norms:
                state.pop(key, None)
                removed += 1
        if removed:
            _save_planner_state(state)
        return removed
    except Exception:
        return 0


def _purge_plan_artifacts_for_scenarios(names: list[str]) -> int:
    try:
        norms = {_normalize_scenario_label(n) for n in (names or []) if _normalize_scenario_label(n)}
        if not norms:
            return 0
        plans_dir = Path(_outputs_dir()) / 'plans'
        if not plans_dir.is_dir():
            return 0
        removed = 0
        for scen_norm in norms:
            try:
                canonical_path = _canonical_plan_path_for_scenario_norm(scen_norm)
                if os.path.exists(canonical_path):
                    os.remove(canonical_path)
                    removed += 1
            except Exception:
                pass
        for p in plans_dir.glob('*.json'):
            try:
                with open(p, 'r', encoding='utf-8') as f:
                    payload = json.load(f) or {}
                meta = payload.get('metadata') if isinstance(payload, dict) else None
                scen = ''
                if isinstance(meta, dict):
                    scen = str(meta.get('scenario') or '').strip()
                    if not scen:
                        flow_meta = meta.get('flow') if isinstance(meta.get('flow'), dict) else None
                        if isinstance(flow_meta, dict):
                            scen = str(flow_meta.get('scenario') or '').strip()
                if not scen and isinstance(payload, dict):
                    flow_meta = payload.get('flow') if isinstance(payload.get('flow'), dict) else None
                    if isinstance(flow_meta, dict):
                        scen = str(flow_meta.get('scenario') or '').strip()
                if _normalize_scenario_label(scen) in norms:
                    try:
                        os.remove(p)
                        removed += 1
                    except Exception:
                        pass
            except Exception:
                continue
        return removed
    except Exception:
        return 0


def _attach_latest_flow_into_plan_payload(payload: dict[str, Any], *, scenario: str) -> None:
    """Best-effort: merge saved flow metadata from XML into a plan payload.

    Flow state is sourced only from the scenario XML (FlagSequencing/FlowState).
    """
    try:
        if not isinstance(payload, dict):
            return
        scenario_norm = _normalize_scenario_label(scenario or '')
        if not scenario_norm:
            return

        full_prev = payload.get('full_preview')
        if not isinstance(full_prev, dict):
            return

        flow_meta = _flow_state_from_latest_xml(scenario_norm)
        if not isinstance(flow_meta, dict):
            return

        # Repair saved Flow metadata against the *current* full_preview topology.
        # This prevents stale chains/assignments from pointing at nodes that are no
        # longer eligible (e.g., non-docker/non-vuln hosts).
        try:
            repaired = _flow_repair_saved_flow_for_preview(full_prev, flow_meta)
            if not isinstance(repaired, dict):
                return
            flow_meta = repaired
        except Exception:
            return

        payload.setdefault('metadata', {})
        if isinstance(payload.get('metadata'), dict):
            payload['metadata']['flow'] = flow_meta
    except Exception:
        return


def _latest_flow_plan_for_scenario_norm(scenario_norm: str) -> Optional[str]:
    """Return canonical plan path for scenario (single plan only)."""
    try:
        scenario_norm = _normalize_scenario_label(scenario_norm)
        if not scenario_norm:
            return None
        xml_path = _latest_xml_path_for_scenario(scenario_norm)
        if not xml_path:
            return None
        payload = _load_plan_preview_from_xml(xml_path, scenario_norm)
        return xml_path if payload else None
    except Exception:
        return None


def _attach_latest_flow_into_full_preview(full_prev: dict, scenario: Optional[str], *, repair: bool = True) -> dict | None:
    """Best-effort: load saved Flag Sequencing (flow) metadata for a full_preview payload.

    Flow state is sourced only from the scenario XML (FlagSequencing/FlowState).
    Returns the flow metadata without mutating the full_preview.
    """
    try:
        if not isinstance(full_prev, dict):
            return None
        scenario_norm = _normalize_scenario_label(scenario or '')
        if not scenario_norm:
            return None
        flow_meta = None
        try:
            flow_meta = _flow_state_from_latest_xml(scenario_norm)
        except Exception:
            flow_meta = None
        if not isinstance(flow_meta, dict):
            return None

        # Repair saved Flow metadata against the *current* full_preview topology.
        # Preview plans can change over time; keep the UI from marking/assigning
        # flags to nodes that are no longer eligible.
        if repair:
            try:
                repaired = _flow_repair_saved_flow_for_preview(full_prev, flow_meta)
                if not isinstance(repaired, dict):
                    return None
                flow_meta = repaired
            except Exception:
                return None

        return flow_meta
    except Exception:
        return None


def _flow_node_is_docker_role(node: dict[str, Any] | None) -> bool:
    try:
        if not isinstance(node, dict):
            return False
        t_raw = str(node.get('type') or '')
        t = t_raw.strip().lower()
        if ('docker' in t) or (t_raw.strip().upper() == 'DOCKER'):
            return True
        # Preview graph nodes also carry compose metadata for docker-role hosts.
        comp = str(node.get('compose') or '').strip()
        comp_name = str(node.get('compose_name') or '').strip()
        return bool(comp or comp_name)
    except Exception:
        return False


def _flow_node_is_vuln(node: dict[str, Any] | None) -> bool:
    try:
        if not isinstance(node, dict):
            return False
        return bool(node.get('is_vuln')) or bool(node.get('is_vulnerability')) or bool(node.get('is_vulnerable')) or bool(node.get('vulnerabilities'))
    except Exception:
        return False


def _get_flow_seed(preview: dict[str, Any] | None, flow_seed_override: int | None = None) -> int:
    """Return the flow seed for chain/generator selection.

    Priority:
    1. flow_seed_override parameter (e.g., from retry increment)
    2. preview.flow_seed (explicit flow seed stored in preview)
    3. preview.seed (topology seed as fallback for backward compatibility)
    4. 0 (default)
    """
    if flow_seed_override is not None:
        try:
            return int(flow_seed_override)
        except (ValueError, TypeError):
            pass
    if isinstance(preview, dict):
        flow_seed = preview.get('flow_seed')
        if flow_seed is not None:
            try:
                return int(flow_seed)
            except (ValueError, TypeError):
                pass
        seed = preview.get('seed')
        if seed is not None:
            try:
                return int(seed)
            except (ValueError, TypeError):
                pass
    return 0


def _flow_repair_saved_flow_for_preview(full_prev: dict, flow_meta: dict) -> dict | None:

    """Best-effort: repair a persisted Flow chain/assignments against the current preview.

    This is intentionally conservative: if we cannot build a valid chain of the same
    length, we return None and the caller should skip attaching Flow.

    Placement rules enforced:
    - flag-generator: allowed on vuln nodes only
    - flag-node-generator: allowed only on non-vuln docker-role nodes
    """
    if not isinstance(full_prev, dict) or not isinstance(flow_meta, dict):
        return None

    chain_in = flow_meta.get('chain')
    if not isinstance(chain_in, list) or not chain_in:
        return None

    assignments_in = flow_meta.get('flag_assignments')
    assignments: list[dict[str, Any]] | None = None
    if isinstance(assignments_in, list) and assignments_in:
        assignments = [a for a in assignments_in if isinstance(a, dict)]

    nodes, _links, _adj = _build_topology_graph_from_preview_plan(full_prev)
    id_map = {str(n.get('id') or '').strip(): n for n in (nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()}
    if not id_map:
        return None

    # Candidate pools.
    eligible_any: list[dict[str, Any]] = []
    eligible_nonvuln_docker: list[dict[str, Any]] = []
    for n in nodes or []:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if not nid:
            continue
        is_docker = _flow_node_is_docker_role(n)
        is_vuln = _flow_node_is_vuln(n)
        if is_vuln:
            eligible_any.append(n)
        if is_docker and (not is_vuln):
            eligible_nonvuln_docker.append(n)
    eligible_any.sort(key=lambda x: str(x.get('id') or ''))
    eligible_nonvuln_docker.sort(key=lambda x: str(x.get('id') or ''))

    # Saved chains may intentionally include duplicate nodes (e.g., when the user
    # allows duplicates with different generator seeds). Only enforce uniqueness
    # when the saved chain is itself unique.
    try:
        chain_in_ids = [
            str((step or {}).get('id') or '').strip()
            for step in (chain_in or [])
            if isinstance(step, dict) and str((step or {}).get('id') or '').strip()
        ]
        enforce_unique = len(set(chain_in_ids)) == len(chain_in_ids)
    except Exception:
        enforce_unique = True

    used: set[str] = set()
    chain_out: list[dict[str, str]] = []
    assignments_out: list[dict[str, Any]] | None = [] if assignments is not None else None

    for i, step in enumerate(chain_in):
        step_id = str((step or {}).get('id') or '').strip() if isinstance(step, dict) else ''

        kind = 'flag-generator'
        if assignments is not None and i < len(assignments):
            try:
                kind = str((assignments[i] or {}).get('type') or '').strip() or 'flag-generator'
            except Exception:
                kind = 'flag-generator'

        need_nonvuln_docker = (kind == 'flag-node-generator')

        cand = id_map.get(step_id) if step_id else None
        if cand is not None:
            nid = str(cand.get('id') or '').strip()
            ok = bool(nid) and ((not enforce_unique) or (nid not in used))
            if ok:
                is_docker = _flow_node_is_docker_role(cand)
                is_vuln = _flow_node_is_vuln(cand)
                if need_nonvuln_docker:
                    ok = bool(is_docker and (not is_vuln))
                else:
                    ok = bool(is_vuln)
            if not ok:
                cand = None

        if cand is None:
            pool = eligible_nonvuln_docker if need_nonvuln_docker else eligible_any
            pick = None
            for n in pool:
                nid = str(n.get('id') or '').strip()
                if not nid:
                    continue
                if enforce_unique and nid in used:
                    continue
                pick = n
                break
            cand = pick

        if not isinstance(cand, dict):
            return None

        nid = str(cand.get('id') or '').strip()
        if not nid:
            return None
        if enforce_unique:
            used.add(nid)

        chain_out.append({
            'id': nid,
            'name': str(cand.get('name') or nid),
            'type': str(cand.get('type') or 'node'),
        })

        if assignments is not None and assignments_out is not None and i < len(assignments):
            a2 = dict(assignments[i] or {})
            a2['node_id'] = nid
            assignments_out.append(a2)

    out = dict(flow_meta)
    out['chain'] = chain_out
    out['length'] = len(chain_out)
    if assignments_out is not None:
        out['flag_assignments'] = assignments_out
    return out


def _build_topology_graph_from_preview_plan(preview: Dict[str, Any]) -> Tuple[list[dict[str, Any]], list[dict[str, str]], dict[str, set[str]]]:
    """Build a lightweight graph from a full_preview payload.

    This is used for Flow generation based on persisted preview artifacts
    (no CORE session XML required).
    """
    nodes_out: list[dict[str, Any]] = []
    links_out: list[dict[str, str]] = []
    by_id: dict[str, dict[str, Any]] = {}

    def _add_node(
        nid: str,
        name: str,
        ntype: str,
        *,
        compose: str = '',
        compose_name: str = '',
        is_vuln: bool = False,
        ip4: str = '',
    ) -> None:
        if not nid or nid in by_id:
            return
        rec = {
            'id': nid,
            'name': name or nid,
            'type': ntype or 'node',
            'compose': compose or '',
            'compose_name': compose_name or '',
            'is_vuln': bool(is_vuln),
            'ip4': str(ip4 or '').strip(),
            'interfaces': [],
            'services': [],
        }
        by_id[nid] = rec
        nodes_out.append(rec)

    def _add_link(a: str, b: str) -> None:
        a = str(a or '').strip()
        b = str(b or '').strip()
        if not a or not b or a == b:
            return
        if a not in by_id or b not in by_id:
            return
        links_out.append({'node1': a, 'node2': b})

    # Add routers
    for r in (preview.get('routers') or []):
        if not isinstance(r, dict):
            continue
        rid = str(r.get('node_id') or '').strip()
        if not rid:
            continue
        _add_node(rid, str(r.get('name') or f'router-{rid}'), 'router')

    # Add switches
    for s in (preview.get('switches') or []):
        if not isinstance(s, dict):
            continue
        sid = str(s.get('node_id') or '').strip()
        if not sid:
            continue
        _add_node(sid, str(s.get('name') or f'switch-{sid}'), 'switch')

    vuln_ids: set[str] = set()
    try:
        vb = preview.get('vulnerabilities_by_node') if isinstance(preview, dict) else None
        if isinstance(vb, dict):
            for k, v in vb.items():
                if not v:
                    continue
                kk = str(k or '').strip()
                if kk:
                    vuln_ids.add(kk)
    except Exception:
        vuln_ids = set()

    # Add hosts
    for h in (preview.get('hosts') or []):
        if not isinstance(h, dict):
            continue
        hid = str(h.get('node_id') or '').strip()
        if not hid:
            continue
        role = str(h.get('role') or 'Host')
        role_norm = role.strip().lower()
        is_docker_role = role_norm == 'docker'
        vulns = h.get('vulnerabilities') if isinstance(h.get('vulnerabilities'), list) else []
        is_vuln = bool(vulns) or (hid in vuln_ids)
        # IMPORTANT: vulnerabilities should not "take up" Docker node slots.
        # Only Docker-role hosts are treated as docker nodes for Flow eligibility.
        node_type = 'docker' if is_docker_role else 'host'
        compose = ''
        compose_name = ''
        if is_docker_role:
            compose_name = 'standard-ubuntu-docker-core'
            compose = 'scripts/standard-ubuntu-docker-core/docker-compose.yml'
        # Best-effort: pull an IPv4 address from host payload if present.
        host_ip4 = ''
        try:
            host_ip4 = _first_valid_ipv4(h.get('ip4') or h.get('ipv4') or h.get('ip'))
        except Exception:
            host_ip4 = ''
        if not host_ip4:
            try:
                ifaces = h.get('interfaces') if isinstance(h.get('interfaces'), list) else []
                for iface in ifaces:
                    if not isinstance(iface, dict):
                        continue
                    host_ip4 = _first_valid_ipv4(iface.get('ip4') or iface.get('ipv4') or iface.get('ip') or iface.get('address'))
                    if host_ip4:
                        break
            except Exception:
                host_ip4 = ''
        _add_node(
            hid,
            str(h.get('name') or f'host-{hid}'),
            node_type,
            compose=compose,
            compose_name=compose_name,
            is_vuln=is_vuln,
            ip4=host_ip4,
        )

    # Router-to-router links
    for l in (preview.get('r2r_links_preview') or []):
        if not isinstance(l, dict):
            continue
        routers = l.get('routers')
        if not isinstance(routers, list) or len(routers) < 2:
            continue
        try:
            a = str((routers[0] or {}).get('id') or '').strip()
            b = str((routers[1] or {}).get('id') or '').strip()
        except Exception:
            continue
        _add_link(a, b)

    # Router-switch-host grouping
    try:
        for detail in (preview.get('switches_detail') or []):
            if not isinstance(detail, dict):
                continue
            sid = str(detail.get('switch_id') or '').strip()
            rid = str(detail.get('router_id') or '').strip()
            if sid and rid:
                _add_link(sid, rid)
            for hid in (detail.get('hosts') or []):
                _add_link(sid, str(hid))
    except Exception:
        pass

    # Direct host-router attachment (when no switches)
    try:
        hrm = preview.get('host_router_map') or {}
        if isinstance(hrm, dict):
            for hid, rid in hrm.items():
                _add_link(str(hid), str(rid))
    except Exception:
        pass

    adj: dict[str, set[str]] = {nid: set() for nid in by_id.keys()}
    for l in links_out:
        a = str(l.get('node1') or '').strip()
        b = str(l.get('node2') or '').strip()
        if not a or not b:
            continue
        adj.setdefault(a, set()).add(b)
        adj.setdefault(b, set()).add(a)

    return nodes_out, links_out, adj


@app.route('/api/flag-sequencing/latest_preview_plan')
def api_flow_latest_preview_plan():
    scenario_label = (request.args.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    xml_hint = (request.args.get('xml_path') or '').strip()
    xml_path_for_core = ''
    try:
        if xml_hint:
            xml_path_for_core = os.path.abspath(xml_hint)
    except Exception:
        xml_path_for_core = xml_hint
    if not xml_path_for_core:
        xml_path_for_core = _latest_xml_path_for_scenario(scenario_norm) or ''

    flow_run_remote = False
    flow_remote_forced = False
    flow_core_cfg: Dict[str, Any] | None = None
    try:
        if 'run_remote' in j:
            flow_run_remote = _coerce_bool(j.get('run_remote'))
            flow_remote_forced = flow_run_remote
        if 'run_local' in j and _coerce_bool(j.get('run_local')):
            flow_run_remote = False
            flow_remote_forced = False
    except Exception:
        pass
    if not flow_run_remote:
        try:
            selected_cfg = _core_config_from_xml_path(xml_path_for_core, scenario_norm, include_password=True)
            if isinstance(selected_cfg, dict):
                selected_cfg = _apply_core_secret_to_config(selected_cfg, scenario_norm)
            if isinstance(selected_cfg, dict) and _coerce_bool(selected_cfg.get('ssh_enabled')):
                flow_core_cfg = selected_cfg
                flow_run_remote = True
        except Exception:
            flow_core_cfg = None
    if flow_run_remote and not flow_core_cfg:
        try:
            selected_cfg = _core_config_from_xml_path(xml_path_for_core, scenario_norm, include_password=True)
            if isinstance(selected_cfg, dict):
                flow_core_cfg = _apply_core_secret_to_config(selected_cfg, scenario_norm)
        except Exception:
            flow_core_cfg = None

    core_validated = False
    try:
        if isinstance(flow_core_cfg, dict):
            core_validated = _coerce_bool(flow_core_cfg.get('validated'))
            if not core_validated:
                status = str(flow_core_cfg.get('last_tested_status') or '').strip().lower()
                if status == 'success':
                    core_validated = True
            if not core_validated:
                try:
                    hv_map = _load_scenario_hitl_validation_from_disk()
                    hv = None
                    if isinstance(hv_map, dict):
                        hv = hv_map.get(scenario_norm)
                        if hv is None:
                            try:
                                key = _scenario_match_key(scenario_norm)
                            except Exception:
                                key = ''
                            if key:
                                for k, v in hv_map.items():
                                    try:
                                        if _scenario_match_key(k) == key:
                                            hv = v
                                            break
                                    except Exception:
                                        continue
                    hv_core = hv.get('core') if isinstance(hv, dict) else None
                    if isinstance(hv_core, dict):
                        if _coerce_bool(hv_core.get('validated')):
                            core_validated = True
                        else:
                            hv_status = str(hv_core.get('last_tested_status') or '').strip().lower()
                            if hv_status == 'success':
                                core_validated = True
                        if not core_validated and str(hv_core.get('core_secret_id') or '').strip():
                            core_validated = True
                except Exception:
                    pass
                if not core_validated:
                    try:
                        secret_record = _select_latest_core_secret_record(scenario_norm or None)
                    except Exception:
                        secret_record = None
                    if secret_record and str(secret_record.get('identifier') or '').strip():
                        core_validated = True
    except Exception:
        core_validated = False
    if not core_validated:
        return jsonify({
            'ok': False,
            'error': 'Flag sequencing requires a validated CORE VM. Validate in VM / Access and save XML, then retry.',
        }), 422
    flow_run_remote = True
    core_validated = False
    try:
        if isinstance(flow_core_cfg, dict):
            core_validated = _coerce_bool(flow_core_cfg.get('validated'))
            if not core_validated:
                status = str(flow_core_cfg.get('last_tested_status') or '').strip().lower()
                if status == 'success':
                    core_validated = True
            if not core_validated:
                try:
                    hv_map = _load_scenario_hitl_validation_from_disk()
                    hv = None
                    if isinstance(hv_map, dict):
                        hv = hv_map.get(scenario_norm)
                        if hv is None:
                            try:
                                key = _scenario_match_key(scenario_norm)
                            except Exception:
                                key = ''
                            if key:
                                for k, v in hv_map.items():
                                    try:
                                        if _scenario_match_key(k) == key:
                                            hv = v
                                            break
                                    except Exception:
                                        continue
                    hv_core = hv.get('core') if isinstance(hv, dict) else None
                    if isinstance(hv_core, dict):
                        if _coerce_bool(hv_core.get('validated')):
                            core_validated = True
                        else:
                            hv_status = str(hv_core.get('last_tested_status') or '').strip().lower()
                            if hv_status == 'success':
                                core_validated = True
                        if not core_validated and str(hv_core.get('core_secret_id') or '').strip():
                            core_validated = True
                except Exception:
                    pass
                if not core_validated:
                    try:
                        secret_record = _select_latest_core_secret_record(scenario_norm or None)
                    except Exception:
                        secret_record = None
                    if secret_record and str(secret_record.get('identifier') or '').strip():
                        core_validated = True
    except Exception:
        core_validated = False
    def _flow_eligibility_from_payload(payload: dict) -> tuple[int, int, bool]:
        docker_count = 0
        vuln_count = 0
        try:
            preview = payload.get('full_preview') if isinstance(payload, dict) else None
            if isinstance(preview, dict):
                role_counts = preview.get('role_counts') if isinstance(preview.get('role_counts'), dict) else None
                if isinstance(role_counts, dict):
                    try:
                        docker_count = int(role_counts.get('Docker') or 0)
                    except Exception:
                        docker_count = 0
                hosts = preview.get('hosts') if isinstance(preview.get('hosts'), list) else []
                if isinstance(hosts, list):
                    for h in hosts:
                        if not isinstance(h, dict):
                            continue
                        role = str(h.get('role') or '').strip().lower()
                        if role == 'docker':
                            docker_count += 1
                        vulns = h.get('vulnerabilities') if isinstance(h.get('vulnerabilities'), list) else []
                        if vulns:
                            vuln_count += 1
                vuln_by_node = preview.get('vulnerabilities_by_node') if isinstance(preview.get('vulnerabilities_by_node'), dict) else None
                if isinstance(vuln_by_node, dict):
                    vuln_count = max(vuln_count, len([k for k, v in vuln_by_node.items() if v]))
        except Exception:
            docker_count = docker_count
            vuln_count = vuln_count
        flow_eligible = bool((docker_count or 0) > 0 or (vuln_count or 0) > 0)
        return docker_count, vuln_count, flow_eligible

    if xml_hint:
        try:
            xml_abs = os.path.abspath(xml_hint)
            if os.path.exists(xml_abs) and xml_abs.lower().endswith('.xml'):
                payload = _load_plan_preview_from_xml(xml_abs, scenario_label or scenario_norm)
                if payload and isinstance(payload, dict):
                    meta = payload.get('metadata') if isinstance(payload.get('metadata'), dict) else {}
                    docker_count, vuln_count, flow_eligible = _flow_eligibility_from_payload(payload)
                    scen_chk = str(meta.get('scenario') or '').strip()
                    if not scen_chk or _normalize_scenario_label(scen_chk) == scenario_norm:
                        return jsonify({
                            'ok': True,
                            'scenario': scenario_label or scenario_norm,
                            'preview_plan_path': xml_abs,
                            'preview_source': 'xml',
                            'metadata': meta or {},
                            'docker_count': docker_count,
                            'vuln_count': vuln_count,
                            'flow_eligible': flow_eligible,
                            'core_validated': core_validated,
                        })
        except Exception:
            pass
    try:
        xml_path = _latest_xml_path_for_scenario(scenario_norm)
        if xml_path:
            payload = _load_plan_preview_from_xml(xml_path, scenario_label or scenario_norm)
            if payload and isinstance(payload, dict):
                meta = payload.get('metadata') if isinstance(payload.get('metadata'), dict) else {}
                docker_count, vuln_count, flow_eligible = _flow_eligibility_from_payload(payload)
                return jsonify({
                    'ok': True,
                    'scenario': scenario_label or scenario_norm,
                    'preview_plan_path': xml_path,
                    'preview_source': 'xml',
                    'metadata': meta or {},
                    'docker_count': docker_count,
                    'vuln_count': vuln_count,
                    'flow_eligible': flow_eligible,
                    'core_validated': core_validated,
                })
    except Exception:
        pass
    return jsonify({'ok': False, 'error': 'No XML found for this scenario. Save XML with a PlanPreview first.'}), 404


@app.route('/api/flag-sequencing/test_core_connection', methods=['POST'])
def api_flow_test_core_connection():
    j = request.get_json(silent=True) or {}
    scenario_label = str(j.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400
    try:
        xml_hint = str(j.get('xml_path') or '').strip()
        xml_path_for_core = ''
        if xml_hint:
            try:
                xml_path_for_core = os.path.abspath(xml_hint)
            except Exception:
                xml_path_for_core = xml_hint
        if not xml_path_for_core:
            xml_path_for_core = _latest_xml_path_for_scenario(scenario_norm) or ''
        if not xml_path_for_core:
            return jsonify({'ok': False, 'error': 'No XML found for this scenario.'}), 404
        flow_core_cfg = _core_config_from_xml_path(xml_path_for_core, scenario_norm, include_password=True)
        if isinstance(flow_core_cfg, dict):
            flow_core_cfg = _apply_core_secret_to_config(flow_core_cfg, scenario_norm)
    except Exception:
        flow_core_cfg = None
    if not isinstance(flow_core_cfg, dict):
        return jsonify({'ok': False, 'error': 'No CoreConnection configured in XML for this scenario.'}), 404
    core_validated = False
    try:
        core_validated = _coerce_bool(flow_core_cfg.get('validated'))
        if not core_validated:
            status = str(flow_core_cfg.get('last_tested_status') or '').strip().lower()
            if status == 'success':
                core_validated = True
        if not core_validated:
            try:
                hv_map = _load_scenario_hitl_validation_from_disk()
                hv = None
                if isinstance(hv_map, dict):
                    hv = hv_map.get(scenario_norm)
                    if hv is None:
                        try:
                            key = _scenario_match_key(scenario_norm)
                        except Exception:
                            key = ''
                        if key:
                            for k, v in hv_map.items():
                                try:
                                    if _scenario_match_key(k) == key:
                                        hv = v
                                        break
                                except Exception:
                                    continue
                hv_core = hv.get('core') if isinstance(hv, dict) else None
                if isinstance(hv_core, dict):
                    if _coerce_bool(hv_core.get('validated')):
                        core_validated = True
                    else:
                        hv_status = str(hv_core.get('last_tested_status') or '').strip().lower()
                        if hv_status == 'success':
                            core_validated = True
                    if not core_validated and str(hv_core.get('core_secret_id') or '').strip():
                        core_validated = True
            except Exception:
                pass
        if not core_validated:
            try:
                secret_record = _select_latest_core_secret_record(scenario_norm or None)
            except Exception:
                secret_record = None
            if secret_record and str(secret_record.get('identifier') or '').strip():
                core_validated = True
    except Exception:
        core_validated = False
    if not core_validated:
        return jsonify({'ok': False, 'error': 'CORE VM is not validated. Validate in VM / Access and save XML, then retry.'}), 422
    try:
        if not _coerce_bool(flow_core_cfg.get('ssh_enabled')):
            return jsonify({
                'ok': False,
                'error': 'SSH is required for CORE VM access. Enable SSH in VM / Access and re-validate.',
                'detail': 'ssh_enabled=false',
            }), 422
        flow_core_cfg = _require_core_ssh_credentials(flow_core_cfg)
        # Mirror execute path: validate via SSH on the CORE VM, no local tunnel.
        _ensure_core_daemon_listening(flow_core_cfg, timeout=5.0)
    except Exception as exc:
        target = f"{flow_core_cfg.get('host')}:{flow_core_cfg.get('port')}"
        return jsonify({
            'ok': False,
            'error': f'CORE connection failed to {target}: {exc}',
            'detail': str(exc),
        }), 502
    return jsonify({
        'ok': True,
        'host': flow_core_cfg.get('host'),
        'port': flow_core_cfg.get('port'),
    })


@app.route('/api/flag-sequencing/revalidate_flow', methods=['POST'])
def api_flow_revalidate_flow():
    j = request.get_json(silent=True) or {}
    scenario_label = str(j.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    xml_hint = str(j.get('xml_path') or '').strip()
    xml_path = ''
    if xml_hint:
        try:
            xml_path = os.path.abspath(xml_hint)
        except Exception:
            xml_path = xml_hint
    if not xml_path:
        xml_path = _latest_xml_path_for_scenario(scenario_norm) or ''
    if not xml_path or not os.path.exists(xml_path):
        return jsonify({'ok': False, 'error': 'No XML found for this scenario.'}), 404

    flow_state = _flow_state_from_xml_path(xml_path, scenario_norm)
    if not isinstance(flow_state, dict):
        return jsonify({'ok': False, 'error': 'No FlowState found in XML. Save XML after resolve.'}), 400

    has_resolved = False
    try:
        assigns = flow_state.get('flag_assignments') if isinstance(flow_state.get('flag_assignments'), list) else []
        for entry in assigns or []:
            if not isinstance(entry, dict):
                continue
            ro = entry.get('resolved_outputs')
            if isinstance(ro, dict) and ro:
                has_resolved = True
                break
    except Exception:
        has_resolved = False
    if not has_resolved:
        return jsonify({'ok': False, 'error': 'No resolved outputs saved in XML. Run Generate and Save XML first.'}), 400

    flat_expected: list[tuple[str, str]] = []
    try:
        assigns = flow_state.get('flag_assignments') if isinstance(flow_state.get('flag_assignments'), list) else []
    except Exception:
        assigns = []
    for entry in assigns or []:
        if not isinstance(entry, dict):
            continue
        node_key = str(entry.get('node_id') or entry.get('node_name') or '').strip() or 'node'
        artifacts_dir = str(entry.get('artifacts_dir') or '').strip()
        outputs_manifest = str(entry.get('outputs_manifest') or '').strip()
        run_dir = str(entry.get('run_dir') or '').strip()
        base_dir = run_dir or artifacts_dir or (os.path.dirname(outputs_manifest) if outputs_manifest else '')
        if artifacts_dir:
            flat_expected.append((node_key, artifacts_dir))
        if outputs_manifest:
            flat_expected.append((node_key, outputs_manifest))

        inject_detail = entry.get('inject_files_detail') if isinstance(entry.get('inject_files_detail'), list) else []
        for item in inject_detail or []:
            if not isinstance(item, dict):
                continue
            resolved = str(item.get('resolved') or '').strip()
            path = str(item.get('path') or '').strip()
            rel = ''
            if resolved and not resolved.startswith('/'):
                rel = resolved
            elif resolved.startswith('artifacts/'):
                rel = resolved
            elif path.startswith('/tmp/'):
                rel = os.path.basename(path)
            if base_dir and rel:
                flat_expected.append((node_key, os.path.join(base_dir, rel)))

        try:
            resolved_outputs = entry.get('resolved_outputs') if isinstance(entry.get('resolved_outputs'), dict) else {}
        except Exception:
            resolved_outputs = {}
        try:
            ro_detail = entry.get('resolved_outputs_detail') if isinstance(entry.get('resolved_outputs_detail'), list) else []
            for item in ro_detail:
                if not isinstance(item, dict):
                    continue
                field = str(item.get('field') or '').strip()
                if field != 'File(path)':
                    continue
                val = str(item.get('resolved') or '').strip()
                if val:
                    resolved_outputs.setdefault('File(path)', val)
        except Exception:
            pass
        try:
            file_val = str(resolved_outputs.get('File(path)') or '').strip()
            if file_val:
                if file_val.startswith('/'):
                    flat_expected.append((node_key, file_val))
                elif base_dir:
                    flat_expected.append((node_key, os.path.join(base_dir, file_val)))
                elif file_val.startswith('artifacts/'):
                    # Fallback: no base_dir but we have a relative artifacts path.
                    # Use default CORE VM path pattern for flow generator outputs.
                    fallback_base = f'/tmp/vulns/flag_generators_runs/flow-{scenario_norm}'
                    flat_expected.append((node_key, os.path.join(fallback_base, file_val)))
        except Exception:
            pass

    if not flat_expected:
        return jsonify({'ok': False, 'error': 'No FlowState artifacts to validate. Run Generate and Save XML first.'}), 400

    flow_core_cfg = _core_config_from_xml_path(xml_path, scenario_norm, include_password=True)
    if isinstance(flow_core_cfg, dict):
        flow_core_cfg = _apply_core_secret_to_config(flow_core_cfg, scenario_norm)
    if not isinstance(flow_core_cfg, dict):
        return jsonify({'ok': False, 'error': 'No CoreConnection configured in XML for this scenario.'}), 404

    try:
        flow_core_cfg = _require_core_ssh_credentials(flow_core_cfg)
    except Exception as exc:
        return jsonify({'ok': False, 'error': f'Remote validation requires SSH credentials: {exc}'}), 400

    client = None
    missing: list[str] = []
    present: list[str] = []
    discovered_base_dir: str = ''
    try:
        client = _open_ssh_client(flow_core_cfg)

        # If we have paths with the fallback pattern, try to discover the actual latest run dir.
        needs_discovery = any(f'/tmp/vulns/flag_generators_runs/flow-{scenario_norm}' in p for _, p in flat_expected)
        if needs_discovery:
            try:
                # Find the latest flow run directory for this scenario.
                discover_cmd = f"ls -1td /tmp/vulns/flag_generators_runs/flow-{shlex.quote(scenario_norm)}-* 2>/dev/null | head -1"
                d_code, d_out, _ = _exec_ssh_command(client, f"bash -lc {shlex.quote(discover_cmd)}", timeout=10.0)
                if d_code == 0 and d_out.strip():
                    discovered_base_dir = d_out.strip().split('\n')[0].strip()
                    # Also check flag_node_generators_runs if empty
                if not discovered_base_dir:
                    discover_cmd2 = f"ls -1td /tmp/vulns/flag_node_generators_runs/flow-{shlex.quote(scenario_norm)}-* 2>/dev/null | head -1"
                    d_code2, d_out2, _ = _exec_ssh_command(client, f"bash -lc {shlex.quote(discover_cmd2)}", timeout=10.0)
                    if d_code2 == 0 and d_out2.strip():
                        discovered_base_dir = d_out2.strip().split('\n')[0].strip()
            except Exception:
                discovered_base_dir = ''

            # Replace fallback patterns with discovered actual path.
            if discovered_base_dir:
                fallback_pattern = f'/tmp/vulns/flag_generators_runs/flow-{scenario_norm}'
                flat_expected = [
                    (node_key, path.replace(fallback_pattern, discovered_base_dir))
                    for node_key, path in flat_expected
                ]

        checks: list[str] = []
        for node_key, path in flat_expected:
            q = shlex.quote(path)
            tag = f"{node_key}::{path}"
            checks.append(f"if [ -e {q} ]; then echo ok::{tag}; else echo missing::{tag}; fi")
        cmd = ' ; '.join(checks) if checks else 'true'
        code, out, err = _exec_ssh_command(client, f"bash -lc {shlex.quote(cmd)}", timeout=30.0)
        if code != 0 and not out:
            return jsonify({'ok': False, 'error': (err or 'Remote validation failed.')}), 500
        for raw in (out or '').splitlines():
            line = raw.strip()
            if line.startswith('ok::'):
                present.append(line[len('ok::'):])
            elif line.startswith('missing::'):
                missing.append(line[len('missing::'):])
    except Exception as exc:
        return jsonify({'ok': False, 'error': f'Remote validation failed: {exc}'}), 500
    finally:
        try:
            if client:
                client.close()
        except Exception:
            pass

    missing_sorted = sorted(set(missing))
    present_sorted = sorted(set(present))
    return jsonify({
        'ok': True,
        'missing': missing_sorted,
        'present': present_sorted,
        'expected': [f"{n}::{p}" for n, p in flat_expected],
    })


def _flow_read_flag_value_from_artifacts_dir(artifacts_dir: str) -> str:
    """Read a realized flag value from a generator artifacts directory.

    Only reads from directories under /tmp/vulns to avoid arbitrary file access.
    Returns empty string when not found.
    """
    try:
        d = str(artifacts_dir or '').strip()
        if not d:
            return ''
        base_dir = os.path.abspath(os.path.join('/tmp', 'vulns'))
        dd = os.path.abspath(d)
        if os.path.commonpath([dd, base_dir]) != base_dir:
            return ''
        if not os.path.isdir(dd):
            return ''
        flag_txt = os.path.join(dd, 'flag.txt')
        if os.path.isfile(flag_txt):
            with open(flag_txt, 'r', encoding='utf-8', errors='ignore') as f:
                val = (f.read() or '').strip()
                return val[:4096] if val else ''
        outs_path = os.path.join(dd, 'outputs.json')
        if os.path.isfile(outs_path):
            try:
                with open(outs_path, 'r', encoding='utf-8') as f:
                    m = json.load(f) or {}
                outs = m.get('outputs') if isinstance(m, dict) else None
                if isinstance(outs, dict):
                    v = outs.get('Flag(flag_id)') or outs.get('flag')
                    if isinstance(v, str) and v.strip():
                        vv = v.strip()
                        return vv[:4096]
            except Exception:
                return ''
    except Exception:
        return ''
    return ''


def _flow_read_outputs_map_from_artifacts_dir(artifacts_dir: str) -> dict[str, Any]:
    """Read realized output values from a generator artifacts directory.

    Only reads from directories under /tmp/vulns to avoid arbitrary file access.
    Returns an empty dict when not found.

    Expected shape (best-effort): outputs.json contains {"outputs": {k: v, ...}}.
    """
    try:
        d = str(artifacts_dir or '').strip()
        if not d:
            return {}
        base_dir = os.path.abspath(os.path.join('/tmp', 'vulns'))
        dd = os.path.abspath(d)
        if os.path.commonpath([dd, base_dir]) != base_dir:
            return {}
        if not os.path.isdir(dd):
            return {}
        outs_path = os.path.join(dd, 'outputs.json')
        if not os.path.isfile(outs_path):
            return {}
        with open(outs_path, 'r', encoding='utf-8') as f:
            m = json.load(f) or {}
        outs = m.get('outputs') if isinstance(m, dict) else None
        return outs if isinstance(outs, dict) else {}
    except Exception:
        return {}


def _first_valid_ipv4(value: Any) -> str:
    """Return a best-effort IPv4 address string (no CIDR), or '' if not found."""
    try:
        import ipaddress

        def _norm_one(v: Any) -> str:
            s = str(v or '').strip()
            if not s:
                return ''
            if '/' in s:
                s = s.split('/', 1)[0].strip()
            try:
                ip = ipaddress.ip_address(s)
            except Exception:
                return ''
            return s if ip.version == 4 else ''

        if isinstance(value, list):
            for item in value:
                out = _norm_one(item)
                if out:
                    return out
            return ''
        return _norm_one(value)
    except Exception:
        return ''


def _preview_host_ip4_any(host: dict[str, Any] | None) -> str:
    """Best-effort: return primary IPv4 from a preview host payload."""
    if not isinstance(host, dict):
        return ''
    try:
        ip4 = host.get('ip4')
        if isinstance(ip4, str) and _first_valid_ipv4(ip4):
            return _first_valid_ipv4(ip4)
    except Exception:
        pass
    for key in ('ipv4', 'ip', 'ip_addr', 'address'):
        try:
            v = host.get(key)
        except Exception:
            v = None
        ip_str = _first_valid_ipv4(v)
        if ip_str:
            return ip_str
    try:
        for key in ('ips', 'addresses', 'ip4s', 'ipv4s'):
            v = host.get(key)
            ip_str = _first_valid_ipv4(v)
            if ip_str:
                return ip_str
    except Exception:
        pass
    try:
        ifaces = host.get('interfaces')
        if isinstance(ifaces, list):
            for iface in ifaces:
                if not isinstance(iface, dict):
                    continue
                for key in ('ip4', 'ipv4', 'ip', 'ip_addr', 'address'):
                    ip_str = _first_valid_ipv4(iface.get(key))
                    if ip_str:
                        return ip_str
    except Exception:
        pass
    return ''


@app.route('/api/flag-sequencing/flag_values_for_node')
def api_flow_flag_values_for_node():
    """Return realized flag value(s) for a sequenced node (runtime-only).

    Used by the Scenarios Preview tab graph popup to fetch flag values on-demand.
    """
    scenario_label = (request.args.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    node_id = str((request.args.get('node_id') or '').strip())
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400
    if not node_id:
        return jsonify({'ok': False, 'error': 'No node_id specified.'}), 400

    plan_path = None
    try:
        entry = _planner_get_plan(scenario_norm)
        if entry:
            plan_path = entry.get('plan_path') or plan_path
    except Exception:
        plan_path = plan_path
    if not plan_path:
        plan_path = _latest_preview_plan_for_scenario_norm_origin(scenario_norm, origin='planner')
    if not plan_path:
        plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm, prefer_flow=True)
    if not plan_path or not os.path.exists(plan_path):
        return jsonify({'ok': False, 'error': 'No preview plan found for this scenario.'}), 404

    try:
        payload = _load_preview_payload_from_path(plan_path, scenario_label or scenario_norm)
        if not isinstance(payload, dict):
            return jsonify({'ok': False, 'error': 'Preview plan not embedded in XML.'}), 404
        meta = payload.get('metadata') if isinstance(payload, dict) else None
        flow = (meta or {}).get('flow') if isinstance(meta, dict) else None
        fas = flow.get('flag_assignments') if isinstance(flow, dict) else None
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to load preview plan: {e}'}), 500

    if not isinstance(fas, list) or not fas:
        return jsonify({'ok': True, 'scenario': scenario_label or scenario_norm, 'node_id': node_id, 'flags': []})

    matches = [a for a in fas if isinstance(a, dict) and str(a.get('node_id') or '').strip() == node_id]
    out_flags: list[dict[str, Any]] = []
    for a in (matches or []):
        try:
            artifacts_dir = str(a.get('artifacts_dir') or a.get('run_dir') or '').strip()
            val = _flow_read_flag_value_from_artifacts_dir(artifacts_dir) if artifacts_dir else ''
            out_flags.append({
                'generator_id': str(a.get('id') or ''),
                'generator_name': str(a.get('name') or ''),
                'flag_value': val,
            })
        except Exception:
            out_flags.append({
                'generator_id': str(a.get('id') or ''),
                'generator_name': str(a.get('name') or ''),
                'flag_value': '',
            })

    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'node_id': node_id,
        'flags': out_flags,
    })


def _flow_compute_flag_assignments(
    preview: dict,
    chain_nodes: list[dict[str, Any]],
    scenario_label: str,
    *,
    initial_facts_override: dict[str, list[str]] | None = None,
    goal_facts_override: dict[str, list[str]] | None = None,
    seed_override: int | None = None,
    disallow_generator_reuse: bool = False,
) -> list[dict[str, Any]]:
    """Return a per-position list of flag assignments aligned to chain_ids.

    Each item includes node_id plus the fields used for metadata.flow_flag.
    Deterministic-ish: shuffles eligible flags based on preview.seed.
    """
    if not isinstance(preview, dict) or not isinstance(chain_nodes, list) or not chain_nodes:
        return []

    chain_ids: list[str] = [str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict) and str(n.get('id') or '').strip()]
    if not chain_ids:
        return []

    vuln_ids: set[str] = set()
    vuln_names_by_id: dict[str, list[str]] = {}
    try:
        hosts = preview.get('hosts') if isinstance(preview, dict) else None
        if isinstance(hosts, list):
            for h in hosts:
                if not isinstance(h, dict):
                    continue
                hid = str(h.get('node_id') or '').strip()
                if not hid:
                    continue
                vulns = h.get('vulnerabilities') if isinstance(h.get('vulnerabilities'), list) else []
                if vulns:
                    vuln_ids.add(hid)
                try:
                    names: list[str] = []
                    for v in (vulns or []):
                        if isinstance(v, str):
                            s = v.strip()
                            if s:
                                names.append(s)
                            continue
                        if isinstance(v, dict):
                            for key in ('name', 'title', 'id', 'vuln', 'cve', 'cve_id', 'slug'):
                                val = v.get(key)
                                if isinstance(val, str) and val.strip():
                                    names.append(val.strip())
                                    break
                    if names:
                        uniq = []
                        seen = set()
                        for n in names:
                            if n and n not in seen:
                                seen.add(n)
                                uniq.append(n)
                        vuln_names_by_id[hid] = uniq
                except Exception:
                    pass
    except Exception:
        vuln_ids = set()
    def _extract_vuln_names(obj: dict[str, Any]) -> list[str]:
        names: list[str] = []
        try:
            raw = obj.get('vulnerabilities')
            if isinstance(raw, list):
                for v in raw:
                    if isinstance(v, str):
                        s = v.strip()
                        if s:
                            names.append(s)
                        continue
                    if isinstance(v, dict):
                        for key in ('name', 'title', 'id', 'vuln', 'cve', 'cve_id', 'slug'):
                            val = v.get(key)
                            if isinstance(val, str) and val.strip():
                                names.append(val.strip())
                                break
        except Exception:
            pass
        return names
    try:
        for n in (chain_nodes or []):
            if not isinstance(n, dict):
                continue
            nid = str(n.get('id') or '').strip()
            if not nid:
                continue
            names = _extract_vuln_names(n)
            if names:
                vuln_ids.add(nid)
                if nid not in vuln_names_by_id:
                    uniq = []
                    seen = set()
                    for nm in names:
                        if nm and nm not in seen:
                            seen.add(nm)
                            uniq.append(nm)
                    vuln_names_by_id[nid] = uniq
    except Exception:
        pass

    # Use generator-catalog entries:
    # - flag-generators: artifact/flag generation
    # - flag-node-generators: node/docker-compose generation (Docker-role only)
    try:
        generators, _errors = _flag_generators_from_enabled_sources()
    except Exception:
        generators = []

    try:
        node_generators, _errors2 = _flag_node_generators_from_enabled_sources()
    except Exception:
        node_generators = []

    eligible_gens: list[dict[str, Any]] = []
    for g in (generators or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if not gid:
            continue
        g2 = dict(g)
        g2['_flow_kind'] = 'flag-generator'
        g2['_flow_catalog'] = 'flag_generators'
        eligible_gens.append(g2)

    for g in (node_generators or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if not gid:
            continue
        g2 = dict(g)
        g2['_flow_kind'] = 'flag-node-generator'
        g2['_flow_catalog'] = 'flag_node_generators'
        eligible_gens.append(g2)

    if not eligible_gens:
        return []

    # Deterministic randomness for generator selection using flow_seed.
    try:
        import random as _random
        seed_val = _get_flow_seed(preview, seed_override)
        rnd = _random.Random(seed_val ^ 0xC0FFEE)
    except Exception:
        rnd = None


    # Map ids -> names/ips for THIS/NEXT substitution.
    id_to_name: dict[str, str] = {}
    id_to_ip: dict[str, str] = {}
    id_to_ip: dict[str, str] = {}
    for n in chain_nodes:
        try:
            nid = str(n.get('id') or '').strip()
            nm = str(n.get('name') or '').strip()
            if nid:
                id_to_name[nid] = nm or nid
                ip = ''
                try:
                    ip = _first_valid_ipv4(n.get('ip4') or n.get('ipv4') or n.get('ip') or '')
                except Exception:
                    ip = ''
                if ip:
                    id_to_ip[nid] = ip
        except Exception:
            pass

    id_to_node: dict[str, dict[str, Any]] = {}
    for n in chain_nodes:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if nid:
            id_to_node[nid] = n

    # Prefer v3 plugin contracts for chaining semantics (requires/produces).
    try:
        plugins_by_id = _flow_enabled_plugin_contracts_by_id()
    except Exception:
        plugins_by_id = {}

    def _render_hint(tpl: str, *, this_id: str, next_id: str) -> str:
        try:
            raw_tpl = str(tpl or '').strip() or 'Next: {{NEXT_NODE_NAME}}'
            text = raw_tpl
            next_id_val = str(next_id or '').strip()
            next_name_val = (id_to_name.get(next_id_val) or '').strip() if next_id_val else ''
            next_ip_val = str(id_to_ip.get(next_id_val) or '').strip() if next_id_val else ''
            if not next_id_val:
                return "You've completed this sequence of challenges!"
            if not next_name_val:
                next_name_val = next_id_val
            if ('{{NEXT_NODE_NAME}}' in raw_tpl) and ('{{NEXT_NODE_IP}}' not in raw_tpl) and next_ip_val:
                next_name_val = f"{next_name_val} ({next_ip_val})"
            repl = {
                '{{SCENARIO}}': str(scenario_label or ''),
                '{{THIS_NODE_ID}}': this_id,
                '{{THIS_NODE_NAME}}': id_to_name.get(this_id) or this_id,
                '{{NEXT_NODE_ID}}': next_id_val,
                '{{NEXT_NODE_NAME}}': next_name_val,
                '{{NEXT_NODE_IP}}': next_ip_val,
            }
            for k, v in repl.items():
                text = text.replace(k, str(v))
            return _flow_strip_ids_from_hint(text)
        except Exception:
            return ''

    out: list[dict[str, Any]] = []

    def _artifact_requires_of(gen: dict[str, Any]) -> set[str]:
        required: set[str] = set()
        try:
            pid = str(gen.get('id') or '').strip()
            plugin = plugins_by_id.get(pid)
            if isinstance(plugin, dict) and isinstance(plugin.get('requires'), list):
                for x in (plugin.get('requires') or []):
                    xx = str(x).strip()
                    if xx:
                        required.add(xx)
        except Exception:
            pass
        # Synthesized inputs (e.g., seed/node_name) are *fields*, not chain artifacts.
        # Filter them out even if a plugin contract mistakenly lists them in `requires`.
        try:
            required = {x for x in required if x not in _flow_synthesized_inputs()}
        except Exception:
            pass
        return required

    def _required_input_fields_of(gen: dict[str, Any]) -> set[str]:
        """Return required input field names for a generator.

        Optional inputs (required=False) are intentionally excluded.
        """
        required: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    name = str(inp.get('name') or '').strip()
                    if not name:
                        continue
                    if inp.get('required') is False:
                        continue
                    required.add(name)
        except Exception:
            pass
        return required

    def _all_input_fields_of(gen: dict[str, Any]) -> set[str]:
        fields: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    name = str(inp.get('name') or '').strip()
                    if name:
                        fields.add(name)
        except Exception:
            pass
        return fields

    def _required_inputs_of(gen: dict[str, Any]) -> set[str]:
        # Effective union: artifact dependencies + required runtime input fields.
        # If a plugin-level "requires" token is also present as an *optional* input
        # field (required=False), treat it as optional and exclude it.
        try:
            req_fields = _required_input_fields_of(gen)
            opt_fields = _all_input_fields_of(gen) - set(req_fields)
            req_artifacts = _artifact_requires_of(gen)
            if opt_fields:
                req_artifacts = {r for r in req_artifacts if r not in opt_fields}
            return req_artifacts | set(req_fields)
        except Exception:
            return set()

    def _artifact_produces_of(gen: dict[str, Any]) -> set[str]:
        provides: set[str] = set()
        try:
            pid = str(gen.get('id') or '').strip()
            plugin = plugins_by_id.get(pid)
            if isinstance(plugin, dict) and isinstance(plugin.get('produces'), list):
                for item in (plugin.get('produces') or []):
                    if not isinstance(item, dict):
                        continue
                    a = str(item.get('artifact') or '').strip()
                    if a:
                        provides.add(a)
        except Exception:
            pass
        return provides

    def _output_fields_of(gen: dict[str, Any]) -> set[str]:
        out_fields: set[str] = set()
        try:
            outputs = gen.get('outputs')
            if isinstance(outputs, list):
                for outp in outputs:
                    if not isinstance(outp, dict):
                        continue
                    nm = str(outp.get('name') or '').strip()
                    if nm:
                        out_fields.add(nm)
        except Exception:
            pass
        try:
            out_fields |= _artifact_produces_of(gen)
        except Exception:
            pass
        return out_fields

    def _provides_of(gen: dict[str, Any]) -> set[str]:
        provides: set[str] = set()

        # Plugin-level produces (artifact dependencies).
        try:
            provides |= _artifact_produces_of(gen)
        except Exception:
            pass

        try:
            prov = gen.get('provides')
            if isinstance(prov, list):
                for x in prov:
                    s = str(x).strip()
                    if s:
                        provides.add(s)
        except Exception:
            pass
        try:
            provides |= _output_fields_of(gen)
        except Exception:
            pass
        return provides

    # Inputs we can always synthesize deterministically at preview time.
    # Keep this in sync with _flow_default_generator_config() and common generator conventions.
    initial_facts: set[str] = set(_flow_synthesized_inputs())
    try:
        init_override = _flow_normalize_fact_override(initial_facts_override)
        if init_override:
            initial_facts |= set(init_override.get('artifacts') or [])
            initial_facts |= set(init_override.get('fields') or [])
    except Exception:
        pass

    goal_facts: set[str] = set()
    try:
        goal_override = _flow_normalize_fact_override(goal_facts_override)
        if goal_override:
            goal_facts |= set(goal_override.get('artifacts') or [])
            goal_facts |= set(goal_override.get('fields') or [])
    except Exception:
        pass

    state_known: set[str] = set(initial_facts)
    deployed: set[str] = set()

    provides_cache: dict[str, set[str]] = {}
    requires_cache: dict[str, set[str]] = {}

    def _provides_cached(gen: dict[str, Any]) -> set[str]:
        try:
            gid = str(gen.get('id') or '').strip()
        except Exception:
            gid = ''
        if gid and gid in provides_cache:
            return provides_cache[gid]
        try:
            p = set(_provides_of(gen))
        except Exception:
            p = set()
        if gid:
            provides_cache[gid] = p
        return p

    def _requires_cached(gen: dict[str, Any]) -> set[str]:
        try:
            gid = str(gen.get('id') or '').strip()
        except Exception:
            gid = ''
        if gid and gid in requires_cache:
            return requires_cache[gid]
        try:
            r = set(_required_inputs_of(gen))
        except Exception:
            r = set()
        if gid:
            requires_cache[gid] = r
        return r

    pool_by_pos: list[list[dict[str, Any]]] = []
    for cid in chain_ids:
        node = id_to_node.get(str(cid)) or {}
        is_vuln_node = _flow_node_is_vuln(node) or (str(cid) in vuln_ids)
        is_docker_node = _flow_node_is_docker_role(node)
        def _eligible_for_node(g: dict[str, Any]) -> bool:
            k = str(g.get('_flow_kind') or '').strip() or 'flag-generator'
            if k == 'flag-node-generator':
                return bool(is_docker_node and (not is_vuln_node))
            return bool(is_vuln_node)
        pool_by_pos.append([g for g in eligible_gens if _eligible_for_node(g)])

    remaining_union_by_idx: list[set[str]] = []
    try:
        running: set[str] = set()
        for i in range(len(pool_by_pos) - 1, -1, -1):
            pool = pool_by_pos[i] if i < len(pool_by_pos) else []
            for g in pool:
                running |= _provides_cached(g)
            remaining_union_by_idx.append(set(running))
        remaining_union_by_idx = list(reversed(remaining_union_by_idx))
    except Exception:
        remaining_union_by_idx = [set() for _ in range(len(pool_by_pos))]

    def _candidate_list(idx: int, state: set[str], deployed_ids: set[str]) -> list[dict[str, Any]]:
        pool = pool_by_pos[idx] if idx < len(pool_by_pos) else []
        if not pool:
            return []
        candidates = [
            g for g in pool
            if _requires_cached(g).issubset(state) and str(g.get('id') or '').strip() not in deployed_ids
        ]
        if (not candidates) and (not disallow_generator_reuse):
            candidates = [g for g in pool if _requires_cached(g).issubset(state)]
        if not candidates:
            if disallow_generator_reuse:
                return []
            # Best-effort fallback: allow assignment even if requires are not yet met.
            # Validation will surface missing dependencies, but we avoid empty assignments.
            candidates = list(pool)
        return candidates

    def _score_order(cands: list[dict[str, Any]], state: set[str]) -> list[dict[str, Any]]:
        scored: list[tuple[dict[str, Any], int]] = []
        for g in cands:
            provides = _provides_cached(g)
            new_facts = set(provides) - set(state)
            goal_new = (set(goal_facts) - set(state)) & set(provides)
            score = max(1, len(new_facts) + (3 * len(goal_new)))
            scored.append((g, score))
        if rnd is not None:
            try:
                rnd.shuffle(scored)
            except Exception:
                pass
        scored.sort(key=lambda x: x[1], reverse=True)
        return [g for g, _ in scored]

    chosen_gens: list[dict[str, Any]] | None = None
    if goal_facts:
        memo: set[tuple[int, frozenset[str], frozenset[str]]] = set()
        try:
            import time as _time
            _deadline = _time.monotonic() + 30.0
        except Exception:
            _deadline = None

        def _dfs(idx: int, state: set[str], deployed_ids: set[str]) -> list[dict[str, Any]] | None:
            if _deadline is not None:
                try:
                    if _time.monotonic() >= _deadline:
                        return None
                except Exception:
                    pass
            key = (idx, frozenset(state), frozenset(deployed_ids))
            if key in memo:
                return None
            if idx >= len(chain_ids):
                return [] if set(goal_facts).issubset(state) else None
            remaining_goal = set(goal_facts) - set(state)
            if remaining_goal:
                possible = set(state)
                try:
                    possible |= remaining_union_by_idx[idx]
                except Exception:
                    pass
                if not remaining_goal.issubset(possible):
                    memo.add(key)
                    return None
            candidates = _candidate_list(idx, state, deployed_ids)
            if not candidates:
                memo.add(key)
                return None
            ordered = _score_order(candidates, state)
            for g in ordered:
                provides = _provides_cached(g)
                gid = str(g.get('id') or '').strip()
                next_state = set(state) | set(provides)
                next_deployed = set(deployed_ids)
                if gid:
                    next_deployed.add(gid)
                tail = _dfs(idx + 1, next_state, next_deployed)
                if tail is not None:
                    return [g] + tail
            memo.add(key)
            return None

        try:
            chosen = _dfs(0, set(state_known), set(deployed))
            if chosen and len(chosen) == len(chain_ids):
                chosen_gens = chosen
        except Exception:
            chosen_gens = None

    for i, cid in enumerate(chain_ids):
        pool = pool_by_pos[i] if i < len(pool_by_pos) else []
        if not pool:
            return []

        if chosen_gens is not None:
            gen = chosen_gens[i]
        else:
            candidates = [
                g for g in pool
                if _requires_cached(g).issubset(state_known) and str(g.get('id') or '').strip() not in deployed
            ]
            if (not candidates) and (not disallow_generator_reuse):
                candidates = [g for g in pool if _requires_cached(g).issubset(state_known)]
            if not candidates:
                if disallow_generator_reuse:
                    return []
                # Best-effort fallback: allow assignment even if requires are not yet met.
                # Validation will surface missing dependencies, but we avoid empty assignments.
                candidates = list(pool)

            remaining_goal = set(goal_facts) - set(state_known)
            if remaining_goal:
                if i == (len(chain_ids) - 1):
                    goal_candidates = [g for g in candidates if remaining_goal & _provides_cached(g)]
                    if goal_candidates:
                        candidates = goal_candidates
                else:
                    try:
                        remaining_union: set[str] = set()
                        for j in range(i + 1, len(chain_ids)):
                            for g in (pool_by_pos[j] if j < len(pool_by_pos) else []):
                                remaining_union |= _provides_cached(g)
                        filtered: list[dict[str, Any]] = []
                        for g in candidates:
                            future_possible = set(state_known) | _provides_cached(g) | remaining_union
                            if remaining_goal.issubset(future_possible):
                                filtered.append(g)
                        if filtered:
                            candidates = filtered
                    except Exception:
                        pass

            scored: list[tuple[dict[str, Any], int]] = []
            for g in candidates:
                provides = _provides_cached(g)
                new_facts = set(provides) - set(state_known)
                goal_new = (set(goal_facts) - set(state_known)) & set(provides)
                score = max(1, len(new_facts) + (3 * len(goal_new)))
                scored.append((g, score))

            try:
                if rnd is not None:
                    total = sum(max(1, int(s)) for _, s in scored)
                    pick = rnd.random() * float(total)
                    acc = 0.0
                    gen = scored[0][0]
                    for g, s in scored:
                        acc += max(1, int(s))
                        if pick <= acc:
                            gen = g
                            break
                else:
                    gen = scored[0][0]
            except Exception:
                gen = scored[0][0]

        # Update outputs for the next hop.
        try:
            produced = _provides_cached(gen)
            state_known |= set(produced)
        except Exception:
            pass
        try:
            gid = str(gen.get('id') or '').strip()
            if gid:
                deployed.add(gid)
        except Exception:
            pass

        next_id = chain_ids[i + 1] if (i + 1) < len(chain_ids) else ''
        hint_templates = _flow_hint_templates_from_generator(gen)
        hint_tpl = hint_templates[0] if hint_templates else 'Next: {{NEXT_NODE_NAME}}'
        rendered_hints = [
            _render_hint(t, this_id=str(cid), next_id=str(next_id))
            for t in (hint_templates or [])
        ]

        requires_artifacts = sorted(list(_artifact_requires_of(gen)))
        produces_artifacts = sorted(list(_artifact_produces_of(gen)))
        input_fields_required = sorted(list(_required_input_fields_of(gen)))
        input_fields_all = sorted(list(_all_input_fields_of(gen)))
        input_fields_optional = sorted([x for x in input_fields_all if x and x not in set(input_fields_required)])
        output_fields = sorted(list(_output_fields_of(gen)))

        # Preview-level input overrides are optional; these are primarily relevant
        # for UI preview and are filtered to known inputs + synthesized fields.
        raw_overrides = preview.get('config_overrides')
        if not isinstance(raw_overrides, dict):
            raw_overrides = preview.get('inputs_overrides')
        if not isinstance(raw_overrides, dict):
            raw_overrides = preview.get('input_overrides')

        allowed_override_keys: set[str] = set(input_fields_all)
        try:
            allowed_override_keys |= set(_flow_synthesized_inputs())
        except Exception:
            pass

        config_overrides: dict[str, Any] = {}
        if isinstance(raw_overrides, dict):
            for k, v in raw_overrides.items():
                kk = str(k or '').strip()
                if not kk:
                    continue
                if allowed_override_keys and kk not in allowed_override_keys:
                    continue
                # Preserve explicit clears (None/empty) as an override.
                config_overrides[kk] = v

        # If an artifact "requires" token also appears as an optional input field,
        # treat it as optional (do not block chaining/validation on it).
        try:
            optional_field_set = set(input_fields_optional)
            requires_artifacts = sorted([x for x in requires_artifacts if x and x not in optional_field_set])
        except Exception:
            pass

        out.append({
            'node_id': str(cid),
            'id': str(gen.get('id') or ''),
            'name': str(gen.get('name') or ''),
            'description': str(gen.get('description') or ''),
            'type': str(gen.get('_flow_kind') or 'flag-generator'),
            'flag_generator': str(gen.get('_source_name') or '').strip() or 'unknown',
            'generator_catalog': str(gen.get('_flow_catalog') or 'flag_generators'),
            'language': str(gen.get('language') or ''),
            'vulnerabilities': list(vuln_names_by_id.get(str(cid), []) or []),
            'description_hints': list(gen.get('description_hints') or []) if isinstance(gen.get('description_hints'), list) else [],
            'inject_files': list(gen.get('inject_files') or []) if isinstance(gen.get('inject_files'), list) else [],
            # Effective union (used for chaining feasibility / ordering validation).
            'inputs': sorted(list(_required_inputs_of(gen))),
            'outputs': sorted(list(_provides_of(gen))),

            # Split-out views for UI transparency.
            'requires': requires_artifacts,
            'produces': produces_artifacts,
            'input_fields': input_fields_all,
            'input_fields_required': input_fields_required,
            'input_fields_optional': input_fields_optional,
            'output_fields': output_fields,
            'hint_template': hint_tpl,
            'hint_templates': hint_templates,
            'hint': rendered_hints[0] if rendered_hints else _render_hint(hint_tpl, this_id=str(cid), next_id=str(next_id)),
            'hints': rendered_hints,
            'next_node_id': str(next_id),
            'next_node_name': str(id_to_name.get(str(next_id)) or ''),
        })
    return out


def _flow_synthesized_inputs() -> set[str]:
    """Inputs we can always synthesize deterministically at preview time.

    Keep this in sync with _flow_compute_flag_assignments() and _flow_default_generator_config().
    """
    return {
        'seed',
        'seed_ts',
        'secret',
        'env_name',
        'challenge',
        'flag_prefix',
        'flag_seed',
        'username_prefix',
        'key_len',
        'node_name',
        # Optional per-node context injected by Flow (may not be declared by a generator).
        # These are safe to ignore in "dropped" input reporting.
        'Knowledge(ip)',
        'Hostname(host)',
        'host_ip',
        'target_ip',
        'ip4',
        'ipv4',
    }

def _flow_normalize_fact_override(raw: Any) -> dict[str, list[str]] | None:
    if not isinstance(raw, dict):
        return None
    arts = raw.get('artifacts') if isinstance(raw.get('artifacts'), list) else []
    fields = raw.get('fields') if isinstance(raw.get('fields'), list) else []
    def _is_flag_fact(name: str) -> bool:
        s = str(name or '').strip()
        if not s:
            return False
        if s.lower() == 'flag(flag_id)' or s == 'Flag':
            return True
        return s.startswith('Flag(') or s.startswith('flag(')

    out_artifacts = [
        str(x or '').strip()
        for x in (arts or [])
        if str(x or '').strip()
    ]
    out_fields = [
        str(x or '').strip()
        for x in (fields or [])
        if str(x or '').strip()
    ]
    if not out_artifacts and not out_fields:
        return None
    return {
        'artifacts': out_artifacts,
        'fields': out_fields,
    }


def _flow_generator_seed(*, base_seed: Any, scenario_norm: str, node_id: str, gen_id: str, occurrence_idx: int = 0) -> str:
    """Build the deterministic generator seed for Flow.

    NOTE: This seed intentionally includes an occurrence index so a Flow chain may repeat the
    same (node_id, generator_id) without producing an *exact* duplicate configuration.
    """
    return _flow_generator_seed_impl(
        base_seed=base_seed,
        scenario_norm=scenario_norm,
        node_id=node_id,
        gen_id=gen_id,
        occurrence_idx=occurrence_idx,
    )



def _flow_strip_runtime_sensitive_fields(flag_assignments: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """Return a copy of assignments safe to persist in preview-plan metadata.

    NOTE: As of the "flags are required outputs" contract, realized flag strings are part
    of the sequencing contract and are allowed to persist so the UI can display them.
    """
    if not isinstance(flag_assignments, list):
        return flag_assignments
    out: list[dict[str, Any]] = []
    for a in (flag_assignments or []):
        if not isinstance(a, dict):
            continue
        a2 = dict(a)
        a2.pop('runtime_flags', None)
        a2.pop('runtime_outputs', None)
        # Keep resolved inputs/outputs so the Flow UI can restore values after refresh.
        # Effective generator config may include secrets and should not be persisted.
        a2.pop('config', None)
        out.append(a2)
    return out


def _flow_enrich_saved_flag_assignments(
    flag_assignments: list[dict[str, Any]],
    chain_nodes: list[dict[str, Any]],
    *,
    scenario_label: str,
) -> list[dict[str, Any]]:
    """Best-effort enrichment for persisted Flow assignments.

    Older preview plans may persist `flag_assignments` that predate newer UI fields
    (e.g., `description_hints`) or contain hints that still include ids.

    We keep the chosen generator per node, but refresh:
    - `description_hints` from current enabled catalogs
    - `hint_templates`/`hint_template` from current enabled catalogs
    - rendered `hints`/`hint` for the current chain order
    - `next_node_id`/`next_node_name`
    """
    if not isinstance(flag_assignments, list) or not isinstance(chain_nodes, list):
        return flag_assignments

    # Map ids -> names for THIS/NEXT substitution.
    id_to_name: dict[str, str] = {}
    id_to_ip: dict[str, str] = {}
    chain_ids: list[str] = []
    vuln_names_by_id: dict[str, list[str]] = {}
    resolved_ip_by_id: dict[str, str] = {}
    try:
        for fa in (flag_assignments or []):
            if not isinstance(fa, dict):
                continue
            nid = str(fa.get('node_id') or '').strip()
            if not nid:
                continue
            ri = fa.get('resolved_inputs') if isinstance(fa.get('resolved_inputs'), dict) else None
            if not isinstance(ri, dict):
                continue
            for key in ('Knowledge(ip)', 'target_ip', 'host_ip', 'ip4', 'ipv4', 'ip'):
                val = ri.get(key)
                if val is None:
                    continue
                ip_val = _first_valid_ipv4(val)
                if ip_val:
                    resolved_ip_by_id[nid] = ip_val
                    break
    except Exception:
        resolved_ip_by_id = {}
    for n in (chain_nodes or []):
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if not nid:
            continue
        chain_ids.append(nid)
        nm = str(n.get('name') or '').strip()
        id_to_name[nid] = nm or nid
        try:
            ip_val = resolved_ip_by_id.get(nid) or _first_valid_ipv4(n.get('ip4') or n.get('ipv4') or n.get('ip') or '')
            if ip_val:
                id_to_ip[nid] = ip_val
        except Exception:
            pass
        try:
            raw = n.get('vulnerabilities')
            names: list[str] = []
            if isinstance(raw, list):
                for v in raw:
                    if isinstance(v, str):
                        s = v.strip()
                        if s:
                            names.append(s)
                        continue
                    if isinstance(v, dict):
                        for key in ('name', 'title', 'id', 'vuln', 'cve', 'cve_id', 'slug'):
                            val = v.get(key)
                            if isinstance(val, str) and val.strip():
                                names.append(val.strip())
                                break
            if names:
                uniq = []
                seen = set()
                for nm in names:
                    if nm and nm not in seen:
                        seen.add(nm)
                        uniq.append(nm)
                vuln_names_by_id[nid] = uniq
        except Exception:
            pass

    # Build a by-id view of currently enabled generators.
    by_id: dict[str, dict[str, Any]] = {}
    try:
        gens, _ = _flag_generators_from_enabled_sources()
        for g in (gens or []):
            if not isinstance(g, dict):
                continue
            gid = str(g.get('id') or '').strip()
            if gid and gid not in by_id:
                by_id[gid] = g
    except Exception:
        pass
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
        for g in (node_gens or []):
            if not isinstance(g, dict):
                continue
            gid = str(g.get('id') or '').strip()
            if gid and gid not in by_id:
                by_id[gid] = g
    except Exception:
        pass

    out: list[dict[str, Any]] = []
    for i, a in enumerate(flag_assignments or []):
        if not isinstance(a, dict):
            continue
        a2 = dict(a)
        this_id = str(a2.get('node_id') or '').strip() or (chain_ids[i] if i < len(chain_ids) else '')
        next_id = chain_ids[i + 1] if (i + 1) < len(chain_ids) else ''
        a2['node_id'] = this_id
        a2['next_node_id'] = str(next_id)
        a2['next_node_name'] = str(id_to_name.get(str(next_id)) or '')
        try:
            existing_vulns = a2.get('vulnerabilities')
            has_existing = isinstance(existing_vulns, list) and any(str(x or '').strip() for x in existing_vulns)
        except Exception:
            has_existing = False
        if not has_existing:
            try:
                a2['vulnerabilities'] = list(vuln_names_by_id.get(str(this_id), []) or [])
            except Exception:
                pass

        gen_id = str(a2.get('id') or '').strip()
        gen_def = by_id.get(gen_id) if gen_id else None

        # Backfill source label for older persisted assignments.
        # Historically this was often the generic string "manifest"; now we prefer
        # the containing pack/bundle label (or "repo") from the current catalog.
        try:
            existing_src = str(a2.get('flag_generator') or '').strip()
        except Exception:
            existing_src = ''
        try:
            catalog_src = str((gen_def or {}).get('_source_name') or '').strip() if isinstance(gen_def, dict) else ''
        except Exception:
            catalog_src = ''
        if catalog_src and (not existing_src or existing_src.lower() == 'manifest'):
            a2['flag_generator'] = catalog_src

        # Backfill inject_files for older persisted assignments.
        try:
            existing_injects = a2.get('inject_files')
            if not (isinstance(existing_injects, list) and any(str(x or '').strip() for x in existing_injects)):
                if isinstance(gen_def, dict):
                    inj = gen_def.get('inject_files')
                    if isinstance(inj, list):
                        a2['inject_files'] = [str(x or '').strip() for x in inj if str(x or '').strip()]
        except Exception:
            pass

        # Ensure generator description exists (if the catalog provides it).
        try:
            existing_desc = str(a2.get('description') or '').strip()
        except Exception:
            existing_desc = ''
        if (not existing_desc) and isinstance(gen_def, dict):
            try:
                a2['description'] = str(gen_def.get('description') or '')
            except Exception:
                pass

        # Ensure description hints exist (if the catalog provides them).
        try:
            existing = a2.get('description_hints')
            if not (isinstance(existing, list) and any(str(x or '').strip() for x in existing)):
                dh = (gen_def or {}).get('description_hints') if isinstance(gen_def, dict) else None
                if isinstance(dh, list):
                    a2['description_hints'] = [str(x or '').strip() for x in dh if str(x or '').strip()]
        except Exception:
            pass

        # Ensure input/output descriptors exist (if the catalog provides them).
        try:
            existing_in_defs = a2.get('input_defs')
            if not (isinstance(existing_in_defs, list) and existing_in_defs):
                if isinstance(gen_def, dict) and isinstance(gen_def.get('inputs'), list):
                    a2['input_defs'] = list(gen_def.get('inputs') or [])
        except Exception:
            pass
        try:
            existing_out_defs = a2.get('output_defs')
            if not (isinstance(existing_out_defs, list) and existing_out_defs):
                if isinstance(gen_def, dict) and isinstance(gen_def.get('outputs'), list):
                    a2['output_defs'] = list(gen_def.get('outputs') or [])
        except Exception:
            pass

        # If hint overrides exist, treat them as authoritative.
        try:
            if 'hint_overrides' in a2:
                if a2.get('hint_overrides') is None:
                    a2.pop('hint_overrides', None)
                elif isinstance(a2.get('hint_overrides'), list):
                    ovr = [str(x or '').strip() for x in (a2.get('hint_overrides') or [])]
                    ovr = [x for x in ovr if x]
                    a2['hint_overrides'] = ovr
                    a2['hints'] = ovr
                    a2['hint'] = ovr[0] if ovr else ''
        except Exception:
            pass

        # Normalize optional persisted flag override.
        try:
            if 'flag_override' in a2:
                if a2.get('flag_override') is None:
                    a2.pop('flag_override', None)
                elif isinstance(a2.get('flag_override'), str):
                    s = str(a2.get('flag_override') or '').strip()
                    if s:
                        a2['flag_override'] = s
                    else:
                        a2.pop('flag_override', None)
                else:
                    a2.pop('flag_override', None)
        except Exception:
            pass

        # Normalize optional persisted output overrides (dict of output_key -> value).
        try:
            if 'output_overrides' in a2:
                if a2.get('output_overrides') is None:
                    a2.pop('output_overrides', None)
                elif isinstance(a2.get('output_overrides'), dict):
                    cleaned: dict[str, Any] = {}
                    for k, v in (a2.get('output_overrides') or {}).items():
                        kk = str(k or '').strip()
                        if not kk:
                            continue
                        cleaned[kk] = v
                    if cleaned:
                        a2['output_overrides'] = cleaned
                    else:
                        a2.pop('output_overrides', None)
                else:
                    a2.pop('output_overrides', None)
        except Exception:
            pass

        # Normalize optional persisted inject-files override (list of paths).
        try:
            if 'inject_files_override' in a2:
                if a2.get('inject_files_override') is None:
                    a2.pop('inject_files_override', None)
                elif isinstance(a2.get('inject_files_override'), list):
                    cleaned = [str(x or '').strip() for x in (a2.get('inject_files_override') or [])]
                    cleaned = [x for x in cleaned if x]
                    a2['inject_files_override'] = cleaned
                    # Mirror into inject_files so the UI/runner sees the effective list.
                    a2['inject_files'] = cleaned
                else:
                    a2.pop('inject_files_override', None)
        except Exception:
            pass

        # Preserve persisted hint text when present (Flow persistence contract), but
        # strip ids if they exist. Only regenerate hints from catalogs when missing.
        has_hints = False
        try:
            if isinstance(a2.get('hints'), list) and any(str(x or '').strip() for x in (a2.get('hints') or [])):
                has_hints = True
            elif str(a2.get('hint') or '').strip():
                has_hints = True
        except Exception:
            has_hints = False

        def _apply_ip_to_hint_text(s: str) -> str:
            try:
                if not s:
                    return s
                out_s = str(s)
                for nid_key, nm_val in (id_to_name or {}).items():
                    if not nm_val:
                        continue
                    ip_val = id_to_ip.get(nid_key)
                    if not ip_val:
                        continue
                    if nm_val in out_s and (f"{nm_val} @" not in out_s):
                        out_s = out_s.replace(nm_val, f"{nm_val} @ {ip_val}")
                return out_s
            except Exception:
                return s

        if has_hints:
            try:
                if isinstance(a2.get('hints'), list) and a2.get('hints'):
                    raw = [str(x or '') for x in (a2.get('hints') or [])]
                    # Only mutate if the text actually contains ids/placeholders.
                    if any(('(id=' in s.lower()) or ('{{NEXT_NODE_ID}}' in s) or ('{{THIS_NODE_ID}}' in s) for s in raw):
                        a2['hints'] = [_flow_strip_ids_from_hint(s) for s in raw]
                        a2['hints'] = [x for x in (a2.get('hints') or []) if str(x).strip()]
                        if a2['hints']:
                            a2['hint'] = a2['hints'][0]
                    try:
                        a2['hints'] = [_apply_ip_to_hint_text(s) for s in (a2.get('hints') or [])]
                        if a2.get('hints'):
                            a2['hint'] = a2['hints'][0]
                    except Exception:
                        pass
                elif a2.get('hint'):
                    s = str(a2.get('hint') or '')
                    if ('(id=' in s.lower()) or ('{{NEXT_NODE_ID}}' in s) or ('{{THIS_NODE_ID}}' in s):
                        a2['hint'] = _flow_strip_ids_from_hint(s)
                    try:
                        a2['hint'] = _apply_ip_to_hint_text(str(a2.get('hint') or ''))
                    except Exception:
                        pass
            except Exception:
                pass
        elif isinstance(gen_def, dict):
            # No saved hints: fall back to catalog templates and render for current chain order.
            hint_templates: list[str] = []
            try:
                hint_templates = _flow_hint_templates_from_generator(gen_def)
            except Exception:
                hint_templates = []
            hint_tpl = hint_templates[0] if hint_templates else 'Next: {{NEXT_NODE_NAME}}'
            a2['hint_templates'] = hint_templates
            a2['hint_template'] = hint_tpl
            rendered = [
                _flow_render_hint_template(t, scenario_label=scenario_label, id_to_name=id_to_name, id_to_ip=id_to_ip, this_id=str(this_id), next_id=str(next_id))
                for t in (hint_templates or [hint_tpl])
            ]
            a2['hints'] = rendered
            a2['hint'] = rendered[0] if rendered else _flow_render_hint_template(hint_tpl, scenario_label=scenario_label, id_to_name=id_to_name, id_to_ip=id_to_ip, this_id=str(this_id), next_id=str(next_id))

        out.append(a2)
    return out


def _infer_flow_artifacts_dir_from_assignment(assignment: dict[str, Any], *, max_dirs: int = 60) -> dict[str, str]:
    """Infer artifacts/run/mount dirs from existing assignment data.

    Returns a dict with any of: artifacts_dir, mount_dir, run_dir, outputs_manifest.
    """
    if not isinstance(assignment, dict):
        return {}

    rel_candidates: list[str] = []
    for key in ('resolved_outputs',):
        ro = assignment.get(key)
        if isinstance(ro, dict):
            for v in ro.values():
                if isinstance(v, str) and v.strip():
                    rel_candidates.append(v.strip())
    detail_list = assignment.get('inject_files_detail') if isinstance(assignment.get('inject_files_detail'), list) else []
    for item in detail_list:
        if not isinstance(item, dict):
            continue
        raw = str(item.get('resolved') or '').strip()
        if raw:
            rel_candidates.append(raw)

    def _to_rel(raw: str) -> str:
        s = str(raw or '').replace('\\', '/').strip()
        if not s:
            return ''
        if '/artifacts/' in s:
            return s.split('/artifacts/', 1)[1].lstrip('/')
        if '/flow_artifacts/' in s:
            return s.split('/flow_artifacts/', 1)[1].lstrip('/')
        if s.startswith('artifacts/'):
            return s
        if s.startswith('/tmp/'):
            s = s[len('/tmp/'):]
        if s.startswith('/'):
            s = s[1:]
        return s

    rels: list[str] = []
    for raw in rel_candidates:
        rel = _to_rel(raw)
        if rel and rel not in rels:
            rels.append(rel)

    if not rels:
        return {}

    outputs_root = _outputs_dir()
    base_dirs = [
        os.path.join('/tmp/vulns', 'flag_generators_runs'),
        os.path.join('/tmp/vulns', 'flag_node_generators_runs'),
        os.path.join(outputs_root, 'flag_generators_runs'),
        os.path.join(outputs_root, 'flag_node_generators_runs'),
        os.path.join(outputs_root, 'vulns', 'flag_generators_runs'),
        os.path.join(outputs_root, 'vulns', 'flag_node_generators_runs'),
    ]
    run_dirs: list[str] = []
    for base in base_dirs:
        if not os.path.isdir(base):
            continue
        try:
            for entry in os.scandir(base):
                if entry.is_dir():
                    run_dirs.append(entry.path)
        except Exception:
            continue
    run_dirs.sort(key=lambda p: os.path.getmtime(p) if os.path.exists(p) else 0, reverse=True)
    if max_dirs and len(run_dirs) > max_dirs:
        run_dirs = run_dirs[:max_dirs]

    def _exists_in_run(run_dir: str, rel: str) -> bool:
        rel_clean = rel.lstrip('/')
        if rel_clean.startswith('artifacts/'):
            rel_clean = rel_clean[len('artifacts/'):]
        for candidate in (
            os.path.join(run_dir, rel),
            os.path.join(run_dir, 'artifacts', rel_clean),
            os.path.join(run_dir, 'injected', rel_clean),
        ):
            if os.path.exists(candidate):
                return True
        return False

    for rd in run_dirs:
        try:
            if any(_exists_in_run(rd, rel) for rel in rels):
                mount_dir = ''
                if os.path.isdir(os.path.join(rd, 'artifacts')):
                    mount_dir = os.path.join(rd, 'artifacts')
                elif os.path.isdir(os.path.join(rd, 'injected')):
                    mount_dir = os.path.join(rd, 'injected')
                else:
                    mount_dir = rd
                out = {
                    'artifacts_dir': rd,
                    'mount_dir': mount_dir,
                    'run_dir': rd,
                }
                manifest = os.path.join(rd, 'outputs.json')
                if os.path.exists(manifest):
                    out['outputs_manifest'] = manifest
                return out
        except Exception:
            continue
    return {}


def _webui_log_path() -> str:
    try:
        port = str(os.environ.get('CORETG_PORT') or '').strip() or '9090'
    except Exception:
        port = '9090'
    try:
        logs_dir = os.path.abspath(os.path.join(_outputs_dir(), 'logs'))
    except Exception:
        logs_dir = os.path.abspath(os.path.join(os.getcwd(), 'outputs', 'logs'))
    return os.path.join(logs_dir, f'webui-{port}.log')


@app.route('/api/webui/log_tail')
def api_webui_log_tail():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'ok': False, 'error': 'Admin privileges required'}), 403
    try:
        offset = int(request.args.get('offset') or 0)
    except Exception:
        offset = 0
    try:
        max_bytes = int(request.args.get('max_bytes') or 50000)
    except Exception:
        max_bytes = 50000
    max_bytes = max(1024, min(max_bytes, 200000))
    path = _webui_log_path()
    if not os.path.exists(path):
        return jsonify({'ok': False, 'error': 'Web UI log not found', 'path': path, 'offset': offset}), 404
    try:
        size = os.path.getsize(path)
    except Exception:
        size = 0
    if offset < 0 or offset > size:
        offset = max(size - max_bytes, 0)
    data = b''
    try:
        with open(path, 'rb') as f:
            f.seek(offset)
            data = f.read(max_bytes)
    except Exception as exc:
        return jsonify({'ok': False, 'error': f'Failed reading log: {exc}', 'path': path, 'offset': offset}), 500
    new_offset = offset + len(data)
    try:
        text = data.decode('utf-8', errors='ignore')
    except Exception:
        text = ''
    return jsonify({'ok': True, 'offset': new_offset, 'text': text})


@app.route('/api/webui/log_clear', methods=['POST'])
def api_webui_log_clear():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'ok': False, 'error': 'Admin privileges required'}), 403
    path = _webui_log_path()
    if not os.path.exists(path):
         return jsonify({'ok': True, 'users_cleared': False})
    try:
        with open(path, 'w') as f:
            f.truncate(0)
    except Exception as exc:
        return jsonify({'ok': False, 'error': f'Failed clearing log: {exc}'}), 500
    return jsonify({'ok': True})


def _enrich_flow_state_with_artifacts(flow_state: dict[str, Any]) -> dict[str, Any]:
    if not isinstance(flow_state, dict):
        return flow_state
    assigns = flow_state.get('flag_assignments') if isinstance(flow_state.get('flag_assignments'), list) else []
    if not assigns:
        return flow_state
    changed = False
    enriched: list[dict[str, Any]] = []
    for a in assigns:
        if not isinstance(a, dict):
            continue
        a2 = dict(a)
        try:
            injects = a2.get('inject_files') if isinstance(a2.get('inject_files'), list) else None
            if injects:
                fixed = []
                for raw in injects:
                    s = str(raw or '').strip()
                    if s.startswith('/tmp/tmp/'):
                        s = '/tmp/' + s[len('/tmp/tmp/') :]
                    fixed.append(s)
                a2['inject_files'] = fixed
                changed = True
        except Exception:
            pass
        try:
            detail = a2.get('inject_files_detail') if isinstance(a2.get('inject_files_detail'), list) else None
            if detail:
                fixed_detail = []
                for item in detail:
                    if not isinstance(item, dict):
                        fixed_detail.append(item)
                        continue
                    item2 = dict(item)
                    path = str(item2.get('path') or '').strip()
                    if path.startswith('/tmp/tmp/'):
                        path = '/tmp/' + path[len('/tmp/tmp/') :]
                        item2['path'] = path
                    fixed_detail.append(item2)
                a2['inject_files_detail'] = fixed_detail
                changed = True
        except Exception:
            pass
        try:
            ro = a2.get('resolved_outputs') if isinstance(a2.get('resolved_outputs'), dict) else None
            if not ro:
                detail = a2.get('resolved_outputs_detail') if isinstance(a2.get('resolved_outputs_detail'), list) else None
                if detail:
                    ro_map: dict[str, Any] = {}
                    for item in detail:
                        if not isinstance(item, dict):
                            continue
                        field = str(item.get('field') or '').strip()
                        if not field:
                            continue
                        val = item.get('resolved')
                        if val is not None and field not in ro_map:
                            ro_map[field] = val
                    if ro_map:
                        a2['resolved_outputs'] = ro_map
                        changed = True
        except Exception:
            pass
        try:
            ro = a2.get('resolved_outputs') if isinstance(a2.get('resolved_outputs'), dict) else None
            if not ro:
                manifest_path = str(a2.get('outputs_manifest') or '').strip()
                if manifest_path and os.path.exists(manifest_path):
                    with open(manifest_path, 'r', encoding='utf-8') as mf:
                        m = json.load(mf) or {}
                    outs = m.get('outputs') if isinstance(m, dict) else None
                    if isinstance(outs, dict) and outs:
                        a2['resolved_outputs'] = outs
                        changed = True
        except Exception:
            pass
        needs = not any(a2.get(k) for k in ('artifacts_dir', 'mount_dir', 'run_dir', 'outputs_manifest'))
        if needs:
            inferred = _infer_flow_artifacts_dir_from_assignment(a2)
            if inferred:
                a2.update(inferred)
                changed = True
        # Resolve relative paths in inject_files_detail and resolved_outputs to absolute paths.
        # This ensures docker-compose generation gets full paths when loading from saved XML.
        try:
            art_dir = str(a2.get('artifacts_dir') or a2.get('mount_dir') or a2.get('run_dir') or '').strip()
            if art_dir and os.path.isabs(art_dir):
                # Resolve relative paths in inject_files_detail.resolved
                detail = a2.get('inject_files_detail') if isinstance(a2.get('inject_files_detail'), list) else None
                if detail:
                    fixed_detail = []
                    for item in detail:
                        if not isinstance(item, dict):
                            fixed_detail.append(item)
                            continue
                        item2 = dict(item)
                        resolved = str(item2.get('resolved') or '').strip()
                        if resolved and not os.path.isabs(resolved):
                            # Resolve relative path to absolute using artifacts_dir
                            full_path = os.path.join(art_dir, resolved.lstrip('/'))
                            if not os.path.exists(full_path):
                                # Try without 'artifacts/' prefix if it exists
                                if resolved.startswith('artifacts/'):
                                    alt_path = os.path.join(art_dir, resolved[len('artifacts/'):])
                                    if os.path.exists(alt_path):
                                        full_path = alt_path
                            item2['resolved'] = full_path
                            changed = True
                        fixed_detail.append(item2)
                    a2['inject_files_detail'] = fixed_detail
                # Resolve relative paths in resolved_outputs values
                ro = a2.get('resolved_outputs') if isinstance(a2.get('resolved_outputs'), dict) else None
                if ro:
                    fixed_ro = {}
                    for k, v in ro.items():
                        if isinstance(v, str) and v.strip() and not os.path.isabs(v):
                            # Check if this looks like a file path (contains slash or file extension)
                            if '/' in v or ('.' in v and not v.startswith('{')):
                                full_path = os.path.join(art_dir, v.lstrip('/'))
                                if not os.path.exists(full_path) and v.startswith('artifacts/'):
                                    alt_path = os.path.join(art_dir, v[len('artifacts/'):])
                                    if os.path.exists(alt_path):
                                        full_path = alt_path
                                fixed_ro[k] = full_path
                                changed = True
                            else:
                                fixed_ro[k] = v
                        else:
                            fixed_ro[k] = v
                    a2['resolved_outputs'] = fixed_ro
        except Exception:
            pass
        enriched.append(a2)
    if changed:
        flow_state = dict(flow_state)
        flow_state['flag_assignments'] = enriched
    return flow_state



def _flow_parse_bool(value: Any, *, default: bool = False) -> bool:
    """Parse a truthy/falsey value commonly used in query args/JSON."""
    if value is None:
        return default
    try:
        if isinstance(value, bool):
            return bool(value)
        s = str(value).strip().lower()
    except Exception:
        return default
    if s in ('1', 'true', 'yes', 'y', 'on'):
        return True
    if s in ('0', 'false', 'no', 'n', 'off'):
        return False
    return default


def _flow_extract_node_authoring_doc(node: dict[str, Any]) -> dict[str, Any] | None:
    if not isinstance(node, dict):
        return None
    meta = node.get('metadata') if isinstance(node.get('metadata'), dict) else {}
    candidates: list[Any] = []
    for key in ('node_authoring', 'node_schema', 'node_spec', 'node_definition', 'authoring'):
        if key in node:
            candidates.append(node.get(key))
        if isinstance(meta, dict) and key in meta:
            candidates.append(meta.get(key))
    for raw in candidates:
        if raw is None:
            continue
        if isinstance(raw, dict):
            return raw
        if isinstance(raw, str):
            try:
                import yaml  # type: ignore
                doc = yaml.safe_load(raw)
            except Exception:
                doc = None
            if isinstance(doc, dict):
                return doc
    return None


def _flow_validate_chain_nodes_against_node_schema(
    chain_nodes: list[dict[str, Any]],
    *,
    scenario_label: str,
) -> list[str]:
    schema = _load_node_schema_validation()
    if not isinstance(schema, dict):
        return []
    try:
        from core_topo_gen.sequencer.schemas import validate_against_schema
    except Exception:
        return []
    errors: list[str] = []
    for node in (chain_nodes or []):
        if not isinstance(node, dict):
            continue
        doc = _flow_extract_node_authoring_doc(node)
        if not isinstance(doc, dict):
            continue
        ok, errs = validate_against_schema(doc, schema)
        if not ok:
            node_id = str(node.get('id') or node.get('node_id') or node.get('name') or '').strip()
            prefix = f"{node_id}: " if node_id else ''
            for err in errs:
                errors.append(f"{prefix}node authoring invalid: {err}")
    if errors:
        errors.insert(0, f"node authoring schema validation failed for scenario '{scenario_label}'")
    return errors


def _flow_validate_chain_order_by_requires_produces(
    chain_nodes: list[dict[str, Any]],
    flag_assignments: list[dict[str, Any]],
    *,
    scenario_label: str,
    plugins_by_id_override: dict[str, dict[str, Any]] | None = None,
) -> tuple[bool, list[str]]:
    """Validate that the given chain order is solvable by requires/produces.

    This is a strict, linear check in the *current* order:
    - Each step's effective requires must be satisfied by synthesized inputs or prior produces.
    - Effective requires/produces are derived from v3 plugin contracts, plus assignment-declared
      inputs/outputs (best-effort) to support coarse catalogs.

    Returns: (ok, errors)
    """
    errors: list[str] = []
    if not isinstance(chain_nodes, list) or not chain_nodes:
        return False, ['missing chain nodes']
    if not isinstance(flag_assignments, list) or not flag_assignments:
        try:
            chain_count = len(chain_nodes or []) if isinstance(chain_nodes, list) else 0
        except Exception:
            chain_count = 0
        try:
            vuln_nodes = len([n for n in (chain_nodes or []) if isinstance(n, dict) and _flow_node_is_vuln(n)])
        except Exception:
            vuln_nodes = 0
        try:
            docker_nodes = len([n for n in (chain_nodes or []) if isinstance(n, dict) and _flow_node_is_docker_role(n)])
        except Exception:
            docker_nodes = 0
        try:
            gens_enabled, _ = _flag_generators_from_enabled_sources()
        except Exception:
            gens_enabled = []
        try:
            node_gens_enabled, _ = _flag_node_generators_from_enabled_sources()
        except Exception:
            node_gens_enabled = []
        try:
            eligible_flag_gens = len([g for g in (gens_enabled or []) if isinstance(g, dict)])
        except Exception:
            eligible_flag_gens = 0
        try:
            eligible_node_gens = len([g for g in (node_gens_enabled or []) if isinstance(g, dict)])
        except Exception:
            eligible_node_gens = 0
        return False, [
            'missing flag assignments',
            f"assignments=0 chain_nodes={chain_count}",
            f"chain_vuln_nodes={vuln_nodes}",
            f"chain_docker_nodes={docker_nodes}",
            f"eligible_flag_generators={eligible_flag_gens}",
            f"eligible_flag_node_generators={eligible_node_gens}",
        ]

    chain_ids: list[str] = [
        str(n.get('id') or '').strip()
        for n in chain_nodes
        if isinstance(n, dict) and str(n.get('id') or '').strip()
    ]
    if not chain_ids:
        return False, ['empty chain']

    node_schema_errors = _flow_validate_chain_nodes_against_node_schema(
        chain_nodes,
        scenario_label=scenario_label,
    )
    if node_schema_errors:
        errors.extend(node_schema_errors)

    assign_by_node: dict[str, dict[str, Any]] = {}
    for a in flag_assignments:
        if not isinstance(a, dict):
            continue
        nid = str(a.get('node_id') or '').strip()
        if nid:
            assign_by_node[nid] = a

    # If assignments are positionally aligned but missing node_id fields, map by position.
    try:
        if any(cid not in assign_by_node for cid in chain_ids):
            if len(flag_assignments) == len(chain_ids):
                for i, cid in enumerate(chain_ids):
                    if cid in assign_by_node:
                        continue
                    a = flag_assignments[i]
                    if isinstance(a, dict):
                        assign_by_node[cid] = a
    except Exception:
        pass

    missing_assignments = [cid for cid in chain_ids if cid not in assign_by_node]
    if missing_assignments:
        return False, [
            'missing assignment for at least one chain node',
            f"missing_assignment_nodes={','.join(missing_assignments)}",
        ]

    if isinstance(plugins_by_id_override, dict) and plugins_by_id_override:
        plugins_by_id = plugins_by_id_override
    else:
        plugins_by_id = _flow_enabled_plugin_contracts_by_id()

    # Best-effort: load enabled generator definitions so we can infer optional
    # input fields even if the assignment payload doesn't include them.
    gen_defs_by_id: dict[str, dict[str, Any]] = {}
    try:
        gens, _ = _flag_generators_from_enabled_sources()
        for g in (gens or []):
            if not isinstance(g, dict):
                continue
            gid = str(g.get('id') or '').strip()
            if gid and gid not in gen_defs_by_id:
                gen_defs_by_id[gid] = g
    except Exception:
        pass
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
        for g in (node_gens or []):
            if not isinstance(g, dict):
                continue
            gid = str(g.get('id') or '').strip()
            if gid and gid not in gen_defs_by_id:
                gen_defs_by_id[gid] = g
    except Exception:
        pass

    available: set[str] = set(_flow_synthesized_inputs())

    for cid in chain_ids:
        a = assign_by_node.get(cid) or {}
        plugin_id = str(a.get('id') or '').strip()
        if not plugin_id:
            errors.append(f"{cid}: missing generator id")
            continue

        plugin = plugins_by_id.get(plugin_id)
        if not isinstance(plugin, dict):
            errors.append(f"{cid}: unknown plugin '{plugin_id}'")
            continue

        inferred_requires = {str(x).strip() for x in (a.get('inputs') or []) if str(x).strip()}
        inferred_produces = {str(x).strip() for x in (a.get('outputs') or []) if str(x).strip()}

        # Optional dependency tokens (do not block ordering). Prefer the assignment
        # field when present; fall back to the generator catalog schema.
        opt_set: set[str] = set()
        try:
            opt_fields = a.get('input_fields_optional') or []
            if isinstance(opt_fields, list):
                opt_set |= {str(x).strip() for x in opt_fields if str(x).strip()}
        except Exception:
            pass
        if not opt_set:
            try:
                gen_def = gen_defs_by_id.get(plugin_id)
                inputs = (gen_def or {}).get('inputs') if isinstance(gen_def, dict) else None
                if isinstance(inputs, list):
                    for inp in inputs:
                        if not isinstance(inp, dict):
                            continue
                        if inp.get('required') is False:
                            nm = str(inp.get('name') or '').strip()
                            if nm:
                                opt_set.add(nm)
            except Exception:
                pass

        base_requires = {str(x).strip() for x in (plugin.get('requires') or []) if str(x).strip()} if isinstance(plugin.get('requires'), list) else set()
        if opt_set:
            base_requires = {r for r in base_requires if r not in opt_set}
            inferred_requires = {r for r in inferred_requires if r not in opt_set}

        requires = base_requires | inferred_requires

        base_prod: set[str] = set()
        try:
            for p in (plugin.get('produces') or []):
                if not isinstance(p, dict):
                    continue
                art = str(p.get('artifact') or '').strip()
                if art:
                    base_prod.add(art)
        except Exception:
            base_prod = set()
        produces = base_prod | inferred_produces

        missing = sorted(list({r for r in requires if r not in available}))
        if missing:
            errors.append(f"{cid}: requires {missing} before they are produced")

        available |= produces

    if errors:
        # Include minimal context; callers may surface this in API error details.
        return False, errors
    return True, []


def _flow_enabled_plugin_contracts_by_id() -> dict[str, dict[str, Any]]:
    """Return generator plugin contracts indexed by plugin_id.

    Strict: contracts are derived from per-generator YAML manifests.
    """
    plugins_by_id: dict[str, dict[str, Any]] = {}

    # Important: Flow sequencing must NOT consider disabled generators.
    # We filter plugin contracts to installed+enabled generators only.
    try:
        gens, fg_plugins, _errs = _flag_generators_from_manifests(kind='flag-generator')
        allowed: set[str] = set()
        for g in (gens or []):
            if not isinstance(g, dict):
                continue
            if not _is_installed_generator_view(g):
                continue
            gid = str(g.get('id') or '').strip()
            if not gid:
                continue
            if _is_installed_generator_disabled(kind='flag-generator', generator_id=gid):
                continue
            allowed.add(gid)
        for k, v in (fg_plugins or {}).items():
            if isinstance(k, str) and k in allowed and isinstance(v, dict):
                plugins_by_id[k] = v
    except Exception:
        pass

    try:
        gens2, ng_plugins, _errs2 = _flag_generators_from_manifests(kind='flag-node-generator')
        allowed2: set[str] = set()
        for g in (gens2 or []):
            if not isinstance(g, dict):
                continue
            if not _is_installed_generator_view(g):
                continue
            gid = str(g.get('id') or '').strip()
            if not gid:
                continue
            if _is_installed_generator_disabled(kind='flag-node-generator', generator_id=gid):
                continue
            allowed2.add(gid)
        for k, v in (ng_plugins or {}).items():
            if isinstance(k, str) and k in allowed2 and k not in plugins_by_id and isinstance(v, dict):
                plugins_by_id[k] = v
    except Exception:
        pass

    return plugins_by_id


def _flow_reorder_chain_by_generator_dag(
    chain_nodes: list[dict[str, Any]],
    flag_assignments: list[dict[str, Any]],
    *,
    scenario_label: str,
    plugins_by_id_override: dict[str, dict[str, Any]] | None = None,
    return_debug: bool = False,
) -> tuple[list[dict[str, Any]], list[dict[str, Any]], dict[str, Any] | None]:
    """Best-effort: reorder the chain using generator artifact dependencies.

    This does not change generator selection; it only reorders the existing
    chain nodes + assignments so that inputs are satisfied by some producer.

    If the DAG cannot be built, returns inputs unchanged.
    """
    try:
        if not isinstance(chain_nodes, list) or not chain_nodes:
            return chain_nodes, flag_assignments, None
        if not isinstance(flag_assignments, list) or not flag_assignments:
            return chain_nodes, flag_assignments, None

        from core_topo_gen.sequencer.dag import build_dag
        from core_topo_gen.sequencer.chain import validate_chain_doc, validate_linear_chain

        node_by_id: dict[str, dict[str, Any]] = {}
        for n in chain_nodes:
            if not isinstance(n, dict):
                continue
            nid = str(n.get('id') or '').strip()
            if nid:
                node_by_id[nid] = n

        assign_by_node: dict[str, dict[str, Any]] = {}
        for a in flag_assignments:
            if not isinstance(a, dict):
                continue
            nid = str(a.get('node_id') or '').strip()
            if nid:
                assign_by_node[nid] = a

        # Only reorder when we have a 1:1 mapping.
        chain_ids = [str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict) and str(n.get('id') or '').strip()]
        if not chain_ids:
            return chain_nodes, flag_assignments, None
        if any(cid not in assign_by_node for cid in chain_ids):
            return chain_nodes, flag_assignments, None

        plugins_by_id: dict[str, dict[str, Any]] = {}
        if isinstance(plugins_by_id_override, dict) and plugins_by_id_override:
            plugins_by_id = plugins_by_id_override
        else:
            plugins_by_id = _flow_enabled_plugin_contracts_by_id()

        # Build a real sequencer chain instance (YAML-shape) from the current Flow chain.
        #
        # Important: some catalogs may omit requires/produces details or keep them coarse.
        # For Flow ordering we also incorporate the assignment-derived inputs/outputs as
        # additional requires/produces to avoid a no-op DAG.
        chain_doc: dict[str, Any] = {
            'ctf': {
                'id': str(scenario_label or '').strip() or 'flow',
                'version': 'flow',
                'difficulty': 'unknown',
                'description': 'Generated from Flow Sequencing assignments',
            },
            'challenges': [],
        }

        effective_plugins_by_id: dict[str, dict[str, Any]] = dict(plugins_by_id or {})
        challenges: list[dict[str, Any]] = []
        for cid in chain_ids:
            a = assign_by_node.get(cid) or {}
            plugin_id = str(a.get('id') or '').strip()
            if not plugin_id:
                return chain_nodes, flag_assignments, None
            plugin = plugins_by_id.get(plugin_id)
            if not isinstance(plugin, dict):
                # Without the plugin contract we can't build a correct artifact DAG.
                return chain_nodes, flag_assignments, None

            inferred_requires = [str(x).strip() for x in (a.get('inputs') or []) if str(x).strip()]
            inferred_produces = [str(x).strip() for x in (a.get('outputs') or []) if str(x).strip()]

            # Build an "effective" plugin contract for DAG purposes.
            try:
                eff = dict(plugin)
            except Exception:
                eff = plugin

            try:
                base_req = [str(x).strip() for x in (eff.get('requires') or []) if str(x).strip()] if isinstance(eff, dict) else []
            except Exception:
                base_req = []
            eff_requires = sorted(list({*base_req, *inferred_requires}))
            if isinstance(eff, dict):
                eff['requires'] = eff_requires

            base_prod_set: set[str] = set()
            try:
                for p in (eff.get('produces') or []) if isinstance(eff, dict) else []:
                    if not isinstance(p, dict):
                        continue
                    art = str(p.get('artifact') or '').strip()
                    if art:
                        base_prod_set.add(art)
            except Exception:
                base_prod_set = set()
            eff_prod_set = {*(base_prod_set), *(set(inferred_produces))}
            if isinstance(eff, dict):
                eff['produces'] = [{'artifact': x} for x in sorted(list(eff_prod_set))]

            effective_plugins_by_id[plugin_id] = eff

            kind = str(plugin.get('plugin_type') or a.get('type') or 'flag-generator').strip() or 'flag-generator'

            # Populate YAML-chain fields for validation/debug. Note: instance.requires is
            # intentionally empty; plugin.requires drives dependencies.
            produces_list: list[dict[str, str]] = []
            try:
                for p in (eff.get('produces') or []):
                    if not isinstance(p, dict):
                        continue
                    art = str(p.get('artifact') or '').strip()
                    if art:
                        produces_list.append({'name': art, 'artifact': art})
            except Exception:
                produces_list = []

            requires_list: list[dict[str, str]] = []
            try:
                for art in (eff.get('requires') or []):
                    a2 = str(art or '').strip()
                    if a2:
                        requires_list.append({'artifact': a2})
            except Exception:
                requires_list = []

            ch = {
                'challenge_id': cid,
                'kind': kind,
                'generator': {'plugin': plugin_id},
                'requires': requires_list,
                'produces': produces_list,
            }
            chain_doc['challenges'].append(ch)

            # Also build the challenge instances for build_dag (it reads generator.plugin).
            challenges.append(ch)

        chain_ok, chain_errors, chain_norm = validate_chain_doc(chain_doc)

        dag, errors = build_dag(challenges, plugins_by_id=effective_plugins_by_id, initial_artifacts=sorted(list(_flow_synthesized_inputs())))
        debug: dict[str, Any] | None = None
        if return_debug:
            try:
                debug = {
                    'ok': bool(dag is not None),
                    'errors': list(errors or []),
                    'initial_artifacts': sorted(list(_flow_synthesized_inputs())),
                    'chain_ok': bool(chain_ok),
                    'chain_errors': list(chain_errors or []),
                }
                if dag is not None:
                    debug['order'] = list(dag.order)
                    debug['edges'] = [
                        {'src': e.src, 'dst': e.dst, 'artifact': e.artifact}
                        for e in (dag.edges or ())
                    ]
            except Exception:
                debug = None
        if dag is None:
            return chain_nodes, flag_assignments, debug

        order = [str(x) for x in (dag.order or ()) if str(x).strip()]
        if not order:
            return chain_nodes, flag_assignments, debug
        if set(order) != set(chain_ids):
            return chain_nodes, flag_assignments, debug

        # Reorder nodes + assignments.
        new_chain_nodes = [node_by_id[cid] for cid in order if cid in node_by_id]
        new_assignments = [assign_by_node[cid] for cid in order if cid in assign_by_node]
        if len(new_chain_nodes) != len(chain_nodes) or len(new_assignments) != len(flag_assignments):
            return chain_nodes, flag_assignments, debug

        # Optional: validate that the new order is linearly solvable by the chain spec.
        # This uses only instance produces/requires; since we keep instance.requires empty,
        # this is expected to succeed (and is primarily a sanity check).
        if debug is not None:
            try:
                reordered_doc = dict(chain_norm or {})
                reordered_doc['challenges'] = [
                    next((c for c in (chain_norm.get('challenges') or []) if isinstance(c, dict) and str(c.get('challenge_id') or '') == cid), None)
                    for cid in order
                ]
                reordered_doc['challenges'] = [c for c in reordered_doc['challenges'] if isinstance(c, dict)]
                lin_ok, lin_errors = validate_linear_chain(reordered_doc, initial_artifacts=sorted(list(_flow_synthesized_inputs())))
                debug['linear_ok'] = bool(lin_ok)
                debug['linear_errors'] = list(lin_errors or [])
            except Exception:
                pass

        # Update NEXT placeholders in hints for the new order.
        id_to_name: dict[str, str] = {}
        for n in new_chain_nodes:
            try:
                nid = str(n.get('id') or '').strip()
                nm = str(n.get('name') or '').strip()
                if nid:
                    id_to_name[nid] = nm or nid
            except Exception:
                continue

        def _render_hint(tpl: str, *, this_id: str, next_id: str) -> str:
            try:
                text = str(tpl or '').strip() or 'Next: {{NEXT_NODE_NAME}}'
                next_id_val = str(next_id or '').strip()
                next_name_val = (id_to_name.get(next_id_val) or '').strip() if next_id_val else ''
                if not next_id_val:
                    return "You've completed this sequence of challenges!"
                if not next_name_val:
                    next_name_val = next_id_val
                repl = {
                    '{{SCENARIO}}': str(scenario_label or ''),
                    '{{THIS_NODE_ID}}': this_id,
                    '{{THIS_NODE_NAME}}': id_to_name.get(this_id) or this_id,
                    '{{NEXT_NODE_ID}}': next_id_val,
                    '{{NEXT_NODE_NAME}}': next_name_val,
                }
                for k, v in repl.items():
                    text = text.replace(k, str(v))
                return _flow_strip_ids_from_hint(text)
            except Exception:
                return ''

        for i, a in enumerate(new_assignments):
            try:
                this_id = str(a.get('node_id') or '').strip()
                next_id = order[i + 1] if (i + 1) < len(order) else ''
                hint_templates: list[str] = []
                try:
                    ht = a.get('hint_templates')
                    if isinstance(ht, list) and ht:
                        hint_templates = [str(x or '').strip() for x in ht if str(x or '').strip()]
                except Exception:
                    hint_templates = []
                tpl = str(a.get('hint_template') or '').strip() or (hint_templates[0] if hint_templates else 'Next: {{NEXT_NODE_NAME}}')
                a['next_node_id'] = str(next_id)
                a['next_node_name'] = str(id_to_name.get(str(next_id)) or '')
                rendered = [_render_hint(t, this_id=this_id, next_id=str(next_id)) for t in (hint_templates or [tpl])]
                a['hint'] = rendered[0] if rendered else _render_hint(tpl, this_id=this_id, next_id=str(next_id))
                a['hints'] = rendered
            except Exception:
                continue

        return new_chain_nodes, new_assignments, debug
    except Exception:
        return chain_nodes, flag_assignments, None


@app.route('/api/flag-sequencing/attackflow_preview')
def api_flow_attackflow_preview():
    scenario_label = (request.args.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    preset = str(request.args.get('preset') or '').strip()
    mode = str(request.args.get('mode') or '').strip().lower()
    xml_hint = (request.args.get('xml_path') or '').strip()
    length_raw = request.args.get('length')
    try:
        length = int(length_raw) if length_raw is not None else 5
    except Exception:
        length = 5
    preset_steps = _flow_preset_steps(preset)
    if preset_steps:
        length = len(preset_steps)
    length = max(1, min(length, 50))
    requested_length = length

    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    prefer_preview = str(request.args.get('prefer_preview') or '').strip().lower() in ('1', 'true', 'yes', 'y')
    force_preview = str(request.args.get('force_preview') or '').strip().lower() in ('1', 'true', 'yes', 'y')
    prefer_flow = str(request.args.get('prefer_flow') or '').strip().lower() in ('1', 'true', 'yes', 'y')
    best_effort_query = str(request.args.get('best_effort') or '').strip().lower() in ('1', 'true', 'yes', 'y')
    allow_node_duplicates = str(request.args.get('allow_node_duplicates') or request.args.get('allow_duplicates') or '').strip().lower() in ('1', 'true', 'yes', 'y')
    debug_mode = str(request.args.get('debug') or '').strip().lower() in ('1', 'true', 'yes', 'y')

    # When Generate forces preview selection, we still want to use the same *topology*
    # that refresh would choose (to avoid eligible counts flipping), but we should not
    # reuse a previously saved chain/assignments.
    ignore_saved_flow = bool(force_preview)

    selected_by = 'xml'

    preview_plan_path = (request.args.get('preview_plan') or '').strip() or None
    if preview_plan_path:
        try:
            preview_plan_path = os.path.abspath(preview_plan_path)
            if (not preview_plan_path.lower().endswith('.xml')) or (not os.path.exists(preview_plan_path)):
                preview_plan_path = None
        except Exception:
            preview_plan_path = None

    if not preview_plan_path and xml_hint:
        try:
            xml_abs = os.path.abspath(xml_hint)
            if os.path.exists(xml_abs) and xml_abs.lower().endswith('.xml'):
                payload_hint = _load_plan_preview_from_xml(xml_abs, scenario_norm)
                if isinstance(payload_hint, dict):
                    meta_hint = payload_hint.get('metadata') if isinstance(payload_hint.get('metadata'), dict) else {}
                    scen_hint = str(meta_hint.get('scenario') or '').strip()
                    if (not scen_hint) or _normalize_scenario_label(scen_hint) == scenario_norm:
                        preview_plan_path = xml_abs
                        selected_by = 'xml_hint'
        except Exception:
            pass

    if not preview_plan_path:
        preview_plan_path = _latest_xml_path_for_scenario(scenario_norm)
        if preview_plan_path:
            selected_by = 'latest_xml'

    if not preview_plan_path:
        return jsonify({'ok': False, 'error': 'No XML found for this scenario. Save XML with a PlanPreview first.'}), 404

    # Prefer the canonical plan; flow metadata is stored in metadata.flow.

    payload = {}
    preview = None
    try:
        # If a preview_plan was supplied but does not match the requested scenario,
        # ignore it and fall back to the latest plan for this scenario.
        attempts = 0
        while attempts < 2:
            attempts += 1
            payload = _load_preview_payload_from_path(preview_plan_path, scenario_norm)
            if not isinstance(payload, dict):
                return jsonify({'ok': False, 'error': 'Preview plan not embedded in XML.'}), 404
            meta_chk = payload.get('metadata') if isinstance(payload, dict) else None
            scen_chk = ''
            if isinstance(meta_chk, dict):
                scen_chk = str(meta_chk.get('scenario') or '').strip()
                flow_chk = meta_chk.get('flow') if isinstance(meta_chk.get('flow'), dict) else None
                if not scen_chk and isinstance(flow_chk, dict):
                    scen_chk = str(flow_chk.get('scenario') or '').strip()
            if scen_chk:
                scen_chk_norm = _normalize_scenario_label(scen_chk)
            else:
                scen_chk_norm = ''
            if scen_chk_norm and scen_chk_norm != scenario_norm:
                # Mismatch: pick the latest plan for this scenario.
                preview_plan_path = _latest_preview_plan_for_scenario_norm_origin(scenario_norm, origin='planner')
                if not preview_plan_path:
                    return jsonify({'ok': False, 'error': 'No preview plan found for this scenario. Generate a Full Preview first.'}), 404
                continue
            break
        preview = payload.get('full_preview') if isinstance(payload, dict) else None
        if not isinstance(preview, dict):
            return jsonify({'ok': False, 'error': 'Preview plan is missing full_preview.'}), 422
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to load preview plan: {e}'}), 500

    def _docker_count_from_preview(full_preview: dict) -> int:
        try:
            hosts = full_preview.get('hosts') or []
        except Exception:
            hosts = []
        if not isinstance(hosts, list):
            return 0
        total = 0
        for host in hosts:
            if not isinstance(host, dict):
                continue
            role = str(host.get('role') or '').strip().lower()
            if role == 'docker':
                total += 1
        return total

    def _docker_count_from_editor_snapshot(snapshot: dict, scen_norm: str) -> int:
        try:
            scenarios = snapshot.get('scenarios') or []
        except Exception:
            scenarios = []
        if not isinstance(scenarios, list):
            return 0
        match = None
        for scen in scenarios:
            if not isinstance(scen, dict):
                continue
            nm = _normalize_scenario_label(scen.get('name') or '')
            if nm and nm == scen_norm:
                match = scen
                break
        if not isinstance(match, dict):
            return 0
        sec = (match.get('sections') or {}).get('Node Information')
        if not isinstance(sec, dict):
            return 0
        items = sec.get('items') or []
        if not isinstance(items, list):
            return 0
        total = 0
        for item in items:
            if not isinstance(item, dict):
                continue
            metric = str(item.get('v_metric') or 'Weight').strip()
            if metric != 'Count':
                continue
            sel = str(item.get('selected') or '').strip().lower()
            if sel != 'docker':
                continue
            try:
                total += max(0, int(item.get('v_count') or 0))
            except Exception:
                continue
        return total

    def _plan_epoch_seconds(plan_path: str, plan_payload: dict) -> float:
        try:
            meta = plan_payload.get('metadata') if isinstance(plan_payload, dict) else None
            if isinstance(meta, dict):
                ts = _parse_iso_ts(meta.get('created_at'))
                if ts > 0:
                    return ts
        except Exception:
            pass
        try:
            return float(os.path.getmtime(plan_path))
        except Exception:
            return 0.0

    def _editor_snapshot_epoch_seconds(owner: Optional[dict]) -> float:
        try:
            snap_path = _editor_state_snapshot_path(owner)
            if os.path.exists(snap_path):
                return float(os.path.getmtime(snap_path))
        except Exception:
            pass
        return 0.0

    # Planner owns preview plan generation; this endpoint only reads plans.

    # If we loaded a preview plan (topology) but the user has a saved Flow chain,
    # merge that Flow metadata into this payload so saved chain/assignments still apply.
    try:
        meta0 = payload.get('metadata') if isinstance(payload, dict) else None
        flow0 = (meta0 or {}).get('flow') if isinstance(meta0, dict) else None
        if (not ignore_saved_flow) and (not isinstance(flow0, dict)):
            _attach_latest_flow_into_plan_payload(payload, scenario=(scenario_label or scenario_norm))
    except Exception:
        pass

    nodes, _links, adj = _build_topology_graph_from_preview_plan(preview)
    stats = _flow_compose_docker_stats(nodes)

    # If the latest plan is flow-modified, prefer the saved chain order.
    chain_nodes: list[dict[str, Any]] = []
    used_saved_chain = False
    if (not ignore_saved_flow) and (not preset_steps):
        try:
            meta = payload.get('metadata') if isinstance(payload, dict) else None
            flow_meta = meta.get('flow') if isinstance(meta, dict) else None
            saved_chain = flow_meta.get('chain') if isinstance(flow_meta, dict) else None
            saved_ids: list[str] = []
            if isinstance(saved_chain, list) and saved_chain:
                for entry in saved_chain:
                    if not isinstance(entry, dict):
                        continue
                    cid = str(entry.get('id') or '').strip()
                    if cid:
                        saved_ids.append(cid)
            if saved_ids:
                if (not allow_node_duplicates) and (len(set(saved_ids)) != len(saved_ids)):
                    saved_ids = []
                id_map = {str(n.get('id') or '').strip(): n for n in (nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()}
                # Honor requested length (truncate) but do not try to auto-extend.
                desired = saved_ids[:length]
                chain_nodes = [id_map[cid] for cid in desired if cid in id_map]
                if chain_nodes:
                    # Drop saved chains that contain nodes that are neither docker-role nor vuln nodes.
                    try:
                        for n in chain_nodes:
                            if not isinstance(n, dict):
                                continue
                            t_raw = str(n.get('type') or '')
                            t = t_raw.strip().lower()
                            is_docker = ('docker' in t) or (t_raw.strip().upper() == 'DOCKER')
                            is_vuln = _flow_node_is_vuln(n)
                            if (not is_docker) and (not is_vuln):
                                chain_nodes = []
                                break
                    except Exception:
                        chain_nodes = []
                if chain_nodes:
                    used_saved_chain = True
        except Exception:
            chain_nodes = []

    if not chain_nodes:
        if preset_steps:
            chain_nodes = _pick_flag_chain_nodes_for_preset(nodes, adj, steps=preset_steps)
        else:
            if allow_node_duplicates:
                try:
                    seed_val = int((preview.get('seed') if isinstance(preview, dict) else None) or 0)
                except Exception:
                    seed_val = 0
                chain_nodes = _pick_flag_chain_nodes_allow_duplicates(nodes, adj, length=length, seed=seed_val)
            else:
                chain_nodes = _pick_flag_chain_nodes(nodes, adj, length=length)

    warning: str | None = None

    # If we loaded a saved chain that is shorter than the requested length (e.g. the UI
    # reset its length input to the default on refresh), treat the saved chain as
    # authoritative and respond with its effective length.
    if used_saved_chain:
        try:
            eff = len(chain_nodes)
            if eff > 0:
                length = eff
        except Exception:
            pass

    # Optional best-effort mode: if the user requests a longer chain than we can
    # build from eligible nodes, clamp to available rather than returning 422.
    if (not used_saved_chain) and (not preset_steps) and best_effort_query:
        try:
            available = len(chain_nodes)
        except Exception:
            available = 0
        if available > 0 and available < length:
            warning = f"Only {available} eligible nodes found; using chain length {available} instead of requested {length}."
            length = available

    # Inject preview host IPv4/interface details onto chain nodes for UI display.
    try:
        host_by_id: dict[str, dict[str, Any]] = {}
        hosts = preview.get('hosts') if isinstance(preview, dict) else None
        if isinstance(hosts, list):
            for h in hosts:
                if not isinstance(h, dict):
                    continue
                hid = str(h.get('node_id') or h.get('id') or '').strip()
                if hid:
                    host_by_id[hid] = h
        if host_by_id:
            for n in (chain_nodes or []):
                if not isinstance(n, dict):
                    continue
                nid = str(n.get('id') or '').strip()
                if not nid:
                    continue
                h = host_by_id.get(nid)
                if not isinstance(h, dict):
                    continue
                try:
                    ip_val = _preview_host_ip4_any(h)
                except Exception:
                    ip_val = ''
                if ip_val:
                    if not (n.get('ip4') or n.get('ipv4') or n.get('ip') or n.get('address')):
                        n['ip4'] = ip_val
                        n['ipv4'] = ip_val
                try:
                    ifaces = h.get('interfaces') if isinstance(h.get('interfaces'), list) else None
                except Exception:
                    ifaces = None
                if ifaces and not n.get('interfaces'):
                    n['interfaces'] = ifaces
    except Exception:
        pass

    # Build a stable node-id -> ip map for UI consumption.
    host_ip_map: dict[str, str] = {}
    try:
        for hid, h in (host_by_id or {}).items():
            ip_val = _preview_host_ip4_any(h)
            if ip_val:
                host_ip_map[str(hid)] = ip_val
    except Exception:
        host_ip_map = {}
    # Prefer persisted assignments when the plan comes from a prior Flow save.
    flag_assignments: list[dict[str, Any]] = []
    flow_state_from_xml: dict[str, Any] | None = None
    try:
        if not ignore_saved_flow:
            flow_state_from_xml = _flow_state_from_latest_xml(scenario_norm)
        if flow_state_from_xml and isinstance(flow_state_from_xml.get('chain_ids'), list):
            saved_ids = [str(x).strip() for x in (flow_state_from_xml.get('chain_ids') or []) if str(x).strip()]
            if saved_ids:
                id_map = {str(n.get('id') or '').strip(): n for n in (nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()}
                candidate_nodes = [id_map[cid] for cid in saved_ids if cid in id_map]
                if candidate_nodes:
                    try:
                        invalid = any(
                            (not _flow_node_is_docker_role(n)) and (not _flow_node_is_vuln(n))
                            for n in candidate_nodes
                            if isinstance(n, dict)
                        )
                    except Exception:
                        invalid = True
                    if not invalid:
                        chain_nodes = candidate_nodes
                        # Use saved assignments from XML (resolved values) if provided.
                        fas = flow_state_from_xml.get('flag_assignments') if isinstance(flow_state_from_xml, dict) else None
                        if isinstance(fas, list) and fas:
                            ordered: list[dict[str, Any]] = []
                            for i in range(len(chain_nodes)):
                                a = fas[i] if i < len(fas) else {}
                                if not isinstance(a, dict):
                                    ordered.append({})
                                    continue
                                a2 = dict(a)
                                try:
                                    a2['node_id'] = str((chain_nodes[i] or {}).get('id') or '').strip()
                                except Exception:
                                    pass
                                ordered.append(a2)
                            flag_assignments = ordered
                            try:
                                flag_assignments = _flow_enrich_saved_flag_assignments(
                                    flag_assignments,
                                    chain_nodes,
                                    scenario_label=(scenario_label or scenario_norm),
                                )
                            except Exception:
                                pass
    except Exception:
        flow_state_from_xml = None
    try:
        meta = payload.get('metadata') if isinstance(payload, dict) else None
        flow_meta = meta.get('flow') if isinstance(meta, dict) else None
        saved_assignments = flow_meta.get('flag_assignments') if isinstance(flow_meta, dict) else None
        if (not flag_assignments) and (not ignore_saved_flow) and isinstance(saved_assignments, list) and saved_assignments:
            # Saved assignments are persisted positionally, aligned to the saved chain.
            # Do not re-key by node_id; chains may intentionally contain duplicates.
            try:
                desired_len = len(chain_nodes or [])
            except Exception:
                desired_len = 0
            if desired_len and len(saved_assignments) >= desired_len:
                ordered: list[dict[str, Any]] = []
                for i in range(desired_len):
                    a = saved_assignments[i]
                    if not isinstance(a, dict):
                        ordered.append({})
                        continue
                    a2 = dict(a)
                    try:
                        a2['node_id'] = str((chain_nodes[i] or {}).get('id') or '').strip()
                    except Exception:
                        pass
                    ordered.append(a2)
                if all(isinstance(a, dict) and str(a.get('id') or '').strip() for a in ordered):
                    flag_assignments = ordered
                    try:
                        flag_assignments = _flow_enrich_saved_flag_assignments(
                            flag_assignments,
                            chain_nodes,
                            scenario_label=(scenario_label or scenario_norm),
                        )
                    except Exception:
                        pass
                    # Validate saved assignments against node eligibility (vuln vs docker).
                    # If incompatible, discard and recompute assignments.
                    try:
                        try:
                            gens_enabled, _ = _flag_generators_from_enabled_sources()
                        except Exception:
                            gens_enabled = []
                        try:
                            node_gens_enabled, _ = _flag_node_generators_from_enabled_sources()
                        except Exception:
                            node_gens_enabled = []
                        flag_gen_ids: set[str] = set()
                        node_gen_ids: set[str] = set()
                        for g in (gens_enabled or []):
                            if isinstance(g, dict):
                                gid = str(g.get('id') or '').strip()
                                if gid:
                                    flag_gen_ids.add(gid)
                        for g in (node_gens_enabled or []):
                            if isinstance(g, dict):
                                gid = str(g.get('id') or '').strip()
                                if gid:
                                    node_gen_ids.add(gid)

                        vuln_ids: set[str] = set()
                        try:
                            hosts = preview.get('hosts') if isinstance(preview, dict) else None
                            if isinstance(hosts, list):
                                for h in hosts:
                                    if not isinstance(h, dict):
                                        continue
                                    hid = str(h.get('node_id') or '').strip()
                                    if not hid:
                                        continue
                                    vulns = h.get('vulnerabilities') if isinstance(h.get('vulnerabilities'), list) else []
                                    if vulns:
                                        vuln_ids.add(hid)
                        except Exception:
                            vuln_ids = set()

                        def _infer_kind(a: dict[str, Any]) -> str:
                            try:
                                k = str(a.get('type') or '').strip()
                                if k:
                                    return k
                            except Exception:
                                pass
                            try:
                                cat = str(a.get('generator_catalog') or '').strip().lower()
                                if cat == 'flag_node_generators':
                                    return 'flag-node-generator'
                                if cat == 'flag_generators':
                                    return 'flag-generator'
                            except Exception:
                                pass
                            gid = str(a.get('id') or a.get('generator_id') or '').strip()
                            if gid:
                                if gid in node_gen_ids:
                                    return 'flag-node-generator'
                                if gid in flag_gen_ids:
                                    return 'flag-generator'
                            return ''

                        invalid = False
                        for i, a in enumerate(flag_assignments or []):
                            if i >= len(chain_nodes):
                                break
                            node = chain_nodes[i] if i < len(chain_nodes) else {}
                            if not isinstance(node, dict):
                                continue
                            nid = str(node.get('id') or '').strip()
                            is_vuln_node = bool(node.get('is_vuln')) or bool(node.get('vulnerabilities')) or (nid in vuln_ids)
                            is_docker_node = _flow_node_is_docker_role(node)
                            kind = _infer_kind(a)
                            if kind == 'flag-generator' and not is_vuln_node:
                                invalid = True
                                break
                            if kind == 'flag-node-generator' and not (is_docker_node and (not is_vuln_node)):
                                invalid = True
                                break
                        if invalid:
                            flag_assignments = []
                    except Exception:
                        pass
    except Exception:
        flag_assignments = []

    initial_facts_override: dict[str, list[str]] | None = None
    goal_facts_override: dict[str, list[str]] | None = None
    try:
        flow_for_facts = None
        if flow_state_from_xml and isinstance(flow_state_from_xml, dict):
            flow_for_facts = flow_state_from_xml
        if flow_for_facts is None:
            meta_for_facts = payload.get('metadata') if isinstance(payload, dict) else None
            flow_for_facts = meta_for_facts.get('flow') if isinstance(meta_for_facts, dict) else None
        if isinstance(flow_for_facts, dict):
            initial_facts_override = _flow_normalize_fact_override(flow_for_facts.get('initial_facts'))
            goal_facts_override = _flow_normalize_fact_override(flow_for_facts.get('goal_facts'))
    except Exception:
        initial_facts_override = None
        goal_facts_override = None

    if not flag_assignments:
        if preset_steps and not used_saved_chain:
            preset_assignments, preset_err = _flow_compute_flag_assignments_for_preset(preview, chain_nodes, scenario_label or scenario_norm, preset)
            if preset_err:
                return jsonify({'ok': False, 'error': f'Error: {preset_err}', 'stats': stats, 'preview_plan_path': preview_plan_path}), 422
            flag_assignments = preset_assignments
        else:
            flag_assignments = _flow_compute_flag_assignments(
                preview,
                chain_nodes,
                scenario_label or scenario_norm,
                initial_facts_override=initial_facts_override,
                goal_facts_override=goal_facts_override,
                disallow_generator_reuse=(not allow_node_duplicates),
            )
            if (not flag_assignments) and (not allow_node_duplicates):
                flag_assignments = _flow_compute_flag_assignments(
                    preview,
                    chain_nodes,
                    scenario_label or scenario_norm,
                    initial_facts_override=initial_facts_override,
                    goal_facts_override=goal_facts_override,
                    disallow_generator_reuse=False,
                )
                if flag_assignments:
                    try:
                        warning = warning or 'Not enough unique generators for this chain length; generator reuse was enabled.'
                    except Exception:
                        pass

    # Fallback: if no assignments could be computed, pick the first eligible
    # generator per node so the UI can proceed and provide a clearer error.
    if not flag_assignments and chain_nodes:
        try:
            gens_enabled, _ = _flag_generators_from_enabled_sources()
        except Exception:
            gens_enabled = []
        try:
            node_gens_enabled, _ = _flag_node_generators_from_enabled_sources()
        except Exception:
            node_gens_enabled = []

        try:
            gens_enabled = [g for g in (gens_enabled or []) if isinstance(g, dict) and str(g.get('id') or '').strip()]
            node_gens_enabled = [g for g in (node_gens_enabled or []) if isinstance(g, dict) and str(g.get('id') or '').strip()]
        except Exception:
            gens_enabled = []
            node_gens_enabled = []

        fallback: list[dict[str, Any]] = []
        if gens_enabled or node_gens_enabled:
            for n in (chain_nodes or []):
                if not isinstance(n, dict):
                    continue
                nid = str(n.get('id') or '').strip()
                if not nid:
                    continue
                is_vuln = _flow_node_is_vuln(n)
                is_docker = _flow_node_is_docker_role(n)
                if is_vuln and gens_enabled:
                    g = gens_enabled[0]
                    fallback.append({
                        'node_id': nid,
                        'id': str(g.get('id') or ''),
                        'name': str(g.get('name') or ''),
                        'type': 'flag-generator',
                        'flag_generator': str(g.get('_source_name') or g.get('source') or '').strip() or 'unknown',
                        'generator_catalog': 'flag_generators',
                    })
                elif (not is_vuln) and is_docker and node_gens_enabled:
                    g = node_gens_enabled[0]
                    fallback.append({
                        'node_id': nid,
                        'id': str(g.get('id') or ''),
                        'name': str(g.get('name') or ''),
                        'type': 'flag-node-generator',
                        'flag_generator': str(g.get('_source_name') or g.get('source') or '').strip() or 'unknown',
                        'generator_catalog': 'flag_node_generators',
                    })
        if fallback and len(fallback) == len(chain_nodes):
            flag_assignments = fallback

    # For auto-generated (non-preset) chains only, prefer a dependency-consistent ordering.
    # Presets force an intended generator order and should not be reordered.
    try:
        _ids = [str(n.get('id') or '').strip() for n in (chain_nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()]
        has_dupes = len(set(_ids)) != len(_ids)
    except Exception:
        has_dupes = False

    if (not used_saved_chain) and (not preset_steps) and (not has_dupes):
        debug_dag = str(request.args.get('debug_dag') or '').strip().lower() in ('1', 'true', 'yes', 'y')
        chain_nodes, flag_assignments, dag_debug = _flow_reorder_chain_by_generator_dag(
            chain_nodes,
            flag_assignments,
            scenario_label=(scenario_label or scenario_norm),
            return_debug=bool(debug_dag),
        )
    else:
        debug_dag = str(request.args.get('debug_dag') or '').strip().lower() in ('1', 'true', 'yes', 'y')
        dag_debug = None

    # Ensure assignments are aligned to the chain (node_id + length).
    try:
        if isinstance(flag_assignments, list) and isinstance(chain_nodes, list):
            desired_len = len(chain_nodes)
            if desired_len:
                if len(flag_assignments) != desired_len:
                    flag_assignments = list(flag_assignments[:desired_len])
                    while len(flag_assignments) < desired_len:
                        flag_assignments.append({})
                for i in range(desired_len):
                    a = flag_assignments[i]
                    if not isinstance(a, dict):
                        a = {}
                        flag_assignments[i] = a
                    try:
                        nid = str((chain_nodes[i] or {}).get('id') or '').strip()
                    except Exception:
                        nid = ''
                    if nid:
                        a.setdefault('node_id', nid)
    except Exception:
        pass

    # Ensure inject_files are present from generator definitions when missing.
    try:
        for fa in (flag_assignments or []):
            if not isinstance(fa, dict):
                continue
            existing = fa.get('inject_files') if isinstance(fa.get('inject_files'), list) else []
            if any(str(x or '').strip() for x in (existing or [])):
                continue
            gid = str(fa.get('id') or fa.get('generator_id') or '').strip()
            if not gid:
                continue
            gen_def = _gen_by_id.get(gid) if isinstance(_gen_by_id, dict) else None
            if not isinstance(gen_def, dict):
                continue
            inj = gen_def.get('inject_files')
            if isinstance(inj, list) and inj:
                fa['inject_files'] = [str(x or '').strip() for x in inj if str(x or '').strip()]
    except Exception:
        pass

    # Validate (non-blocking): if invalid, the UI should warn and execution should
    # proceed without flags.
    if not flag_assignments:
        flow_valid = False
        flow_errors = ['missing flag assignments']
        try:
            gens_enabled, _ = _flag_generators_from_enabled_sources()
        except Exception:
            gens_enabled = []
        try:
            node_gens_enabled, _ = _flag_node_generators_from_enabled_sources()
        except Exception:
            node_gens_enabled = []
        try:
            eligible_flag_gens = len([g for g in (gens_enabled or []) if isinstance(g, dict)])
        except Exception:
            eligible_flag_gens = 0
        try:
            eligible_node_gens = len([g for g in (node_gens_enabled or []) if isinstance(g, dict)])
        except Exception:
            eligible_node_gens = 0
        try:
            vuln_nodes = len([n for n in (chain_nodes or []) if isinstance(n, dict) and _flow_node_is_vuln(n)])
        except Exception:
            vuln_nodes = 0
        try:
            docker_nodes = len([n for n in (chain_nodes or []) if isinstance(n, dict) and _flow_node_is_docker_role(n)])
        except Exception:
            docker_nodes = 0
        flow_errors.extend([
            f"eligible_flag_generators={eligible_flag_gens}",
            f"eligible_flag_node_generators={eligible_node_gens}",
            f"chain_nodes={len(chain_nodes or [])}",
            f"chain_vuln_nodes={vuln_nodes}",
            f"chain_docker_nodes={docker_nodes}",
        ])
    else:
        flow_valid, flow_errors = _flow_validate_chain_order_by_requires_produces(
            chain_nodes,
            flag_assignments,
            scenario_label=(scenario_label or scenario_norm),
        )

        # Attach diagnostic details to help UI surface why validation failed.
        try:
            assign_ids = [str(a.get('id') or a.get('generator_id') or '').strip() for a in (flag_assignments or []) if isinstance(a, dict)]
            chain_ids_dbg = [str(n.get('id') or '').strip() for n in (chain_nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()]
            flow_errors_detail = (
                f"assignments={len(flag_assignments or [])} "
                f"assignments_with_id={len([x for x in assign_ids if x])} "
                f"chain_nodes={len(chain_nodes or [])} "
                f"chain_ids={','.join(chain_ids_dbg)}"
            )
        except Exception:
            flow_errors_detail = None

    # Additional validation: any referenced generator must exist and be enabled.
    # If not, disable flags and surface a clear error.
    try:
        gens_enabled, _ = _flag_generators_from_enabled_sources()
    except Exception:
        gens_enabled = []
    try:
        node_gens_enabled, _ = _flag_node_generators_from_enabled_sources()
    except Exception:
        node_gens_enabled = []
    enabled_ids: set[str] = set()
    for g in (gens_enabled or []):
        if isinstance(g, dict):
            gid = str(g.get('id') or '').strip()
            if gid:
                enabled_ids.add(gid)
    for g in (node_gens_enabled or []):
        if isinstance(g, dict):
            gid = str(g.get('id') or '').strip()
            if gid:
                enabled_ids.add(gid)

    missing_refs: list[str] = []
    try:
        for fa in (flag_assignments or []):
            if not isinstance(fa, dict):
                continue
            gid = str(fa.get('id') or fa.get('generator_id') or '').strip()
            if not gid:
                continue
            if gid not in enabled_ids:
                missing_refs.append(gid)
    except Exception:
        missing_refs = []

    missing_refs = sorted(list(dict.fromkeys(missing_refs)))
    if missing_refs:
        # Attempt to recover by recomputing assignments without missing generators.
        try:
            if not preset_steps:
                flag_assignments = _flow_compute_flag_assignments(
                    preview,
                    chain_nodes,
                    scenario_label or scenario_norm,
                    initial_facts_override=initial_facts_override,
                    goal_facts_override=goal_facts_override,
                )
                # Re-evaluate missing refs after recompute.
                missing_refs = []
                try:
                    for fa in (flag_assignments or []):
                        if not isinstance(fa, dict):
                            continue
                        gid = str(fa.get('id') or fa.get('generator_id') or '').strip()
                        if gid and gid not in enabled_ids:
                            missing_refs.append(gid)
                except Exception:
                    missing_refs = []
                missing_refs = sorted(list(dict.fromkeys(missing_refs)))
        except Exception:
            pass
    if missing_refs:
        flow_errors = list(flow_errors or []) + [f"generator not found/enabled: {gid}" for gid in missing_refs]
        flow_valid = False

    flags_enabled = bool(flow_valid)
    run_generators = bool(flags_enabled or (mode in {'resolve', 'resolve_hints', 'hint', 'hint_only'}))

    # Ensure preview IP context is aligned to the current chain order so the UI
    # shows Knowledge(ip) for the correct sequence node.
    try:
        host_by_id: dict[str, dict[str, Any]] = {}
        hosts = preview.get('hosts') if isinstance(preview, dict) else None
        if isinstance(hosts, list):
            for h in hosts:
                if not isinstance(h, dict):
                    continue
                hid = str(h.get('node_id') or '').strip()
                if hid:
                    host_by_id[hid] = h
    except Exception:
        host_by_id = {}
    try:
        vuln_by_node = preview.get('vulnerabilities_by_node') if isinstance(preview, dict) else None
        if not isinstance(vuln_by_node, dict):
            vuln_by_node = {}
    except Exception:
        vuln_by_node = {}

    def _preview_host_ip4(host: dict) -> str:
        try:
            ip4 = host.get('ip4')
            if isinstance(ip4, str) and _first_valid_ipv4(ip4):
                return _first_valid_ipv4(ip4)
        except Exception:
            pass
        for key in ('ipv4', 'ip', 'ip_addr', 'address'):
            try:
                v = host.get(key)
            except Exception:
                v = None
            ip_str = _first_valid_ipv4(v)
            if ip_str:
                return ip_str
        # Some previews store IPs under lists or interfaces.
        try:
            for key in ('ips', 'addresses', 'ip4s', 'ipv4s'):
                v = host.get(key)
                ip_str = _first_valid_ipv4(v)
                if ip_str:
                    return ip_str
        except Exception:
            pass
        try:
            ifaces = host.get('interfaces')
            if isinstance(ifaces, list):
                for iface in ifaces:
                    if not isinstance(iface, dict):
                        continue
                    for key in ('ip4', 'ipv4', 'ip', 'ip_addr', 'address'):
                        ip_str = _first_valid_ipv4(iface.get(key))
                        if ip_str:
                            return ip_str
        except Exception:
            pass
        return ''

    try:
        for fa in (flag_assignments or []):
            if not isinstance(fa, dict):
                continue
            nid = str(fa.get('node_id') or '').strip()
            if not nid:
                continue
            host = host_by_id.get(nid)
            preview_ip4 = _preview_host_ip4(host) if isinstance(host, dict) else ''
            if not preview_ip4:
                try:
                    node = next((n for n in (chain_nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip() == nid), None)
                except Exception:
                    node = None
                if isinstance(node, dict):
                    preview_ip4 = _first_valid_ipv4(node.get('ip4') or node.get('ipv4') or node.get('ip') or '')
            if not preview_ip4:
                continue
            ri = fa.get('resolved_inputs') if isinstance(fa.get('resolved_inputs'), dict) else None
            if ri is None:
                ri = {}
                fa['resolved_inputs'] = ri
            ri['Knowledge(ip)'] = preview_ip4
            ri['target_ip'] = preview_ip4
            ri['host_ip'] = preview_ip4
            ri['ip4'] = preview_ip4
            ri['ipv4'] = preview_ip4
    except Exception:
        pass

    if len(chain_nodes) < 1:
        return jsonify({'ok': False, 'error': 'No eligible nodes found in preview plan (vulnerability nodes only for flag-generators).', 'stats': stats, 'preview_plan_path': preview_plan_path}), 422
    if (not used_saved_chain) and (not allow_node_duplicates) and len(chain_nodes) < length:
        return jsonify({
            'ok': False,
            'error': f'Only {len(chain_nodes)} eligible nodes found for chain length {length}.',
            'available': len(chain_nodes),
            'stats': stats,
            'preview_plan_path': preview_plan_path,
        }), 422

    host_ip_map: dict[str, str] = {}
    try:
        for hid, h in (host_by_id or {}).items():
            ip_val = _preview_host_ip4(h) if isinstance(h, dict) else ''
            if ip_val:
                host_ip_map[str(hid)] = ip_val
    except Exception:
        host_ip_map = {}

    chain_out: list[dict[str, Any]] = []
    try:
        for n in (chain_nodes or []):
            if not isinstance(n, dict):
                continue
            nid = str(n.get('id') or '').strip()
            h = host_by_id.get(nid) if nid else None
            ip_val = _preview_host_ip4(h) if isinstance(h, dict) else ''
            if not ip_val:
                ip_val = _first_valid_ipv4(n.get('ip4') or n.get('ipv4') or n.get('ip') or '')
            ifaces = None
            try:
                ifaces = h.get('interfaces') if isinstance(h, dict) and isinstance(h.get('interfaces'), list) else None
            except Exception:
                ifaces = None
            vulns: list[str] = []
            try:
                if isinstance(h, dict) and isinstance(h.get('vulnerabilities'), list):
                    vulns = [str(v).strip() for v in (h.get('vulnerabilities') or []) if str(v).strip()]
            except Exception:
                vulns = []
            if not vulns:
                try:
                    if isinstance(n.get('vulnerabilities'), list):
                        vulns = [str(v).strip() for v in (n.get('vulnerabilities') or []) if str(v).strip()]
                except Exception:
                    vulns = []
            if (not vulns) and nid:
                try:
                    raw_v = vuln_by_node.get(nid)
                    if isinstance(raw_v, list):
                        vulns = [str(v).strip() for v in raw_v if str(v).strip()]
                except Exception:
                    vulns = []
            is_vuln = bool(vulns) or bool(n.get('is_vuln')) or bool(n.get('is_vulnerability')) or bool(n.get('is_vulnerable'))
            chain_out.append({
                'id': str(n.get('id') or ''),
                'name': str(n.get('name') or ''),
                'type': str(n.get('type') or ''),
                'is_vuln': bool(is_vuln),
                'vulnerabilities': list(vulns or []),
                'ip4': str(ip_val or ''),
                'ipv4': str(ip_val or ''),
                'interfaces': list(ifaces or []) if isinstance(ifaces, list) else [],
            })
    except Exception:
        chain_out = [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or ''), 'is_vuln': bool(n.get('is_vuln'))} for n in (chain_nodes or []) if isinstance(n, dict)]

    out = {
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'length': length,
        'requested_length': requested_length,
        'preview_plan_path': preview_plan_path,
        'chain': chain_out,
        'flag_assignments': flag_assignments,
        'stats': stats,
        'flow_valid': bool(flow_valid),
        'flow_errors': list(flow_errors or []),
        **({'flow_errors_detail': flow_errors_detail} if flow_errors_detail else {}),
        'flags_enabled': bool(flags_enabled),
        'allow_node_duplicates': bool(allow_node_duplicates),
        **({'host_ip_map': host_ip_map} if host_ip_map else {}),
    }
    if flow_errors_detail:
        out['flow_errors_detail'] = flow_errors_detail
    try:
        if not flow_valid:
            app.logger.warning(
                '[flow.attackflow_preview] invalid flow: %s',
                (flow_errors_detail or (flow_errors or [])),
            )
    except Exception:
        pass
    if initial_facts_override:
        out['initial_facts'] = initial_facts_override
    if goal_facts_override:
        out['goal_facts'] = goal_facts_override
    if warning:
        out['warning'] = warning
    if debug_mode:
        try:
            meta_dbg = payload.get('metadata') if isinstance(payload, dict) else None
        except Exception:
            meta_dbg = None
        out['debug'] = {
            'selected_by': selected_by,
            'prefer_preview': bool(prefer_preview),
            'force_preview': bool(force_preview),
            'ignore_saved_flow': bool(ignore_saved_flow),
            'used_saved_chain': bool(used_saved_chain),
            'preview_plan_path': preview_plan_path,
            'metadata': (meta_dbg if isinstance(meta_dbg, dict) else {}),
        }
    if debug_dag:
        out['sequencer_dag'] = dag_debug or {'ok': False, 'errors': ['not computed (saved chain)']}

    # STIX/AttackFlow bundle export has been removed; keep this endpoint for chain preview only.
    if (request.args.get('download') or '').strip() in {'1', 'true', 'yes'}:
        return jsonify({
            'ok': False,
            'error': 'STIX bundle export has been removed. Use /api/flag-sequencing/afb_from_chain.',
        }), 410

    try:
        try:
            plan_basename = os.path.basename(str(preview_plan_path or ''))
        except Exception:
            plan_basename = str(preview_plan_path or '')
        try:
            preview_seed = (payload.get('metadata') or {}).get('seed') if isinstance(payload, dict) else None
        except Exception:
            preview_seed = None
        if preview_seed is None:
            try:
                preview_seed = preview.get('seed') if isinstance(preview, dict) else None
            except Exception:
                preview_seed = None
        app.logger.info(
            '[flow.attackflow_preview] scenario=%s chain_len=%s flow_valid=%s flow_errors=%s selected_by=%s plan=%s seed=%s',
            scenario_norm,
            len(chain_nodes or []),
            bool(flow_valid),
            (flow_errors or []),
            selected_by,
            plan_basename,
            preview_seed,
        )
    except Exception:
        pass

    return jsonify(out)


@app.route('/api/flag-sequencing/sequence_preview_plan', methods=['POST'])
def api_flow_sequence_preview_plan():
    """Generate a Flow chain from an existing preview plan and persist sequence metadata.

    This does NOT regenerate topology/IPs; it only selects chain nodes and assigns generators.
    """
    j = request.get_json(silent=True) or {}
    scenario_label = str(j.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    preset = str(j.get('preset') or '').strip()
    allow_node_duplicates = str(j.get('allow_node_duplicates') or j.get('allow_duplicates') or '').strip().lower() in ('1', 'true', 'yes', 'y')
    length = 5
    try:
        length = int(j.get('length') or 5)
    except Exception:
        length = 5
    preset_steps = _flow_preset_steps(preset)
    if preset_steps:
        length = len(preset_steps)
    length = max(1, min(length, 50))
    requested_length = length
    best_effort = bool(j.get('best_effort'))

    # Parse flow_seed from request (for explicit control over chain/generator selection)
    flow_seed_param: int | None = None
    try:
        fs_raw = j.get('flow_seed')
        if fs_raw is not None:
            flow_seed_param = int(fs_raw)
    except (ValueError, TypeError):
        flow_seed_param = None

    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    preview_plan_path = str(j.get('preview_plan') or '').strip() or None
    xml_hint = str(j.get('xml_path') or '').strip()
    if preview_plan_path:
        try:
            preview_plan_path = os.path.abspath(preview_plan_path)
            if (not preview_plan_path.lower().endswith('.xml')) or (not os.path.exists(preview_plan_path)):
                preview_plan_path = None
        except Exception:
            preview_plan_path = None

    if not preview_plan_path and xml_hint:
        try:
            xml_abs = os.path.abspath(xml_hint)
            if os.path.exists(xml_abs) and xml_abs.lower().endswith('.xml'):
                payload_hint = _load_plan_preview_from_xml(xml_abs, scenario_norm)
                if isinstance(payload_hint, dict):
                    meta_hint = payload_hint.get('metadata') if isinstance(payload_hint.get('metadata'), dict) else {}
                    scen_hint = str(meta_hint.get('scenario') or '').strip()
                    if (not scen_hint) or _normalize_scenario_label(scen_hint) == scenario_norm:
                        preview_plan_path = xml_abs
        except Exception:
            pass

    if not preview_plan_path:
        preview_plan_path = _latest_xml_path_for_scenario(scenario_norm)

    if not preview_plan_path:
        return jsonify({'ok': False, 'error': 'No XML found for this scenario. Save XML with a PlanPreview first.'}), 404

    try:
        payload = _load_preview_payload_from_path(preview_plan_path, scenario_norm)
        if not isinstance(payload, dict):
            return jsonify({'ok': False, 'error': 'Preview plan not embedded in XML.'}), 422
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to load preview plan: {e}'}), 422

    preview = payload.get('full_preview') if isinstance(payload, dict) else None
    if not isinstance(preview, dict):
        return jsonify({'ok': False, 'error': 'Preview plan is missing full_preview.'}), 422

    nodes, _links, adj = _build_topology_graph_from_preview_plan(preview)
    stats = _flow_compose_docker_stats(nodes)

    # Select chain nodes from preview topology only.
    if preset_steps:
        chain_nodes = _pick_flag_chain_nodes_for_preset(nodes, adj, steps=preset_steps)
    else:
        if allow_node_duplicates:
            # Use flow_seed for chain selection (allows retrying with different combinations)
            seed_val = _get_flow_seed(preview, flow_seed_param)
            chain_nodes = _pick_flag_chain_nodes_allow_duplicates(nodes, adj, length=length, seed=seed_val)
        else:
            chain_nodes = _pick_flag_chain_nodes(nodes, adj, length=length)

    if not chain_nodes:
        return jsonify({'ok': False, 'error': 'No eligible nodes found in preview plan.', 'available': 0, 'requested_length': requested_length, 'stats': stats}), 422

    warning: str | None = None

    if (not preset_steps) and (not allow_node_duplicates) and len(chain_nodes) < length:
        if best_effort:
            warning = f"Only {len(chain_nodes)} eligible nodes found; using chain length {len(chain_nodes)} instead of requested {length}."
            length = len(chain_nodes)
        else:
            return jsonify({
                'ok': False,
                'error': 'Not enough eligible nodes in preview plan to build the requested chain.',
                'available': len(chain_nodes),
                'requested_length': requested_length,
                'stats': stats,
            }), 422

    # Inject preview host IP/interface details onto chain nodes for UI display.
    host_by_id: dict[str, dict[str, Any]] = {}
    try:
        hosts = preview.get('hosts') if isinstance(preview, dict) else None
        if isinstance(hosts, list):
            for h in hosts:
                if not isinstance(h, dict):
                    continue
                hid = str(h.get('node_id') or h.get('id') or '').strip()
                if hid:
                    host_by_id[hid] = h
    except Exception:
        host_by_id = {}
    try:
        vuln_by_node = preview.get('vulnerabilities_by_node') if isinstance(preview, dict) else None
        if not isinstance(vuln_by_node, dict):
            vuln_by_node = {}
    except Exception:
        vuln_by_node = {}
    try:
        if host_by_id:
            for n in (chain_nodes or []):
                if not isinstance(n, dict):
                    continue
                nid = str(n.get('id') or '').strip()
                if not nid:
                    continue
                h = host_by_id.get(nid)
                if not isinstance(h, dict):
                    continue
                ip_val = _preview_host_ip4_any(h)
                if ip_val:
                    if not (n.get('ip4') or n.get('ipv4') or n.get('ip') or n.get('address')):
                        n['ip4'] = ip_val
                        n['ipv4'] = ip_val
                ifaces = h.get('interfaces') if isinstance(h.get('interfaces'), list) else None
                if ifaces and not n.get('interfaces'):
                    n['interfaces'] = ifaces
    except Exception:
        pass

    initial_facts_override = _flow_normalize_fact_override(j.get('initial_facts'))
    goal_facts_override = _flow_normalize_fact_override(j.get('goal_facts'))
    retry_index = 0
    try:
        retry_index = int(j.get('retry_index') or 0)
    except Exception:
        retry_index = 0

    if preset_steps:
        flag_assignments, preset_err = _flow_compute_flag_assignments_for_preset(preview, chain_nodes, scenario_label or scenario_norm, preset)
        if preset_err:
            return jsonify({'ok': False, 'error': f'Error: {preset_err}', 'stats': stats}), 422
    else:
        # Use flow_seed as base, with retry_index variation
        base_seed = _get_flow_seed(preview, flow_seed_param)
        seed_override = base_seed ^ (retry_index * 0x9E3779B1) if retry_index else flow_seed_param
        flag_assignments = _flow_compute_flag_assignments(
            preview,
            chain_nodes,
            scenario_label or scenario_norm,
            initial_facts_override=initial_facts_override,
            goal_facts_override=goal_facts_override,
            seed_override=seed_override,
            disallow_generator_reuse=(not allow_node_duplicates),
        )
        if (not flag_assignments) and (not allow_node_duplicates):
            # Fallback: allow generator reuse if unique generators are insufficient.
            flag_assignments = _flow_compute_flag_assignments(
                preview,
                chain_nodes,
                scenario_label or scenario_norm,
                initial_facts_override=initial_facts_override,
                goal_facts_override=goal_facts_override,
                seed_override=seed_override,
                disallow_generator_reuse=False,
            )
            if flag_assignments:
                warning = warning or 'Not enough unique generators for this chain length; generator reuse was enabled.'

    # For auto-generated (non-preset) chains only, prefer a dependency-consistent ordering.
    # Presets force an intended generator order and should not be reordered.
    try:
        _ids = [str(n.get('id') or '').strip() for n in (chain_nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()]
        has_dupes = len(set(_ids)) != len(_ids)
    except Exception:
        has_dupes = False

    if (not preset_steps) and (not has_dupes):
        try:
            debug_dag = bool(j.get('debug_dag'))
        except Exception:
            debug_dag = False
        chain_nodes, flag_assignments, _dag_debug = _flow_reorder_chain_by_generator_dag(
            chain_nodes,
            flag_assignments,
            scenario_label=(scenario_label or scenario_norm),
            return_debug=bool(debug_dag),
        )

    try:
        flow_valid, flow_errors = _flow_validate_chain_order_by_requires_produces(
            chain_nodes,
            flag_assignments,
            scenario_label=(scenario_label or scenario_norm),
        )
    except Exception:
        flow_valid, flow_errors = True, []

    if not allow_node_duplicates:
        try:
            gen_ids = [str(a.get('id') or a.get('generator_id') or '').strip() for a in (flag_assignments or []) if isinstance(a, dict)]
            gen_ids = [g for g in gen_ids if g]
            if len(set(gen_ids)) != len(gen_ids):
                return jsonify({
                    'ok': False,
                    'error': 'Duplicate generators detected while duplicates are disabled. Reduce chain length or enable duplicates.',
                    'scenario': scenario_label or scenario_norm,
                    'length': len(chain_nodes or []),
                    'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or '')} for n in (chain_nodes or []) if isinstance(n, dict)],
                    'flag_assignments': flag_assignments,
                }), 422
        except Exception:
            pass

    host_ip_map: dict[str, str] = {}
    try:
        for hid, h in (host_by_id or {}).items():
            ip_val = _preview_host_ip4_any(h)
            if ip_val:
                host_ip_map[str(hid)] = ip_val
    except Exception:
        host_ip_map = {}

    chain_payload: list[dict[str, Any]] = []
    try:
        for n in (chain_nodes or []):
            if not isinstance(n, dict):
                continue
            nid = str(n.get('id') or '').strip()
            h = host_by_id.get(nid) if nid else None
            ip_val = _preview_host_ip4_any(h) if isinstance(h, dict) else ''
            if not ip_val:
                ip_val = _first_valid_ipv4(n.get('ip4') or n.get('ipv4') or n.get('ip') or '')
            ifaces = None
            try:
                ifaces = h.get('interfaces') if isinstance(h, dict) and isinstance(h.get('interfaces'), list) else None
            except Exception:
                ifaces = None
            vulns: list[str] = []
            try:
                if isinstance(h, dict) and isinstance(h.get('vulnerabilities'), list):
                    vulns = [str(v).strip() for v in (h.get('vulnerabilities') or []) if str(v).strip()]
            except Exception:
                vulns = []
            if not vulns:
                try:
                    if isinstance(n.get('vulnerabilities'), list):
                        vulns = [str(v).strip() for v in (n.get('vulnerabilities') or []) if str(v).strip()]
                except Exception:
                    vulns = []
            if (not vulns) and nid:
                try:
                    raw_v = vuln_by_node.get(nid)
                    if isinstance(raw_v, list):
                        vulns = [str(v).strip() for v in raw_v if str(v).strip()]
                except Exception:
                    vulns = []
            is_vuln = bool(vulns) or bool(n.get('is_vuln')) or bool(n.get('is_vulnerability')) or bool(n.get('is_vulnerable'))
            chain_payload.append({
                'id': str(n.get('id') or ''),
                'name': str(n.get('name') or ''),
                'type': str(n.get('type') or ''),
                'is_vuln': bool(is_vuln),
                'vulnerabilities': list(vulns or []),
                'ip4': str(ip_val or ''),
                'ipv4': str(ip_val or ''),
                'interfaces': list(ifaces or []) if isinstance(ifaces, list) else [],
            })
    except Exception:
        chain_payload = [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or ''), 'is_vuln': bool(n.get('is_vuln'))} for n in (chain_nodes or []) if isinstance(n, dict)]

    # Persist updated preview plan with sequence metadata (no topology mutation).
    try:
        meta = payload.get('metadata') if isinstance(payload, dict) else {}
        flow_meta = {
            'source_preview_plan_path': preview_plan_path,
            'scenario': scenario_label or scenario_norm,
            'length': len(chain_nodes),
            'requested_length': requested_length,
            'allow_node_duplicates': bool(allow_node_duplicates),
            'chain': list(chain_payload or []),
            'flag_assignments': _flow_strip_runtime_sensitive_fields(flag_assignments),
            'flags_enabled': bool(flow_valid),
            'flow_valid': bool(flow_valid),
            'flow_errors': list(flow_errors or []),
            'modified_at': _iso_now(),
        }
        if initial_facts_override:
            flow_meta['initial_facts'] = initial_facts_override
        if goal_facts_override:
            flow_meta['goal_facts'] = goal_facts_override
        if warning:
            flow_meta['warning'] = warning
        if isinstance(meta, dict):
            meta = dict(meta)
            meta['flow'] = flow_meta
        else:
            meta = {'flow': flow_meta}
        payload['metadata'] = meta
        if isinstance(preview, dict):
            preview.setdefault('metadata', {})
    except Exception:
        pass

    try:
        if isinstance(meta, dict):
            meta = dict(meta)
            meta['updated_at'] = _iso_now()
        xml_path_for_plan = None
        try:
            if preview_plan_path and str(preview_plan_path).lower().endswith('.xml'):
                xml_path_for_plan = os.path.abspath(preview_plan_path)
        except Exception:
            xml_path_for_plan = None
        if not xml_path_for_plan:
            try:
                meta_xml = str((meta or {}).get('xml_path') or '').strip()
                if meta_xml:
                    xml_path_for_plan = os.path.abspath(meta_xml)
            except Exception:
                xml_path_for_plan = None
        if not xml_path_for_plan and xml_hint:
            try:
                xml_hint_abs = os.path.abspath(xml_hint)
                if xml_hint_abs.lower().endswith('.xml'):
                    xml_path_for_plan = xml_hint_abs
            except Exception:
                xml_path_for_plan = None
        if not xml_path_for_plan:
            try:
                xml_path_for_plan = _latest_xml_path_for_scenario(scenario_norm)
            except Exception:
                xml_path_for_plan = None
        if not xml_path_for_plan or not os.path.exists(xml_path_for_plan):
            return jsonify({'ok': False, 'error': 'Failed to persist sequence plan: XML path not found.'}), 500

        plan_payload = {
            'full_preview': preview,
            'metadata': meta,
        }
        ok, err = _update_plan_preview_in_xml(xml_path_for_plan, scenario_label or scenario_norm, plan_payload)
        if not ok:
            return jsonify({'ok': False, 'error': f'Failed to persist sequence plan: {err}'}), 500
        try:
            _update_flow_state_in_xml(xml_path_for_plan, scenario_label or scenario_norm, flow_meta)
        except Exception:
            pass
        try:
            _planner_set_plan(scenario_norm, plan_path=xml_path_for_plan, xml_path=xml_path_for_plan, seed=(meta or {}).get('seed'))
        except Exception:
            pass
        out_path = xml_path_for_plan
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to persist sequence plan: {e}'}), 500

    # Include flow_seed in response (the effective seed used for chain/generator selection)
    response_flow_seed = _get_flow_seed(preview, flow_seed_param)

    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'length': len(chain_nodes),
        'requested_length': requested_length,
        'stats': stats,
        'chain': list(chain_payload or []),
        'flag_assignments': flag_assignments,
        'flags_enabled': bool(flow_valid),
        'flow_valid': bool(flow_valid),
        'flow_errors': list(flow_errors or []),
        'flow_seed': response_flow_seed,
        'preview_plan_path': out_path,
        'base_preview_plan_path': preview_plan_path,
        **({'warning': warning} if warning else {}),
        **({'host_ip_map': host_ip_map} if host_ip_map else {}),
    })



@app.route('/api/flag-sequencing/prepare_preview_for_execute', methods=['POST'])
def api_flow_prepare_preview_for_execute():
    # Stub for early progress calls; overridden later if generator runs occur.
    def _flow_progress(msg: str) -> None:
        try:
            app.logger.info('[flow.progress] %s', msg)
        except Exception:
            pass

    j = request.get_json(silent=True) or {}
    scenario_label = str(j.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    preset = str(j.get('preset') or '').strip()
    mode = str(j.get('mode') or '').strip().lower()
    best_effort = bool(j.get('best_effort')) or (mode in {'hint', 'hint_only', 'resolve_hints', 'preview'})
    run_generators_request = bool(mode in {'resolve', 'resolve_hints', 'hint', 'hint_only'})
    allow_node_duplicates = str(j.get('allow_node_duplicates') or j.get('allow_duplicates') or '').strip().lower() in ('1', 'true', 'yes', 'y')
    total_timeout_s: int | None = None
    try:
        total_timeout_s = int(j.get('timeout_s') or 0)
    except Exception:
        total_timeout_s = None
    if total_timeout_s is not None and total_timeout_s <= 0:
        total_timeout_s = None
    if best_effort and total_timeout_s is None:
        # UI hint resolution is optional; keep it bounded by default.
        total_timeout_s = 30
    try:
        length = int(j.get('length') or 5)
    except Exception:
        length = 5
    preset_steps = _flow_preset_steps(preset)
    if preset_steps:
        length = len(preset_steps)
    length = max(1, min(length, 50))
    requested_length = length

    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    flow_run_remote = False
    flow_remote_forced = False
    flow_core_cfg: Dict[str, Any] | None = None
    try:
        if 'run_remote' in j:
            flow_run_remote = _coerce_bool(j.get('run_remote'))
            flow_remote_forced = flow_run_remote
        if 'run_local' in j and _coerce_bool(j.get('run_local')):
            flow_run_remote = False
            flow_remote_forced = False
    except Exception:
        pass

    canonical_plan_path = _canonical_plan_path_for_scenario_norm(scenario_norm)

    base_plan_path = str(j.get('preview_plan') or '').strip() or None
    if base_plan_path:
        try:
            base_plan_path = os.path.abspath(base_plan_path)
            plans_dir = os.path.abspath(os.path.join(_outputs_dir(), 'plans'))
            if os.path.commonpath([base_plan_path, plans_dir]) != plans_dir:
                base_plan_path = None
            elif not os.path.exists(base_plan_path):
                base_plan_path = None
        except Exception:
            base_plan_path = None
    try:
        if canonical_plan_path and os.path.exists(canonical_plan_path):
            if not base_plan_path or os.path.abspath(base_plan_path) != os.path.abspath(canonical_plan_path):
                base_plan_path = canonical_plan_path
    except Exception:
        pass
    if not base_plan_path:
        try:
            entry = _planner_get_plan(scenario_norm)
            if entry:
                base_plan_path = entry.get('plan_path') or base_plan_path
        except Exception:
            base_plan_path = base_plan_path

    if not base_plan_path:
        base_plan_path = _latest_preview_plan_for_scenario_norm_origin(scenario_norm, origin='planner')

    if not base_plan_path:
        base_plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm)

    if not base_plan_path:
        return jsonify({'ok': False, 'error': 'No preview plan found for this scenario. Generate a Full Preview first.'}), 404

    try:
        flow_core_cfg = _core_config_from_xml_path(base_plan_path, scenario_norm, include_password=True)
        if isinstance(flow_core_cfg, dict):
            flow_core_cfg = _apply_core_secret_to_config(flow_core_cfg, scenario_norm)
    except Exception:
        flow_core_cfg = None
    if not flow_remote_forced and isinstance(flow_core_cfg, dict) and _coerce_bool(flow_core_cfg.get('ssh_enabled')):
        flow_run_remote = True
    if flow_run_remote and not isinstance(flow_core_cfg, dict):
        return jsonify({'ok': False, 'error': 'No CoreConnection configured in XML for this scenario.'}), 404

    initial_facts_override: dict[str, list[str]] | None = None
    goal_facts_override: dict[str, list[str]] | None = None
    try:
        initial_facts_override = _flow_normalize_fact_override(j.get('initial_facts'))
        goal_facts_override = _flow_normalize_fact_override(j.get('goal_facts'))
    except Exception:
        initial_facts_override = None
        goal_facts_override = None

    started_at = time.monotonic()
    try:
        plan_basename = os.path.basename(base_plan_path)
    except Exception:
        plan_basename = str(base_plan_path or '')
    app.logger.info(
        '[flow.prepare_preview_for_execute] start scenario=%s requested_length=%s preset=%s best_effort=%s timeout_s=%s base_plan=%s',
        scenario_norm,
        requested_length,
        (preset or ''),
        bool(best_effort),
        (total_timeout_s if total_timeout_s is not None else 'none'),
        plan_basename,
    )
    try:
        _flow_progress(f"Prepare start: scenario={scenario_norm} length={requested_length}")
    except Exception:
        pass

    try:
        payload = _load_preview_payload_from_path(base_plan_path, scenario_norm)
        if not isinstance(payload, dict):
            return jsonify({'ok': False, 'error': 'Preview plan not embedded in XML. Save XML with Preview first.'}), 404
        meta = payload.get('metadata') if isinstance(payload, dict) else {}
        preview = payload.get('full_preview') if isinstance(payload, dict) else None
        if not isinstance(preview, dict):
            return jsonify({'ok': False, 'error': 'Preview plan is missing full_preview.'}), 422
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to load preview plan: {e}'}), 500

    # Best-effort guard: the UI expects JSON errors (avoid Flask HTML 500s).
    warning: str | None = None
    try:
        nodes, _links, adj = _build_topology_graph_from_preview_plan(preview)
        stats = _flow_compose_docker_stats(nodes)

        def _eligible_debug_summary(all_nodes: list[Any], max_items: int = 50) -> dict[str, Any]:
            vuln_nodes: list[dict[str, str]] = []
            nonvuln_docker_nodes: list[dict[str, str]] = []
            try:
                for n in (all_nodes or []):
                    if not isinstance(n, dict):
                        continue
                    nid = str(n.get('id') or '').strip()
                    name = str(n.get('name') or nid).strip()
                    if not nid:
                        continue
                    is_vuln = _flow_node_is_vuln(n)
                    is_docker = _flow_node_is_docker_role(n)
                    if is_vuln:
                        if len(vuln_nodes) < max_items:
                            vuln_nodes.append({'id': nid, 'name': name})
                        continue
                    if is_docker:
                        if len(nonvuln_docker_nodes) < max_items:
                            nonvuln_docker_nodes.append({'id': nid, 'name': name})
            except Exception:
                pass
            return {
                'vuln_nodes_count': sum(1 for n in (all_nodes or []) if isinstance(n, dict) and _flow_node_is_vuln(n)),
                'nonvuln_docker_nodes_count': sum(1 for n in (all_nodes or []) if isinstance(n, dict) and _flow_node_is_docker_role(n) and (not _flow_node_is_vuln(n))),
                'vuln_nodes_sample': vuln_nodes,
                'nonvuln_docker_nodes_sample': nonvuln_docker_nodes,
            }
        eligible_debug = _eligible_debug_summary(nodes)

        # Allow caller to provide an explicit ordered chain.
        chain_ids_in = j.get('chain_ids')
        if (not chain_ids_in) and (not preset_steps):
            try:
                flow_meta = meta.get('flow') if isinstance(meta, dict) else None
                saved_chain = flow_meta.get('chain') if isinstance(flow_meta, dict) else None
                saved_ids: list[str] = []
                if isinstance(saved_chain, list) and saved_chain:
                    for entry in saved_chain:
                        if not isinstance(entry, dict):
                            continue
                        cid = str(entry.get('id') or '').strip()
                        if cid:
                            saved_ids.append(cid)
                if saved_ids:
                    chain_ids_in = saved_ids
            except Exception:
                pass
        chain_ids: list[str] = []
        if isinstance(chain_ids_in, list) and chain_ids_in:
            for cid in chain_ids_in:
                c = str(cid or '').strip()
                if c:
                    chain_ids.append(c)
            chain_ids = chain_ids[:length]

        explicit_chain = bool(chain_ids)

        if chain_ids:
            id_map = {str(n.get('id') or '').strip(): n for n in (nodes or []) if isinstance(n, dict)}
            # Preserve positional intent: keep placeholders for missing ids.
            chain_nodes: list[Any] = []
            missing_chain_ids: list[str] = []
            for cid in chain_ids:
                if cid in id_map:
                    chain_nodes.append(id_map[cid])
                else:
                    chain_nodes.append(None)
                    missing_chain_ids.append(cid)

            # Best-effort repair: if some ids don't exist in the selected plan (common when a
            # stale plan path is passed), fill missing positions with unused eligible nodes.
            # If we cannot repair fully, fail loudly (prevents UI from collapsing chain length).
            if missing_chain_ids:
                try:
                    used = {
                        str(n.get('id') or '').strip()
                        for n in chain_nodes
                        if isinstance(n, dict) and str(n.get('id') or '').strip()
                    }

                    def _needs_nonvuln_docker(pos: int) -> bool:
                        if not preset_steps:
                            return False
                        if pos < 0 or pos >= len(preset_steps):
                            return False
                        return (str((preset_steps[pos] or {}).get('kind') or '').strip() == 'flag-node-generator')

                    def _eligible(cand: dict, pos: int) -> bool:
                        try:
                            cid0 = str(cand.get('id') or '').strip()
                            if not cid0:
                                return False
                            if (not allow_node_duplicates) and cid0 in used:
                                return False
                            is_docker = _flow_node_is_docker_role(cand)
                            is_vuln = bool(cand.get('is_vuln')) or bool(cand.get('vulnerabilities'))
                            if _needs_nonvuln_docker(pos):
                                return bool(is_docker) and (not is_vuln)
                            return bool(is_vuln)
                        except Exception:
                            return False

                    for i, node in enumerate(chain_nodes):
                        if isinstance(node, dict):
                            continue
                        replacement = None
                        for cand in (nodes or []):
                            if not isinstance(cand, dict):
                                continue
                            if not _eligible(cand, i):
                                continue
                            replacement = cand
                            break
                        if replacement is not None:
                            rid = str(replacement.get('id') or '').strip()
                            if rid:
                                chain_nodes[i] = replacement
                                chain_ids[i] = rid
                                used.add(rid)

                    # Drop any unrepaired placeholders.
                    chain_nodes = [n for n in chain_nodes if isinstance(n, dict)]
                    if len(chain_nodes) < length:
                        return jsonify({
                            'ok': False,
                            'error': 'Provided chain_ids do not match the selected preview plan (stale preview_plan?) and could not be fully repaired.',
                            'requested_length': requested_length,
                            'matched_length': len(chain_nodes),
                            'missing_chain_ids': missing_chain_ids,
                            'stats': stats,
                            'best_effort': bool(best_effort),
                        }), 422
                except Exception:
                    # Conservative: don't silently shrink.
                    return jsonify({
                        'ok': False,
                        'error': 'Provided chain_ids did not match the selected preview plan and repair failed.',
                        'requested_length': requested_length,
                        'missing_chain_ids': missing_chain_ids,
                        'stats': stats,
                        'best_effort': bool(best_effort),
                    }), 422

            # Presets require certain steps to run on non-vulnerability docker nodes.
            # If the UI provided a chain that violates this (common when many vuln nodes exist),
            # best-effort swap in an eligible docker node.
            if preset_steps and chain_nodes:
                try:
                    used = {str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict)}
                    for i, step in enumerate(preset_steps[:len(chain_nodes)]):
                        if str((step or {}).get('kind') or '').strip() != 'flag-node-generator':
                            continue
                        node = chain_nodes[i] if i < len(chain_nodes) else None
                        if not isinstance(node, dict):
                            continue
                        if not bool(node.get('is_vuln')):
                            continue
                        replacement = None
                        for cand in (nodes or []):
                            if not isinstance(cand, dict):
                                continue
                            cid = str(cand.get('id') or '').strip()
                            if not cid:
                                continue
                            if (not allow_node_duplicates) and cid in used:
                                continue
                            t_raw = str(cand.get('type') or '')
                            t = t_raw.strip().lower()
                            is_docker = ('docker' in t) or (t_raw.strip().upper() == 'DOCKER')
                            if is_docker and not bool(cand.get('is_vuln')):
                                replacement = cand
                                break
                        if replacement is not None:
                            rid = str(replacement.get('id') or '').strip()
                            if rid:
                                chain_nodes[i] = replacement
                                chain_ids[i] = rid
                                used.add(rid)
                except Exception:
                    pass

            # Enforce Flow placement rules:
            # - flag-generators may be placed on vulnerability nodes only
            # - flag-node-generators must be placed on non-vulnerability docker-role nodes
            # If the caller provided a chain that violates this, best-effort replace nodes
            # with unused eligible nodes; otherwise fail with a clear error.
            if chain_nodes:
                try:
                    used = {str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict) and str(n.get('id') or '').strip()}
                    reselect_chain = False
                    reselect_reason = ''
                    for i, node in enumerate(chain_nodes):
                        if not isinstance(node, dict):
                            continue
                        t_raw = str(node.get('type') or '')
                        t = t_raw.strip().lower()
                        is_docker = ('docker' in t) or (t_raw.strip().upper() == 'DOCKER')
                        is_vuln = _flow_node_is_vuln(node)

                        need_nonvuln_docker = False
                        if preset_steps and i < len(preset_steps):
                            need_nonvuln_docker = (str((preset_steps[i] or {}).get('kind') or '').strip() == 'flag-node-generator')

                        if need_nonvuln_docker:
                            if is_docker and (not is_vuln):
                                continue
                        else:
                            if is_vuln:
                                continue

                        replacement = None
                        for cand in (nodes or []):
                            if not isinstance(cand, dict):
                                continue
                            cid = str(cand.get('id') or '').strip()
                            if not cid:
                                continue
                            if (not allow_node_duplicates) and cid in used:
                                continue
                            ct_raw = str(cand.get('type') or '')
                            ct = ct_raw.strip().lower()
                            cand_is_docker = ('docker' in ct) or (ct_raw.strip().upper() == 'DOCKER')
                            cand_is_vuln = _flow_node_is_vuln(cand)
                            if need_nonvuln_docker:
                                if not cand_is_docker:
                                    continue
                                if cand_is_vuln:
                                    continue
                            else:
                                if not cand_is_vuln:
                                    continue
                            replacement = cand
                            break

                        if replacement is None:
                            if best_effort or (mode in {'resolve', 'resolve_hints', 'hint', 'hint_only', 'preview'}):
                                reselect_chain = True
                                reselect_reason = 'Provided chain was incompatible with placement rules; selected a new valid chain.'
                                break
                            return jsonify({
                                'ok': False,
                                'error': 'Not enough eligible nodes for the provided chain. Flag-generators require vulnerability nodes; flag-node-generators require non-vulnerability docker-role nodes.',
                                'stats': stats,
                                'eligible': eligible_debug,
                            }), 422

                        rid = str(replacement.get('id') or '').strip()
                        if rid:
                            chain_nodes[i] = replacement
                            chain_ids[i] = rid
                            used.add(rid)

                    if reselect_chain:
                        if preset_steps:
                            chain_nodes = _pick_flag_chain_nodes_for_preset(nodes, adj, steps=preset_steps)
                        else:
                            if allow_node_duplicates:
                                try:
                                    seed_val = int((preview.get('seed') if isinstance(preview, dict) else None) or 0)
                                except Exception:
                                    seed_val = 0
                                chain_nodes = _pick_flag_chain_nodes_allow_duplicates(nodes, adj, length=length, seed=seed_val)
                            else:
                                chain_nodes = _pick_flag_chain_nodes(nodes, adj, length=length)
                        if len(chain_nodes) < 1:
                            return jsonify({
                                'ok': False,
                                'error': 'No eligible nodes found in preview plan (vulnerability nodes only for flag-generators).',
                                'available': len(chain_nodes),
                                'stats': stats,
                            }), 422
                        if (not allow_node_duplicates) and len(chain_nodes) < length:
                            return jsonify({
                                'ok': False,
                                'error': 'Not enough eligible nodes in preview plan to build the requested chain.',
                                'available': len(chain_nodes),
                                'stats': stats,
                                'eligible': eligible_debug,
                            }), 422
                        chain_ids = [str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict) and str(n.get('id') or '').strip()]
                        explicit_chain = False
                        warning = reselect_reason or warning
                except Exception:
                    pass

            # Enforce no-duplicates when requested (even for saved chains).
            if (not allow_node_duplicates) and chain_nodes:
                try:
                    seen = set()
                    unique_nodes: list[dict[str, Any]] = []
                    for n in chain_nodes:
                        if not isinstance(n, dict):
                            continue
                        nid = str(n.get('id') or '').strip()
                        if not nid or nid in seen:
                            continue
                        seen.add(nid)
                        unique_nodes.append(n)
                    if len(unique_nodes) != len(chain_nodes):
                        chain_nodes = unique_nodes
                        used_saved_chain = False
                except Exception:
                    pass

            # If de-duping reduced length, reselect a fresh chain to fill to requested length.
            if (not allow_node_duplicates) and len(chain_nodes) < length:
                if preset_steps:
                    chain_nodes = _pick_flag_chain_nodes_for_preset(nodes, adj, steps=preset_steps)
                else:
                    chain_nodes = _pick_flag_chain_nodes(nodes, adj, length=length)
            if len(chain_nodes) < 1:
                return jsonify({'ok': False, 'error': 'Provided chain_ids did not match any nodes in the preview plan.', 'stats': stats}), 422
        else:
            if preset_steps:
                chain_nodes = _pick_flag_chain_nodes_for_preset(nodes, adj, steps=preset_steps)
            else:
                if allow_node_duplicates:
                    try:
                        seed_val = int((preview.get('seed') if isinstance(preview, dict) else None) or 0)
                    except Exception:
                        seed_val = 0
                    chain_nodes = _pick_flag_chain_nodes_allow_duplicates(nodes, adj, length=length, seed=seed_val)
                else:
                    chain_nodes = _pick_flag_chain_nodes(nodes, adj, length=length)
            if len(chain_nodes) < 1:
                return jsonify({'ok': False, 'error': 'No eligible nodes found in preview plan (vulnerability nodes only for flag-generators).', 'available': len(chain_nodes), 'stats': stats, 'eligible': eligible_debug}), 422
            if (not allow_node_duplicates) and len(chain_nodes) < length:
                return jsonify({'ok': False, 'error': 'Not enough eligible nodes in preview plan to build the requested chain.', 'available': len(chain_nodes), 'stats': stats, 'eligible': eligible_debug}), 422
            chain_ids = [str(n.get('id') or '').strip() for n in chain_nodes if str(n.get('id') or '').strip()]
    except Exception as e:
        app.logger.exception('[flow.prepare_preview_for_execute] internal error: %s', e)
        return jsonify({
            'ok': False,
            'error': f'Internal error preparing preview for execution: {e}',
            'base_preview_plan_path': base_plan_path,
        }), 500

    # Caller may pass fewer ids than requested length; persist effective length.
    try:
        length = len(chain_nodes)
    except Exception:
        pass

    # Inject generator metadata into chain nodes.
    # Flag-generators should NOT create nodes/services; they generate artifacts/flags that can be
    # inserted into existing Docker nodes.
    flag_assignments: list[dict[str, Any]] = []
    # Prefer saved Flow assignments if the caller passed a plan payload that
    # already includes metadata.flow) and it fully covers this chain.
    try:
        flow_meta = meta.get('flow') if isinstance(meta, dict) else None
        saved_assignments = flow_meta.get('flag_assignments') if isinstance(flow_meta, dict) else None
        if isinstance(saved_assignments, list) and saved_assignments:
            # Saved assignments are persisted positionally, aligned with the saved chain.
            # Do not re-key by node_id; chains may intentionally contain duplicates.
            try:
                desired_len = len(chain_nodes or [])
            except Exception:
                desired_len = 0
            if desired_len and len(saved_assignments) >= desired_len:
                ordered: list[dict[str, Any]] = []
                for i in range(desired_len):
                    a = saved_assignments[i]
                    if not isinstance(a, dict):
                        ordered.append({})
                        continue
                    a2 = dict(a)
                    try:
                        a2['node_id'] = str((chain_nodes[i] or {}).get('id') or '').strip()
                    except Exception:
                        pass
                    ordered.append(a2)
                if all(isinstance(a, dict) and str(a.get('id') or '').strip() for a in ordered):
                    flag_assignments = ordered
                try:
                    flag_assignments = _flow_enrich_saved_flag_assignments(
                        flag_assignments,
                        chain_nodes,
                        scenario_label=(scenario_label or scenario_norm),
                    )
                except Exception:
                    pass

                # Do not reuse saved assignments if they violate placement rules for
                # the current chain nodes (prevents Preview from showing flags on
                # non-docker/non-vuln nodes or flag-node-generators on vuln nodes).
                try:
                    if flag_assignments and len(flag_assignments) == len(chain_nodes):
                        for i, n in enumerate(chain_nodes):
                            a = flag_assignments[i] if i < len(flag_assignments) else {}
                            if not isinstance(n, dict) or not isinstance(a, dict):
                                raise ValueError('invalid chain/assignment')
                            nid = str(n.get('id') or '').strip()
                            aid = str(a.get('node_id') or '').strip()
                            if nid and aid and nid != aid:
                                raise ValueError('assignment node mismatch')
                            is_docker = _flow_node_is_docker_role(n)
                            is_vuln = bool(n.get('is_vuln'))
                            kind = str(a.get('type') or '').strip() or 'flag-generator'
                            if kind == 'flag-node-generator':
                                if not (is_docker and (not is_vuln)):
                                    raise ValueError('flag-node-generator on ineligible node')
                            else:
                                if not (is_docker or is_vuln):
                                    raise ValueError('flag-generator on ineligible node')
                except Exception:
                    flag_assignments = []
    except Exception:
        flag_assignments = []

    if not flag_assignments:
        if preset_steps:
            preset_assignments, preset_err = _flow_compute_flag_assignments_for_preset(preview, chain_nodes, scenario_label or scenario_norm, preset)
            if preset_err:
                return jsonify({'ok': False, 'error': f'Error: {preset_err}', 'stats': stats}), 422
            flag_assignments = preset_assignments
        else:
            flag_assignments = _flow_compute_flag_assignments(
                preview,
                chain_nodes,
                scenario_label or scenario_norm,
                initial_facts_override=initial_facts_override,
                goal_facts_override=goal_facts_override,
            )

    # For auto-generated (non-preset) chains only, prefer a dependency-consistent ordering.
    # Note: chain_ids is populated for both explicit and auto-picked chains; use explicit_chain.
    try:
        _ids = [str(n.get('id') or '').strip() for n in (chain_nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()]
        has_dupes = len(set(_ids)) != len(_ids)
    except Exception:
        has_dupes = False

    if (not explicit_chain) and (not preset_steps) and (not has_dupes):
        debug_dag = bool(j.get('debug_dag'))
        chain_nodes, flag_assignments, dag_debug = _flow_reorder_chain_by_generator_dag(
            chain_nodes,
            flag_assignments,
            scenario_label=(scenario_label or scenario_norm),
            return_debug=bool(debug_dag),
        )
        try:
            chain_ids = [str(n.get('id') or '').strip() for n in chain_nodes if isinstance(n, dict) and str(n.get('id') or '').strip()]
        except Exception:
            pass
    else:
        debug_dag = bool(j.get('debug_dag'))
        dag_debug = None

    # Validate (non-blocking): if invalid, we still allow execution but we do not
    # inject or run any flags.
    has_assignment_ids = False
    try:
        has_assignment_ids = any(
            isinstance(a, dict) and str(a.get('id') or a.get('generator_id') or '').strip()
            for a in (flag_assignments or [])
        )
    except Exception:
        has_assignment_ids = False

    if (not flag_assignments) or (not has_assignment_ids):
        flow_valid = False
        flow_errors = ['missing flag assignments']
        try:
            gens_enabled, _ = _flag_generators_from_enabled_sources()
        except Exception:
            gens_enabled = []
        try:
            node_gens_enabled, _ = _flag_node_generators_from_enabled_sources()
        except Exception:
            node_gens_enabled = []
        try:
            eligible_flag_gens = len([g for g in (gens_enabled or []) if isinstance(g, dict)])
        except Exception:
            eligible_flag_gens = 0
        try:
            eligible_node_gens = len([g for g in (node_gens_enabled or []) if isinstance(g, dict)])
        except Exception:
            eligible_node_gens = 0
        try:
            vuln_nodes = len([n for n in (chain_nodes or []) if isinstance(n, dict) and _flow_node_is_vuln(n)])
        except Exception:
            vuln_nodes = 0
        try:
            docker_nodes = len([n for n in (chain_nodes or []) if isinstance(n, dict) and _flow_node_is_docker_role(n)])
        except Exception:
            docker_nodes = 0
        flow_errors.extend([
            f"eligible_flag_generators={eligible_flag_gens}",
            f"eligible_flag_node_generators={eligible_node_gens}",
            f"chain_nodes={len(chain_nodes or [])}",
            f"chain_vuln_nodes={vuln_nodes}",
            f"chain_docker_nodes={docker_nodes}",
        ])
    else:
        flow_valid, flow_errors = _flow_validate_chain_order_by_requires_produces(
            chain_nodes,
            flag_assignments,
            scenario_label=(scenario_label or scenario_norm),
        )
    # Provide detailed diagnostics for UI display.
    try:
        assign_ids = [str(a.get('id') or a.get('generator_id') or '').strip() for a in (flag_assignments or []) if isinstance(a, dict)]
        chain_ids_dbg = [str(n.get('id') or '').strip() for n in (chain_nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()]
        flow_errors_detail = (
            f"assignments={len(flag_assignments or [])} "
            f"assignments_with_id={len([x for x in assign_ids if x])} "
            f"chain_nodes={len(chain_nodes or [])} "
            f"chain_ids={','.join(chain_ids_dbg)}"
        )
    except Exception:
        flow_errors_detail = None
    flags_enabled = bool(flow_valid)
    run_generators = bool(flags_enabled or run_generators_request)
    try:
        if not flow_valid:
            app.logger.warning(
                '[flow.prepare_preview_for_execute] invalid flow: %s',
                (flow_errors_detail or (flow_errors or [])),
            )
    except Exception:
        pass

    # Apply Flow modifications and run generators when requested (resolve/hint) or valid.
    flow_remote_repo_dir: str | None = None
    if run_generators and flow_run_remote:
        if isinstance(flow_core_cfg, dict):
            try:
                app.logger.info('[flow.generator] syncing repo to CORE VM before remote generator run')
            except Exception:
                pass
            allowed_outputs_override = None
            include_repo_paths = None
            try:
                allowed_outputs_override = _flow_required_installed_generator_outputs(
                    flag_assignments,
                    repo_root=_get_repo_root(),
                )
                include_repo_paths = _flow_required_generator_repo_paths(
                    flag_assignments,
                    repo_root=_get_repo_root(),
                )
                if not include_repo_paths:
                    raise ValueError('No generator paths resolved for Flow sync.')
                try:
                    generator_only = [
                        p for p in (allowed_outputs_override or [])
                        if p.startswith('outputs/installed_generators/')
                    ]
                except Exception:
                    generator_only = []
                if generator_only:
                    app.logger.info('[flow.generator] Syncing repo to CORE VM (generators: %d)', len(generator_only))
                else:
                    app.logger.info('[flow.generator] Syncing repo to CORE VM (no installed generators needed)')
            except Exception as exc:
                app.logger.error('[flow.generator] failed to resolve generator paths: %s', exc, exc_info=True)
                allowed_outputs_override = None
                include_repo_paths = None
            try:
                if not allowed_outputs_override:
                    app.logger.info('[flow.generator] Syncing repo to CORE VM (reduced snapshot)')
            except Exception:
                pass
            try:
                if not include_repo_paths:
                    # No files to sync? That's fine, just skip.
                    app.logger.info('[flow.generator] repo sync skipped (no generator paths resolved)')
                else:
                    _push_repo_to_remote(
                        flow_core_cfg,
                        logger=app.logger,
                        upload_only_injected_artifacts=True,
                        allowed_outputs_override=allowed_outputs_override,
                        include_repo_paths=include_repo_paths,
                    )
                    try:
                        _flow_progress('Repo sync complete')
                    except Exception:
                        pass
            except Exception as exc:
                if flow_remote_forced:
                    return jsonify({'ok': False, 'error': f'Failed to sync repo to CORE VM: {exc}'}), 500
                try:
                    app.logger.warning('[flow.generator] repo sync failed (continuing): %s', exc)
                except Exception:
                    pass
        if not isinstance(flow_core_cfg, dict):
            if flow_remote_forced:
                return jsonify({'ok': False, 'error': 'Remote Flow generation requested but no CORE VM config was found.'}), 400
            flow_run_remote = False
        else:
            try:
                flow_core_cfg = _require_core_ssh_credentials(flow_core_cfg)
            except Exception as exc:
                if flow_remote_forced:
                    return jsonify({'ok': False, 'error': f'Remote Flow generation requires SSH credentials: {exc}'}), 400
                flow_run_remote = False
            if flow_run_remote:
                client = None
                sftp = None
                try:
                    client = _open_ssh_client(flow_core_cfg)
                    sftp = client.open_sftp()
                    flow_remote_repo_dir = _remote_static_repo_dir(sftp)
                    runner_path = _remote_path_join(flow_remote_repo_dir, 'scripts', 'run_flag_generator.py')
                    sftp.stat(flow_remote_repo_dir)
                    sftp.stat(runner_path)
                except Exception as exc:
                    if flow_remote_forced:
                        return jsonify({'ok': False, 'error': f'Remote Flow generation requires repo on CORE VM: {exc}'}), 400
                    flow_run_remote = False
                    flow_remote_repo_dir = None
                finally:
                    try:
                        if sftp:
                            sftp.close()
                    except Exception:
                        pass
                    try:
                        if client:
                            client.close()
                    except Exception:
                        pass

    if run_generators:
        # Promote chain nodes to Docker role (flag payloads attach to Docker nodes).
        try:
            hosts = preview.get('hosts') or []
            if isinstance(hosts, list):
                for h in hosts:
                    if not isinstance(h, dict):
                        continue
                    hid = str(h.get('node_id') or '').strip()
                    if hid and hid in chain_ids:
                        h['role'] = 'Docker'
        except Exception:
            pass

    # Load enabled generator catalogs once so we can prune config to declared inputs.
    # We do this even when flags are disabled so the UI can still show resolved inputs.
    try:
        _gens_for_cfg, _ = _flag_generators_from_enabled_sources()
    except Exception:
        _gens_for_cfg = []
    try:
        _node_gens_for_cfg, _ = _flag_node_generators_from_enabled_sources()
    except Exception:
        _node_gens_for_cfg = []

    _gen_by_id: dict[str, dict[str, Any]] = {}
    try:
        for _g in (_gens_for_cfg or []):
            if not isinstance(_g, dict):
                continue
            _gid = str(_g.get('id') or '').strip()
            if _gid:
                _gen_by_id[_gid] = _g
        for _g in (_node_gens_for_cfg or []):
            if not isinstance(_g, dict):
                continue
            _gid = str(_g.get('id') or '').strip()
            if _gid and _gid not in _gen_by_id:
                _gen_by_id[_gid] = _g
    except Exception:
        _gen_by_id = {}

    def _flow_is_file_input_type(type_value: Any) -> bool:
        try:
            t = str(type_value or '').strip().lower()
        except Exception:
            return False
        if not t:
            return False
        if t in {'file', 'file_list'}:
            return True
        if t in {'file', 'filepath', 'file_path', 'path', 'pathname'}:
            return True
        if 'file' in t:
            return True
        if 'path' in t:
            return True
        return False

    def _flow_is_file_list_input_type(type_value: Any) -> bool:
        try:
            t = str(type_value or '').strip().lower()
        except Exception:
            return False
        if not t:
            return False
        if t == 'file_list':
            return True
        # Back-compat heuristic.
        if 'list' in t and ('file' in t or 'path' in t):
            return True
        return False

    def _flow_is_allowed_upload_path(path_value: Any) -> bool:
        """Only allow staging from known safe roots (outputs/flow_uploads and uploads/)."""
        if not path_value:
            return False
        try:
            ap = os.path.abspath(str(path_value))
        except Exception:
            return False
        try:
            if not os.path.isfile(ap):
                return False
        except Exception:
            return False
        try:
            allowed_roots = [
                os.path.abspath(os.path.join(_outputs_dir(), 'flow_uploads')),
                os.path.abspath(_uploads_dir()),
            ]
            for root in allowed_roots:
                try:
                    if os.path.commonpath([ap, root]) == root:
                        return True
                except Exception:
                    continue
        except Exception:
            return False
        return False

    def _flow_unique_dest_filename(dir_path: str, filename: str) -> str:
        base = secure_filename(filename) or 'upload'
        cand = base
        root, ext = os.path.splitext(base)
        i = 1
        while os.path.exists(os.path.join(dir_path, cand)):
            cand = f"{root}_{i}{ext}"
            i += 1
            if i > 5000:
                break
        return cand

    def _flow_stage_file_inputs_for_generator(cfg_to_pass: dict[str, Any], gen_def: dict[str, Any], *, run_dir: str) -> None:
        if not isinstance(cfg_to_pass, dict) or not isinstance(gen_def, dict):
            return
        if not run_dir:
            return
        inputs = gen_def.get('inputs') if isinstance(gen_def, dict) else None
        inputs_list = inputs if isinstance(inputs, list) else []

        file_input_names: set[str] = set()
        file_list_input_names: set[str] = set()
        for inp in inputs_list:
            if not isinstance(inp, dict):
                continue
            nm = str(inp.get('name') or '').strip()
            if not nm:
                continue
            if _flow_is_file_list_input_type(inp.get('type')):
                file_list_input_names.add(nm)
                continue
            if _flow_is_file_input_type(inp.get('type')):
                file_input_names.add(nm)

        if not file_input_names and not file_list_input_names:
            return

        inputs_dir = os.path.join(str(run_dir), 'inputs')
        os.makedirs(inputs_dir, exist_ok=True)

        for key in list(cfg_to_pass.keys()):
            # file
            if key in file_input_names:
                val = cfg_to_pass.get(key)
                if not isinstance(val, str):
                    continue
                raw = val.strip()
                if not raw:
                    continue
                # Already a container path.
                if raw.startswith('/inputs/'):
                    continue
                if not _flow_is_allowed_upload_path(raw):
                    continue
                try:
                    src = os.path.abspath(raw)
                except Exception:
                    continue
                try:
                    base = os.path.basename(src) or f"{key}.upload"
                except Exception:
                    base = f"{key}.upload"
                dest_name = _flow_unique_dest_filename(inputs_dir, base)
                dest = os.path.join(inputs_dir, dest_name)
                try:
                    shutil.copyfile(src, dest)
                    cfg_to_pass[key] = f"/inputs/{dest_name}"
                except Exception:
                    continue

            # file_list
            if key in file_list_input_names:
                val = cfg_to_pass.get(key)
                if not isinstance(val, list):
                    continue
                staged: list[str] = []
                for item in (val or []):
                    if not isinstance(item, str):
                        continue
                    raw = item.strip()
                    if not raw:
                        continue
                    if raw.startswith('/inputs/'):
                        staged.append(raw)
                        continue
                    if not _flow_is_allowed_upload_path(raw):
                        continue
                    try:
                        src = os.path.abspath(raw)
                    except Exception:
                        continue
                    try:
                        base = os.path.basename(src) or f"{key}.upload"
                    except Exception:
                        base = f"{key}.upload"
                    dest_name = _flow_unique_dest_filename(inputs_dir, base)
                    dest = os.path.join(inputs_dir, dest_name)
                    try:
                        shutil.copyfile(src, dest)
                        staged.append(f"/inputs/{dest_name}")
                    except Exception:
                        continue
                if staged:
                    cfg_to_pass[key] = staged

    # Hard validation: if flags are enabled, every referenced generator must exist and be enabled.
    # If not, fail with a clear error so the user can fix assignments.
    if flags_enabled:
        missing_or_disabled: list[dict[str, Any]] = []
        try:
            for i, fa in enumerate(flag_assignments or []):
                if not isinstance(fa, dict):
                    continue
                generator_id = str(fa.get('id') or fa.get('generator_id') or '').strip()
                if not generator_id:
                    continue
                assignment_type = str(fa.get('type') or '').strip() or 'flag-generator'
                # Map assignment type to disable-kind.
                kind = 'flag-node-generator' if assignment_type == 'flag-node-generator' else 'flag-generator'
                exists_enabled = generator_id in _gen_by_id
                disabled = _is_installed_generator_disabled(kind=kind, generator_id=generator_id)
                if (not exists_enabled) or disabled:
                    node_id = ''
                    node_name = ''
                    try:
                        node_id = str(fa.get('node_id') or '').strip()
                    except Exception:
                        node_id = ''
                    try:
                        if i < len(chain_nodes or []):
                            n = chain_nodes[i] if isinstance(chain_nodes[i], dict) else {}
                            node_name = str(n.get('name') or '').strip()
                    except Exception:
                        node_name = ''
                    reason = 'disabled' if disabled else 'not found/enabled'
                    missing_or_disabled.append({
                        'index': i,
                        'node_id': node_id,
                        'node_name': node_name,
                        'generator_id': generator_id,
                        'type': assignment_type,
                        'reason': reason,
                    })
        except Exception:
            missing_or_disabled = []

        if missing_or_disabled:
            bad = missing_or_disabled[0]
            msg = f"Generator {bad.get('generator_id')} ({bad.get('type')}) is {bad.get('reason')}."
            try:
                nid = str(bad.get('node_id') or '').strip()
                if nid:
                    msg += f" Node: {nid}"
                nm = str(bad.get('node_name') or '').strip()
                if nm:
                    msg += f" ({nm})"
            except Exception:
                pass
            return jsonify({
                'ok': False,
                'error': msg,
                'details': missing_or_disabled,
                'scenario': scenario_label or scenario_norm,
                'length': len(chain_nodes or []),
                'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or '')} for n in (chain_nodes or []) if isinstance(n, dict)],
                'flag_assignments': flag_assignments,
            }), 422

    flag_seed_epoch: int | None = None
    try:
        candidate_paths: list[str] = []
        if base_plan_path:
            candidate_paths.append(str(base_plan_path))
        if preview_plan_path:
            candidate_paths.append(str(preview_plan_path))
        for path in candidate_paths:
            if not path:
                continue
            try:
                abs_path = os.path.abspath(path)
            except Exception:
                abs_path = path
            if abs_path and os.path.exists(abs_path):
                try:
                    flag_seed_epoch = int(os.path.getmtime(abs_path))
                    break
                except Exception:
                    continue
    except Exception:
        flag_seed_epoch = None
    if flag_seed_epoch is None:
        try:
            flag_seed_epoch = int(time.time())
        except Exception:
            flag_seed_epoch = None

    def _all_input_names_of(gen: dict[str, Any]) -> set[str]:
        names: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    nm = str(inp.get('name') or '').strip()
                    if nm:
                        names.add(nm)
        except Exception:
            pass
        return names

    def _required_input_names_of(gen: dict[str, Any]) -> set[str]:
        names: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    nm = str(inp.get('name') or '').strip()
                    if not nm:
                        continue
                    if inp.get('required') is False:
                        continue
                    names.add(nm)
        except Exception:
            pass
        return names

    def _flow_default_generator_config(assignment: dict[str, Any], *, seed_val: Any, occurrence_idx: int = 0) -> dict[str, Any]:
        """Synthesize deterministic default inputs for the seeded generators."""
        node_id = str(assignment.get('node_id') or '').strip()
        gen_id = str(assignment.get('id') or '').strip()
        base_seed = str(seed_val if seed_val not in (None, '') else '0')
        flag_seed = str(flag_seed_epoch if flag_seed_epoch not in (None, '') else '0')
        try:
            base_seed = f"{base_seed}:{flag_seed}"
        except Exception:
            pass
        return {
            # Allow duplicates only when seeds differ per occurrence.
            'seed': _flow_generator_seed(
                base_seed=base_seed,
                scenario_norm=scenario_norm,
                node_id=node_id,
                gen_id=gen_id,
                occurrence_idx=int(occurrence_idx or 0),
            ),
            'flag_prefix': 'FLAG',
            'flag_seed': f"{flag_seed}",
            'secret': f"FLOWSECRET_{base_seed}_{scenario_norm}_{node_id}_{flag_seed}",
            'env_name': f"env_{scenario_norm}_{node_id}",
            'challenge': f"challenge_{scenario_norm}_{node_id}",
            'username_prefix': 'user',
            'key_len': 16,
        }

    def _flow_try_run_generator(
        generator_id: str,
        *,
        out_dir: str,
        config: dict[str, Any],
        kind: str = 'flag-generator',
        timeout_s: int = 120,
        inject_files_override: list[str] | None = None,
    ) -> tuple[bool, str, str | None, str | None, str | None]:
        """Best-effort run of scripts/run_flag_generator.py.

        Returns: (ok, note_or_error, manifest_path, stdout_tail, stderr_tail)
        """
        try:
            repo_root = _get_repo_root()
            runner_path = os.path.join(repo_root, 'scripts', 'run_flag_generator.py')
            if not os.path.exists(runner_path):
                return False, 'runner script not found', None, None, None

            cmd = [
                sys.executable or 'python',
                runner_path,
                '--kind',
                str(kind or 'flag-generator'),
                '--generator-id',
                generator_id,
                '--out-dir',
                out_dir,
                '--config',
                json.dumps(config, ensure_ascii=False),
                '--repo-root',
                repo_root,
            ]
            env = None
            try:
                if isinstance(inject_files_override, list):
                    env = dict(os.environ)
                    env['CORETG_INJECT_FILES_JSON'] = json.dumps(list(inject_files_override))
            except Exception:
                env = None

            p = subprocess.run(
                cmd,
                cwd=repo_root,
                check=False,
                capture_output=True,
                text=True,
                timeout=max(1, int(timeout_s or 120)),
                env=env,
            )
            manifest_path = os.path.join(out_dir, 'outputs.json')
            stdout_tail = (p.stdout or '').strip()[-4000:]
            stderr_tail = (p.stderr or '').strip()[-4000:]
            if p.returncode != 0:
                err = (p.stderr or p.stdout or '').strip()
                if err:
                    err = err[-800:]
                return False, f'generator failed (rc={p.returncode}): {err}', (manifest_path if os.path.exists(manifest_path) else None), stdout_tail, stderr_tail
            if os.path.exists(manifest_path):
                return True, 'ok', manifest_path, stdout_tail, stderr_tail
            try:
                app.logger.warning('[flow.generator] outputs.json missing for generator=%s kind=%s out_dir=%s stdout_tail=%s stderr_tail=%s',
                                   generator_id,
                                   kind,
                                   out_dir,
                                   (p.stdout or '').strip()[-400:],
                                   (p.stderr or '').strip()[-400:])
            except Exception:
                pass
            return True, 'ok (no outputs.json)', None, stdout_tail, stderr_tail
        except subprocess.TimeoutExpired:
            return False, 'generator timed out', None, None, None
        except Exception as exc:
            return False, f'generator exception: {exc}', None, None, None

    def _flow_try_run_generator_remote(
        generator_id: str,
        *,
        out_dir: str,
        config: dict[str, Any],
        kind: str = 'flag-generator',
        timeout_s: int = 120,
        inject_files_override: list[str] | None = None,
        core_cfg: dict[str, Any],
        repo_dir: str,
    ) -> tuple[bool, str, str | None, dict[str, Any] | None, str | None, str | None]:
        """Run generator on CORE VM and return outputs map when available."""
        try:
            timeout_literal = str(int(timeout_s or 120))
        except Exception:
            timeout_literal = '120'
        try:
            cfg_json = json.dumps(config or {}, ensure_ascii=False)
        except Exception:
            cfg_json = '{}'
        sudo_pw = None
        try:
            sudo_pw = str(core_cfg.get('ssh_password') or '').strip() if isinstance(core_cfg, dict) else None
        except Exception:
            sudo_pw = None
        inject_json = None
        try:
            if isinstance(inject_files_override, list):
                inject_json = json.dumps(list(inject_files_override))
        except Exception:
            inject_json = None
        script = (
            "import json, os, subprocess, sys\n"
            f"REPO={json.dumps(str(repo_dir))}\n"
            f"OUT={json.dumps(str(out_dir))}\n"
            f"GEN={json.dumps(str(generator_id))}\n"
            f"KIND={json.dumps(str(kind or 'flag-generator'))}\n"
            f"CFG={json.dumps(cfg_json)}\n"
            f"INJECT={inject_json if inject_json is not None else 'None'}\n"
            "os.makedirs(OUT, exist_ok=True)\n"
            "runner=os.path.join(REPO,'scripts','run_flag_generator.py')\n"
            "env=os.environ.copy()\n"
            "env['CORETG_DOCKER_USE_SUDO']='1'\n"
            "env['CORETG_DOCKER_HOST_NETWORK']='1'\n"
            f"SUDO_PW={json.dumps(str(sudo_pw or ''))}\n"
            "if SUDO_PW:\n"
            "  env['CORETG_DOCKER_SUDO_PASSWORD']=SUDO_PW\n"
            "if INJECT is not None:\n"
            "  try:\n"
            "    env['CORETG_INJECT_FILES_JSON']=json.dumps(json.loads(INJECT))\n"
            "  except Exception:\n"
            "    env['CORETG_INJECT_FILES_JSON']=INJECT\n"
            "cmd=[sys.executable, runner, '--kind', KIND, '--generator-id', GEN, '--out-dir', OUT, '--config', CFG, '--repo-root', REPO]\n"
            f"p=subprocess.run(cmd, cwd=REPO, env=env, check=False, capture_output=True, text=True, timeout=max(1, int({timeout_literal})))\n"
            "manifest=os.path.join(OUT,'outputs.json')\n"
            "outputs=None\n"
            "if os.path.exists(manifest):\n"
            "  try:\n"
            "    with open(manifest,'r',encoding='utf-8') as f:\n"
            "      m=json.load(f) or {}\n"
            "    outputs=m.get('outputs') if isinstance(m, dict) else None\n"
            "  except Exception:\n"
            "    outputs=None\n"
            "if outputs is None:\n"
            "  try:\n"
            "    flag_path=os.path.join(OUT,'flag.txt')\n"
            "    if os.path.exists(flag_path):\n"
            "      flag_val=open(flag_path,'r',encoding='utf-8',errors='ignore').read().strip()\n"
            "      if flag_val:\n"
            "        outputs={'Flag(flag_id)': flag_val, 'flag': flag_val}\n"
            "  except Exception:\n"
            "    outputs=outputs\n"
            "print(json.dumps({\n"
            "  'ok': bool(p.returncode==0),\n"
            "  'rc': int(p.returncode or 0),\n"
            "  'stdout': (p.stdout or '')[-4000:],\n"
            "  'stderr': (p.stderr or '')[-4000:],\n"
            "  'manifest': manifest if os.path.exists(manifest) else None,\n"
            "  'outputs': outputs,\n"
            "}))\n"
        )
        try:
            payload = _run_remote_python_json(
                core_cfg,
                script,
                logger=app.logger,
                label=f'flow.generator.remote.{generator_id}',
                timeout=max(30.0, float(timeout_s or 120)),
            )
        except Exception as exc:
            return False, f'remote generator exception: {exc}', None, None, None, None

        ok = bool(payload.get('ok')) if isinstance(payload, dict) else False
        rc = payload.get('rc') if isinstance(payload, dict) else None
        stdout = str(payload.get('stdout') or '') if isinstance(payload, dict) else ''
        stderr = str(payload.get('stderr') or '') if isinstance(payload, dict) else ''
        manifest_path = payload.get('manifest') if isinstance(payload, dict) else None
        outputs = payload.get('outputs') if isinstance(payload, dict) else None
        note = 'ok'
        if not ok:
            tail = (stderr or stdout).strip()
            note = f'remote generator failed (rc={rc}): {tail[-800:] if tail else "(no output)"}'
        elif not manifest_path:
            tail = (stderr or stdout).strip()
            if tail:
                note = f'no outputs.json (stdout/stderr): {tail[-800:]}'
            else:
                note = 'no outputs.json'
        return ok, note, (str(manifest_path) if manifest_path else None), (outputs if isinstance(outputs, dict) else None), (stdout or ''), (stderr or '')

    def _redact_kv_for_ui(kv: Any) -> dict[str, Any]:
        """Best-effort redaction for UI display.

        The Flow UI is an organizer/admin surface, but we still avoid blasting obvious
        secrets (and flag material) into the main chain tables by default.
        """
        if not isinstance(kv, dict) or not kv:
            return {}

        # Redaction disabled for Flow UI (show full values).
        return dict(kv)

        # (Unreachable) retained for reference.
    def _preview_host_ip4(host: dict) -> str:
        """Best-effort: return the primary IPv4 address for a preview host."""
        try:
            ip4 = host.get('ip4')
            if isinstance(ip4, str) and _first_valid_ipv4(ip4):
                return _first_valid_ipv4(ip4)
        except Exception:
            pass
        for key in ('ipv4', 'ip', 'ip_addr', 'address'):
            try:
                v = host.get(key)
            except Exception:
                v = None
            ip_str = _first_valid_ipv4(v)
            if ip_str:
                return ip_str
        return ''

    host_by_id: dict[str, dict[str, Any]] = {}
    try:
        hosts = preview.get('hosts') or []
        if isinstance(hosts, list):
            for h in hosts:
                if not isinstance(h, dict):
                    continue
                host_by_id[str(h.get('node_id') or '').strip()] = h
    except Exception:
        host_by_id = {}

    if run_generators:
        try:

            # Flow has a "god-eye" view of generator outputs across the chain. As we run each
            # generator, we capture outputs.json and feed those values into subsequent generator
            # configs when they declare matching input names (e.g. Knowledge(ip), Credential(user, password)).
            flow_context: dict[str, Any] = {}
            # Track absolute paths of file-like artifacts produced by generators
            # so we can resolve inject_files for downstream consumers.
            artifact_context: dict[str, str] = {}

            def _apply_outputs_to_hint_text(text_in: str, outs: dict[str, Any]) -> str:
                """Replace {{OUTPUT.key}} and {{OUTPUT.key:transform}} placeholders."""
                try:
                    text = str(text_in or '')
                except Exception:
                    return str(text_in or '')
                if not text or not isinstance(outs, dict) or not outs:
                    return text
                try:
                    pattern = re.compile(r"\{\{OUTPUT\.([^}:]+?)(?::([^}]+?))?\}\}")
                except Exception:
                    return text

                def _render_value(val: Any) -> str:
                    if isinstance(val, (dict, list)):
                        return json.dumps(val, ensure_ascii=False)
                    return str(val)

                def _transform(val: Any, tf: str) -> str:
                    t = (tf or '').strip().lower()
                    s = _render_value(val)
                    if not t:
                        return s
                    if t in {'last_octet', 'octet4'}:
                        try:
                            parts = s.strip().split('.')
                            if len(parts) == 4:
                                return parts[3]
                        except Exception:
                            pass
                        return s
                    if t in {'subnet24', 'cidr24'}:
                        try:
                            parts = s.strip().split('.')
                            if len(parts) == 4:
                                return f"{parts[0]}.{parts[1]}.{parts[2]}.0/24"
                        except Exception:
                            pass
                        return s
                    if t in {'redact', 'masked'}:
                        try:
                            parts = s.strip().split('.')
                            if len(parts) == 4:
                                return f"{parts[0]}.{parts[1]}.{parts[2]}.x"
                        except Exception:
                            pass
                        return s
                    return s

                def _repl(match: re.Match) -> str:
                    key = (match.group(1) or '').strip()
                    tf = (match.group(2) or '').strip()
                    if not key:
                        return match.group(0)
                    if key not in outs:
                        return match.group(0)
                    return _transform(outs.get(key), tf)

                try:
                    return pattern.sub(_repl, text)
                except Exception:
                    return text

            def _apply_node_placeholders(text_in: str, *, node_ip4: str) -> str:
                """Replace legacy placeholders like <node-ip> with the preview host IP."""
                try:
                    text = str(text_in or '')
                except Exception:
                    return str(text_in or '')
                if not text or not node_ip4:
                    return text
                for token in ('<node-ip>', '<node_ip>', '<host-ip>', '<host_ip>', '<target-ip>', '<target_ip>'):
                    try:
                        text = text.replace(token, node_ip4)
                    except Exception:
                        continue
                return text

            generation_failures: list[dict[str, Any]] = []
            generation_skipped: list[dict[str, Any]] = []
            generator_runs: list[dict[str, Any]] = []
            created_run_dirs: list[str] = []
            failed_run_dirs: list[str] = []
            progress_log: list[str] = []

            def _flow_progress(msg: str) -> None:
                try:
                    progress_log.append(str(msg))
                except Exception:
                    pass
                try:
                    app.logger.info('[flow.progress] %s', msg)
                except Exception:
                    pass
            seen_flag_values: set[str] = set()

            deadline = (started_at + float(total_timeout_s)) if total_timeout_s is not None else None
            occurrence_ctr: dict[tuple[str, str], int] = {}
            total_assignments = len([x for x in (flag_assignments or []) if isinstance(x, dict)])
            run_index = 0
            for fa in (flag_assignments or []):
                if not isinstance(fa, dict):
                    continue
                cid = str(fa.get('node_id') or '').strip()
                h = host_by_id.get(cid)
                if not h or not isinstance(h, dict):
                    continue
                preview_ip4 = _preview_host_ip4(h)

                meta_h = h.get('metadata')
                if not isinstance(meta_h, dict):
                    meta_h = {}
                    h['metadata'] = meta_h

                generator_id = str(fa.get('id') or '').strip()
                assignment_type = str(fa.get('type') or '').strip() or 'flag-generator'
                generator_catalog = str(fa.get('generator_catalog') or '').strip() or 'flag_generators'
                seed_val = preview.get('seed') if isinstance(preview, dict) else None

                occ_key = (cid, generator_id)
                occ = int(occurrence_ctr.get(occ_key, 0) or 0)
                occurrence_ctr[occ_key] = occ + 1

                cfg_full = _flow_default_generator_config(fa, seed_val=seed_val, occurrence_idx=occ)

                # Ensure per-use uniqueness by mixing a runtime timestamp into the seed/secret.
                try:
                    seed_ts = int(time.time() * 1000.0)
                except Exception:
                    seed_ts = None
                if seed_ts is not None:
                    try:
                        cfg_full['seed_ts'] = seed_ts
                        cfg_full['seed'] = f"{cfg_full.get('seed')}:{seed_ts}"
                        cfg_full['secret'] = f"{cfg_full.get('secret')}:{seed_ts}"
                    except Exception:
                        pass

                # Provide per-node network context. Some generators output Knowledge(ip) and then
                # hint templates reference it via {{OUTPUT.Knowledge(ip)}}; make it match Preview.
                try:
                    if preview_ip4:
                        cfg_full.setdefault('Knowledge(ip)', preview_ip4)
                        cfg_full.setdefault('target_ip', preview_ip4)
                        cfg_full.setdefault('host_ip', preview_ip4)
                        cfg_full.setdefault('ip4', preview_ip4)
                        cfg_full.setdefault('ipv4', preview_ip4)
                except Exception:
                    pass

                # Provide basic per-node context for generators that want it.
                try:
                    node_name_val = str(h.get('name') or '').strip()
                    if node_name_val:
                        cfg_full['node_name'] = node_name_val
                except Exception:
                    pass

                # Apply any user-provided input overrides persisted in Flow metadata.
                try:
                    raw_overrides = fa.get('config_overrides') or fa.get('inputs_overrides') or fa.get('input_overrides')
                    if isinstance(raw_overrides, dict) and raw_overrides:
                        for k, v in raw_overrides.items():
                            kk = str(k or '').strip()
                            if not kk:
                                continue
                            # Preserve explicit clears; generators may treat empty/null specially.
                            cfg_full[kk] = v
                        # Ensure the UI sees a normalized dict.
                        fa['config_overrides'] = dict(raw_overrides)
                except Exception:
                    pass

                cfg = cfg_full
                inputs_mismatch: dict[str, Any] = {}
                try:
                    gen_def = _gen_by_id.get(generator_id)
                    if isinstance(gen_def, dict):
                        allowed = _all_input_names_of(gen_def)

                        # Apply manifest defaults for any inputs not already provided.
                        try:
                            inputs_def = gen_def.get('inputs')
                            if isinstance(inputs_def, list):
                                for inp in inputs_def:
                                    if not isinstance(inp, dict):
                                        continue
                                    name = str(inp.get('name') or '').strip()
                                    if not name:
                                        continue
                                    if 'default' not in inp:
                                        continue
                                    if name in cfg_full:
                                        v = cfg_full.get(name)
                                        if v is not None and (not isinstance(v, str) or v.strip()):
                                            continue
                                    cfg_full[name] = inp.get('default')
                        except Exception:
                            pass

                        declared_required = None
                        try:
                            if isinstance(fa.get('input_fields_required'), list):
                                declared_required = {str(x).strip() for x in (fa.get('input_fields_required') or []) if str(x).strip()}
                        except Exception:
                            declared_required = None
                        if declared_required is None:
                            declared_required = _required_input_names_of(gen_def)

                        # Inject prior outputs into this generator's config, but only for inputs it declares.
                        try:
                            if allowed and flow_context:
                                blocked_keys = {'Flag(flag_id)', 'flag'}
                                for k in allowed:
                                    if k in cfg_full:
                                        continue
                                    if k in blocked_keys and k not in (declared_required or set()):
                                        continue
                                    if k in flow_context:
                                        cfg_full[k] = flow_context[k]
                        except Exception:
                            pass

                        # If the generator declares inputs, only pass those (keeps Flow configs relevant).
                        # HOWEVER: some catalogs/definitions can drift; if the assignment marks an input as
                        # required (e.g., seed, node_name), ensure it is passed even if not in `allowed`.
                        cfg_to_pass = cfg_full
                        if allowed:
                            keep = set(allowed)
                            try:
                                keep |= set(declared_required or set())
                            except Exception:
                                pass
                            try:
                                keep |= set(_flow_synthesized_inputs())
                            except Exception:
                                pass
                            cfg_to_pass = {k: v for k, v in cfg_full.items() if k in keep}
                        cfg = cfg_to_pass

                        try:
                            provided_keys = {str(k).strip() for k in (cfg_to_pass or {}).keys() if str(k).strip()}
                        except Exception:
                            provided_keys = set()

                        missing_required = sorted([k for k in (declared_required or set()) if k not in provided_keys])

                        unset_required: list[str] = []
                        try:
                            for k in sorted(list(declared_required or set())):
                                if k not in (cfg_to_pass or {}):
                                    continue
                                v = (cfg_to_pass or {}).get(k)
                                if v is None:
                                    unset_required.append(k)
                                elif isinstance(v, str) and (not v.strip()):
                                    unset_required.append(k)
                        except Exception:
                            unset_required = []

                        dropped_keys: list[str] = []
                        try:
                            if allowed:
                                dropped_keys = sorted([k for k in (cfg_full or {}).keys() if k not in (cfg_to_pass or {})])
                                # Many keys are synthesized by Flow and carried in cfg_full for
                                # convenience (e.g., hint rendering). Generators only receive
                                # inputs they declare, so these show up as "dropped" even though
                                # it's expected. Don't surface them as mismatches.
                                try:
                                    synthesized = set(_flow_synthesized_inputs())
                                except Exception:
                                    synthesized = set()
                                dropped_keys = [k for k in dropped_keys if str(k) not in synthesized]
                        except Exception:
                            dropped_keys = []

                        inputs_mismatch = {
                            'declared_required': sorted(list(declared_required or set())),
                            'provided': sorted(list(provided_keys)),
                            'missing_required': missing_required,
                            'unset_required': unset_required,
                            'dropped': dropped_keys,
                            # Consider dropped keys a mismatch so the UI can surface it.
                            'ok': (not missing_required and not unset_required and not dropped_keys),
                        }
                except Exception:
                    cfg = cfg_full
                    inputs_mismatch = {}

                flow_out_dir = ''
                ok_run = False
                note = ''
                manifest_path = None
                actual_output_keys: list[str] = []
                declared_output_keys: list[str] = []
                try:
                    # IMPORTANT: outputs.json is a runtime manifest of output KEYS.
                    # Use output_fields (runtime) when available; fall back to legacy outputs.
                    declared_src = (fa.get('output_fields') if isinstance(fa.get('output_fields'), list) else None)
                    if declared_src is None:
                        declared_src = (fa.get('outputs') if isinstance(fa.get('outputs'), list) else [])
                    declared_output_keys = sorted([str(x) for x in (declared_src or []) if str(x).strip()])
                    # Ensure artifact produces are treated as declared outputs.
                    prod_src = (fa.get('produces') if isinstance(fa.get('produces'), list) else [])
                    prod_keys = [str(x).strip() for x in (prod_src or []) if str(x).strip()]
                    if prod_keys:
                        declared_output_keys = sorted(set(declared_output_keys) | set(prod_keys))
                except Exception:
                    declared_output_keys = []
                mismatch: dict[str, Any] = {}

                # UI convenience: expose resolved inputs (redacted) even if generator execution
                # is skipped or fails.
                try:
                    fa['resolved_inputs'] = _redact_kv_for_ui(cfg)
                    try:
                        if isinstance(fa.get('resolved_inputs'), dict) and isinstance(cfg_full, dict):
                            if 'Knowledge(ip)' in cfg_full and 'Knowledge(ip)' not in fa['resolved_inputs']:
                                fa['resolved_inputs']['Knowledge(ip)'] = cfg_full.get('Knowledge(ip)')
                    except Exception:
                        pass
                except Exception:
                    pass

                # Apply inject-files override early so downstream hint.txt allowlist uses it.
                try:
                    inj_ovr = (fa or {}).get('inject_files_override')
                    if isinstance(inj_ovr, list):
                        cleaned = [str(x or '').strip() for x in (inj_ovr or [])]
                        cleaned = [x for x in cleaned if x]
                        fa['inject_files'] = cleaned
                except Exception:
                    pass

                # If the user provided an explicit FLAG override, expose it even before
                # generator execution (so UI can show Resolved outputs / Flag).
                try:
                    flag_override = str((fa or {}).get('flag_override') or '').strip()
                    if flag_override:
                        fa['flag_value'] = flag_override
                        fa['resolved_outputs'] = _redact_kv_for_ui({'Flag(flag_id)': flag_override})
                except Exception:
                    pass

                # If the user provided output overrides, expose them even before generator
                # execution (useful when running best-effort or when generator outputs are unknown).
                try:
                    out_ovr = (fa or {}).get('output_overrides')
                    if isinstance(out_ovr, dict) and out_ovr:
                        cleaned: dict[str, Any] = {}
                        for k, v in (out_ovr or {}).items():
                            kk = str(k or '').strip()
                            if not kk:
                                continue
                            cleaned[kk] = v
                        if cleaned:
                            fa['resolved_outputs'] = _redact_kv_for_ui(cleaned)
                            # Prefer showing overridden flag value in the UI's Flag row.
                            try:
                                if isinstance(cleaned.get('Flag(flag_id)'), str) and str(cleaned.get('Flag(flag_id)') or '').strip():
                                    fa['flag_value'] = str(cleaned.get('Flag(flag_id)') or '').strip()
                                elif isinstance(cleaned.get('flag'), str) and str(cleaned.get('flag') or '').strip():
                                    fa['flag_value'] = str(cleaned.get('flag') or '').strip()
                            except Exception:
                                pass
                except Exception:
                    pass

                try:
                    run_index += 1
                except Exception:
                    run_index = run_index
                try:
                    if generator_id:
                        if deadline is not None and time.monotonic() >= deadline:
                            generation_skipped.append({
                                'node_id': cid,
                                'node_name': str(h.get('name') or ''),
                                'generator_id': generator_id,
                                'reason': 'time budget exceeded',
                            })
                            break

                        try:
                            _flow_progress(
                                f"Running generator {run_index}/{total_assignments}: {generator_id} @ {str(h.get('name') or '')}"
                            )
                        except Exception:
                            pass

                        flow_run_id = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:10]
                        # IMPORTANT: stage under /tmp/vulns so the existing sync pipeline can ship
                        # these artifacts to the CORE host (where docker-compose paths resolve).
                        subdir = 'flag_node_generators_runs' if assignment_type == 'flag-node-generator' else 'flag_generators_runs'
                        flow_out_dir = os.path.join('/tmp/vulns', subdir, f"flow-{scenario_norm}-{flow_run_id}")
                        if not flow_run_remote:
                            os.makedirs(flow_out_dir, exist_ok=True)
                        try:
                            created_run_dirs.append(str(flow_out_dir))
                        except Exception:
                            pass

                        # If any config inputs are declared as file/path types, and the user provided
                        # an override pointing to a previously-uploaded server file, stage it into
                        # the run's inputs/ directory and rewrite the config to /inputs/<filename>.
                        if not flow_run_remote:
                            try:
                                gen_def = _gen_by_id.get(generator_id)
                                if isinstance(gen_def, dict) and isinstance(cfg, dict):
                                    _flow_stage_file_inputs_for_generator(cfg, gen_def, run_dir=str(flow_out_dir))
                            except Exception:
                                pass

                        remaining = None
                        if deadline is not None:
                            try:
                                remaining = int(max(1.0, deadline - time.monotonic()))
                            except Exception:
                                remaining = 1
                        gen_timeout_s = 120
                        if remaining is not None:
                            gen_timeout_s = min(gen_timeout_s, remaining)

                        def _split_inject_spec(raw: str) -> tuple[str, str]:
                            text = str(raw or '').strip()
                            if not text:
                                return '', ''
                            for sep in ('->', '=>'):
                                if sep in text:
                                    left, right = text.split(sep, 1)
                                    return left.strip(), right.strip()
                            return text, ''

                        def _stage_inject_uploads(injects: list[str], run_dir: str) -> list[str]:
                            if not injects:
                                return []
                            try:
                                repo_root = _get_repo_root()
                            except Exception:
                                repo_root = os.getcwd()
                            allowed_root = os.path.abspath(_flow_inject_uploads_dir())
                            art_dir = os.path.join(run_dir, 'artifacts')
                            os.makedirs(art_dir, exist_ok=True)

                            out_list: list[str] = []
                            for raw in injects:
                                src_raw, dest_raw = _split_inject_spec(str(raw))
                                src = str(src_raw or '').strip()
                                if src.startswith('upload:'):
                                    src = src[len('upload:'):].strip()
                                if src:
                                    try:
                                        abs_src = os.path.abspath(src)
                                    except Exception:
                                        abs_src = ''
                                else:
                                    abs_src = ''

                                if abs_src and os.path.exists(abs_src):
                                    try:
                                        if os.path.commonpath([allowed_root, abs_src]) == allowed_root:
                                            base = os.path.basename(abs_src.rstrip('/')) or 'upload'
                                            dest_path = os.path.join(art_dir, base)
                                            if os.path.isdir(abs_src):
                                                shutil.copytree(abs_src, dest_path, dirs_exist_ok=True)
                                            else:
                                                shutil.copy2(abs_src, dest_path)
                                            new_src = f"artifacts/{base}"
                                            if dest_raw:
                                                out_list.append(f"{new_src} -> {dest_raw}")
                                            else:
                                                out_list.append(new_src)
                                            continue
                                    except Exception:
                                        pass

                                # Fallback: keep original entry.
                                out_list.append(str(raw))
                            return out_list

                        # Resolve inject_files using prior generator outputs (flow key -> absolute path).
                        try:
                            raw_injects = fa.get('inject_files')
                            if isinstance(raw_injects, list):
                                resolved_injects = []
                                for item in raw_injects:
                                    s = str(item or '').strip()
                                    if not s:
                                        continue
                                    
                                    # Handle "SRC -> DEST" syntax.
                                    src, dest = _split_inject_spec(s)
                                    final_src = src
                                    
                                    # If the source matches a known artifact from a prior generator,
                                    # substitute the absolute path.
                                    if src in artifact_context:
                                        final_src = artifact_context[src]
                                    elif src in flow_context:
                                        # Fallback: if flow_context has a string value that looks like a path, try it?
                                        # For now, trust artifact_context which is explicitly validated against disk.
                                        pass
                                        
                                    if dest:
                                        resolved_injects.append(f"{final_src} -> {dest}")
                                    else:
                                        resolved_injects.append(final_src)
                                fa['inject_files'] = resolved_injects
                        except Exception:
                            pass

                        effective_injects = None
                        if not flow_run_remote:
                            try:
                                eff = fa.get('inject_files') if isinstance(fa, dict) else None
                                if isinstance(eff, list) and eff:
                                    effective_injects = _stage_inject_uploads(list(eff), str(flow_out_dir))
                                    fa['inject_files'] = list(effective_injects)
                            except Exception:
                                effective_injects = None

                        manifest_outputs: dict[str, Any] | None = None
                        run_stdout: str | None = None
                        run_stderr: str | None = None
                        if flow_run_remote and flow_remote_repo_dir and isinstance(flow_core_cfg, dict):
                            ok_run, note, manifest_path, manifest_outputs, run_stdout, run_stderr = _flow_try_run_generator_remote(
                                generator_id,
                                out_dir=flow_out_dir,
                                config=cfg,
                                kind=assignment_type,
                                timeout_s=gen_timeout_s,
                                inject_files_override=effective_injects,
                                core_cfg=flow_core_cfg,
                                repo_dir=flow_remote_repo_dir,
                            )
                        else:
                            ok_run, note, manifest_path, run_stdout, run_stderr = _flow_try_run_generator(
                                generator_id,
                                out_dir=flow_out_dir,
                                config=cfg,
                                kind=assignment_type,
                                timeout_s=gen_timeout_s,
                                inject_files_override=effective_injects,
                            )

                            if ok_run and not manifest_path and flow_out_dir:
                                try:
                                    flag_path = os.path.join(flow_out_dir, 'flag.txt')
                                    if os.path.exists(flag_path):
                                        with open(flag_path, 'r', encoding='utf-8', errors='ignore') as f:
                                            flag_val = (f.read() or '').strip()
                                        if flag_val:
                                            manifest_outputs = {'Flag(flag_id)': flag_val, 'flag': flag_val}
                                except Exception:
                                    manifest_outputs = manifest_outputs

                        try:
                            app.logger.info(
                                '[flow.generator] node=%s generator=%s ok=%s note=%s manifest=%s out_dir=%s',
                                cid,
                                generator_id,
                                bool(ok_run),
                                str(note or ''),
                                str(manifest_path or ''),
                                str(flow_out_dir or ''),
                            )
                        except Exception:
                            pass

                        log_path = None
                        try:
                            logs_root = os.path.join(_outputs_dir(), 'logs', 'flow_generator_logs')
                            os.makedirs(logs_root, exist_ok=True)
                            safe_gen = re.sub(r'[^A-Za-z0-9_.-]+', '_', str(generator_id or 'generator')).strip('_') or 'generator'
                            safe_node = re.sub(r'[^A-Za-z0-9_.-]+', '_', str(h.get('name') or cid or 'node')).strip('_') or 'node'
                            safe_run = re.sub(r'[^A-Za-z0-9_.-]+', '_', str(flow_run_id or uuid.uuid4().hex))
                            filename = f"{safe_gen}_{safe_node}_{safe_run}.log"
                            log_path = os.path.join(logs_root, filename)
                            with open(log_path, 'w', encoding='utf-8') as lf:
                                lf.write(f"generator_id={generator_id}\n")
                                lf.write(f"node={safe_node}\n")
                                lf.write(f"ok={bool(ok_run)}\n")
                                lf.write(f"note={str(note or '')}\n\n")
                                if isinstance(run_stdout, str) and run_stdout.strip():
                                    lf.write("--- stdout ---\n")
                                    lf.write(run_stdout.strip())
                                    lf.write("\n")
                                if isinstance(run_stderr, str) and run_stderr.strip():
                                    lf.write("--- stderr ---\n")
                                    lf.write(run_stderr.strip())
                                    lf.write("\n")
                        except Exception:
                            log_path = None

                        try:
                            generator_runs.append({
                                'node_id': cid,
                                'node_name': str(h.get('name') or ''),
                                'generator_id': generator_id,
                                'type': assignment_type,
                                'ok': bool(ok_run),
                                'note': str(note or ''),
                                'out_dir': str(flow_out_dir or ''),
                                'manifest': str(manifest_path or ''),
                                'stdout': (run_stdout or '')[-4000:] if isinstance(run_stdout, str) else '',
                                'stderr': (run_stderr or '')[-4000:] if isinstance(run_stderr, str) else '',
                                'log_path': str(log_path or ''),
                            })
                        except Exception:
                            pass

                        try:
                            _flow_progress(
                                f"Completed generator {run_index}/{total_assignments}: {generator_id} -> {'ok' if ok_run else 'failed'}"
                            )
                        except Exception:
                            pass

                        if ok_run and (not manifest_path) and declared_output_keys and not manifest_outputs:
                            ok_run = False
                            note = f"outputs.json missing for generator={generator_id}"

                        if not ok_run:
                            generation_failures.append({
                                'node_id': cid,
                                'node_name': str(h.get('name') or ''),
                                'generator_id': generator_id,
                                'error': str(note or 'generator execution failed'),
                                'run_dir': str(flow_out_dir or ''),
                            })
                            try:
                                if flow_out_dir:
                                    failed_run_dirs.append(str(flow_out_dir))
                            except Exception:
                                pass

                        # If the generator produced a manifest, capture the actual output keys.
                        if ok_run and (manifest_outputs is not None or (manifest_path and os.path.exists(manifest_path))):
                            try:
                                outs = None
                                if isinstance(manifest_outputs, dict):
                                    outs = manifest_outputs
                                elif manifest_path and os.path.exists(manifest_path):
                                    with open(manifest_path, 'r', encoding='utf-8') as f:
                                        m = json.load(f) or {}
                                    outs = m.get('outputs') if isinstance(m, dict) else None
                                if isinstance(outs, dict):
                                    # Apply user override for FLAG value (if provided).
                                    try:
                                        flag_override = str((fa or {}).get('flag_override') or '').strip()
                                        if flag_override:
                                            outs['Flag(flag_id)'] = flag_override
                                            outs['flag'] = flag_override
                                    except Exception:
                                        pass

                                    # Apply user output overrides (if provided).
                                    try:
                                        out_ovr = (fa or {}).get('output_overrides')
                                        if isinstance(out_ovr, dict) and out_ovr:
                                            for k, v in (out_ovr or {}).items():
                                                kk = str(k or '').strip()
                                                if not kk:
                                                    continue
                                                outs[kk] = v
                                    except Exception:
                                        pass
                                    # Normalize legacy 'flag' output to the fact-style key.
                                    try:
                                        if isinstance(outs, dict) and 'flag' in outs and 'Flag(flag_id)' not in outs:
                                            outs['Flag(flag_id)'] = outs.get('flag')
                                    except Exception:
                                        pass
                                    # Ensure any IP-like outputs align with the preview host IP.
                                    # This prevents resolved hints from drifting away from Preview
                                    # even if a generator invents its own Knowledge(ip).
                                    try:
                                        if preview_ip4:
                                            ip_keys = {
                                                'Knowledge(ip)',
                                                'host.ip',
                                                'host_ip',
                                                'target_ip',
                                                'ip',
                                                'ip4',
                                                'ipv4',
                                                'address',
                                            }
                                            for k in list(outs.keys()):
                                                kk = str(k)
                                                if kk not in ip_keys:
                                                    continue
                                                old = outs.get(k)
                                                old_ip = _first_valid_ipv4(old)
                                                if old_ip and old_ip != preview_ip4:
                                                    outs[k] = preview_ip4
                                                    try:
                                                        app.logger.info(
                                                            '[flow.prepare_preview_for_execute] clamped %s=%s -> %s for node=%s',
                                                            kk,
                                                            old_ip,
                                                            preview_ip4,
                                                            cid,
                                                        )
                                                    except Exception:
                                                        pass
                                    except Exception:
                                        pass

                                    actual_output_keys = sorted([str(k) for k in outs.keys() if str(k).strip()])

                                    # Propagate outputs forward so later generators can consume concrete values.
                                    try:
                                        for k, v in outs.items():
                                            kk = str(k)
                                            if not kk:
                                                continue
                                            flow_context[kk] = v
                                    except Exception:
                                        pass

                                    # UI convenience: expose resolved outputs (redacted).
                                    try:
                                        fa['resolved_outputs'] = _redact_kv_for_ui(outs)
                                    except Exception:
                                        pass

                                    # Substitute output placeholders into hints (best-effort).
                                    try:
                                        if isinstance(fa.get('hints'), list) and fa.get('hints'):
                                            new_hints = [_apply_outputs_to_hint_text(str(t), outs) for t in (fa.get('hints') or [])]
                                            new_hints = [_apply_node_placeholders(str(t), node_ip4=preview_ip4) for t in new_hints]
                                            fa['hints'] = new_hints
                                            fa['hint'] = str(new_hints[0] or '') if new_hints else str(fa.get('hint') or '')
                                        else:
                                            hint_final = _apply_outputs_to_hint_text(str(fa.get('hint') or ''), outs)
                                            hint_final = _apply_node_placeholders(str(hint_final), node_ip4=preview_ip4)
                                            if hint_final and hint_final != str(fa.get('hint') or ''):
                                                fa['hint'] = hint_final
                                    except Exception:
                                        pass

                                    # Best-effort: if the generator emitted a flag value,
                                    # also write a plain flag.txt for easier participant discovery.
                                    try:
                                        flag_val = outs.get('Flag(flag_id)') or outs.get('flag')
                                        if not (isinstance(flag_val, str) and flag_val.strip()):
                                            try:
                                                app.logger.warning(
                                                    '[flow.generator] no flag output for generator=%s node=%s out_dir=%s keys=%s',
                                                    generator_id,
                                                    cid,
                                                    flow_out_dir,
                                                    sorted(list(outs.keys())) if isinstance(outs, dict) else [],
                                                )
                                            except Exception:
                                                pass
                                        if isinstance(flag_val, str) and flag_val.strip():
                                            flag_val_clean = flag_val.strip()
                                            if flag_val_clean in seen_flag_values:
                                                raise RuntimeError(f'duplicate flag value: {flag_val_clean}')
                                            seen_flag_values.add(flag_val_clean)
                                        if ok_run and flow_out_dir and isinstance(flag_val, str) and flag_val.strip():
                                            with open(os.path.join(flow_out_dir, 'flag.txt'), 'w', encoding='utf-8') as ff:
                                                ff.write(flag_val.strip() + "\n")
                                            # UI convenience: expose the realized flag value (runtime-only).
                                            # IMPORTANT: this must not be persisted into saved plans.
                                            try:
                                                fa['flag_value'] = flag_val.strip()
                                            except Exception:
                                                pass
                                    except RuntimeError:
                                        raise
                                    except Exception:
                                        pass
                            except RuntimeError:
                                raise
                            except Exception:
                                actual_output_keys = []

                        # Capture absolute paths for file-like outputs to support inject_files resolution.
                        try:
                            if flow_out_dir:
                                for k, v in outs.items():
                                    if not isinstance(v, str):
                                        continue
                                    # Heuristic: if the value is a filename/path and exists in the run dir,
                                    # track it as a resolvable artifact.
                                    candidate = os.path.join(flow_out_dir, v)
                                    if os.path.exists(candidate):
                                        # Strip leading slash if present in key (unlikely for well-formed keys but safe).
                                        artifact_context[str(k).strip()] = candidate
                        except Exception:
                            pass

                        # Materialize a human-readable hint file in the artifacts directory.
                        # IMPORTANT: do this after outputs.json has been applied to hints so
                        # the CORE VM sees resolved template values.
                        try:
                            # Prefer staged injected artifacts when present (inject_files allowlist).
                            # Fall back to artifacts/, then the run dir.
                            flow_mount_dir = str(flow_out_dir or '')
                            allow_hint_file = True
                            if not flow_run_remote:
                                try:
                                    if flow_out_dir:
                                        injected = os.path.join(flow_out_dir, 'injected')
                                        artifacts = os.path.join(flow_out_dir, 'artifacts')
                                        if os.path.isdir(injected):
                                            flow_mount_dir = injected
                                        elif os.path.isdir(artifacts):
                                            flow_mount_dir = artifacts
                                except Exception:
                                    flow_mount_dir = str(flow_out_dir or '')
                            else:
                                try:
                                    flow_mount_dir = posixpath.join(str(flow_out_dir or ''), 'artifacts')
                                except Exception:
                                    flow_mount_dir = str(flow_out_dir or '')

                            # Respect inject_files allowlist: only place hint.txt into the
                            # injected mount dir if the allowlist appears to include it.
                            try:
                                inj_list = fa.get('inject_files') if isinstance(fa, dict) else None
                                if isinstance(inj_list, list) and inj_list:
                                    allow_hint = False
                                    for raw in inj_list:
                                        s = str(raw or '').strip().replace('\\', '/')
                                        for sep in ('->', '=>'):
                                            if sep in s:
                                                s = s.split(sep, 1)[0].strip()
                                                break
                                        if not s:
                                            continue
                                        if s == 'hint.txt' or s.endswith('/hint.txt'):
                                            allow_hint = True
                                            break
                                    if not allow_hint:
                                        allow_hint_file = False
                            except Exception:
                                pass

                            if not allow_hint_file:
                                raise RuntimeError('hint.txt not allowlisted')

                            hint_texts: list[str] = []
                            if isinstance(fa.get('hints'), list):
                                hint_texts = [str(x or '').strip() for x in (fa.get('hints') or []) if str(x or '').strip()]
                            if not hint_texts:
                                single = str(fa.get('hint') or '').strip()
                                if single:
                                    hint_texts = [single]
                            if flow_mount_dir and hint_texts:
                                if flow_run_remote and isinstance(flow_core_cfg, dict):
                                    hint_payload = "".join(
                                        [
                                            (f"Hint {idx + 1}/{len(hint_texts)}: {ht}\n" if len(hint_texts) > 1 else f"{ht}\n")
                                            for idx, ht in enumerate(hint_texts)
                                        ]
                                    )
                                    script = (
                                        "import os\n"
                                        f"p={json.dumps(posixpath.join(flow_mount_dir, 'hint.txt'))}\n"
                                        f"d=os.path.dirname(p)\n"
                                        "os.makedirs(d, exist_ok=True)\n"
                                        f"open(p,'w',encoding='utf-8').write({json.dumps(hint_payload)})\n"
                                        "print('{}')\n"
                                    )
                                    try:
                                        _run_remote_python_json(
                                            flow_core_cfg,
                                            script,
                                            logger=app.logger,
                                            label='flow.hint.remote',
                                            timeout=20.0,
                                        )
                                    except Exception:
                                        pass
                                else:
                                    with open(os.path.join(flow_mount_dir, 'hint.txt'), 'w', encoding='utf-8') as hf:
                                        if len(hint_texts) == 1:
                                            hf.write(hint_texts[0] + "\n")
                                        else:
                                            for idx, ht in enumerate(hint_texts, start=1):
                                                hf.write(f"Hint {idx}/{len(hint_texts)}: {ht}\n")
                        except Exception:
                            pass

                        # Compare declared vs actual output keys (best-effort).
                        try:
                            if ok_run and actual_output_keys:
                                # Some generators include metadata echo outputs (e.g., node_name).
                                # These are not meaningful artifacts for Flow chaining and shouldn't
                                # trigger contract mismatch warnings.
                                ignore_actual = {
                                    'node_name',
                                    'nodename',
                                    'nodeName',
                                }
                                declared_set = set(declared_output_keys or [])
                                actual_set = set([k for k in (actual_output_keys or []) if k not in ignore_actual])
                                missing = sorted(list(declared_set - actual_set))
                                extra = sorted(list(actual_set - declared_set))
                                mismatch = {
                                    'declared': declared_output_keys,
                                    'actual': actual_output_keys,
                                    'missing': missing,
                                    'extra': extra,
                                    'ok': (not missing and not extra),
                                }
                        except Exception:
                            mismatch = {}
                except Exception as exc:
                    if 'duplicate flag value' in str(exc):
                        raise
                    ok_run, note, manifest_path = False, f'generator exception: {exc}', None
                    if generator_id:
                        generation_failures.append({
                            'node_id': cid,
                            'node_name': str(h.get('name') or ''),
                            'generator_id': generator_id,
                            'error': str(note or ''),
                            'run_dir': str(flow_out_dir or ''),
                        })

                def _normalize_inject_src_for_copy(raw_src: str, source_dir: str) -> str:
                    src = str(raw_src or '').strip()
                    if not src:
                        return ''
                    if not os.path.isabs(src):
                        return src
                    try:
                        if source_dir:
                            src_abs = os.path.abspath(src)
                            base_abs = os.path.abspath(source_dir)
                            if os.path.commonpath([src_abs, base_abs]) == base_abs:
                                return os.path.relpath(src_abs, base_abs)
                    except Exception:
                        pass
                    for marker in ('/artifacts/', '/flow_artifacts/'):
                        if marker in src:
                            return src.split(marker, 1)[1].lstrip('/')
                    return ''

                def _normalize_inject_spec_for_copy(raw: str, source_dir: str) -> str:
                    text = str(raw or '').strip()
                    if not text:
                        return ''
                    sep = '->' if '->' in text else '=>' if '=>' in text else ''
                    if sep:
                        left, right = text.split(sep, 1)
                        src_norm = _normalize_inject_src_for_copy(left.strip(), source_dir)
                        if not src_norm:
                            return ''
                        dest = right.strip()
                        return f"{src_norm} -> {dest}" if dest else src_norm
                    src_norm = _normalize_inject_src_for_copy(text, source_dir)
                    return src_norm

                def _inject_files_for_copy_from_detail(detail_list: Any, source_dir: str) -> list[str]:
                    if not isinstance(detail_list, list):
                        return []
                    out: list[str] = []
                    for entry in detail_list:
                        if not isinstance(entry, dict):
                            continue
                        resolved = str(entry.get('resolved') or '').strip()
                        path = str(entry.get('path') or '').strip()
                        if not path:
                            continue
                        dest_dir = os.path.dirname(path) if path.startswith('/') else ''
                        if dest_dir == '/' and path.startswith('/'):
                            # Single-segment absolute path (e.g., /exports) should be treated as the directory.
                            dest_dir = path.rstrip('/') or '/'
                        if not dest_dir or dest_dir == '/':
                            continue
                        src_norm = _normalize_inject_src_for_copy(resolved, source_dir)
                        if not src_norm:
                            continue
                        out.append(f"{src_norm} -> {dest_dir}")
                    return out

                if flow_run_remote:
                    inject_source_dir = str(posixpath.join(str(flow_out_dir or ''), 'artifacts')) if flow_out_dir else ''
                else:
                    inject_source_dir = str(
                        os.path.join(flow_out_dir, 'injected')
                        if flow_out_dir and os.path.isdir(os.path.join(flow_out_dir, 'injected'))
                        else (
                            os.path.join(flow_out_dir, 'artifacts')
                            if flow_out_dir and os.path.isdir(os.path.join(flow_out_dir, 'artifacts'))
                            else (flow_out_dir or '')
                        )
                    )

                meta_h['flow_flag'] = {
                    'type': assignment_type,
                    'generator_catalog': generator_catalog,
                    'generator_id': generator_id,
                    'generator_name': str(fa.get('name') or ''),
                    'generator_language': str(fa.get('language') or ''),
                    'generator_source': str(fa.get('flag_generator') or ''),
                    'artifacts_dir': str(flow_out_dir or ''),
                    'mount_dir': inject_source_dir,
                    'inject_files': _inject_files_for_copy_from_detail(fa.get('inject_files_detail'), inject_source_dir) or (
                        [x for x in (_normalize_inject_spec_for_copy(r, inject_source_dir) for r in (fa.get('inject_files') or []) if r is not None) if x]
                        if isinstance(fa.get('inject_files'), list)
                        else []
                    ),
                    'inputs': list(fa.get('inputs') or []) if isinstance(fa.get('inputs'), list) else [],
                    'outputs': list(fa.get('outputs') or []) if isinstance(fa.get('outputs'), list) else [],
                    'hint_template': str(fa.get('hint_template') or ''),
                    'hint': str(fa.get('hint') or ''),
                    'next_node_id': str(fa.get('next_node_id') or ''),
                    'next_node_name': str(fa.get('next_node_name') or ''),
                    'generated': bool(ok_run),
                    'generation_note': str(note or ''),
                    'run_dir': str(flow_out_dir or ''),
                    'outputs_manifest': str(manifest_path or ''),
                    'actual_outputs': actual_output_keys,
                    'declared_outputs': declared_output_keys,
                    'outputs_match': bool(mismatch.get('ok')) if isinstance(mismatch, dict) and mismatch else True,
                    'outputs_mismatch': mismatch,
                    'inputs_match': bool(inputs_mismatch.get('ok')) if isinstance(inputs_mismatch, dict) and inputs_mismatch else True,
                    'inputs_mismatch': inputs_mismatch,
                    'config': cfg,
                }

                # Also enrich the assignment itself for the Flow UI response.
                try:
                    fa['generated'] = bool(ok_run)
                    fa['generation_note'] = str(note or '')
                    fa['artifacts_dir'] = str(flow_out_dir or '')
                    fa['mount_dir'] = inject_source_dir
                    fa['inject_files'] = list(fa.get('inject_files') or []) if isinstance(fa.get('inject_files'), list) else []
                    fa['outputs_manifest'] = str(manifest_path or '')
                    fa['declared_outputs'] = declared_output_keys
                    fa['actual_outputs'] = actual_output_keys
                    fa['outputs_match'] = bool(mismatch.get('ok')) if isinstance(mismatch, dict) and mismatch else True
                    fa['outputs_mismatch'] = mismatch
                    fa['inputs_match'] = bool(inputs_mismatch.get('ok')) if isinstance(inputs_mismatch, dict) and inputs_mismatch else True
                    fa['inputs_mismatch'] = inputs_mismatch
                    fa['config'] = cfg
                except Exception:
                    pass

            try:
                realized_flags: list[str] = []
                for fa in (flag_assignments or []):
                    if not isinstance(fa, dict):
                        continue
                    ro = fa.get('resolved_outputs') if isinstance(fa.get('resolved_outputs'), dict) else {}
                    flag_val = None
                    if isinstance(ro, dict):
                        flag_val = ro.get('Flag(flag_id)') or ro.get('flag')
                    if not flag_val:
                        flag_val = fa.get('flag_value')
                    if not flag_val:
                        try:
                            manifest_path = str(fa.get('outputs_manifest') or '').strip()
                            if (not flow_run_remote) and manifest_path and os.path.exists(manifest_path):
                                with open(manifest_path, 'r', encoding='utf-8') as mf:
                                    m = json.load(mf) or {}
                                outs = m.get('outputs') if isinstance(m, dict) else None
                                if isinstance(outs, dict):
                                    flag_val = outs.get('Flag(flag_id)') or outs.get('flag')
                        except Exception:
                            flag_val = None
                    if isinstance(flag_val, str) and flag_val.strip():
                        realized_flags.append(flag_val.strip())
                if realized_flags:
                    if len(set(realized_flags)) != len(realized_flags):
                        raise RuntimeError('duplicate flag value detected after resolve')
            except RuntimeError:
                raise
            except Exception:
                pass

            if generation_failures:
                force_fail = False
                try:
                    force_fail = any('duplicate flag value' in str(x.get('error', '') or '') for x in (generation_failures or []) if isinstance(x, dict))
                except Exception:
                    force_fail = False
                # Cleanup partial artifacts so we don't leave confusing residues behind.
                try:
                    base_dir = os.path.abspath(os.path.join('/tmp', 'vulns'))
                    to_rm = (created_run_dirs or [])
                    if best_effort:
                        to_rm = (failed_run_dirs or [])
                    for d in to_rm:
                        try:
                            dd = os.path.abspath(str(d))
                            if os.path.commonpath([dd, base_dir]) != base_dir:
                                continue
                            shutil.rmtree(dd, ignore_errors=True)
                        except Exception:
                            continue
                except Exception:
                    pass
                if (not best_effort) or force_fail:
                    return jsonify({
                        'ok': False,
                        'error': f"{len(generation_failures)} generator run(s) failed; cannot prepare preview for execute.",
                        'scenario': scenario_label or scenario_norm,
                        'length': length,
                        'stats': stats,
                        'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or '')} for n in chain_nodes],
                        'flag_assignments': flag_assignments,
                        'generation_failures': generation_failures,
                        'generation_skipped': generation_skipped,
                        'base_preview_plan_path': base_plan_path,
                        'best_effort': bool(best_effort),
                    }), 422
        except Exception:
            pass
    else:
        # Ensure the UI gets a consistent signal when flags are disabled.
        try:
            occurrence_ctr: dict[tuple[str, str], int] = {}
            for fa in (flag_assignments or []):
                if not isinstance(fa, dict):
                    continue
                cid = str(fa.get('node_id') or '').strip()
                h = host_by_id.get(cid)
                preview_ip4 = _preview_host_ip4(h) if isinstance(h, dict) else ''

                generator_id = str(fa.get('id') or '').strip()
                seed_val = preview.get('seed') if isinstance(preview, dict) else None

                occ_key = (cid, generator_id)
                occ = int(occurrence_ctr.get(occ_key, 0) or 0)
                occurrence_ctr[occ_key] = occ + 1

                # Compute a best-effort effective config so the UI can show resolved inputs
                # even when dependency order is invalid.
                try:
                    cfg_full = _flow_default_generator_config(fa, seed_val=seed_val, occurrence_idx=occ)

                    # Ensure per-use uniqueness by mixing a runtime timestamp into the seed/secret.
                    try:
                        seed_ts = int(time.time() * 1000.0)
                    except Exception:
                        seed_ts = None
                    if seed_ts is not None:
                        try:
                            cfg_full['seed_ts'] = seed_ts
                            cfg_full['seed'] = f"{cfg_full.get('seed')}:{seed_ts}"
                            cfg_full['secret'] = f"{cfg_full.get('secret')}:{seed_ts}"
                        except Exception:
                            pass
                    if preview_ip4:
                        cfg_full.setdefault('Knowledge(ip)', preview_ip4)
                        cfg_full.setdefault('target_ip', preview_ip4)
                        cfg_full.setdefault('host_ip', preview_ip4)
                        cfg_full.setdefault('ip4', preview_ip4)
                        cfg_full.setdefault('ipv4', preview_ip4)
                    try:
                        node_name_val = str((h or {}).get('name') or '').strip() if isinstance(h, dict) else ''
                        if node_name_val:
                            cfg_full['node_name'] = node_name_val
                    except Exception:
                        pass

                    raw_overrides = fa.get('config_overrides') or fa.get('inputs_overrides') or fa.get('input_overrides')
                    if isinstance(raw_overrides, dict) and raw_overrides:
                        for k, v in raw_overrides.items():
                            kk = str(k or '').strip()
                            if not kk:
                                continue
                            cfg_full[kk] = v
                        fa['config_overrides'] = dict(raw_overrides)

                    cfg = cfg_full
                    gen_def = _gen_by_id.get(generator_id)
                    if isinstance(gen_def, dict):
                        try:
                            inputs_def = gen_def.get('inputs')
                            if isinstance(inputs_def, list):
                                for inp in inputs_def:
                                    if not isinstance(inp, dict):
                                        continue
                                    name = str(inp.get('name') or '').strip()
                                    if not name:
                                        continue
                                    if 'default' not in inp:
                                        continue
                                    if name in cfg_full:
                                        v = cfg_full.get(name)
                                        if v is not None and (not isinstance(v, str) or v.strip()):
                                            continue
                                    cfg_full[name] = inp.get('default')
                        except Exception:
                            pass
                        allowed = _all_input_names_of(gen_def)
                        declared_required = _required_input_names_of(gen_def)
                        if allowed:
                            keep = set(allowed)
                            try:
                                keep |= set(declared_required or set())
                            except Exception:
                                pass
                            try:
                                keep |= set(_flow_synthesized_inputs())
                            except Exception:
                                pass
                            cfg = {k: v for k, v in (cfg_full or {}).items() if k in keep}

                    fa['config'] = cfg
                    fa['resolved_inputs'] = _redact_kv_for_ui(cfg)
                    try:
                        if isinstance(fa.get('resolved_inputs'), dict) and isinstance(cfg_full, dict):
                            if 'Knowledge(ip)' in cfg_full and 'Knowledge(ip)' not in fa['resolved_inputs']:
                                fa['resolved_inputs']['Knowledge(ip)'] = cfg_full.get('Knowledge(ip)')
                    except Exception:
                        pass
                except Exception:
                    pass

                # Apply inject-files override even when flags are disabled.
                try:
                    inj_ovr = (fa or {}).get('inject_files_override')
                    if isinstance(inj_ovr, list):
                        cleaned = [str(x or '').strip() for x in (inj_ovr or [])]
                        cleaned = [x for x in cleaned if x]
                        fa['inject_files'] = cleaned
                except Exception:
                    pass

                # Preserve FLAG override visibility even when flags are disabled.
                try:
                    flag_override = str((fa or {}).get('flag_override') or '').strip()
                    if flag_override:
                        fa['flag_value'] = flag_override
                        fa['resolved_outputs'] = _redact_kv_for_ui({'Flag(flag_id)': flag_override})
                except Exception:
                    pass

                # Preserve output override visibility even when flags are disabled.
                try:
                    out_ovr = (fa or {}).get('output_overrides')
                    if isinstance(out_ovr, dict) and out_ovr:
                        cleaned: dict[str, Any] = {}
                        for k, v in (out_ovr or {}).items():
                            kk = str(k or '').strip()
                            if not kk:
                                continue
                            cleaned[kk] = v
                        if cleaned:
                            fa['resolved_outputs'] = _redact_kv_for_ui(cleaned)
                            try:
                                if isinstance(cleaned.get('Flag(flag_id)'), str) and str(cleaned.get('Flag(flag_id)') or '').strip():
                                    fa['flag_value'] = str(cleaned.get('Flag(flag_id)') or '').strip()
                                elif isinstance(cleaned.get('flag'), str) and str(cleaned.get('flag') or '').strip():
                                    fa['flag_value'] = str(cleaned.get('flag') or '').strip()
                            except Exception:
                                pass
                except Exception:
                    pass

                fa['generated'] = False
                fa['generation_note'] = 'flags disabled (invalid dependency order)'
        except Exception:
            pass

    try:
        persisted_flag_assignments = _flow_strip_runtime_sensitive_fields(flag_assignments)
        flow_meta = {
            'source_preview_plan_path': base_plan_path,
            'scenario': scenario_label or scenario_norm,
            'length': length,
            'requested_length': requested_length,
            'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or '')} for n in chain_nodes],
            # Persist all Flow decisions so returning to Flag Sequencing shows the same
            # chain and generator selections/hints.
            'flag_assignments': persisted_flag_assignments,
            'flags_enabled': bool(flags_enabled),
            'flow_valid': bool(flow_valid),
            'flow_errors': list(flow_errors or []),
            'modified_at': _iso_now(),
        }
        try:
            flow_existing = meta.get('flow') if isinstance(meta, dict) else None
            if isinstance(flow_existing, dict):
                init_facts = _flow_normalize_fact_override(flow_existing.get('initial_facts'))
                goal_facts = _flow_normalize_fact_override(flow_existing.get('goal_facts'))
                if init_facts:
                    flow_meta['initial_facts'] = init_facts
                if goal_facts:
                    flow_meta['goal_facts'] = goal_facts
        except Exception:
            pass
        if isinstance(meta, dict):
            meta = dict(meta)
            meta['flow'] = flow_meta
        else:
            meta = {'flow': flow_meta}
    except Exception:
        pass

    # NOTE: Flow state persistence is now manual (Save XML) to avoid auto-saving.

    # Persist a single per-scenario plan artifact so /run_cli_async can safely consume it.
    try:
        if isinstance(meta, dict):
            meta = dict(meta)
            meta['updated_at'] = _iso_now()
        out_path = _canonical_plan_path_for_scenario(scenario_label or scenario_norm, xml_path=str((meta or {}).get('xml_path') or ''), create_dir=True)
        out_payload = {
            'full_preview': preview,
            'metadata': meta,
        }
        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(out_payload, f, indent=2)
        try:
            _planner_set_plan(scenario_norm, plan_path=out_path, xml_path=str((meta or {}).get('xml_path') or ''), seed=(meta or {}).get('seed'))
        except Exception:
            pass
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to persist flow-modified preview plan: {e}'}), 500

    try:
        app.logger.info(
            '[flow.prepare_preview_for_execute] done scenario=%s chain_len=%s flow_valid=%s flow_errors=%s detail=%s',
            scenario_norm,
            len(chain_nodes or []),
            bool(flow_valid),
            (flow_errors or []),
            (flow_errors_detail or ''),
        )
    except Exception:
        pass

    host_ip_map: dict[str, str] = {}
    try:
        for hid, h in (host_by_id or {}).items():
            ip_val = _preview_host_ip4(h) if isinstance(h, dict) else ''
            if ip_val:
                host_ip_map[str(hid)] = ip_val
    except Exception:
        host_ip_map = {}

    try:
        realized_flags: list[str] = []
        for fa in (flag_assignments or []):
            if not isinstance(fa, dict):
                continue
            ro = fa.get('resolved_outputs') if isinstance(fa.get('resolved_outputs'), dict) else {}
            flag_val = None
            if isinstance(ro, dict):
                flag_val = ro.get('Flag(flag_id)') or ro.get('flag')
            if not flag_val:
                flag_val = fa.get('flag_value')
            if isinstance(flag_val, str) and flag_val.strip():
                realized_flags.append(flag_val.strip())
        if realized_flags and len(set(realized_flags)) != len(realized_flags):
            return jsonify({
                'ok': False,
                'error': 'Duplicate flag value detected during resolve; retry with a different chain.',
                'scenario': scenario_label or scenario_norm,
                'length': length,
                'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or '')} for n in chain_nodes],
                'flag_assignments': flag_assignments,
            }), 422
    except Exception:
        pass

    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'length': length,
        'requested_length': requested_length,
        'stats': stats,
        'chain': [
            {
                'id': str(n.get('id') or ''),
                'name': str(n.get('name') or ''),
                'type': str(n.get('type') or ''),
                'is_vuln': bool(n.get('is_vuln')),
                'ip4': str(n.get('ip4') or ''),
                'ipv4': str(n.get('ipv4') or ''),
                'interfaces': list(n.get('interfaces') or []) if isinstance(n.get('interfaces'), list) else [],
            }
            for n in chain_nodes
        ],
        'flag_assignments': flag_assignments,
        'flags_enabled': bool(flags_enabled),
        'flow_valid': bool(flow_valid),
        'flow_errors': list(flow_errors or []),
        **({'flow_errors_detail': flow_errors_detail} if flow_errors_detail else {}),
        **({'host_ip_map': host_ip_map} if host_ip_map else {}),
        'xml_path': str((meta or {}).get('xml_path') or ''),
        'preview_plan_path': out_path,
        'base_preview_plan_path': base_plan_path,
        'best_effort': bool(best_effort),
        'elapsed_s': round(float(time.monotonic() - started_at), 3),
        'generator_runs': generator_runs,
        'progress_log': progress_log,
        'generation_failures': generation_failures,
        'generation_skipped': generation_skipped,
        'created_run_dirs': created_run_dirs,
        'failed_run_dirs': failed_run_dirs,
        **({'sequencer_dag': (dag_debug or {'ok': False, 'errors': ['not computed (explicit chain)']})} if debug_dag else {}),
        **({'warning': warning} if warning else {}),
    })


@app.route('/api/flag-sequencing/save_flow_substitutions', methods=['POST'])
def api_flow_save_flow_substitutions():
    """Persist a user-edited chain + generator assignments (no generator runs).

    This updates the single per-scenario plan file with metadata.flow.chain and
    metadata.flow.flag_assignments so future preview/prepare/execute honors the
    user's substitutions.
    """
    j = request.get_json(silent=True) or {}
    warning: str | None = None
    scenario_label = str(j.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    canonical_plan_path = _canonical_plan_path_for_scenario_norm(scenario_norm)

    allow_node_duplicates = False
    try:
        allow_node_duplicates = str(j.get('allow_node_duplicates') or '').strip().lower() in {
            '1', 'true', 't', 'yes', 'y', 'on'
        }
    except Exception:
        allow_node_duplicates = False

    chain_ids_in = j.get('chain_ids')
    if not isinstance(chain_ids_in, list) or not chain_ids_in:
        return jsonify({'ok': False, 'error': 'Missing chain_ids.'}), 400
    chain_ids: list[str] = [str(x or '').strip() for x in chain_ids_in if str(x or '').strip()]
    if not chain_ids:
        return jsonify({'ok': False, 'error': 'Missing chain_ids.'}), 400

    base_plan_path = str(j.get('preview_plan') or '').strip() or None
    if base_plan_path:
        try:
            base_plan_path = os.path.abspath(base_plan_path)
            plans_dir = os.path.abspath(os.path.join(_outputs_dir(), 'plans'))
            if os.path.commonpath([base_plan_path, plans_dir]) != plans_dir:
                base_plan_path = None
            elif not os.path.exists(base_plan_path):
                base_plan_path = None
        except Exception:
            base_plan_path = None
    try:
        if canonical_plan_path and os.path.exists(canonical_plan_path):
            if not base_plan_path or os.path.abspath(base_plan_path) != os.path.abspath(canonical_plan_path):
                base_plan_path = canonical_plan_path
    except Exception:
        pass
    if not base_plan_path:
        try:
            entry = _planner_get_plan(scenario_norm)
            if entry:
                base_plan_path = entry.get('plan_path') or base_plan_path
        except Exception:
            base_plan_path = base_plan_path

    if not base_plan_path:
        base_plan_path = _latest_preview_plan_for_scenario_norm_origin(scenario_norm, origin='planner')

    if not base_plan_path:
        try:
            entry = _planner_get_plan(scenario_norm)
            if entry:
                base_plan_path = entry.get('plan_path') or base_plan_path
        except Exception:
            base_plan_path = base_plan_path

    if not base_plan_path:
        base_plan_path = _latest_preview_plan_for_scenario_norm_origin(scenario_norm, origin='planner')

    if not base_plan_path:
        base_plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm, prefer_flow=True)
    if not base_plan_path or not os.path.exists(base_plan_path):
        return jsonify({'ok': False, 'error': 'No preview plan found for this scenario. Generate a Full Preview first.'}), 404

    try:
        payload = _load_preview_payload_from_path(base_plan_path, scenario_norm)
        if not isinstance(payload, dict):
            return jsonify({'ok': False, 'error': 'Preview plan not embedded in XML. Save XML with Preview first.'}), 404
        meta = payload.get('metadata') if isinstance(payload, dict) else {}
        preview = payload.get('full_preview') if isinstance(payload, dict) else None
        if not isinstance(preview, dict):
            return jsonify({'ok': False, 'error': 'Preview plan is missing full_preview.'}), 422
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to load preview plan: {e}'}), 500

    initial_facts_override = _flow_normalize_fact_override(j.get('initial_facts'))
    goal_facts_override = _flow_normalize_fact_override(j.get('goal_facts'))
    try:
        flow_existing = meta.get('flow') if isinstance(meta, dict) else None
        if initial_facts_override is None and isinstance(flow_existing, dict):
            initial_facts_override = _flow_normalize_fact_override(flow_existing.get('initial_facts'))
        if goal_facts_override is None and isinstance(flow_existing, dict):
            goal_facts_override = _flow_normalize_fact_override(flow_existing.get('goal_facts'))
    except Exception:
        pass

    # Build chain node dicts with vulnerability flags.
    try:
        nodes, _links, _adj = _build_topology_graph_from_preview_plan(preview)
    except Exception:
        nodes = []
    id_map = {str(n.get('id') or '').strip(): n for n in (nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()}
    chain_nodes: list[dict[str, Any]] = []
    for cid in chain_ids:
        n = id_map.get(str(cid))
        if not isinstance(n, dict):
            return jsonify({'ok': False, 'error': f'Chain node not found in preview plan: {cid}'}), 422
        chain_nodes.append(n)

    # Parse requested assignments (one per chain position).
    fas_in = j.get('flag_assignments')
    if not isinstance(fas_in, list) or len(fas_in) != len(chain_nodes):
        return jsonify({'ok': False, 'error': 'flag_assignments must be a list aligned to chain_ids (same length).'}), 400

    try:
        gens, _ = _flag_generators_from_enabled_sources()
    except Exception:
        gens = []
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
    except Exception:
        node_gens = []
    gen_by_id: dict[str, dict[str, Any]] = {}
    for g in (gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid and gid not in gen_by_id:
            gg = dict(g)
            gg['_flow_kind'] = 'flag-generator'
            gg['_flow_catalog'] = 'flag_generators'
            gen_by_id[gid] = gg
    for g in (node_gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid and gid not in gen_by_id:
            gg = dict(g)
            gg['_flow_kind'] = 'flag-node-generator'
            gg['_flow_catalog'] = 'flag_node_generators'
            gen_by_id[gid] = gg

    try:
        plugins_by_id = _flow_enabled_plugin_contracts_by_id()
    except Exception:
        plugins_by_id = {}

    id_to_name: dict[str, str] = {}
    for n in chain_nodes:
        try:
            nid = str(n.get('id') or '').strip()
            nm = str(n.get('name') or '').strip()
            if nid:
                id_to_name[nid] = nm or nid
        except Exception:
            pass

    def _artifact_requires_of(gen: dict[str, Any]) -> set[str]:
        required: set[str] = set()
        try:
            pid = str(gen.get('id') or '').strip()
            plugin = plugins_by_id.get(pid)
            if isinstance(plugin, dict) and isinstance(plugin.get('requires'), list):
                for x in (plugin.get('requires') or []):
                    xx = str(x).strip()
                    if xx:
                        required.add(xx)
        except Exception:
            pass
        # Synthesized inputs (e.g., seed/node_name) are *fields*, not chain artifacts.
        # Filter them out even if a plugin contract mistakenly lists them in `requires`.
        try:
            required = {x for x in required if x not in _flow_synthesized_inputs()}
        except Exception:
            pass
        return required

    def _artifact_produces_of(gen: dict[str, Any]) -> set[str]:
        provides: set[str] = set()
        try:
            pid = str(gen.get('id') or '').strip()
            plugin = plugins_by_id.get(pid)
            if isinstance(plugin, dict) and isinstance(plugin.get('produces'), list):
                for item in (plugin.get('produces') or []):
                    if not isinstance(item, dict):
                        continue
                    a = str(item.get('artifact') or '').strip()
                    if a:
                        provides.add(a)
        except Exception:
            pass
        return provides

    def _required_input_fields_of(gen: dict[str, Any]) -> set[str]:
        required: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    name = str(inp.get('name') or '').strip()
                    if not name:
                        continue
                    if inp.get('required') is False:
                        continue
                    required.add(name)
        except Exception:
            pass
        return required

    def _all_input_fields_of(gen: dict[str, Any]) -> set[str]:
        fields: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    name = str(inp.get('name') or '').strip()
                    if name:
                        fields.add(name)
        except Exception:
            pass
        return fields

    def _output_fields_of(gen: dict[str, Any]) -> set[str]:
        out_fields: set[str] = set()
        try:
            outputs = gen.get('outputs')
            if isinstance(outputs, list):
                for outp in outputs:
                    if not isinstance(outp, dict):
                        continue
                    nm = str(outp.get('name') or '').strip()
                    if nm:
                        out_fields.add(nm)
        except Exception:
            pass
        try:
            out_fields |= _artifact_produces_of(gen)
        except Exception:
            pass
        return out_fields

    def _provides_of(gen: dict[str, Any]) -> set[str]:
        provides: set[str] = set()
        try:
            provides |= _artifact_produces_of(gen)
        except Exception:
            pass
        try:
            prov = gen.get('provides')
            if isinstance(prov, list):
                for x in prov:
                    s = str(x).strip()
                    if s:
                        provides.add(s)
        except Exception:
            pass
        try:
            provides |= _output_fields_of(gen)
        except Exception:
            pass
        return provides

    out_assignments: list[dict[str, Any]] = []
    for i, (cid, raw_a) in enumerate(zip(chain_ids, (fas_in or []))):
        if not isinstance(raw_a, dict):
            raw_a = {}

        generator_id = str(raw_a.get('id') or raw_a.get('generator_id') or '').strip()
        if not generator_id:
            return jsonify({'ok': False, 'error': f'Missing generator id for position {i}.'}), 400

        gen = gen_by_id.get(generator_id)
        if not isinstance(gen, dict):
            return jsonify({'ok': False, 'error': f'Generator not found/enabled: {generator_id}'}), 422

        # Keep assignment aligned to the provided chain_ids order.
        a2 = dict(raw_a)
        a2['node_id'] = str(cid)
        a2['id'] = str(generator_id)
        a2['name'] = str(gen.get('name') or '')
        a2['description'] = str(gen.get('description') or '')
        a2['type'] = str(gen.get('_flow_kind') or a2.get('type') or 'flag-generator')
        a2['flag_generator'] = str(gen.get('_source_name') or '').strip() or 'unknown'
        a2['generator_catalog'] = str(gen.get('_flow_catalog') or a2.get('generator_catalog') or 'flag_generators')
        a2['language'] = str(gen.get('language') or '')

        hint_templates = _flow_hint_templates_from_generator(gen)
        a2['hint_templates'] = hint_templates
        a2['hint_template'] = str((hint_templates[0] if hint_templates else '') or '')

        try:
            next_id = chain_ids[i + 1] if (i + 1) < len(chain_ids) else ''
        except Exception:
            next_id = ''
        a2['next_node_id'] = str(next_id)
        a2['next_node_name'] = str(id_to_name.get(str(next_id)) or '')

        # Chain semantics.
        requires_artifacts = sorted(list(_artifact_requires_of(gen)))
        produces_artifacts = sorted(list(_artifact_produces_of(gen)))
        input_fields_required = sorted(list(_required_input_fields_of(gen)))
        input_fields_all = sorted(list(_all_input_fields_of(gen)))
        input_fields_optional = sorted([x for x in input_fields_all if x and x not in set(input_fields_required)])
        output_fields = sorted(list(_output_fields_of(gen)))

        # Filter config_overrides to declared inputs + synthesized inputs.
        allowed_override_keys: set[str] = set(input_fields_all)
        try:
            allowed_override_keys |= set(_flow_synthesized_inputs())
        except Exception:
            pass

        raw_overrides: Any = None
        overrides_present = False
        try:
            if 'config_overrides' in a2:
                overrides_present = True
                raw_overrides = a2.get('config_overrides')
            elif 'inputs_overrides' in a2:
                overrides_present = True
                raw_overrides = a2.get('inputs_overrides')
            elif 'input_overrides' in a2:
                overrides_present = True
                raw_overrides = a2.get('input_overrides')
        except Exception:
            overrides_present = False
            raw_overrides = None

        if overrides_present:
            if raw_overrides is None:
                a2.pop('config_overrides', None)
            elif isinstance(raw_overrides, dict):
                config_overrides: dict[str, Any] = {}
                for k, v in (raw_overrides or {}).items():
                    kk = str(k or '').strip()
                    if not kk:
                        continue
                    if kk not in allowed_override_keys:
                        continue
                    config_overrides[kk] = v
                if config_overrides:
                    a2['config_overrides'] = dict(config_overrides)
                else:
                    a2.pop('config_overrides', None)
            else:
                a2.pop('config_overrides', None)

        # Drop legacy keys if present (we normalize into config_overrides).
        a2.pop('inputs_overrides', None)
        a2.pop('input_overrides', None)

        a2['requires'] = requires_artifacts
        a2['produces'] = produces_artifacts
        a2['input_fields'] = input_fields_all
        a2['input_fields_required'] = input_fields_required
        a2['input_fields_optional'] = input_fields_optional
        a2['output_fields'] = output_fields

        a2['inputs'] = sorted(list((_artifact_requires_of(gen) | set(_required_input_fields_of(gen)))))
        a2['outputs'] = sorted(list(_provides_of(gen)))

        # Preserve/derive resolved values for refresh persistence.
        try:
            resolved_inputs: dict[str, Any] | None = None
            resolved_outputs: dict[str, Any] | None = None
            if 'resolved_inputs' in raw_a and isinstance(raw_a.get('resolved_inputs'), dict):
                resolved_inputs = dict(raw_a.get('resolved_inputs') or {})
            if 'resolved_outputs' in raw_a and isinstance(raw_a.get('resolved_outputs'), dict):
                resolved_outputs = dict(raw_a.get('resolved_outputs') or {})

            cfg_ovr = a2.get('config_overrides') if isinstance(a2, dict) else None
            if isinstance(cfg_ovr, dict) and cfg_ovr:
                resolved_inputs = dict(resolved_inputs or {})
                resolved_inputs.update(cfg_ovr)

            out_ovr = a2.get('output_overrides') if isinstance(a2, dict) else None
            if isinstance(out_ovr, dict) and out_ovr:
                resolved_outputs = dict(resolved_outputs or {})
                resolved_outputs.update(out_ovr)

            flag_val = None
            if isinstance(raw_a.get('flag_value'), str) and str(raw_a.get('flag_value') or '').strip():
                flag_val = str(raw_a.get('flag_value') or '').strip()
            if isinstance(a2.get('flag_override'), str) and str(a2.get('flag_override') or '').strip():
                flag_val = str(a2.get('flag_override') or '').strip()
            if flag_val:
                resolved_outputs = dict(resolved_outputs or {})
                resolved_outputs['Flag(flag_id)'] = flag_val
                a2['flag_value'] = flag_val

            if isinstance(resolved_inputs, dict) and resolved_inputs:
                a2['resolved_inputs'] = resolved_inputs
            if isinstance(resolved_outputs, dict) and resolved_outputs:
                a2['resolved_outputs'] = resolved_outputs
        except Exception:
            pass

        # Normalize persisted overrides / new fields.
        out_assignments.append(a2)

    try:
        out_assignments = _flow_enrich_saved_flag_assignments(
            out_assignments,
            chain_nodes,
            scenario_label=(scenario_label or scenario_norm),
        )
    except Exception:
        pass

    try:
        flow_valid, flow_errors = _flow_validate_chain_order_by_requires_produces(
            chain_nodes,
            out_assignments,
            scenario_label=(scenario_label or scenario_norm),
        )
    except Exception:
        flow_valid, flow_errors = True, []
    try:
        assign_ids = [str(a.get('id') or a.get('generator_id') or '').strip() for a in (out_assignments or []) if isinstance(a, dict)]
        chain_ids_dbg = [str(n.get('id') or '').strip() for n in (chain_nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()]
        vuln_nodes_dbg = len([n for n in (chain_nodes or []) if isinstance(n, dict) and _flow_node_is_vuln(n)])
        docker_nodes_dbg = len([n for n in (chain_nodes or []) if isinstance(n, dict) and _flow_node_is_docker_role(n)])
        flow_errors_detail = (
            f"assignments={len(out_assignments or [])} "
            f"assignments_with_id={len([x for x in assign_ids if x])} "
            f"chain_nodes={len(chain_nodes or [])} "
            f"chain_vuln_nodes={vuln_nodes_dbg} "
            f"chain_docker_nodes={docker_nodes_dbg} "
            f"chain_ids={','.join(chain_ids_dbg)} "
            f"base_plan={os.path.basename(str(base_plan_path or ''))}"
        )
    except Exception:
        flow_errors_detail = None
    try:
        app.logger.info(
            '[flow.save_flow_substitutions] scenario=%s flow_valid=%s flow_errors=%s detail=%s',
            scenario_norm,
            bool(flow_valid),
            (flow_errors or []),
            (flow_errors_detail or ''),
        )
    except Exception:
        pass
    flags_enabled = bool(flow_valid)

    # Persist into the single per-scenario plan.
    try:
        persisted_flag_assignments = _flow_strip_runtime_sensitive_fields(out_assignments)
        flow_meta = {
            'source_preview_plan_path': base_plan_path,
            'scenario': scenario_label or scenario_norm,
            'length': len(chain_nodes),
            'requested_length': len(chain_nodes),
            'allow_node_duplicates': bool(allow_node_duplicates),
            'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or '')} for n in chain_nodes],
            'flag_assignments': persisted_flag_assignments,
            'flags_enabled': bool(flags_enabled),
            'flow_valid': bool(flow_valid),
            'flow_errors': list(flow_errors or []),
            'modified_at': _iso_now(),
        }
        if initial_facts_override:
            flow_meta['initial_facts'] = initial_facts_override
        if goal_facts_override:
            flow_meta['goal_facts'] = goal_facts_override
        if isinstance(meta, dict):
            meta2 = dict(meta)
            meta2['flow'] = flow_meta
        else:
            meta2 = {'flow': flow_meta}
    except Exception:
        meta2 = meta

    try:
        if isinstance(meta2, dict):
            meta2 = dict(meta2)
            meta2['updated_at'] = _iso_now()
        out_path = _canonical_plan_path_for_scenario(scenario_label or scenario_norm, xml_path=str((meta2 or {}).get('xml_path') or ''), create_dir=True)
        out_payload = {
            'full_preview': preview,
            'metadata': meta2,
        }
        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(out_payload, f, indent=2)
        try:
            xml_target = str((meta2 or {}).get('xml_path') or '').strip()
            if not xml_target:
                xml_target = _latest_xml_path_for_scenario(scenario_norm)
            if xml_target and os.path.exists(xml_target):
                _update_flow_state_in_xml(xml_target, scenario_label or scenario_norm, flow_meta)
        except Exception:
            pass
        try:
            _planner_set_plan(scenario_norm, plan_path=out_path, xml_path=str((meta2 or {}).get('xml_path') or ''), seed=(meta2 or {}).get('seed'))
        except Exception:
            pass
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to persist flow-modified preview plan: {e}'}), 500

    # Stats for UI.
    try:
        stats = _flow_compose_docker_stats(nodes)
    except Exception:
        stats = {}

    host_ip_map: dict[str, str] = {}
    try:
        hosts = preview.get('hosts') if isinstance(preview, dict) else None
        if isinstance(hosts, list):
            for h in hosts:
                if not isinstance(h, dict):
                    continue
                hid = str(h.get('node_id') or h.get('id') or '').strip()
                if not hid:
                    continue
                ip_val = _preview_host_ip4_any(h)
                if ip_val:
                    host_ip_map[hid] = ip_val
    except Exception:
        host_ip_map = {}

    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'length': len(chain_nodes),
        'stats': stats,
        'chain': [
            {
                'id': str(n.get('id') or ''),
                'name': str(n.get('name') or ''),
                'type': str(n.get('type') or ''),
                'is_vuln': bool(n.get('is_vuln')),
                'ip4': str(n.get('ip4') or ''),
                'ipv4': str(n.get('ipv4') or ''),
                'interfaces': list(n.get('interfaces') or []) if isinstance(n.get('interfaces'), list) else [],
            }
            for n in chain_nodes
        ],
        'flag_assignments': out_assignments,
        'flags_enabled': bool(flags_enabled),
        'flow_valid': bool(flow_valid),
        'flow_errors': list(flow_errors or []),
        **({'flow_errors_detail': flow_errors_detail} if flow_errors_detail else {}),
        **({'host_ip_map': host_ip_map} if host_ip_map else {}),
        'preview_plan_path': out_path,
        'base_preview_plan_path': base_plan_path,
        'allow_node_duplicates': bool(allow_node_duplicates),
        **({'warning': warning} if warning else {}),
        **({'initial_facts': initial_facts_override} if initial_facts_override else {}),
        **({'goal_facts': goal_facts_override} if goal_facts_override else {}),
    })


def _flow_uploads_dir() -> str:
    d = os.path.join(_outputs_dir(), 'flow_uploads')
    os.makedirs(d, exist_ok=True)
    return d


def _flow_inject_uploads_dir() -> str:
    d = os.path.join(_outputs_dir(), 'flow_inject_uploads')
    os.makedirs(d, exist_ok=True)
    return d


@app.route('/api/flag-sequencing/upload_flow_input_file', methods=['POST'])
def api_flow_upload_flow_input_file():
    """Upload a file to be used as a Flow generator input override.

    The Flow Value Override dialog can upload a file, store it under outputs/flow_uploads,
    and persist the returned absolute path in config_overrides. During
    /api/flag-sequencing/prepare_preview_for_execute, the server will stage the file into
    the generator run's inputs/ directory and rewrite the config value to /inputs/<file>.
    """
    scenario_label = str(request.form.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    step_index_raw = str(request.form.get('step_index') or '').strip()
    input_name = str(request.form.get('input_name') or '').strip()
    generator_id = str(request.form.get('generator_id') or '').strip()

    f = request.files.get('file')
    if not f or not getattr(f, 'filename', ''):
        return jsonify({'ok': False, 'error': 'No file provided.'}), 400

    # Basic size guard (best-effort). Flask may not expose content_length reliably.
    max_bytes = 10 * 1024 * 1024
    try:
        clen = request.content_length
        if isinstance(clen, int) and clen > max_bytes:
            return jsonify({'ok': False, 'error': 'File too large (max 10MB).'}), 413
    except Exception:
        pass

    def _unique_dest_filename(dir_path: str, filename: str) -> str:
        base = secure_filename(filename) or 'upload'
        cand = base
        root, ext = os.path.splitext(base)
        i = 1
        while os.path.exists(os.path.join(dir_path, cand)):
            cand = f"{root}_{i}{ext}"
            i += 1
            if i > 5000:
                break
        return cand

    original_filename = str(getattr(f, 'filename', '') or '')
    safe_filename = secure_filename(original_filename) or 'upload'
    unique = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:8]
    base_dir = os.path.join(_flow_uploads_dir(), scenario_norm, unique)
    os.makedirs(base_dir, exist_ok=True)

    prefix = (secure_filename(input_name) or 'input')
    stored_name = _unique_dest_filename(base_dir, f"{prefix}__{safe_filename}")
    stored_path = os.path.join(base_dir, stored_name)
    try:
        f.save(stored_path)
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed saving upload: {e}'}), 400

    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'scenario_norm': scenario_norm,
        'step_index': step_index_raw,
        'generator_id': generator_id,
        'input_name': input_name,
        'original_filename': original_filename,
        'stored_filename': stored_name,
        'stored_path': os.path.abspath(stored_path),
    })


@app.route('/api/flag-sequencing/upload_flow_inject_file', methods=['POST'])
def api_flow_upload_flow_inject_file():
    """Upload a file to be used as a Flow inject override.

    Stores the file under outputs/flow_inject_uploads and returns an inject
    reference token (upload:<abs_path>). At runtime, Flow stages these into
    the run's artifacts/ directory and rewrites inject entries accordingly.
    """
    scenario_label = str(request.form.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    step_index_raw = str(request.form.get('step_index') or '').strip()
    generator_id = str(request.form.get('generator_id') or '').strip()

    f = request.files.get('file')
    if not f or not getattr(f, 'filename', ''):
        return jsonify({'ok': False, 'error': 'No file provided.'}), 400

    max_bytes = 10 * 1024 * 1024
    try:
        clen = request.content_length
        if isinstance(clen, int) and clen > max_bytes:
            return jsonify({'ok': False, 'error': 'File too large (max 10MB).'}), 413
    except Exception:
        pass

    def _unique_dest_filename(dir_path: str, filename: str) -> str:
        base = secure_filename(filename) or 'upload'
        cand = base
        root, ext = os.path.splitext(base)
        i = 1
        while os.path.exists(os.path.join(dir_path, cand)):
            cand = f"{root}_{i}{ext}"
            i += 1
            if i > 5000:
                break
        return cand

    original_filename = str(getattr(f, 'filename', '') or '')
    safe_filename = secure_filename(original_filename) or 'upload'
    unique = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:8]
    base_dir = os.path.join(_flow_inject_uploads_dir(), scenario_norm, unique)
    os.makedirs(base_dir, exist_ok=True)

    stored_name = _unique_dest_filename(base_dir, safe_filename)
    stored_path = os.path.join(base_dir, stored_name)
    try:
        f.save(stored_path)
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed saving upload: {e}'}), 400

    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'scenario_norm': scenario_norm,
        'step_index': step_index_raw,
        'generator_id': generator_id,
        'original_filename': original_filename,
        'stored_filename': stored_name,
        'stored_path': os.path.abspath(stored_path),
        'inject_value': f"upload:{os.path.abspath(stored_path)}",
    })

    out_assignments: list[dict[str, Any]] = []
    for i, cid in enumerate(chain_ids):
        req = fas_in[i] if i < len(fas_in) else {}
        if not isinstance(req, dict):
            return jsonify({'ok': False, 'error': 'flag_assignments entries must be objects.'}), 400
        node_id = str(req.get('node_id') or '').strip()
        if node_id != str(cid):
            return jsonify({'ok': False, 'error': 'flag_assignments must align to chain_ids (node_id mismatch).'}), 400
        gen_id = str(req.get('id') or req.get('generator_id') or '').strip()
        if not gen_id:
            return jsonify({'ok': False, 'error': f'Missing generator id for node {node_id}.'}), 400
        gen = gen_by_id.get(gen_id)
        if not isinstance(gen, dict):
            return jsonify({'ok': False, 'error': f'Generator not found/enabled: {gen_id}'}), 422

        node = chain_nodes[i] if i < len(chain_nodes) else {}
        is_vuln_node = bool(node.get('is_vuln'))
        is_docker_node = False
        try:
            is_docker_node = bool(_flow_node_is_docker_role(node))
        except Exception:
            try:
                t_raw = str(node.get('type') or '')
                t = t_raw.strip().lower()
                is_docker_node = ('docker' in t) or (t_raw.strip().upper() == 'DOCKER')
            except Exception:
                is_docker_node = False

        kind = str(gen.get('_flow_kind') or 'flag-generator')
        if is_vuln_node and kind != 'flag-generator':
            return jsonify({'ok': False, 'error': f'Generator {gen_id} is not compatible with vulnerability node {node_id} (must be flag-generator).'}), 422
        if kind == 'flag-node-generator':
            if is_vuln_node or (not is_docker_node):
                return jsonify({'ok': False, 'error': f'Generator {gen_id} is not compatible with node {node_id} (flag-node-generator requires docker-role, non-vulnerability node).'}), 422
        else:
            # flag-generator
            if not is_vuln_node:
                return jsonify({'ok': False, 'error': f'Generator {gen_id} is not compatible with node {node_id} (flag-generator requires vulnerability node).'}), 422

        next_id = chain_ids[i + 1] if (i + 1) < len(chain_ids) else ''
        hint_templates = _flow_hint_templates_from_generator(gen)
        hint_tpl = hint_templates[0] if hint_templates else 'Next: {{NEXT_NODE_NAME}}'
        rendered_hints = [
            _flow_render_hint_template(t, scenario_label=(scenario_label or scenario_norm), id_to_name=id_to_name, this_id=str(node_id), next_id=str(next_id))
            for t in (hint_templates or [])
        ]

        # Optional user overrides for hint text. Contract:
        # - If key is absent: use generated hints.
        # - If key is present and value is null: clear any override and use generated hints.
        # - If key is present and value is a list (possibly empty): use it verbatim (after trimming).
        hint_overrides_present = False
        raw_hint_overrides: Any = None
        try:
            hint_overrides_present = 'hint_overrides' in req
            raw_hint_overrides = req.get('hint_overrides')
        except Exception:
            hint_overrides_present = False
            raw_hint_overrides = None

        hint_overrides: list[str] | None = None
        clear_hint_overrides = False
        if hint_overrides_present:
            if raw_hint_overrides is None:
                clear_hint_overrides = True
                hint_overrides = None
            elif isinstance(raw_hint_overrides, list):
                cleaned = [str(x or '').strip() for x in (raw_hint_overrides or [])]
                cleaned = [x for x in cleaned if x]
                hint_overrides = cleaned
            elif isinstance(raw_hint_overrides, str):
                s = str(raw_hint_overrides or '').strip()
                hint_overrides = [s] if s else []
            else:
                # Unsupported type: ignore.
                hint_overrides = None

        # Optional user override for the realized FLAG value.
        flag_override_present = False
        raw_flag_override: Any = None
        try:
            flag_override_present = 'flag_override' in req
            raw_flag_override = req.get('flag_override')
        except Exception:
            flag_override_present = False
            raw_flag_override = None

        flag_override: str | None = None
        clear_flag_override = False
        if flag_override_present:
            if raw_flag_override is None:
                clear_flag_override = True
                flag_override = None
            elif isinstance(raw_flag_override, str):
                s = str(raw_flag_override or '').strip()
                flag_override = s if s else None
            else:
                flag_override = None

        # Optional user overrides for outputs (dict of output_key -> value).
        output_overrides_present = False
        raw_output_overrides: Any = None
        try:
            output_overrides_present = 'output_overrides' in req
            raw_output_overrides = req.get('output_overrides')
        except Exception:
            output_overrides_present = False
            raw_output_overrides = None

        output_overrides: dict[str, Any] | None = None
        clear_output_overrides = False
        if output_overrides_present:
            if raw_output_overrides is None:
                clear_output_overrides = True
                output_overrides = None
            elif isinstance(raw_output_overrides, dict):
                cleaned: dict[str, Any] = {}
                for k, v in (raw_output_overrides or {}).items():
                    kk = str(k or '').strip()
                    if not kk:
                        continue
                    cleaned[kk] = v
                output_overrides = cleaned
            else:
                output_overrides = None

        # Optional override for inject_files allowlist.
        inject_files_override_present = False
        raw_inject_files_override: Any = None
        try:
            inject_files_override_present = 'inject_files_override' in req
            raw_inject_files_override = req.get('inject_files_override')
        except Exception:
            inject_files_override_present = False
            raw_inject_files_override = None

        inject_files_override: list[str] | None = None
        clear_inject_files_override = False
        if inject_files_override_present:
            if raw_inject_files_override is None:
                clear_inject_files_override = True
                inject_files_override = None
            elif isinstance(raw_inject_files_override, list):
                cleaned = [str(x or '').strip() for x in (raw_inject_files_override or [])]
                cleaned = [x for x in cleaned if x]
                inject_files_override = cleaned
            else:
                inject_files_override = None

        requires_artifacts = sorted(list(_artifact_requires_of(gen)))
        produces_artifacts = sorted(list(_artifact_produces_of(gen)))
        input_fields_required = sorted(list(_required_input_fields_of(gen)))
        input_fields_all = sorted(list(_all_input_fields_of(gen)))
        input_fields_optional = sorted([x for x in input_fields_all if x and x not in set(input_fields_required)])
        output_fields = sorted(list(_output_fields_of(gen)))

        raw_overrides = req.get('config_overrides')
        if not isinstance(raw_overrides, dict):
            raw_overrides = req.get('inputs_overrides')
        if not isinstance(raw_overrides, dict):
            raw_overrides = req.get('input_overrides')

        allowed_override_keys: set[str] = set(input_fields_all)
        try:
            allowed_override_keys |= set(_flow_synthesized_inputs())
        except Exception:
            pass

        config_overrides: dict[str, Any] = {}
        if isinstance(raw_overrides, dict):
            for k, v in (raw_overrides or {}).items():
                kk = str(k or '').strip()
                if kk and kk in allowed_override_keys:
                    config_overrides[kk] = v

        # If an artifact "requires" token also appears as an optional input field,
        # treat it as optional (exclude from effective chaining requirements).
        try:
            optional_field_set = set(input_fields_optional)
            requires_effective = [x for x in (requires_artifacts or []) if x and x not in optional_field_set]
        except Exception:
            requires_effective = list(requires_artifacts or [])

        # Effective union for chaining.
        inputs_effective = sorted(list(set(requires_effective) | set(input_fields_required)))
        outputs_effective = sorted(list(_provides_of(gen)))

        out_a: dict[str, Any] = {
            'node_id': str(node_id),
            'id': str(gen.get('id') or ''),
            'name': str(gen.get('name') or ''),
            'type': kind,
            'flag_generator': str(gen.get('_source_name') or '').strip() or 'unknown',
            'generator_catalog': str(gen.get('_flow_catalog') or 'flag_generators'),
            'language': str(gen.get('language') or ''),
            'description_hints': list(gen.get('description_hints') or []) if isinstance(gen.get('description_hints'), list) else [],
            'config_overrides': dict(config_overrides),
            'inputs': inputs_effective,
            'outputs': outputs_effective,
            'requires': requires_artifacts,
            'produces': produces_artifacts,
            'input_fields': input_fields_all,
            'input_fields_required': input_fields_required,
            'input_fields_optional': input_fields_optional,
            'output_fields': output_fields,
            'input_defs': list(gen.get('inputs') or []) if isinstance(gen.get('inputs'), list) else [],
            'output_defs': list(gen.get('outputs') or []) if isinstance(gen.get('outputs'), list) else [],
            'hint_template': hint_tpl,
            'hint_templates': hint_templates,
            'hint': rendered_hints[0] if rendered_hints else _flow_render_hint_template(hint_tpl, scenario_label=(scenario_label or scenario_norm), id_to_name=id_to_name, this_id=str(node_id), next_id=str(next_id)),
            'hints': rendered_hints,
            'next_node_id': str(next_id),
            'next_node_name': str(id_to_name.get(str(next_id)) or ''),
        }

        if hint_overrides_present:
            if clear_hint_overrides:
                # Explicit clear: do not persist overrides.
                out_a.pop('hint_overrides', None)
            elif hint_overrides is not None:
                out_a['hint_overrides'] = list(hint_overrides)
                out_a['hints'] = list(hint_overrides)
                out_a['hint'] = hint_overrides[0] if hint_overrides else ''

        if flag_override_present:
            if clear_flag_override:
                out_a.pop('flag_override', None)
            elif flag_override is not None:
                out_a['flag_override'] = str(flag_override)

        if output_overrides_present:
            if clear_output_overrides:
                out_a.pop('output_overrides', None)
            elif output_overrides is not None:
                if output_overrides:
                    out_a['output_overrides'] = dict(output_overrides)
                else:
                    out_a.pop('output_overrides', None)

        if inject_files_override_present:
            if clear_inject_files_override:
                out_a.pop('inject_files_override', None)
            elif inject_files_override is not None:
                out_a['inject_files_override'] = list(inject_files_override)
                # Mirror to inject_files for effective view.
                out_a['inject_files'] = list(inject_files_override)

        out_assignments.append(out_a)

    # Validate (non-blocking)
    try:
        flow_valid, flow_errors = _flow_validate_chain_order_by_requires_produces(
            chain_nodes,
            out_assignments,
            scenario_label=(scenario_label or scenario_norm),
        )
    except Exception:
        flow_valid, flow_errors = True, []
    flags_enabled = bool(flow_valid)

    # Persist into the single per-scenario plan.
    try:
        persisted_flag_assignments = _flow_strip_runtime_sensitive_fields(out_assignments)
        flow_meta = {
            'source_preview_plan_path': base_plan_path,
            'scenario': scenario_label or scenario_norm,
            'length': len(chain_nodes),
            'requested_length': len(chain_nodes),
            'allow_node_duplicates': bool(allow_node_duplicates),
            'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or '')} for n in chain_nodes],
            'flag_assignments': persisted_flag_assignments,
            'flags_enabled': bool(flags_enabled),
            'flow_valid': bool(flow_valid),
            'flow_errors': list(flow_errors or []),
            'modified_at': _iso_now(),
        }
        if isinstance(meta, dict):
            meta2 = dict(meta)
            meta2['flow'] = flow_meta
        else:
            meta2 = {'flow': flow_meta}
    except Exception:
        meta2 = meta

    try:
        if isinstance(meta2, dict):
            meta2 = dict(meta2)
            meta2['updated_at'] = _iso_now()
        out_path = _canonical_plan_path_for_scenario(scenario_label or scenario_norm, xml_path=str((meta2 or {}).get('xml_path') or ''), create_dir=True)
        out_payload = {
            'full_preview': preview,
            'metadata': meta2,
        }
        with open(out_path, 'w', encoding='utf-8') as f:
            json.dump(out_payload, f, indent=2)
        try:
            _planner_set_plan(scenario_norm, plan_path=out_path, xml_path=str((meta2 or {}).get('xml_path') or ''), seed=(meta2 or {}).get('seed'))
        except Exception:
            pass
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to persist flow-modified preview plan: {e}'}), 500

    # Stats for UI.
    try:
        stats = _flow_compose_docker_stats(nodes)
    except Exception:
        stats = {}

    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'length': len(chain_nodes),
        'stats': stats,
        'chain': [{'id': str(n.get('id') or ''), 'name': str(n.get('name') or ''), 'type': str(n.get('type') or ''), 'is_vuln': bool(n.get('is_vuln'))} for n in chain_nodes],
        'flag_assignments': out_assignments,
        'flags_enabled': bool(flags_enabled),
        'flow_valid': bool(flow_valid),
        'flow_errors': list(flow_errors or []),
        'preview_plan_path': out_path,
        'base_preview_plan_path': base_plan_path,
        'allow_node_duplicates': bool(allow_node_duplicates),
    })


@app.route('/api/flag-sequencing/substitution_candidates', methods=['POST'])
def api_flow_substitution_candidates():
    """Return candidate generators with per-position compatibility info.

    The client uses this to gray out incompatible generators and show why.
    Compatibility here means: for the chain *prefix* (positions < index), the
    candidate's required artifacts/fields are satisfied.
    """
    j = request.get_json(silent=True) or {}
    scenario_label = str(j.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    canonical_plan_path = _canonical_plan_path_for_scenario_norm(scenario_norm)

    index_raw = j.get('index')
    try:
        index = int(index_raw)
    except Exception:
        return jsonify({'ok': False, 'error': 'Invalid index.'}), 400
    if index < 0:
        return jsonify({'ok': False, 'error': 'Invalid index.'}), 400

    chain_ids_in = j.get('chain_ids')
    if not isinstance(chain_ids_in, list) or not chain_ids_in:
        return jsonify({'ok': False, 'error': 'Missing chain_ids.'}), 400
    chain_ids: list[str] = [str(x or '').strip() for x in chain_ids_in if str(x or '').strip()]
    if not chain_ids:
        return jsonify({'ok': False, 'error': 'Missing chain_ids.'}), 400
    if index >= len(chain_ids):
        return jsonify({'ok': False, 'error': 'Index out of range.'}), 400

    kind = str(j.get('kind') or 'flag-generator').strip() or 'flag-generator'
    if kind not in {'flag-generator', 'flag-node-generator'}:
        kind = 'flag-generator'

    allow_node_duplicates = False
    try:
        allow_node_duplicates = str(j.get('allow_node_duplicates') or '').strip().lower() in {
            '1', 'true', 't', 'yes', 'y', 'on'
        }
    except Exception:
        allow_node_duplicates = False

    base_plan_path = str(j.get('preview_plan') or '').strip() or None
    if base_plan_path:
        try:
            base_plan_path = os.path.abspath(base_plan_path)
            plans_dir = os.path.abspath(os.path.join(_outputs_dir(), 'plans'))
            if os.path.commonpath([base_plan_path, plans_dir]) != plans_dir:
                base_plan_path = None
            elif not os.path.exists(base_plan_path):
                base_plan_path = None
        except Exception:
            base_plan_path = None
    try:
        if canonical_plan_path and os.path.exists(canonical_plan_path):
            if not base_plan_path or os.path.abspath(base_plan_path) != os.path.abspath(canonical_plan_path):
                base_plan_path = canonical_plan_path
    except Exception:
        pass
    if not base_plan_path:
        base_plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm, prefer_flow=True)
    if not base_plan_path or not os.path.exists(base_plan_path):
        return jsonify({'ok': False, 'error': 'No preview plan found for this scenario. Generate a Full Preview first.'}), 404

    try:
        payload = _load_preview_payload_from_path(base_plan_path, scenario_norm)
        if not isinstance(payload, dict):
            return jsonify({'ok': False, 'error': 'Preview plan not embedded in XML. Save XML with Preview first.'}), 404
        preview = payload.get('full_preview') if isinstance(payload, dict) else None
        if not isinstance(preview, dict):
            return jsonify({'ok': False, 'error': 'Preview plan is missing full_preview.'}), 422
    except Exception as e:
        return jsonify({'ok': False, 'error': f'Failed to load preview plan: {e}'}), 500

    # Build chain nodes and check the target node vulnerability.
    try:
        nodes, _links, _adj = _build_topology_graph_from_preview_plan(preview)
    except Exception:
        nodes = []
    id_map = {str(n.get('id') or '').strip(): n for n in (nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()}
    chain_nodes: list[dict[str, Any]] = []
    for cid in chain_ids:
        n = id_map.get(str(cid))
        if not isinstance(n, dict):
            return jsonify({'ok': False, 'error': f'Chain node not found in preview plan: {cid}'}), 422
        chain_nodes.append(n)
    is_vuln_node = bool((chain_nodes[index] if index < len(chain_nodes) else {}).get('is_vuln'))
    if is_vuln_node:
        kind = 'flag-generator'

    # Node candidates for the selected chain position.
    # The Flow UI can optionally allow changing the *node* at this step, but only to nodes
    # compatible with the selected generator kind.
    def _node_is_docker_role(n: dict[str, Any]) -> bool:
        try:
            t_raw = str(n.get('type') or '')
            t = t_raw.strip().lower()
            return ('docker' in t) or (t_raw.strip().upper() == 'DOCKER') or bool(_flow_node_is_docker_role(n))
        except Exception:
            try:
                return bool(_flow_node_is_docker_role(n))
            except Exception:
                return False

    def _node_compatible_for_kind(n: dict[str, Any], desired_kind: str) -> tuple[bool, list[str]]:
        reasons: list[str] = []
        try:
            is_vuln = bool(n.get('is_vuln'))
            is_docker = _node_is_docker_role(n)
            if desired_kind == 'flag-node-generator':
                if not is_docker:
                    reasons.append('requires docker-role node')
                if is_vuln:
                    reasons.append('requires non-vulnerability node')
                return (bool(is_docker) and (not is_vuln)), reasons
            # flag-generator
            if not is_vuln:
                reasons.append('requires vulnerability node')
                return False, reasons
            return True, []
        except Exception:
            return False, ['compatibility check failed']

    node_candidates: list[dict[str, Any]] = []
    try:
        cur_node = chain_nodes[index] if index < len(chain_nodes) else None
        cur_id = str((cur_node or {}).get('id') or '').strip() if isinstance(cur_node, dict) else ''
        used: set[str] = set()
        if not allow_node_duplicates:
            used = {str(cid).strip() for cid in (chain_ids or []) if str(cid).strip()}
            # Allow re-selecting the current node.
            if cur_id and cur_id in used:
                used.remove(cur_id)

        desired_kind = kind
        # If the current node is vuln, force flag-generator regardless.
        if isinstance(cur_node, dict) and bool(cur_node.get('is_vuln')):
            desired_kind = 'flag-generator'

        # Include current node (even if incompatible) so the UI can show it.
        if isinstance(cur_node, dict) and cur_id:
            ok, blocked = _node_compatible_for_kind(cur_node, desired_kind)
            node_candidates.append({
                'id': cur_id,
                'name': str(cur_node.get('name') or '').strip(),
                'type': str(cur_node.get('type') or '').strip(),
                'is_vuln': bool(cur_node.get('is_vuln')),
                'is_docker': bool(_node_is_docker_role(cur_node)),
                'compatible': bool(ok),
                'blocked_by': blocked,
                'current': True,
            })

        # Add other eligible nodes not already in the chain.
        for n in (nodes or []):
            if not isinstance(n, dict):
                continue
            nid = str(n.get('id') or '').strip()
            if not nid or nid in used:
                continue
            ok, blocked = _node_compatible_for_kind(n, desired_kind)
            if not ok:
                continue
            node_candidates.append({
                'id': nid,
                'name': str(n.get('name') or '').strip(),
                'type': str(n.get('type') or '').strip(),
                'is_vuln': bool(n.get('is_vuln')),
                'is_docker': bool(_node_is_docker_role(n)),
                'compatible': True,
                'blocked_by': [],
                'current': False,
            })

        # Sort: current first, then name/id.
        def _node_sort_key(x: dict[str, Any]) -> tuple[int, str, str]:
            cur = 0 if bool(x.get('current')) else 1
            name = str(x.get('name') or '').lower()
            nid = str(x.get('id') or '')
            return (cur, name, nid)

        node_candidates.sort(key=_node_sort_key)
    except Exception:
        node_candidates = []

    # Parse the current chain assignments (ids only) so we can compute prefix outputs.
    fas_in = j.get('flag_assignments')
    if not isinstance(fas_in, list) or len(fas_in) != len(chain_nodes):
        return jsonify({'ok': False, 'error': 'flag_assignments must be a list aligned to chain_ids (same length).'}), 400
    try:
        from core_topo_gen.utils.flow_substitution import flow_assignment_ids_by_position
        cur_ids_by_pos = flow_assignment_ids_by_position(fas_in)
    except Exception:
        cur_ids_by_pos = ['' for _ in range(len(chain_ids))]
        for pos in range(len(chain_ids)):
            req = fas_in[pos] if pos < len(fas_in) else {}
            if not isinstance(req, dict):
                continue
            gen_id = str(req.get('id') or req.get('generator_id') or '').strip()
            if gen_id:
                cur_ids_by_pos[pos] = gen_id

    # Candidate generator ids (client-filtered).
    cand_ids_in = j.get('candidate_ids')
    candidate_ids: list[str] = []
    if isinstance(cand_ids_in, list) and cand_ids_in:
        for x in cand_ids_in:
            s = str(x or '').strip()
            if s:
                candidate_ids.append(s)
    candidate_ids = list(dict.fromkeys(candidate_ids))

    try:
        gens, _ = _flag_generators_from_enabled_sources()
    except Exception:
        gens = []
    try:
        node_gens, _ = _flag_node_generators_from_enabled_sources()
    except Exception:
        node_gens = []

    # Build generator index by id, annotated with kind.
    gen_by_id: dict[str, dict[str, Any]] = {}
    for g in (gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid and gid not in gen_by_id:
            gg = dict(g)
            gg['_flow_kind'] = 'flag-generator'
            gen_by_id[gid] = gg
    for g in (node_gens or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        if gid and gid not in gen_by_id:
            gg = dict(g)
            gg['_flow_kind'] = 'flag-node-generator'
            gen_by_id[gid] = gg

    try:
        plugins_by_id = _flow_enabled_plugin_contracts_by_id()
    except Exception:
        plugins_by_id = {}

    def _artifact_requires_of(gen: dict[str, Any]) -> set[str]:
        required: set[str] = set()
        try:
            pid = str(gen.get('id') or '').strip()
            plugin = plugins_by_id.get(pid)
            if isinstance(plugin, dict) and isinstance(plugin.get('requires'), list):
                for x in (plugin.get('requires') or []):
                    xx = str(x).strip()
                    if xx:
                        required.add(xx)
        except Exception:
            pass
        # Synthesized inputs (e.g., seed/node_name) are *fields*, not chain artifacts.
        # Filter them out even if a plugin contract mistakenly lists them in `requires`.
        try:
            required = {x for x in required if x not in _flow_synthesized_inputs()}
        except Exception:
            pass
        return required

    def _artifact_produces_of(gen: dict[str, Any]) -> set[str]:
        provides: set[str] = set()
        try:
            pid = str(gen.get('id') or '').strip()
            plugin = plugins_by_id.get(pid)
            if isinstance(plugin, dict) and isinstance(plugin.get('produces'), list):
                for item in (plugin.get('produces') or []):
                    if not isinstance(item, dict):
                        continue
                    a = str(item.get('artifact') or '').strip()
                    if a:
                        provides.add(a)
        except Exception:
            pass
        return provides

    def _required_input_fields_of(gen: dict[str, Any]) -> set[str]:
        required: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    name = str(inp.get('name') or '').strip()
                    if not name:
                        continue
                    if inp.get('required') is False:
                        continue
                    required.add(name)
        except Exception:
            pass
        return required

    def _all_input_fields_of(gen: dict[str, Any]) -> set[str]:
        fields: set[str] = set()
        try:
            inputs = gen.get('inputs')
            if isinstance(inputs, list):
                for inp in inputs:
                    if not isinstance(inp, dict):
                        continue
                    name = str(inp.get('name') or '').strip()
                    if name:
                        fields.add(name)
        except Exception:
            pass
        return fields

    def _output_fields_of(gen: dict[str, Any]) -> set[str]:
        out_fields: set[str] = set()
        try:
            outputs = gen.get('outputs')
            if isinstance(outputs, list):
                for outp in outputs:
                    if not isinstance(outp, dict):
                        continue
                    nm = str(outp.get('name') or '').strip()
                    if nm:
                        out_fields.add(nm)
        except Exception:
            pass
        try:
            out_fields |= _artifact_produces_of(gen)
        except Exception:
            pass
        return out_fields

    # Compute prefix availability (artifacts + fields) from current assignments.
    # Fields the sequencer provides regardless of earlier chain outputs.
    synthesized_fields = {
        'seed',
        'secret',
        'env_name',
        'challenge',
        'flag_prefix',
        'username_prefix',
        'key_len',
        'node_name',
    }
    have_artifacts: set[str] = set()
    have_fields: set[str] = set(synthesized_fields)

    for pos in range(0, max(0, index)):
        gen_id = cur_ids_by_pos[pos] if pos < len(cur_ids_by_pos) else ''
        if not gen_id:
            continue
        g = gen_by_id.get(gen_id)
        if not isinstance(g, dict):
            continue
        try:
            have_artifacts |= _artifact_produces_of(g)
        except Exception:
            pass
        try:
            have_fields |= _output_fields_of(g)
        except Exception:
            pass

    def _blocked_reasons(gen: dict[str, Any]) -> tuple[bool, list[str]]:
        reasons: list[str] = []
        try:
            gkind = str(gen.get('_flow_kind') or '').strip() or 'flag-generator'
            if is_vuln_node and gkind != 'flag-generator':
                reasons.append('Flag-Generator type')
        except Exception:
            pass
        req_a = sorted([x for x in _artifact_requires_of(gen) if x])
        req_f = sorted([x for x in _required_input_fields_of(gen) if x])

        # If an artifact requirement is also present as an *optional* input field,
        # treat it as optional (do not block compatibility on it). This helps
        # avoid flagging things like Credential(user, password) as a hard missing dependency
        # when the generator can operate without it.
        try:
            all_in = _all_input_fields_of(gen)
            req_in = _required_input_fields_of(gen)
            optional_in = set(all_in) - set(req_in)
        except Exception:
            optional_in = set()
        req_a_effective = [x for x in req_a if x not in optional_in]

        missing_a = [x for x in req_a_effective if x not in have_artifacts]
        # Never report sequencer-provided fields as missing.
        missing_f = [x for x in req_f if x not in have_fields and x not in synthesized_fields]
        if missing_a:
            reasons.append('missing inputs (artifacts): ' + ', '.join(missing_a))
        if missing_f:
            reasons.append('missing inputs (fields): ' + ', '.join(missing_f))
        return (len(reasons) == 0), reasons

    out: list[dict[str, Any]] = []
    ids_to_eval = candidate_ids if candidate_ids else list(gen_by_id.keys())
    for gid in ids_to_eval:
        gen = gen_by_id.get(str(gid))
        if not isinstance(gen, dict):
            continue
        gkind = str(gen.get('_flow_kind') or '').strip() or 'flag-generator'
        if gkind != kind:
            continue
        ok, reasons = _blocked_reasons(gen)
        out.append({
            'id': str(gen.get('id') or ''),
            'name': str(gen.get('name') or ''),
            'type': gkind,
            'source': str(gen.get('_source_name') or '').strip() or 'unknown',
            'compatible': bool(ok),
            'blocked_by': reasons,
        })

    out.sort(key=lambda e: (
        0 if bool(e.get('compatible')) else 1,
        str(e.get('name') or '').lower(),
        str(e.get('id') or ''),
    ))

    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'kind': kind,
        'index': index,
        'is_vuln': bool(is_vuln_node),
        'candidates': out,
        'node_candidates': node_candidates,
        'preview_plan_path': base_plan_path,
        'allow_node_duplicates': bool(allow_node_duplicates),
    })


@app.route('/api/flag-sequencing/bundle_from_chain', methods=['POST'])
def api_flow_bundle_from_chain():
    """Deprecated: STIX bundle export has been removed in favor of .afb."""
    return jsonify({
        'ok': False,
        'error': 'STIX bundle export has been removed. Use /api/flag-sequencing/afb_from_chain.',
    }), 410


@app.route('/api/flag-sequencing/afb_from_chain', methods=['POST'])
def api_flow_afb_from_chain():
    """Build an Attack Flow Builder .afb document from a user-specified ordered chain."""
    j = request.get_json(silent=True) or {}
    scenario_label = str(j.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label)
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'No scenario specified.'}), 400

    chain = j.get('chain')
    if not isinstance(chain, list) or not chain:
        return jsonify({'ok': False, 'error': 'Missing chain.'}), 400

    chain_nodes: list[dict[str, Any]] = []
    for n in chain:
        if not isinstance(n, dict):
            continue
        nid = str(n.get('id') or '').strip()
        if not nid:
            continue
        is_vuln = False
        try:
            is_vuln = bool(n.get('is_vuln')) or bool(n.get('is_vulnerability')) or bool(n.get('is_vulnerable'))
        except Exception:
            is_vuln = False
        try:
            vulns = n.get('vulnerabilities') if isinstance(n.get('vulnerabilities'), list) else None
        except Exception:
            vulns = None
        chain_nodes.append({
            'id': nid,
            'name': str(n.get('name') or nid),
            'type': str(n.get('type') or ''),
            'compose': str(n.get('compose') or ''),
            'compose_name': str(n.get('compose_name') or ''),
            'is_vuln': bool(is_vuln),
            **({'vulnerabilities': list(vulns or [])} if vulns else {}),
        })
    if not chain_nodes:
        return jsonify({'ok': False, 'error': 'Chain contained no valid nodes.'}), 400

    # Prefer latest saved Flow assignments (from XML) when available.
    flow_assignments_from_plan: list[dict[str, Any]] = []
    try:
        flow_meta = _flow_state_from_latest_xml(scenario_norm)
        saved_assignments = (flow_meta or {}).get('flag_assignments') if isinstance(flow_meta, dict) else None
        if isinstance(saved_assignments, list) and saved_assignments:
            desired_len = len(chain_nodes)
            ordered: list[dict[str, Any]] = []
            if desired_len and len(saved_assignments) >= desired_len:
                for i in range(desired_len):
                    a = saved_assignments[i]
                    if not isinstance(a, dict):
                        ordered.append({})
                        continue
                    a2 = dict(a)
                    try:
                        a2['node_id'] = str((chain_nodes[i] or {}).get('id') or '').strip()
                    except Exception:
                        pass
                    ordered.append(a2)
            else:
                assign_by_node: dict[str, dict[str, Any]] = {}
                for a in saved_assignments:
                    if not isinstance(a, dict):
                        continue
                    nid = str(a.get('node_id') or '').strip()
                    if nid:
                        assign_by_node[nid] = a
                if assign_by_node:
                    for n in chain_nodes:
                        nid = str((n or {}).get('id') or '').strip()
                        a = assign_by_node.get(nid)
                        if isinstance(a, dict):
                            a2 = dict(a)
                            a2['node_id'] = nid
                            ordered.append(a2)
                        else:
                            ordered.append({})
            if ordered and all(isinstance(a, dict) and str(a.get('id') or a.get('generator_id') or '').strip() for a in ordered):
                flow_assignments_from_plan = ordered
    except Exception:
        flow_assignments_from_plan = []

    flag_assignments: list[dict[str, Any]] = []
    preview: dict[str, Any] | None = None
    try:
        plan_path = None
        try:
            entry = _planner_get_plan(scenario_norm)
            if entry:
                plan_path = entry.get('plan_path') or plan_path
        except Exception:
            plan_path = plan_path
        if not plan_path:
            plan_path = _latest_preview_plan_for_scenario_norm_origin(scenario_norm, origin='planner')
        if not plan_path:
            plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm)
        if plan_path and os.path.exists(plan_path):
            payload = _load_preview_payload_from_path(plan_path, scenario_label or scenario_norm)
            if not isinstance(payload, dict):
                payload = {}

            # Merge latest Flow metadata (including runtime artifacts_dir) into the
            # preview payload so export can include realized flag values.
            try:
                if isinstance(payload, dict):
                    _attach_latest_flow_into_plan_payload(payload, scenario=(scenario_label or scenario_norm))
            except Exception:
                pass

            # Prefer saved flow assignments if present (they may include artifacts_dir).
            try:
                meta = payload.get('metadata') if isinstance(payload, dict) else None
                flow = (meta or {}).get('flow') if isinstance(meta, dict) else None
                fas = flow.get('flag_assignments') if isinstance(flow, dict) else None
                if isinstance(fas, list) and fas:
                    flag_assignments = fas
            except Exception:
                pass

            # If we have flow metadata from a plan, inject it so saved chain/
            # assignments apply to this preview topology.
            if flow_meta_from_plan and isinstance(payload, dict):
                meta_out = payload.get('metadata') if isinstance(payload.get('metadata'), dict) else {}
                meta_out = dict(meta_out or {})
                meta_out['flow'] = flow_meta_from_plan
                payload['metadata'] = meta_out
            preview = payload.get('full_preview') if isinstance(payload, dict) else None
            if isinstance(preview, dict):
                # Enrich chain_nodes with resolved IPv4 + vulnerability info (if present in preview hosts).
                try:
                    id_to_ipv4: dict[str, str] = {}
                    id_to_vuln: dict[str, dict[str, Any]] = {}
                    hosts = preview.get('hosts') if isinstance(preview.get('hosts'), list) else []
                    for h in hosts:
                        if not isinstance(h, dict):
                            continue
                        hid = str(h.get('node_id') or h.get('id') or '').strip()
                        if not hid:
                            continue
                        ip_val = h.get('ipv4')
                        if ip_val is None:
                            ip_val = h.get('ip4')
                        if ip_val is None:
                            ip_val = h.get('ip')
                        ip_str = _first_valid_ipv4(ip_val)
                        if ip_str:
                            id_to_ipv4[hid] = ip_str
                        try:
                            vulns = h.get('vulnerabilities') if isinstance(h.get('vulnerabilities'), list) else None
                        except Exception:
                            vulns = None
                        try:
                            is_vuln = bool(h.get('is_vuln')) or bool(h.get('is_vulnerability')) or bool(h.get('is_vulnerable')) or bool(vulns)
                        except Exception:
                            is_vuln = bool(vulns)
                        if is_vuln or vulns:
                            id_to_vuln[hid] = {
                                'is_vuln': bool(is_vuln),
                                'vulnerabilities': list(vulns or []),
                            }
                    if id_to_ipv4 or id_to_vuln:
                        for n in chain_nodes:
                            if not isinstance(n, dict):
                                continue
                            nid2 = str(n.get('id') or '').strip()
                            if not nid2:
                                continue
                            if not str(n.get('ipv4') or '').strip() and nid2 in id_to_ipv4:
                                n['ipv4'] = id_to_ipv4[nid2]
                            if nid2 in id_to_vuln:
                                meta = id_to_vuln.get(nid2) or {}
                                if 'is_vuln' not in n:
                                    n['is_vuln'] = bool(meta.get('is_vuln'))
                                if 'vulnerabilities' not in n and meta.get('vulnerabilities'):
                                    n['vulnerabilities'] = list(meta.get('vulnerabilities') or [])
                except Exception:
                    pass
                if not flag_assignments:
                    try:
                        meta = payload.get('metadata') if isinstance(payload, dict) else None
                        flow_meta = meta.get('flow') if isinstance(meta, dict) else None
                        init_facts = _flow_normalize_fact_override(flow_meta.get('initial_facts')) if isinstance(flow_meta, dict) else None
                        goal_facts = _flow_normalize_fact_override(flow_meta.get('goal_facts')) if isinstance(flow_meta, dict) else None
                    except Exception:
                        init_facts = None
                        goal_facts = None
                    flag_assignments = _flow_compute_flag_assignments(
                        preview,
                        chain_nodes,
                        scenario_label or scenario_norm,
                        initial_facts_override=init_facts,
                        goal_facts_override=goal_facts,
                    )
    except Exception:
        flag_assignments = []

    if (not flag_assignments) and flow_assignments_from_plan:
        flag_assignments = flow_assignments_from_plan

    try:
        flow_valid, flow_errors = _flow_validate_chain_order_by_requires_produces(
            chain_nodes,
            flag_assignments,
            scenario_label=(scenario_label or scenario_norm),
        )
    except Exception:
        flow_valid, flow_errors = True, []
    try:
        assign_ids = [str(a.get('id') or a.get('generator_id') or '').strip() for a in (flag_assignments or []) if isinstance(a, dict)]
        chain_ids_dbg = [str(n.get('id') or '').strip() for n in (chain_nodes or []) if isinstance(n, dict) and str(n.get('id') or '').strip()]
        flow_errors_detail = (
            f"assignments={len(flag_assignments or [])} "
            f"assignments_with_id={len([x for x in assign_ids if x])} "
            f"chain_nodes={len(chain_nodes or [])} "
            f"chain_ids={','.join(chain_ids_dbg)}"
        )
    except Exception:
        flow_errors_detail = None
    flags_enabled = bool(flow_valid)

    try:
        app.logger.info(
            '[flow.afb_from_chain] scenario=%s chain_len=%s flow_valid=%s flow_errors=%s detail=%s',
            scenario_norm,
            len(chain_nodes or []),
            bool(flow_valid),
            (flow_errors or []),
            (flow_errors_detail or ''),
        )
    except Exception:
        pass

    afb = _attack_flow_builder_afb_for_chain(
        chain_nodes=chain_nodes,
        scenario_label=scenario_label or scenario_norm,
        flag_assignments=flag_assignments,
    )
    attack_graph = _attack_graph_for_chain(
        chain_nodes=chain_nodes,
        scenario_label=scenario_label or scenario_norm,
        flag_assignments=flag_assignments,
    )
    attack_graph_dot = _attack_graph_dot(attack_graph)
    attack_graph_pdf_base64 = _attack_graph_pdf_base64(attack_graph_dot or '')
    return jsonify({
        'ok': True,
        'scenario': scenario_label or scenario_norm,
        'length': len(chain_nodes),
        'chain': chain_nodes,
        'flag_assignments': flag_assignments,
        'afb': afb,
        'attack_graph': attack_graph,
        'attack_graph_dot': attack_graph_dot,
        'attack_graph_pdf_base64': attack_graph_pdf_base64,
        'flow_valid': bool(flow_valid),
        'flow_errors': list(flow_errors or []),
        'flags_enabled': bool(flags_enabled),
        **({'flow_errors_detail': flow_errors_detail} if flow_errors_detail else {}),
    })


@app.route('/api/flag-sequencing/attackflow')
def api_flow_attackflow():
    return jsonify({
        'ok': False,
        'error': 'STIX/AttackFlow bundle export has been removed. Use /api/flag-sequencing/attackflow_preview for chain preview and /api/flag-sequencing/afb_from_chain for Attack Flow Builder export.',
    }), 410


@app.route('/users', methods=['GET'])
def users_page():
    if not _require_admin():
        return redirect(url_for('index'))
    db = _load_users()
    raw_users = db.get('users', [])
    admin_count = sum(
        1
        for entry in raw_users
        if isinstance(entry, dict) and _normalize_role_value(entry.get('role')) == 'admin'
    )
    scenario_names, _scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(user=_current_user())
    scenario_options: list[dict[str, str]] = []
    display_by_norm: dict[str, str] = {}
    for display_name in scenario_names:
        norm = _normalize_scenario_label(display_name)
        if not norm:
            continue
        display_by_norm[norm] = display_name
        scenario_options.append({'value': norm, 'label': display_name})
    # Ensure stable alphabetical order
    scenario_options.sort(key=lambda o: o['label'].lower())
    users: list[dict] = []
    for entry in raw_users:
        if not isinstance(entry, dict):
            continue
        normalized = dict(entry)
        normalized['role'] = _normalize_role_value(entry.get('role'))
        assigned = _normalize_scenario_assignments(entry.get('scenarios'))
        normalized['assigned_scenarios'] = assigned
        normalized['assigned_scenarios_display'] = [display_by_norm.get(norm, norm) for norm in assigned]
        is_only_admin = normalized['role'] == 'admin' and admin_count <= 1
        normalized['role_locked'] = is_only_admin
        if is_only_admin:
            normalized['role_locked_reason'] = 'At least one admin must remain.'
        else:
            normalized['role_locked_reason'] = ''
        users.append(normalized)
    return render_template(
        'users.html',
        users=users,
        scenario_options=scenario_options,
        scenario_lookup=display_by_norm,
        self_change=False,
    )


@app.route('/users', methods=['POST'])
def users_create():
    if not _require_admin():
        return redirect(url_for('index'))
    username = (request.form.get('username') or '').strip()
    password = (request.form.get('password') or '').strip()
    role = _normalize_role_value(request.form.get('role'))
    scenarios = _normalize_scenario_assignments(request.form.getlist('scenarios'))
    if not username or not password:
        flash('Username and password required')
        return redirect(url_for('users_page'))
    db = _load_users()
    users = db.get('users', [])
    if any(u.get('username') == username for u in users):
        flash('Username already exists')
        return redirect(url_for('users_page'))
    users.append({
        'username': username,
        'password_hash': generate_password_hash(password),
        'role': role,
        'scenarios': scenarios,
    })
    db['users'] = users
    _save_users(db)
    flash('User created')
    return redirect(url_for('users_page'))


@app.route('/users/delete/<username>', methods=['POST'])
def users_delete(username: str):
    if not _require_admin():
        return redirect(url_for('users_page'))
    username = (username or '').strip()
    if not username:
        flash('Invalid username')
        return redirect(url_for('users_page'))
    cur = _current_user()
    db = _load_users()
    users = db.get('users', [])
    remain = [u for u in users if u.get('username') != username]
    if cur and username == cur.get('username'):
        flash('Cannot delete your own account')
        return redirect(url_for('users_page'))
    if not any(u.get('role') == 'admin' for u in remain):
        flash('At least one admin must remain')
        return redirect(url_for('users_page'))
    db['users'] = remain
    _save_users(db)
    flash('User deleted')
    return redirect(url_for('users_page'))


@app.route('/users/password/<username>', methods=['POST'])
def users_password(username: str):
    if not _require_admin():
        return redirect(url_for('users_page'))
    new_pwd = request.form.get('password') or ''
    if not new_pwd:
        flash('New password required')
        return redirect(url_for('users_page'))
    db = _load_users()
    changed = False
    for u in db.get('users', []):
        if u.get('username') == username:
            u['password_hash'] = generate_password_hash(new_pwd)
            changed = True
            break
    if changed:
        _save_users(db)
        flash('Password updated')
    else:
        flash('User not found')
    return redirect(url_for('users_page'))


@app.route('/users/role/<username>', methods=['POST'])
def users_update_role(username: str):
    if not _require_admin():
        return redirect(url_for('users_page'))
    username = (username or '').strip()
    role_value = _normalize_role_value(request.form.get('role'))
    if not username or role_value not in _ALLOWED_USER_ROLES:
        flash('Invalid role update request')
        return redirect(url_for('users_page'))
    db = _load_users()
    users = db.get('users', [])
    target = next((u for u in users if u.get('username') == username), None)
    if not target:
        flash('User not found')
        return redirect(url_for('users_page'))
    current_role = _normalize_role_value(target.get('role'))
    if current_role == role_value:
        flash('Role unchanged')
        return redirect(url_for('users_page'))
    if current_role == 'admin' and role_value != 'admin':
        has_other_admin = any(_normalize_role_value(u.get('role')) == 'admin' and u.get('username') != username for u in users)
        if not has_other_admin:
            flash('At least one admin must remain')
            return redirect(url_for('users_page'))
    target['role'] = role_value
    db['users'] = users
    _save_users(db)
    cur = _current_user()
    if cur and cur.get('username') == username:
        _set_current_user({'username': username, 'role': role_value})
    flash(f"Role updated to {role_value}")
    return redirect(url_for('users_page'))


@app.route('/users/scenarios/<username>', methods=['POST'])
def users_assign_scenarios(username: str):
    if not _require_admin():
        return redirect(url_for('users_page'))
    username = (username or '').strip()
    if not username:
        flash('Invalid username')
        return redirect(url_for('users_page'))
    selections = _normalize_scenario_assignments(request.form.getlist('scenarios'))
    db = _load_users()
    users = db.get('users', [])
    updated = False
    for entry in users:
        if entry.get('username') == username:
            entry['scenarios'] = selections
            updated = True
            break
    if updated:
        _save_users(db)
        flash('Scenario assignments updated')
    else:
        flash('User not found')
    return redirect(url_for('users_page'))


@app.route('/me/password', methods=['GET', 'POST'])
def me_password():
    if _current_user() is None:
        return redirect(url_for('login'))
    if request.method == 'GET':
        return render_template('users.html', self_change=True)
    cur = _current_user()
    cur_pwd = request.form.get('current_password') or ''
    new_pwd = request.form.get('password') or ''
    if not cur_pwd or not new_pwd:
        flash('Current and new passwords required')
        return redirect(url_for('me_password'))
    db = _load_users()
    updated = False
    for u in db.get('users', []):
        if u.get('username') == cur.get('username'):
            if not check_password_hash(u.get('password_hash', ''), cur_pwd):
                flash('Current password incorrect')
                return redirect(url_for('me_password'))
            u['password_hash'] = generate_password_hash(new_pwd)
            updated = True
            break
    if updated:
        _save_users(db)
        flash('Password changed')
    else:
        flash('User not found')
    return redirect(url_for('index'))


@app.route('/healthz')
def healthz():
    return Response('ok', mimetype='text/plain')


@app.route('/favicon.ico')
def favicon():
    # The UI uses an inline data-URL favicon in the base template, but some browsers
    # still request /favicon.ico. Return a no-content response to avoid noisy 404s.
    return ('', 204)


# Environment-configurable CORE daemon location (useful inside Docker)
CORE_HOST = os.environ.get('CORE_HOST', 'localhost')
try:
    CORE_PORT = int(os.environ.get('CORE_PORT', '50051'))
except Exception:
    CORE_PORT = 50051

def _default_core_dict():
    return _core_backend_defaults(include_password=False)


def _resolve_cli_venv_bin(
    preferred: Optional[str] = None,
    *,
    allow_fallback: bool = True,
) -> Optional[str]:
    """Return an existing venv/bin directory for local CLI invocations."""

    sanitized_preferred = _sanitize_venv_bin_path(preferred)
    if sanitized_preferred:
        if os.path.isdir(sanitized_preferred):
            return sanitized_preferred
        if not allow_fallback:
            return None

    fallback_candidates = (
        _sanitize_venv_bin_path(os.environ.get('CORE_VENV_BIN')),
        _sanitize_venv_bin_path(DEFAULT_CORE_VENV_BIN),
    )
    for candidate in fallback_candidates:
        if candidate and os.path.isdir(candidate):
            return candidate
    return None


def _prepare_cli_env(
    base_env: Optional[Dict[str, str]] = None,
    preferred_venv_bin: Optional[str] = None,
    *,
    allow_fallback: bool = True,
) -> Dict[str, str]:
    env = dict(base_env or os.environ)
    venv_bin = _resolve_cli_venv_bin(preferred_venv_bin, allow_fallback=allow_fallback)
    if venv_bin:
        original_path = env.get('PATH') or ''
        env['PATH'] = f"{venv_bin}:{original_path}" if original_path else venv_bin
        venv_root = os.path.dirname(venv_bin)
        if venv_root:
            env.setdefault('VIRTUAL_ENV', venv_root)
    return env


def _select_python_interpreter(preferred_venv_bin: Optional[str] = None) -> str:
    """Select the python interpreter to invoke the core_topo_gen CLI.

    Priority order:
    1. Explicit environment override CORE_PY (absolute path wins, otherwise treated as first lookup candidate)
    2. Interpreter binaries that live inside the preferred/core-configured venv bin (falling back to CORE_VENV_BIN env var and /opt/core/venv/bin)
    3. 'core-python', then 'python3', then 'python' discovered via PATH (with the venv bin prepended to PATH)
    4. sys.executable as a final fallback

    Returns the chosen executable string (absolute path or name)."""

    override = os.environ.get('CORE_PY')
    venv_bin = _resolve_cli_venv_bin(preferred_venv_bin)

    name_candidates: list[str] = []

    if override:
        if os.path.isabs(override):
            try:
                if os.access(override, os.X_OK):
                    return override
            except Exception:
                pass
        else:
            name_candidates.append(override)

    if venv_bin:
        preferred_python = _find_python_in_venv_bin(venv_bin)
        if preferred_python:
            return preferred_python

    name_candidates.extend(PYTHON_EXECUTABLE_NAMES)

    search_path = os.environ.get('PATH') or ''
    if venv_bin:
        search_path = f"{venv_bin}:{search_path}" if search_path else venv_bin

    for name in name_candidates:
        try:
            resolved = shutil.which(name, path=search_path)
        except Exception:
            resolved = None
        if resolved:
            return resolved

    return sys.executable or 'python'

def _get_cli_script_path() -> str:
    """Return absolute path to config2scen_core_grpc.py script."""
    return os.path.join(_get_repo_root(), 'config2scen_core_grpc.py')

# Now that helpers can resolve repo root, configure upload folder
UPLOAD_FOLDER = _uploads_dir()
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

UPLOAD_FOLDER = _uploads_dir()
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


# In-memory registry for async runs
RUNS: Dict[str, Dict[str, Any]] = {}

# Run history persistence (simple JSON log)
RUN_HISTORY_PATH = os.path.join(_outputs_dir(), 'run_history.json')
RUN_HISTORY_LOCK = threading.RLock()

def _load_run_history():
    try:
        with RUN_HISTORY_LOCK:
            if os.path.exists(RUN_HISTORY_PATH):
                with open(RUN_HISTORY_PATH, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        return [_normalize_run_history_entry(item) for item in data if isinstance(item, dict)]
                    return []
    except Exception:
        try:
            app.logger.exception('[run_history] failed reading %s', RUN_HISTORY_PATH)
        except Exception:
            pass
    return []

def _append_run_history(entry: dict) -> bool:
    """Append a run history entry to outputs/run_history.json.

    Returns True on success, False on failure.
    """
    # Enforce per-scenario semantics at the write boundary.
    try:
        entry = _normalize_run_history_entry(entry)
    except Exception:
        entry = entry if isinstance(entry, dict) else {}

    run_id = None
    try:
        run_id = (entry.get('run_id') or '').strip() if isinstance(entry.get('run_id'), str) else None
    except Exception:
        run_id = None

    os.makedirs(os.path.dirname(RUN_HISTORY_PATH), exist_ok=True)
    tmp = RUN_HISTORY_PATH + '.tmp'
    with RUN_HISTORY_LOCK:
        history = _load_run_history()
        if run_id:
            try:
                for existing in history:
                    if not isinstance(existing, dict):
                        continue
                    existing_id = existing.get('run_id')
                    if isinstance(existing_id, str) and existing_id.strip() == run_id:
                        return True
            except Exception:
                pass
        history.append(entry)
        try:
            with open(tmp, 'w', encoding='utf-8') as f:
                # Use default=str to avoid silently dropping entries due to an unexpected
                # non-serializable value. Paths/objects become strings.
                json.dump(history, f, indent=2, default=str)
            os.replace(tmp, RUN_HISTORY_PATH)
            return True
        except Exception:
            try:
                app.logger.exception('[run_history] failed writing %s', RUN_HISTORY_PATH)
            except Exception:
                pass
            try:
                if os.path.exists(tmp):
                    os.remove(tmp)
            except Exception:
                pass
            return False


EDITOR_STATE_SNAPSHOT_SUBDIR = 'editor_snapshots'


def _editor_state_snapshot_dir() -> str:
    return _ensure_private_dir(os.path.join(_outputs_dir(), EDITOR_STATE_SNAPSHOT_SUBDIR))


def _editor_state_snapshot_slug(username: Optional[str]) -> str:
    raw = (username or '').strip().lower()
    cleaned = re.sub(r'[^a-z0-9._-]+', '-', raw)
    cleaned = re.sub(r'-{2,}', '-', cleaned).strip('-_.')
    return cleaned or 'anonymous'


def _editor_state_snapshot_path(owner: dict | str | None = None) -> str:
    username: Optional[str]
    if isinstance(owner, dict):
        username = owner.get('username') if owner else None
    elif isinstance(owner, str):
        username = owner
    else:
        username = None
    slug = _editor_state_snapshot_slug(username)
    return os.path.join(_editor_state_snapshot_dir(), f"{slug}.json")


def _sanitize_snapshot_scenario(raw: Any) -> Optional[Dict[str, Any]]:
    if not isinstance(raw, dict):
        return None
    scenario = copy.deepcopy(raw)
    hitl_meta = scenario.get('hitl') if isinstance(scenario.get('hitl'), dict) else None
    if hitl_meta is not None:
        # Keep HITL fields as-is in snapshots (including draft/credential values).
        # Do not scrub or gate on validation so refresh preserves the user's inputs.
        pass
    return scenario


def _build_editor_snapshot_payload(raw_state: Any) -> Optional[Dict[str, Any]]:
    if not isinstance(raw_state, dict):
        return None
    scenarios_raw = raw_state.get('scenarios')
    if not isinstance(scenarios_raw, list):
        return None
    sanitized_scenarios: List[Dict[str, Any]] = []
    for scen in scenarios_raw:
        sanitized = _sanitize_snapshot_scenario(scen)
        if sanitized:
            sanitized_scenarios.append(sanitized)
    if not sanitized_scenarios:
        return None
    snapshot: Dict[str, Any] = {'scenarios': sanitized_scenarios}
    core_meta = _normalize_core_config(raw_state.get('core'), include_password=True)
    if core_meta:
        snapshot['core'] = core_meta
    result_path = raw_state.get('result_path') or raw_state.get('resultPath')
    if isinstance(result_path, str) and result_path.strip():
        snapshot['result_path'] = result_path.strip()
    base_upload = raw_state.get('base_upload')
    if isinstance(base_upload, dict):
        base_copy: Dict[str, Any] = {}
        path_val = base_upload.get('path') or base_upload.get('filepath')
        if isinstance(path_val, str) and path_val:
            base_copy['path'] = path_val
        display_name = base_upload.get('display_name') or base_upload.get('displayName')
        if isinstance(display_name, str) and display_name:
            base_copy['display_name'] = display_name
        if 'valid' in base_upload:
            base_copy['valid'] = bool(base_upload.get('valid'))
        if base_copy:
            snapshot['base_upload'] = base_copy
    host_ifaces = raw_state.get('host_interfaces')
    if isinstance(host_ifaces, list) and host_ifaces:
        snapshot['host_interfaces'] = [iface for iface in host_ifaces if isinstance(iface, dict)]
    for key in ('host_interfaces_source', 'host_interfaces_metadata', 'host_interfaces_fetched_at'):
        value = raw_state.get(key)
        if value is not None:
            snapshot[key] = value
    project_hint = raw_state.get('project_key_hint')
    if isinstance(project_hint, str) and project_hint.strip():
        snapshot['project_key_hint'] = project_hint.strip()
    elif 'result_path' in snapshot:
        snapshot['project_key_hint'] = snapshot['result_path']
    active_index = raw_state.get('active_index')
    if isinstance(active_index, int):
        snapshot['active_index'] = active_index
    elif isinstance(active_index, str):
        try:
            snapshot['active_index'] = int(active_index)
        except Exception:
            pass
    scenario_query = raw_state.get('scenario_query')
    if isinstance(scenario_query, str) and scenario_query.strip():
        snapshot['scenario_query'] = scenario_query.strip()
    return snapshot


def _write_editor_state_snapshot(snapshot: Dict[str, Any], *, user: Optional[dict] = None) -> None:
    if not snapshot:
        return
    path = _editor_state_snapshot_path(user)
    tmp_path = f"{path}.tmp"
    try:
        with open(tmp_path, 'w', encoding='utf-8') as handle:
            json.dump(snapshot, handle, indent=2)
        os.replace(tmp_path, path)
        _ensure_private_file(path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _persist_editor_state_snapshot(raw_state: Any, *, user: Optional[dict] = None) -> None:
    snapshot = _build_editor_snapshot_payload(raw_state)
    if not snapshot:
        return
    _write_editor_state_snapshot(snapshot, user=user)


def _load_editor_state_snapshot(user: Optional[dict] = None) -> Optional[Dict[str, Any]]:
    path = _editor_state_snapshot_path(user)
    if not os.path.exists(path):
        return None
    try:
        with open(path, 'r', encoding='utf-8') as handle:
            data = json.load(handle)
        if isinstance(data, dict):
            snapshot = _scrub_snapshot_transient_errors(data)
            return _sanitize_snapshot_for_user(snapshot, user)
    except Exception:
        pass
    return None


def _delete_editor_state_snapshot(user: Optional[dict] = None) -> bool:
    """Delete the editor snapshot for a specific user (if it exists)."""
    path = _editor_state_snapshot_path(user)
    try:
        if os.path.exists(path):
            os.remove(path)
            return True
    except Exception:
        return False
    return False


def _scrub_snapshot_transient_errors(snapshot: Dict[str, Any]) -> Dict[str, Any]:
    scenarios = snapshot.get('scenarios')
    if isinstance(scenarios, list):
        for scen in scenarios:
            if not isinstance(scen, dict):
                continue
            hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else None
            if not hitl_meta:
                continue
            prox_meta = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else None
            if prox_meta:
                prox_meta.pop('inventory_error', None)
    return snapshot


def _sanitize_snapshot_for_user(snapshot: Dict[str, Any], user: Optional[dict]) -> Dict[str, Any]:
    allowed_norms = _builder_allowed_norms(user)
    if allowed_norms is None:
        snapshot.pop('builder_restricted_scenarios', None)
        snapshot.pop('builder_assigned_scenarios', None)
        snapshot.pop('builder_no_assignments', None)
        return snapshot
    filtered = _filter_scenarios_by_norms(snapshot.get('scenarios') or [], allowed_norms)
    snapshot['scenarios'] = filtered
    if filtered:
        active_idx = snapshot.get('active_index')
        if not isinstance(active_idx, int) or active_idx < 0 or active_idx >= len(filtered):
            snapshot['active_index'] = 0
    else:
        snapshot.pop('active_index', None)
    snapshot['builder_restricted_scenarios'] = True
    snapshot['builder_assigned_scenarios'] = sorted(allowed_norms)
    snapshot['builder_no_assignments'] = not bool(allowed_norms)
    return snapshot

def _scenario_names_from_xml(xml_path: str) -> list[str]:
    names: list[str] = []
    try:
        if not xml_path or not os.path.exists(xml_path):
            return names
        data = _parse_scenarios_xml(xml_path)
        for scen in data.get('scenarios', []):
            nm = scen.get('name')
            if nm and nm not in names:
                names.append(nm)
    except Exception:
        pass
    return names


def _normalize_scenario_label(value: Any) -> str:
    if value is None:
        return ''
    text = value if isinstance(value, str) else str(value)
    text = text.strip().lower()
    return re.sub(r'\s+', ' ', text)


def _scenario_match_key(value: Any) -> str:
    """Return a forgiving key for scenario-name comparisons.

    We keep _normalize_scenario_label() stable for display and storage,
    but compare assignments using this match key so punctuation differences
    don't hide scenarios (e.g., 'Scenario-1' vs 'Scenario 1').
    """
    norm = _normalize_scenario_label(value)
    if not norm:
        return ''
    return re.sub(r'[^a-z0-9]+', '', norm)


def _scenario_display_sort_key(value: Any) -> tuple:
    """Natural-ish sort for scenario display strings.

    Examples:
      - Scenario 1 < Scenario 1b < Scenario 2
      - Anything without a number sorts after numbered scenarios.
    """
    if value is None:
        return (1, '', float('inf'), '', '')
    text = value if isinstance(value, str) else str(value)
    text = text.strip()
    lowered = text.lower()
    m = re.search(r'(\d+)([a-z]*)', lowered)
    if not m:
        return (1, lowered, float('inf'), '', lowered)
    try:
        num = int(m.group(1))
    except Exception:
        num = float('inf')
    suffix = m.group(2) or ''
    prefix = lowered[:m.start(1)].strip()
    return (0, prefix, num, suffix, lowered)


def _select_latest_core_secret_record(scenario_norm: str | None = None) -> Optional[Dict[str, Any]]:
    """Find the most recent stored CORE credential, optionally filtered by scenario."""

    try:
        secret_dir = _core_secret_dir()
        entries = os.listdir(secret_dir)
    except Exception:
        return None
    if not entries:
        return None
    best_for_scenario: tuple[float, Dict[str, Any]] | None = None
    best_overall: tuple[float, Dict[str, Any]] | None = None
    for name in entries:
        if not name.endswith('.json'):
            continue
        identifier = name[:-5]
        try:
            record = _load_core_credentials(identifier)
        except Exception:
            continue
        if not record:
            continue
        stored_at = record.get('stored_at')
        ts_val = 0.0
        if isinstance(stored_at, str) and stored_at:
            try:
                ts_val = datetime.datetime.fromisoformat(stored_at.replace('Z', '+00:00')).timestamp()
            except Exception:
                ts_val = 0.0
        if not ts_val:
            try:
                ts_val = os.path.getmtime(os.path.join(secret_dir, name))
            except Exception:
                ts_val = 0.0
        record_norm = _normalize_scenario_label(record.get('scenario_name')) if record.get('scenario_name') else ''
        if scenario_norm and record_norm == scenario_norm:
            if not best_for_scenario or ts_val > best_for_scenario[0]:
                best_for_scenario = (ts_val, record)
        else:
            if not best_overall or ts_val > best_overall[0]:
                best_overall = (ts_val, record)
    if best_for_scenario:
        return best_for_scenario[1]
    return best_overall[1] if best_overall else None


def _scenario_catalog_file() -> str:
    return os.path.join(_outputs_dir(), 'scenario_catalog.json')


def _scenario_catalog_force_empty() -> bool:
    """Return True if the scenario catalog explicitly indicates a full purge.

    This prevents editor snapshots from re-seeding deleted scenarios after an
    explicit "delete all" action.
    """
    path = _scenario_catalog_file()
    if not os.path.exists(path):
        return False
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception:
        return False
    if isinstance(data, dict) and data.get('force_empty') is True:
        return True
    return False


def _load_scenario_catalog_from_disk() -> tuple[list[str], dict[str, set[str]], dict[str, str]]:
    path = _scenario_catalog_file()
    if not os.path.exists(path):
        return [], {}, {}
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception:
        return [], {}, {}
    names_raw = data.get('names')
    sources_raw = data.get('sources')
    ordered: list[str] = []
    path_map: dict[str, set[str]] = defaultdict(set)
    seen_norms: set[str] = set()
    participant_by_norm: dict[str, str] = {}

    def _record_candidate(norm_key: str, candidate: Any) -> None:
        if not norm_key or candidate in (None, ''):
            return
        if isinstance(candidate, (list, tuple, set)):
            for item in candidate:
                _record_candidate(norm_key, item)
            return
        value = str(candidate).strip()
        if not value:
            return
        try:
            ap = os.path.abspath(value)
        except Exception:
            ap = value
        path_map[norm_key].add(ap)

    if isinstance(names_raw, list):
        for idx, raw_name in enumerate(names_raw):
            name = (str(raw_name).strip() if raw_name is not None else '')
            if not name:
                continue
            norm = _normalize_scenario_label(name)
            if not norm or norm in seen_norms:
                continue
            seen_norms.add(norm)
            ordered.append(name)
            source_candidate: Any = None
            if isinstance(sources_raw, list) and idx < len(sources_raw):
                source_candidate = sources_raw[idx]
            elif isinstance(sources_raw, dict):
                source_candidate = sources_raw.get(name) or sources_raw.get(norm)
            _record_candidate(norm, source_candidate)

    if isinstance(sources_raw, dict):
        for key, candidate in sources_raw.items():
            norm = _normalize_scenario_label(key)
            if not norm:
                continue
            _record_candidate(norm, candidate)

    participant_raw = data.get('participant_urls')
    if isinstance(participant_raw, dict):
        for key, value in participant_raw.items():
            norm = _normalize_scenario_label(key)
            if not norm:
                continue
            # Preserve explicit "cleared" values (empty/invalid) so the UI can override
            # stale scenario XML that might still contain a participant URL.
            url_value = _normalize_participant_proxmox_url(value)
            if url_value:
                participant_by_norm[norm] = url_value
            else:
                participant_by_norm.setdefault(norm, '')

    return ordered, path_map, participant_by_norm


def _persist_scenario_catalog(
    names: Iterable[Any],
    source_path: Optional[Any] = None,
    participant_urls: Optional[Dict[str, Any]] = None,
) -> None:
    catalog_path = _scenario_catalog_file()
    tmp_path = catalog_path + '.tmp'
    ordered: list[str] = []
    seen_norms: set[str] = set()
    participant_map_input = participant_urls or {}
    participant_by_norm: dict[str, str] = {}
    for raw in names or []:
        if raw in (None, ''):
            continue
        name = str(raw).strip()
        if not name:
            continue
        norm = _normalize_scenario_label(name)
        if not norm or norm in seen_norms:
            continue
        seen_norms.add(norm)
        ordered.append(name)
        # Capture participant URL hints for each unique scenario
        if norm in participant_map_input and norm not in participant_by_norm:
            normalized_url = _normalize_participant_proxmox_url(participant_map_input[norm])
            if normalized_url:
                participant_by_norm[norm] = normalized_url
    try:
        sources_out: Any = None
        if source_path:
            # Accept dict/list/str for per-scenario source paths.
            if isinstance(source_path, dict):
                mapped: dict[str, str] = {}
                for key, value in source_path.items():
                    norm = _normalize_scenario_label(key)
                    if not norm:
                        continue
                    try:
                        val = os.path.abspath(str(value))
                    except Exception:
                        val = str(value)
                    if val:
                        mapped[key] = val
                sources_out = mapped
            elif isinstance(source_path, (list, tuple)):
                raw_list = list(source_path)
                if raw_list:
                    if len(raw_list) == 1:
                        candidate = raw_list[0]
                        try:
                            abs_source = os.path.abspath(str(candidate))
                        except Exception:
                            abs_source = str(candidate)
                        sources_out = [abs_source for _ in ordered]
                    else:
                        out_list: list[str] = []
                        for idx, _name in enumerate(ordered):
                            candidate = raw_list[idx] if idx < len(raw_list) else ''
                            try:
                                abs_source = os.path.abspath(str(candidate)) if candidate else ''
                            except Exception:
                                abs_source = str(candidate)
                            out_list.append(abs_source)
                        sources_out = out_list
            else:
                candidate = str(source_path).strip()
                if candidate:
                    try:
                        abs_source = os.path.abspath(candidate)
                    except Exception:
                        abs_source = candidate
                    sources_out = [abs_source for _ in ordered]
        if sources_out is None:
            sources_out = ['' for _ in ordered]
        payload = {
            'names': ordered,
            'sources': sources_out,
            'updated_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
        }
        if participant_by_norm:
            payload['participant_urls'] = participant_by_norm
        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as f:
            json.dump(payload, f, indent=2)
        os.replace(tmp_path, catalog_path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _remove_scenarios_from_catalog(names_to_remove: Iterable[Any]) -> Dict[str, Any]:
    """Remove scenario names from outputs/scenario_catalog.json.

    This is the server-side source of truth for which scenarios are listed on
    refresh, so UI-only deletion must update this file.
    """
    catalog_path = _scenario_catalog_file()
    try:
        if not os.path.exists(catalog_path):
            return {'removed': 0, 'remaining': 0}
    except Exception:
        return {'removed': 0, 'remaining': 0}

    norms_to_remove: set[str] = set()
    for raw in names_to_remove or []:
        norm = _normalize_scenario_label(raw)
        if norm:
            norms_to_remove.add(norm)
    if not norms_to_remove:
        return {'removed': 0, 'remaining': 0}

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return {'removed': 0, 'remaining': 0}

        existing_names = payload.get('names')
        if not isinstance(existing_names, list):
            existing_names = []
        before = list(existing_names)
        kept_names: list[str] = []
        kept_norms: set[str] = set()
        for name in before:
            norm = _normalize_scenario_label(name)
            if not norm:
                continue
            if norm in norms_to_remove:
                continue
            if norm in kept_norms:
                continue
            kept_norms.add(norm)
            kept_names.append(str(name))
        removed = max(0, len(before) - len(kept_names))

        payload['names'] = kept_names
        # Keep sources aligned to names length (list form).
        existing_sources = payload.get('sources')
        if isinstance(existing_sources, list):
            payload['sources'] = [existing_sources[0] if existing_sources else '' for _ in kept_names]
        elif isinstance(existing_sources, dict):
            # Filter dict-form sources.
            payload['sources'] = {
                key: val
                for key, val in existing_sources.items()
                if _normalize_scenario_label(key) in kept_norms
            }

        # Prune hint maps keyed by scenario norm.
        for hint_key in ('participant_urls', 'hitl_validation', 'hitl_config'):
            hint_map = payload.get(hint_key)
            if isinstance(hint_map, dict):
                payload[hint_key] = {
                    key: val
                    for key, val in hint_map.items()
                    if _normalize_scenario_label(key) in kept_norms
                }

        if not kept_names:
            payload['force_empty'] = True
            payload['force_empty_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'
        else:
            payload.pop('force_empty', None)
            payload.pop('force_empty_at', None)

        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
        return {'removed': removed, 'remaining': len(kept_names)}
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass
        raise


def _delete_saved_scenario_xml_artifacts(names_to_remove: Iterable[Any]) -> Dict[str, Any]:
    """Delete saved scenario XML artifacts under outputs/scenarios-*/.

    These artifacts are re-discovered on page load by _collect_scenario_catalog(),
    so UI deletion must remove them to prevent the scenario from reappearing.
    """
    stems: set[str] = set()
    match_keys: set[str] = set()
    for raw in names_to_remove or []:
        try:
            name = str(raw).strip()
        except Exception:
            name = ''
        if not name:
            continue
        mk = _scenario_match_key(name)
        if mk:
            match_keys.add(mk)
        try:
            stem = secure_filename(name).strip('_-.')
        except Exception:
            stem = ''
        if stem:
            stems.add(stem)
    if not stems and not match_keys:
        return {'artifacts_removed': 0}

    outputs_root = os.path.abspath(_outputs_dir())
    removed = 0
    try:
        if not outputs_root or not os.path.isdir(outputs_root):
            return {'artifacts_removed': 0}
    except Exception:
        return {'artifacts_removed': 0}

    # Mirror discovery logic: scan outputs/scenarios-* folders (newest first, bounded).
    dirs: list[tuple[float, str]] = []
    try:
        for entry in os.listdir(outputs_root):
            if not entry.startswith('scenarios-'):
                continue
            folder = os.path.join(outputs_root, entry)
            if not os.path.isdir(folder):
                continue
            try:
                mtime = os.path.getmtime(folder)
            except Exception:
                mtime = 0.0
            dirs.append((mtime, folder))
    except Exception:
        dirs = []
    dirs.sort(key=lambda t: t[0], reverse=True)

    for _mtime, folder in dirs[:500]:
        # Fast path: attempt stem-based delete when possible.
        # NOTE: Some deployments (and some macOS volumes) can be case-sensitive.
        # The UI typically writes files like "Scenario_1.xml" but secure_filename()
        # yields "scenario_1". Use a case-insensitive match within the folder.
        try:
            folder_entries = os.listdir(folder)
        except Exception:
            folder_entries = []

        stem_targets = {f"{stem}.xml".lower() for stem in stems if stem}
        if stem_targets and folder_entries:
            for fname in list(folder_entries):
                try:
                    if fname.lower() not in stem_targets:
                        continue
                    path = os.path.join(folder, fname)
                    if not os.path.isfile(path):
                        continue
                    p_abs = os.path.abspath(path)
                    if not p_abs.startswith(outputs_root + os.sep):
                        continue
                    os.remove(p_abs)
                    removed += 1
                except Exception:
                    continue

        # Robust path: scan XML contents and delete any matching scenario(s).
        if match_keys and folder_entries:
            for fname in list(folder_entries):
                try:
                    if not fname.lower().endswith('.xml'):
                        continue
                    # Skip CORE pre/post captures; only include user-authored scenario XML.
                    if fname.lower().startswith('core-'):
                        continue
                    # Mirror discovery behavior for scenario editor saves (case-insensitive).
                    if not fname.lower().startswith('scenario_'):
                        continue
                    path = os.path.join(folder, fname)
                    if not os.path.isfile(path):
                        continue
                    try:
                        names_in_file = _scenario_names_from_xml(path)
                    except Exception:
                        names_in_file = []
                    if not names_in_file:
                        continue
                    if not any(_scenario_match_key(nm) in match_keys for nm in names_in_file):
                        continue
                    p_abs = os.path.abspath(path)
                    if not p_abs.startswith(outputs_root + os.sep):
                        continue
                    try:
                        os.remove(p_abs)
                        removed += 1
                    except Exception:
                        continue
                except Exception:
                    continue
        # Best-effort cleanup of empty folders
        try:
            if folder.startswith(outputs_root + os.sep) and os.path.isdir(folder) and not os.listdir(folder):
                os.rmdir(folder)
        except Exception:
            pass

    return {'artifacts_removed': removed}


def _remove_scenarios_from_all_editor_snapshots(names_to_remove: Iterable[Any]) -> Dict[str, Any]:
    """Remove scenario entries from all editor snapshots in outputs/editor_snapshots.

    _collect_scenario_catalog() merges names from all snapshots and treats them as
    protected, so we must purge deleted scenarios from these snapshots.
    """
    remove_norms: set[str] = set()
    remove_match: set[str] = set()
    for raw in names_to_remove or []:
        try:
            s = raw if isinstance(raw, str) else str(raw)
        except Exception:
            s = ''
        s = s.strip() if isinstance(s, str) else ''
        if not s:
            continue
        norm = _normalize_scenario_label(s)
        if norm:
            remove_norms.add(norm)
        mk = _scenario_match_key(s)
        if mk:
            remove_match.add(mk)
    if not remove_norms:
        return {'snapshots_updated': 0, 'snapshots_deleted': 0, 'snapshot_scenarios_removed': 0}

    snap_dir = _editor_state_snapshot_dir()
    try:
        entries = [n for n in os.listdir(snap_dir) if n.endswith('.json')]
    except Exception:
        entries = []

    updated = 0
    deleted = 0
    scenarios_removed = 0
    for fname in entries:
        path = os.path.join(snap_dir, fname)
        try:
            with open(path, 'r', encoding='utf-8') as fh:
                snap = json.load(fh)
        except Exception:
            continue
        if not isinstance(snap, dict):
            continue
        scen_list = snap.get('scenarios')
        if not isinstance(scen_list, list) or not scen_list:
            # Still clear scenario_query if it targets a deleted scenario.
            try:
                q = snap.get('scenario_query')
                if q and (_normalize_scenario_label(q) in remove_norms or _scenario_match_key(q) in remove_match):
                    snap['scenario_query'] = ''
                    tmp = path + '.tmp'
                    with open(tmp, 'w', encoding='utf-8') as fh:
                        json.dump(snap, fh, indent=2)
                    os.replace(tmp, path)
                    updated += 1
            except Exception:
                pass
            continue
        kept: list[Any] = []
        removed_here = 0
        for idx, scen in enumerate(scen_list, start=1):
            if not isinstance(scen, dict):
                kept.append(scen)
                continue
            raw_name = scen.get('name')
            display = raw_name.strip() if isinstance(raw_name, str) and raw_name.strip() else f"Scenario {idx}"
            norm = _normalize_scenario_label(display)
            mk = _scenario_match_key(display)
            if (norm and norm in remove_norms) or (mk and mk in remove_match):
                removed_here += 1
                continue
            kept.append(scen)
        if not removed_here:
            # Still clear scenario_query if it targets a deleted scenario.
            try:
                q = snap.get('scenario_query')
                if q and (_normalize_scenario_label(q) in remove_norms or _scenario_match_key(q) in remove_match):
                    snap['scenario_query'] = ''
                    tmp = path + '.tmp'
                    with open(tmp, 'w', encoding='utf-8') as fh:
                        json.dump(snap, fh, indent=2)
                    os.replace(tmp, path)
                    updated += 1
            except Exception:
                pass
            continue
        scenarios_removed += removed_here
        snap['scenarios'] = kept
        # Clear scenario_query if it targets a deleted scenario.
        try:
            q = snap.get('scenario_query')
            if q and (_normalize_scenario_label(q) in remove_norms or _scenario_match_key(q) in remove_match):
                snap['scenario_query'] = ''
        except Exception:
            pass
        try:
            if not kept:
                os.remove(path)
                deleted += 1
            else:
                tmp = path + '.tmp'
                with open(tmp, 'w', encoding='utf-8') as fh:
                    json.dump(snap, fh, indent=2)
                os.replace(tmp, path)
                updated += 1
        except Exception:
            continue

    return {
        'snapshots_updated': updated,
        'snapshots_deleted': deleted,
        'snapshot_scenarios_removed': scenarios_removed,
    }


def _merge_participant_urls_into_scenario_catalog(participant_urls: Dict[str, Any]) -> None:
    """Merge participant URL hints into the scenario catalog without changing names/sources."""
    if not isinstance(participant_urls, dict) or not participant_urls:
        return
    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return
    try:
        if not os.path.exists(catalog_path):
            return
    except Exception:
        return

    normalized_updates: dict[str, str] = {}
    normalized_clears: set[str] = set()
    for key, value in participant_urls.items():
        norm = _normalize_scenario_label(key)
        if not norm:
            continue
        url_value = _normalize_participant_proxmox_url(value)
        if url_value:
            normalized_updates[norm] = url_value
        else:
            # Explicitly cleared values should persist as an override (empty)
            # to prevent falling back to stale scenario XML.
            normalized_clears.add(norm)

    if not normalized_updates and not normalized_clears:
        return

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return
        existing = payload.get('participant_urls')
        if not isinstance(existing, dict):
            existing = {}

        merged = dict(existing)
        for norm_key in normalized_clears:
            merged[norm_key] = ''
        merged.update(normalized_updates)

        payload['participant_urls'] = merged
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _sanitize_hitl_validation_hint(value: Any) -> Optional[Dict[str, Any]]:
    """Return a safe, compact HITL validation hint payload.

    This must never include secret material (passwords, tokens).
    """
    if not isinstance(value, dict):
        return None
    raw = dict(value)
    out: Dict[str, Any] = {}
    for key in (
        'url',
        'port',
        'verify_ssl',
        'secret_id',
        'validated',
        'last_validated_at',
        'stored_at',
        'last_message',
        'vm_key',
        'vm_name',
        'vm_node',
        'grpc_host',
        'grpc_port',
        'ssh_host',
        'ssh_port',
        'core_secret_id',
        'stored_summary',
        'last_tested_at',
        'last_tested_status',
        'last_tested_message',
        'last_tested_host',
        'last_tested_port',
    ):
        if key in raw:
            out[key] = raw.get(key)
    # Normalize identifiers
    if isinstance(out.get('secret_id'), str):
        out['secret_id'] = out['secret_id'].strip() or None
    if isinstance(out.get('core_secret_id'), str):
        out['core_secret_id'] = out['core_secret_id'].strip() or None
    # Drop any known secret material, defensively.
    for secret_field in ('password', 'token_secret', 'api_secret', 'api_token_secret', 'ssh_password'):
        out.pop(secret_field, None)
    if not out:
        return None
    return out


def _load_scenario_hitl_validation_from_disk() -> dict[str, Dict[str, Any]]:
    """Load per-scenario HITL validation hints from scenario_catalog.json."""
    path = _scenario_catalog_file()
    if not os.path.exists(path):
        return {}
    try:
        with open(path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
    except Exception:
        return {}
    if not isinstance(payload, dict):
        return {}
    raw = payload.get('hitl_validation')
    if not isinstance(raw, dict):
        return {}
    out: dict[str, Dict[str, Any]] = {}
    for scen_key, scen_val in raw.items():
        norm = _normalize_scenario_label(scen_key)
        if not norm or not isinstance(scen_val, dict):
            continue
        entry: Dict[str, Any] = {}
        prox = _sanitize_hitl_validation_hint(scen_val.get('proxmox'))
        core = _sanitize_hitl_validation_hint(scen_val.get('core'))
        if prox:
            entry['proxmox'] = prox
        if core:
            entry['core'] = core
        if entry:
            out[norm] = entry
    return out


def _merge_hitl_validation_into_scenario_catalog(
    scenario_name: str,
    *,
    proxmox: Optional[Dict[str, Any]] = None,
    core: Optional[Dict[str, Any]] = None,
) -> None:
    """Merge safe HITL validation hints into the scenario catalog."""
    scenario_norm = _normalize_scenario_label(scenario_name)
    if not scenario_norm:
        return
    prox_clean = _sanitize_hitl_validation_hint(proxmox) if proxmox else None
    core_clean = _sanitize_hitl_validation_hint(core) if core else None
    if not prox_clean and not core_clean:
        return

    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return
    try:
        if not os.path.exists(catalog_path):
            return
    except Exception:
        return

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return
        existing = payload.get('hitl_validation')
        if not isinstance(existing, dict):
            existing = {}
        merged = dict(existing)
        current_entry = merged.get(scenario_norm)
        if not isinstance(current_entry, dict):
            current_entry = {}
        current_entry = dict(current_entry)
        if prox_clean:
            current_entry['proxmox'] = prox_clean
        if core_clean:
            current_entry['core'] = core_clean
        merged[scenario_norm] = current_entry
        payload['hitl_validation'] = merged
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _clear_hitl_validation_in_scenario_catalog(scenario_name: str, *, proxmox: bool = False, core: bool = False) -> None:
    scenario_norm = _normalize_scenario_label(scenario_name)
    if not scenario_norm:
        return
    if not proxmox and not core:
        return
    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return
    try:
        if not os.path.exists(catalog_path):
            return
    except Exception:
        return

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return
        existing = payload.get('hitl_validation')
        if not isinstance(existing, dict):
            return
        merged = dict(existing)
        entry = merged.get(scenario_norm)
        if not isinstance(entry, dict):
            return
        entry = dict(entry)
        if proxmox:
            entry.pop('proxmox', None)
        if core:
            entry.pop('core', None)
        if entry:
            merged[scenario_norm] = entry
        else:
            merged.pop(scenario_norm, None)
        payload['hitl_validation'] = merged
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _clear_hitl_config_in_scenario_catalog(
    scenario_name: str,
    *,
    clear_core_vm: bool = False,
    clear_config: bool = False,
) -> None:
    """Clear builder-safe HITL config hints stored in scenario_catalog.json.

    Notes:
    - This never deletes encrypted secrets; it only clears non-secret hints.
    - clear_core_vm implies clearing all HITL config for that scenario.
    - clear_config clears Steps 35 fields, keeping Step 2 selection if present.
    """

    scenario_norm = _normalize_scenario_label(scenario_name)
    if not scenario_norm:
        return
    if not clear_core_vm and not clear_config:
        return

    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return
    try:
        if not os.path.exists(catalog_path):
            return
    except Exception:
        return

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return

        hc = payload.get('hitl_config')
        if not isinstance(hc, dict):
            hc = {}
        hc = dict(hc)

        hv = payload.get('hitl_validation')
        if not isinstance(hv, dict):
            hv = {}
        hv = dict(hv)

        if clear_core_vm:
            # Drop all config hints for this scenario.
            hc.pop(scenario_norm, None)

            # Also clear CORE VM selection fields from hitl_validation so builder doesn't
            # keep showing a previously-selected CORE VM after admin clears it.
            entry = hv.get(scenario_norm)
            if isinstance(entry, dict):
                entry = dict(entry)
                core_hint = entry.get('core')
                if isinstance(core_hint, dict):
                    core_hint = dict(core_hint)
                    for key in ('vm_key', 'vm_name', 'vm_node', 'vmid'):
                        core_hint.pop(key, None)
                    if core_hint:
                        entry['core'] = core_hint
                    else:
                        entry.pop('core', None)
                if entry:
                    hv[scenario_norm] = entry
                else:
                    hv.pop(scenario_norm, None)

        elif clear_config:
            entry = hc.get(scenario_norm)
            if isinstance(entry, dict):
                entry = dict(entry)
                entry['enabled'] = False
                entry.pop('interfaces', None)
                entry.pop('participant_proxmox_url', None)
                core_cfg = entry.get('core')
                if isinstance(core_cfg, dict):
                    core_cfg = dict(core_cfg)
                    core_cfg.pop('internal_bridge', None)
                    core_cfg.pop('internal_bridge_owner', None)
                    if core_cfg:
                        entry['core'] = core_cfg
                    else:
                        entry.pop('core', None)
                hc[scenario_norm] = entry

        payload['hitl_config'] = hc
        payload['hitl_validation'] = hv
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _scrub_hitl_validation_usernames_in_scenario_catalog() -> bool:
    """Remove any stored usernames/secret material from hitl_validation entries.

    This is an idempotent cleanup for older catalogs that may have persisted
    proxmox/core username fields. Returns True if a write occurred.
    """
    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return False
    try:
        if not os.path.exists(catalog_path):
            return False
    except Exception:
        return False

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return False
        hv = payload.get('hitl_validation')
        if not isinstance(hv, dict) or not hv:
            return False

        changed = False
        cleaned: Dict[str, Any] = {}
        for scen_key, scen_val in hv.items():
            if not isinstance(scen_key, str):
                continue
            if not isinstance(scen_val, dict):
                cleaned[scen_key] = scen_val
                continue
            entry = dict(scen_val)
            prox = entry.get('proxmox')
            if isinstance(prox, dict):
                prox2 = dict(prox)
                for k in ('username', 'user', 'password', 'token_secret', 'api_secret', 'api_token_secret'):
                    if k in prox2:
                        prox2.pop(k, None)
                        changed = True
                entry['proxmox'] = prox2
            core = entry.get('core')
            if isinstance(core, dict):
                core2 = dict(core)
                for k in ('ssh_username', 'username', 'user', 'ssh_password', 'password'):
                    if k in core2:
                        core2.pop(k, None)
                        changed = True
                entry['core'] = core2
            cleaned[scen_key] = entry

        if not changed:
            return False
        payload['hitl_validation'] = cleaned
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
        return True
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass
        return False


def _extract_participant_url_hints_from_scenarios(scenarios: Any) -> dict[str, str]:
    hints: dict[str, str] = {}
    if not isinstance(scenarios, list):
        return hints
    for scen in scenarios:
        if not isinstance(scen, dict):
            continue
        norm = _normalize_scenario_label(scen.get('name'))
        if not norm:
            continue
        hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else None
        if not hitl_meta:
            continue
        url_value = ''
        for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
            normalized = _normalize_participant_proxmox_url(hitl_meta.get(key))
            if normalized:
                url_value = normalized
                break
        if url_value:
            hints[norm] = url_value
    return hints


def _sanitize_hitl_config_hint(value: Any) -> Optional[Dict[str, Any]]:
    """Return a safe HITL config payload suitable for sharing with builder views.

    Must not include passwords/tokens and should avoid usernames.
    """
    if not isinstance(value, dict):
        return None
    raw = dict(value)
    out: Dict[str, Any] = {
        'enabled': bool(raw.get('enabled')),
    }

    # Participant UI URL is non-secret and should persist into builder view.
    try:
        for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
            candidate = raw.get(key)
            normalized = _normalize_participant_proxmox_url(candidate)
            if normalized:
                out['participant_proxmox_url'] = normalized
                break
    except Exception:
        pass

    # Core (keep VM selection + connection endpoints, avoid ssh_username/password)
    core_raw = raw.get('core') if isinstance(raw.get('core'), dict) else {}
    core: Dict[str, Any] = {}
    for key in (
        'vm_key',
        'vm_name',
        'vm_node',
        'grpc_host',
        'grpc_port',
        'ssh_host',
        'ssh_port',
        'internal_bridge',
        'internal_bridge_owner',
        'core_secret_id',
        'validated',
        'last_validated_at',
        'stored_at',
        'last_tested_at',
        'last_tested_status',
        'last_tested_message',
        'last_tested_host',
        'last_tested_port',
    ):
        if key in core_raw:
            core[key] = core_raw.get(key)
    if isinstance(core.get('core_secret_id'), str):
        core['core_secret_id'] = core['core_secret_id'].strip() or None
    if core:
        out['core'] = core

    # Proxmox (keep URL/port + secret_id, avoid username/password)
    prox_raw = raw.get('proxmox') if isinstance(raw.get('proxmox'), dict) else {}
    prox: Dict[str, Any] = {}
    for key in ('url', 'port', 'verify_ssl', 'secret_id', 'validated', 'last_validated_at', 'stored_at'):
        if key in prox_raw:
            prox[key] = prox_raw.get(key)
    if isinstance(prox.get('secret_id'), str):
        prox['secret_id'] = prox['secret_id'].strip() or None
    if prox:
        out['proxmox'] = prox

    # Interfaces + mappings (non-secret)
    iface_list = raw.get('interfaces')

    # Best-effort: if the admin snapshot contains a Proxmox inventory, we can
    # resolve the CORE-side bridge by matching interface MACs.
    inv_vms: List[Dict[str, Any]] = []
    try:
        prox_meta = raw.get('proxmox') if isinstance(raw.get('proxmox'), dict) else {}
        inv = prox_meta.get('inventory') if isinstance(prox_meta.get('inventory'), dict) else {}
        inv_vms_raw = inv.get('vms')
        if isinstance(inv_vms_raw, list):
            inv_vms = [v for v in inv_vms_raw if isinstance(v, dict)]
    except Exception:
        inv_vms = []

    def _parse_vm_key(vm_key: Any) -> tuple[str, str]:
        if not isinstance(vm_key, str):
            return ('', '')
        parts = vm_key.split('::', 1)
        if len(parts) != 2:
            return ('', '')
        return (parts[0].strip(), parts[1].strip())

    def _mac_norm(mac: Any) -> str:
        if mac is None:
            return ''
        s = str(mac).strip().lower()
        return s

    core_node, core_vmid = _parse_vm_key((raw.get('core') or {}).get('vm_key') if isinstance(raw.get('core'), dict) else None)
    if not core_node:
        core_node = (core_raw.get('vm_node') or '').strip() if isinstance(core_raw.get('vm_node'), str) else str(core_raw.get('vm_node') or '').strip()
    if not core_vmid:
        core_vmid = str(core_raw.get('vmid') or '').strip()

    def _find_inventory_vm(node: str, vmid: str) -> Optional[Dict[str, Any]]:
        if not node or not vmid:
            return None
        for vm in inv_vms:
            n = str(vm.get('node') or '').strip()
            v = str(vm.get('vmid') or '').strip()
            if n == node and v == vmid:
                return vm
        return None

    core_inv_vm = _find_inventory_vm(core_node, core_vmid)
    core_inv_ifaces = core_inv_vm.get('interfaces') if (core_inv_vm and isinstance(core_inv_vm.get('interfaces'), list)) else []

    def _bridge_for_mac(mac: Any) -> str:
        macn = _mac_norm(mac)
        if not macn:
            return ''
        for it in core_inv_ifaces:
            if not isinstance(it, dict):
                continue
            inv_mac = _mac_norm(it.get('macaddr') or it.get('mac') or it.get('hwaddr'))
            if inv_mac and inv_mac == macn:
                return str(it.get('bridge') or '').strip()
        return ''

    if isinstance(iface_list, list):
        cleaned_ifaces: List[Dict[str, Any]] = []
        for iface in iface_list:
            if not isinstance(iface, dict):
                continue
            name_raw = iface.get('name')
            name = name_raw.strip() if isinstance(name_raw, str) else str(name_raw or '').strip()
            if not name:
                continue
            mac = iface.get('mac')
            entry: Dict[str, Any] = {
                'name': name,
                'attachment': _normalize_hitl_attachment(iface.get('attachment')),
            }

            # Persist the interface MAC and a best-effort CORE bridge hint.
            if mac:
                entry['mac'] = str(mac).strip()
            core_bridge_existing = iface.get('core_bridge')
            core_bridge_hint = str(core_bridge_existing).strip() if core_bridge_existing not in (None, '') else ''
            if not core_bridge_hint:
                core_bridge_hint = _bridge_for_mac(mac)
            if core_bridge_hint:
                entry['core_bridge'] = core_bridge_hint
            prox_target = iface.get('proxmox_target') if isinstance(iface.get('proxmox_target'), dict) else None
            if prox_target:
                pt: Dict[str, Any] = {}
                for key in ('node', 'vmid', 'interface_id', 'vm_name', 'macaddr', 'bridge', 'model'):
                    if key in prox_target:
                        pt[key] = prox_target.get(key)
                entry['proxmox_target'] = pt
            external = iface.get('external_vm') if isinstance(iface.get('external_vm'), dict) else None
            if external:
                ext: Dict[str, Any] = {}
                for key in ('vm_key', 'vmid', 'interface_id', 'interface_bridge', 'vm_node', 'vm_name'):
                    if key in external:
                        ext[key] = external.get(key)
                entry['external_vm'] = ext
            cleaned_ifaces.append(entry)
        if cleaned_ifaces:
            out['interfaces'] = cleaned_ifaces

    # Defensive drop of any known secret/user fields.
    def _strip(obj: Any) -> Any:
        if isinstance(obj, dict):
            cleaned: Dict[str, Any] = {}
            for k, v in obj.items():
                if k in ('password', 'ssh_password', 'token_secret', 'api_secret', 'api_token_secret'):
                    continue
                if k in ('username', 'ssh_username', 'user'):
                    continue
                cleaned[k] = _strip(v)
            return cleaned
        if isinstance(obj, list):
            return [_strip(v) for v in obj]
        return obj
    out = _strip(out)

    return out or None


def _extract_hitl_config_hints_from_scenarios(scenarios: Any) -> Dict[str, Dict[str, Any]]:
    out: Dict[str, Dict[str, Any]] = {}
    if not isinstance(scenarios, list):
        return out
    for scen in scenarios:
        if not isinstance(scen, dict):
            continue
        norm = _normalize_scenario_label(scen.get('name'))
        if not norm:
            continue
        hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else None
        if not hitl_meta:
            continue
        # Only backfill verified HITL configs.
        if not bool(hitl_meta.get('bridge_validated')):
            continue
        hint = _sanitize_hitl_config_hint(hitl_meta)
        if hint:
            out[norm] = hint
    return out


def _scrub_unverified_hitl_config_in_scenario_catalog() -> bool:
    """Remove any HITL config hints that are not backed by verified credentials.

    This is a safety cleanup for older catalogs/snapshots from before the
    "persist only after verify" rule.

    Returns True if the catalog was modified.
    """

    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return False
    try:
        if not os.path.exists(catalog_path):
            return False
    except Exception:
        return False

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return False

        hc = payload.get('hitl_config')
        hv = payload.get('hitl_validation')
        if not isinstance(hc, dict) or not hc:
            return False
        if not isinstance(hv, dict):
            hv = {}

        changed = False
        cleaned: Dict[str, Any] = {}
        for scen_key, cfg in hc.items():
            norm = _normalize_scenario_label(scen_key)
            if not norm or not isinstance(cfg, dict):
                continue
            val_entry = hv.get(norm)
            if not isinstance(val_entry, dict):
                # No verified credentials recorded -> drop config.
                changed = True
                continue
            prox_ok = False
            core_ok = False
            prox = val_entry.get('proxmox')
            if isinstance(prox, dict):
                prox_ok = bool(prox.get('validated')) and bool((prox.get('secret_id') or '').strip())
            core = val_entry.get('core')
            if isinstance(core, dict):
                core_ok = bool(core.get('validated')) and bool((core.get('core_secret_id') or '').strip())
            if not (prox_ok and core_ok):
                changed = True
                continue
            cleaned[norm] = cfg

        if not changed:
            return False

        payload['hitl_config'] = cleaned
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'
        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
        return True
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass
        return False


def _load_scenario_hitl_config_from_disk() -> dict[str, Dict[str, Any]]:
    """Load per-scenario HITL config hints from scenario_catalog.json."""
    path = _scenario_catalog_file()
    if not os.path.exists(path):
        return {}
    try:
        with open(path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
    except Exception:
        return {}
    if not isinstance(payload, dict):
        return {}
    raw = payload.get('hitl_config')
    if not isinstance(raw, dict):
        return {}
    out: dict[str, Dict[str, Any]] = {}
    for scen_key, scen_val in raw.items():
        norm = _normalize_scenario_label(scen_key)
        if not norm or not isinstance(scen_val, dict):
            continue
        hint = _sanitize_hitl_config_hint(scen_val)
        if hint:
            out[norm] = hint
    return out


def _merge_hitl_config_map_into_scenario_catalog(hitl_configs: Dict[str, Dict[str, Any]]) -> None:
    if not isinstance(hitl_configs, dict) or not hitl_configs:
        return
    catalog_path = _scenario_catalog_file()
    if not catalog_path:
        return
    try:
        if not os.path.exists(catalog_path):
            return
    except Exception:
        return

    normalized_updates: Dict[str, Dict[str, Any]] = {}
    for scen_key, hitl in hitl_configs.items():
        norm = _normalize_scenario_label(scen_key)
        if not norm:
            continue
        clean = _sanitize_hitl_config_hint(hitl)
        if clean:
            # Stamp a write time so "latest" can be selected later.
            stored_at = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'
            if isinstance(clean.get('core'), dict) and 'stored_at' not in clean['core']:
                clean['core']['stored_at'] = stored_at
            if isinstance(clean.get('proxmox'), dict) and 'stored_at' not in clean['proxmox']:
                clean['proxmox']['stored_at'] = stored_at
            normalized_updates[norm] = clean
    if not normalized_updates:
        return

    tmp_path = catalog_path + '.tmp'
    try:
        with open(catalog_path, 'r', encoding='utf-8') as fh:
            payload = json.load(fh)
        if not isinstance(payload, dict):
            return
        existing = payload.get('hitl_config')
        if not isinstance(existing, dict):
            existing = {}
        merged = dict(existing)
        for norm, clean in normalized_updates.items():
            entry = merged.get(norm)
            if not isinstance(entry, dict):
                entry = {}
            entry = dict(entry)
            entry.update(clean)
            merged[norm] = entry
        payload['hitl_config'] = merged
        payload['updated_at'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'

        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
        with open(tmp_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        os.replace(tmp_path, catalog_path)
    except Exception:
        try:
            if os.path.exists(tmp_path):
                os.remove(tmp_path)
        except Exception:
            pass


def _merge_hitl_config_into_scenario_catalog(scenario_name: str, hitl_config: Dict[str, Any]) -> None:
    scenario_norm = _normalize_scenario_label(scenario_name)
    if not scenario_norm:
        return
    _merge_hitl_config_map_into_scenario_catalog({scenario_norm: hitl_config})


def _backfill_hitl_config_from_editor_snapshots() -> bool:
    """Seed scenario_catalog.json hitl_config from any existing editor snapshots.

    This is a startup convenience so builder view reflects admin-configured HITL
    settings immediately after deploy/upgrade.
    """
    catalog_path = _scenario_catalog_file()
    try:
        if not catalog_path or not os.path.exists(catalog_path):
            return False
    except Exception:
        return False

    snapshots_dir = os.path.join(_outputs_dir(), 'editor_snapshots')
    try:
        if not os.path.isdir(snapshots_dir):
            return False
    except Exception:
        return False

    any_updates = False
    try:
        for fname in os.listdir(snapshots_dir):
            if not fname.endswith('.json'):
                continue
            path = os.path.join(snapshots_dir, fname)
            try:
                with open(path, 'r', encoding='utf-8') as fh:
                    snap = json.load(fh)
            except Exception:
                continue
            if not isinstance(snap, dict):
                continue
            hints = _extract_hitl_config_hints_from_scenarios(snap.get('scenarios'))
            if hints:
                _merge_hitl_config_map_into_scenario_catalog(hints)
                any_updates = True
    except Exception:
        return any_updates
    return any_updates


def _collect_scenario_catalog(history: Optional[List[dict]] = None) -> tuple[list[str], dict[str, set[str]], dict[str, str]]:
    catalog_names, catalog_paths, catalog_participants = _load_scenario_catalog_from_disk()
    entries = history if history is not None else _load_run_history()
    display_by_norm: dict[str, str] = {}
    ordered: list[str] = []
    path_map: dict[str, set[str]] = defaultdict(set)
    participant_by_norm: dict[str, str] = dict(catalog_participants)

    def _discover_saved_scenarios_from_outputs() -> list[tuple[str, str]]:
        """Discover scenario names from saved web UI artifacts.

        The web UI saves scenario XMLs under outputs/scenarios-*/Scenario_*.xml.
        These may exist even when they haven't been executed (so run_history won't contain them).
        Builders/participants still need to see these scenarios once assigned.
        Returns a list of (scenario_display_name, xml_path).
        """
        out: list[tuple[str, str]] = []
        try:
            base = _outputs_dir()
            if not base or not os.path.isdir(base):
                return out
            dirs: list[tuple[float, str]] = []
            for entry in os.listdir(base):
                if not entry.startswith('scenarios-'):
                    continue
                full = os.path.join(base, entry)
                if not os.path.isdir(full):
                    continue
                try:
                    mtime = os.path.getmtime(full)
                except Exception:
                    mtime = 0.0
                dirs.append((mtime, full))
            # Limit scan for performance; newest first.
            dirs.sort(key=lambda t: t[0], reverse=True)
            for _mtime, folder in dirs[:250]:
                try:
                    for fname in os.listdir(folder):
                        if not fname.endswith('.xml'):
                            continue
                        # Skip CORE pre/post captures; only include user-authored scenario XML.
                        if fname.startswith('core-'):
                            continue
                        if not fname.startswith('Scenario_'):
                            continue
                        path = os.path.join(folder, fname)
                        if not os.path.isfile(path):
                            continue
                        for name in _scenario_names_from_xml(path):
                            if name:
                                out.append((name, path))
                except Exception:
                    continue
        except Exception:
            return out
        return out

    def _record_path(norm_key: str, path_value: Any) -> None:
        if not path_value or not norm_key:
            return
        if isinstance(path_value, (list, tuple, set)):
            for item in path_value:
                _record_path(norm_key, item)
            return
        try:
            ap = os.path.abspath(str(path_value))
        except Exception:
            ap = str(path_value)
        path_map[norm_key].add(ap)

    for entry in entries:
        # Per-scenario: prefer explicit scenario_name, otherwise scenario_names[0].
        primary_display = ''
        try:
            raw_primary = entry.get('scenario_name')
            if isinstance(raw_primary, str) and raw_primary.strip():
                primary_display = raw_primary.strip()
        except Exception:
            primary_display = ''
        if not primary_display:
            names = entry.get('scenario_names') or []
            if isinstance(names, list) and names:
                raw0 = names[0]
                primary_display = raw0.strip() if isinstance(raw0, str) else str(raw0).strip()
        normalized = _normalize_scenario_label(primary_display)
        if not normalized:
            continue
        if normalized not in display_by_norm:
            display_by_norm[normalized] = primary_display
            ordered.append(primary_display)

        # Prefer a single-scenario XML for path matching.
        single_xml = entry.get('single_scenario_xml_path')
        if single_xml:
            _record_path(normalized, single_xml)
        else:
            # Avoid multi-scenario Scenario Editor outputs like outputs/scenarios-*/scenarios.xml.
            for key in ('scenario_xml_path', 'xml_path'):
                candidate = entry.get(key)
                if not candidate:
                    continue
                try:
                    base = os.path.basename(str(candidate))
                except Exception:
                    base = ''
                if base.lower() == 'scenarios.xml':
                    continue
                _record_path(normalized, candidate)

        # Session captures are useful for participant URL inference and mapping.
        for key in ('session_xml_path', 'post_xml_path'):
            candidate = entry.get(key)
            if candidate:
                _record_path(normalized, candidate)

    # Merge in saved scenario XMLs from outputs/ (covers saved-but-not-executed scenarios like "Scenario 1b").
    try:
        for display_name, xml_path in _discover_saved_scenarios_from_outputs():
            norm = _normalize_scenario_label(display_name)
            if not norm:
                continue
            if norm not in display_by_norm:
                display_by_norm[norm] = display_name
                ordered.append(display_name)
            _record_path(norm, xml_path)
    except Exception:
        pass
    if catalog_paths:
        for norm_key, candidates in catalog_paths.items():
            if not candidates:
                continue
            path_map[norm_key].update(candidates)
    if catalog_names:
        merged_order: list[str] = []
        seen_norms: set[str] = set()
        for display in catalog_names:
            norm = _normalize_scenario_label(display)
            if not norm or norm in seen_norms:
                continue
            seen_norms.add(norm)
            display_by_norm[norm] = display
            path_map.setdefault(norm, set())
            merged_order.append(display)
        for display in ordered:
            norm = _normalize_scenario_label(display)
            if not norm or norm in seen_norms:
                continue
            seen_norms.add(norm)
            merged_order.append(display)
        ordered = merged_order

    # Merge scenario names from all editor snapshots so manually created scenarios are assignable
    snapshot_dir = _editor_state_snapshot_dir()
    try:
        snapshot_entries = [name for name in os.listdir(snapshot_dir) if name.endswith('.json')]
    except Exception:
        snapshot_entries = []
    snapshot_protected_norms: set[str] = set()
    for entry in snapshot_entries:
        full_path = os.path.join(snapshot_dir, entry)
        try:
            with open(full_path, 'r', encoding='utf-8') as handle:
                snapshot_data = json.load(handle)
        except Exception:
            continue
        scen_list = snapshot_data.get('scenarios')
        if not isinstance(scen_list, list):
            continue
        for idx, scen in enumerate(scen_list, start=1):
            if not isinstance(scen, dict):
                continue
            raw_name = scen.get('name')
            if isinstance(raw_name, str) and raw_name.strip():
                display = raw_name.strip()
            else:
                display = f"Scenario {idx}"
            norm = _normalize_scenario_label(display)
            if not norm:
                continue
            if norm not in display_by_norm:
                display_by_norm[norm] = display
                path_map.setdefault(norm, set())
                ordered.append(display)
            snapshot_protected_norms.add(norm)
            hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else None
            if hitl_meta:
                participant_url = ''
                for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                    normalized = _normalize_participant_proxmox_url(hitl_meta.get(key))
                    if normalized:
                        participant_url = normalized
                        break
                if participant_url:
                    participant_by_norm[norm] = participant_url

    # Stable, shared ordering across all pages.
    try:
        ordered = sorted(ordered, key=_scenario_display_sort_key)
    except Exception:
        pass
    protected = snapshot_protected_norms or None
    return _prune_stale_scenario_entries(ordered, path_map, participant_by_norm, protected_norms=protected)


def _prune_stale_scenario_entries(
    scenario_names: list[str],
    scenario_paths: dict[str, set[str]],
    scenario_url_hints: dict[str, str],
    *,
    protected_norms: Optional[set[str]] = None,
) -> tuple[list[str], dict[str, set[str]], dict[str, str]]:
    cleaned_names: list[str] = []
    cleaned_paths: dict[str, set[str]] = {}
    cleaned_hints: dict[str, str] = {}
    seen_norms: set[str] = set()
    protected = {n.strip().lower() for n in (protected_norms or set()) if n}

    for display in scenario_names:
        norm = _normalize_scenario_label(display)
        if not norm or norm in seen_norms:
            continue
        seen_norms.add(norm)
        candidates = scenario_paths.get(norm) or set()
        valid_paths: set[str] = set()
        for candidate in candidates:
            if candidate in (None, ''):
                continue
            try:
                abs_candidate = os.path.abspath(str(candidate))
            except Exception:
                abs_candidate = str(candidate)
            try:
                exists = os.path.exists(abs_candidate)
            except Exception:
                exists = False
            if exists:
                valid_paths.add(abs_candidate)
        if not valid_paths and norm not in protected:
            continue
        cleaned_names.append(display)
        cleaned_paths[norm] = valid_paths
        if norm in scenario_url_hints:
            cleaned_hints[norm] = scenario_url_hints[norm]

    return cleaned_names, cleaned_paths, cleaned_hints


def _merge_editor_scenarios_into_catalog(
    scenario_names: list[str],
    scenario_paths: dict[str, set[str]],
    scenario_url_hints: Optional[Dict[str, str]] = None,
    *,
    user: Optional[dict] = None,
) -> tuple[list[str], dict[str, set[str]], dict[str, str]]:
    snapshot = _load_editor_state_snapshot(user)
    scen_list = snapshot.get('scenarios') if isinstance(snapshot, dict) else None
    if not scen_list or not isinstance(scen_list, list):
        return (
            list(scenario_names or []),
            {k: set(v) for k, v in (scenario_paths or {}).items()},
            dict(scenario_url_hints or {}),
        )

    names_out = list(scenario_names or [])
    paths_out: dict[str, set[str]] = {k: set(v) for k, v in (scenario_paths or {}).items()}
    hints_out: dict[str, str] = dict(scenario_url_hints or {})
    seen_norms: set[str] = set()
    for existing in names_out:
        norm = _normalize_scenario_label(existing)
        if norm:
            seen_norms.add(norm)

    raw_result_path = snapshot.get('result_path') if isinstance(snapshot, dict) else None
    abs_result_path = ''
    if isinstance(raw_result_path, str) and raw_result_path.strip():
        candidate = raw_result_path.strip()
        try:
            abs_result_path = os.path.abspath(candidate)
        except Exception:
            abs_result_path = candidate

    snapshot_norms: set[str] = set()
    for idx, scen in enumerate(scen_list, start=1):
        if not isinstance(scen, dict):
            continue
        raw_name_value = scen.get('name')
        if isinstance(raw_name_value, str):
            display_name = raw_name_value.strip()
        elif raw_name_value not in (None, ''):
            try:
                display_name = str(raw_name_value).strip()
            except Exception:
                display_name = ''
        else:
            display_name = ''
        if not display_name:
            display_name = f"Scenario {idx}"
        norm = _normalize_scenario_label(display_name)
        if not norm:
            continue
        if norm not in seen_norms:
            names_out.append(display_name)
            seen_norms.add(norm)
        target_paths = paths_out.setdefault(norm, set())
        if abs_result_path:
            target_paths.add(abs_result_path)
        base_meta = scen.get('base') if isinstance(scen.get('base'), dict) else None
        base_file = base_meta.get('filepath') if base_meta else None
        if isinstance(base_file, str) and base_file.strip():
            try:
                target_paths.add(os.path.abspath(base_file.strip()))
            except Exception:
                target_paths.add(base_file.strip())
        hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else None
        if hitl_meta:
            participant_url = ''
            for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                normalized = _normalize_participant_proxmox_url(hitl_meta.get(key))
                if normalized:
                    participant_url = normalized
                    break
            if participant_url:
                hints_out[norm] = participant_url
        snapshot_norms.add(norm)

    return _prune_stale_scenario_entries(names_out, paths_out, hints_out, protected_norms=snapshot_norms or None)


def _scenario_catalog_for_user(
    history: Optional[List[dict]] = None,
    *,
    user: Optional[dict] = None,
) -> tuple[list[str], dict[str, set[str]], dict[str, str]]:
    names, paths, hints = _collect_scenario_catalog(history)
    return _merge_editor_scenarios_into_catalog(names, paths, hints, user=user)


_SCENARIO_PARTICIPANT_CACHE: dict[str, dict[str, Any]] = {}
_G_USER_RECORD_SENTINEL = object()
_G_PARTICIPANT_STATE_SENTINEL = object()
_UI_VIEW_SESSION_KEY = 'ui_view_mode'
_UI_VIEW_DEFAULT = 'participant'
_UI_VIEW_ALLOWED = {'participant', 'admin', 'builder'}
_ADMIN_VIEW_ROLES = {'admin', 'builder'}
_ALLOWED_USER_ROLES = {'admin', 'builder', 'participant'}
_ROLE_ALIASES = {'user': 'participant'}
_PARTICIPANT_ROLES = {'participant', 'user'}


def _normalize_role_value(role: Any) -> str:
    text = ''
    if role not in (None, ''):
        try:
            text = str(role).strip().lower()
        except Exception:
            text = ''
    if not text:
        text = 'participant'
    text = _ROLE_ALIASES.get(text, text)
    if text not in _ALLOWED_USER_ROLES:
        text = 'participant'
    return text


def _is_participant_role(role: Optional[str]) -> bool:
    if role is None:
        return False
    return role.strip().lower() in _PARTICIPANT_ROLES


def _is_admin_view_role(role: Optional[str]) -> bool:
    if role is None:
        return False
    return role.strip().lower() in _ADMIN_VIEW_ROLES


def _default_ui_view_mode_for_role(role: Any) -> str:
    normalized = _normalize_role_value(role)
    if normalized == 'admin':
        return 'admin'
    if normalized == 'builder':
        return 'builder'
    return _UI_VIEW_DEFAULT


def _current_ui_view_mode() -> str:
    user = _current_user()
    if not user or user.get('role') not in _ADMIN_VIEW_ROLES:
        if session.get(_UI_VIEW_SESSION_KEY) != _UI_VIEW_DEFAULT:
            session[_UI_VIEW_SESSION_KEY] = _UI_VIEW_DEFAULT
        return _UI_VIEW_DEFAULT

    role = _normalize_role_value(user.get('role'))

    # Admin/builder roles: if no preference saved yet, default by role.
    if _UI_VIEW_SESSION_KEY not in session:
        raw = _default_ui_view_mode_for_role(role)
        session[_UI_VIEW_SESSION_KEY] = raw
        return raw

    raw = session.get(_UI_VIEW_SESSION_KEY, _UI_VIEW_DEFAULT)
    if raw not in _UI_VIEW_ALLOWED:
        raw = _default_ui_view_mode_for_role(role)
        session[_UI_VIEW_SESSION_KEY] = raw
        return raw

    # Builders should never be in admin mode.
    if role == 'builder' and raw == 'admin':
        raw = 'builder'
        session[_UI_VIEW_SESSION_KEY] = raw

    return raw


def _normalize_participant_proxmox_url(value: Any) -> str:
    if value in (None, ''):
        return ''
    text = str(value).strip()
    if not text:
        return ''
    candidate = text
    parsed = urlparse(candidate)
    if not parsed.scheme:
        candidate = f'https://{candidate}'
        parsed = urlparse(candidate)
    scheme = (parsed.scheme or '').lower()
    if scheme not in {'http', 'https'}:
        return ''
    if not parsed.netloc:
        return ''
    return parsed.geturl()


def _participant_url_from_editor(editor: Optional[ET.Element]) -> str:
    if editor is None:
        return ''
    hitl_el = editor.find('HardwareInLoop')
    if hitl_el is None:
        return ''
    for attr in (
        'participant_proxmox_url',
        'participant_ui_url',
        'participant_url',
        'participant',
    ):
        raw = hitl_el.get(attr)
        if raw:
            normalized = _normalize_participant_proxmox_url(raw)
            if normalized:
                return normalized
    return ''


def _participant_urls_from_xml(path: str) -> dict[str, str]:
    if not path:
        return {}
    try:
        abs_path = os.path.abspath(path)
    except Exception:
        abs_path = str(path)
    try:
        mtime = os.path.getmtime(abs_path)
    except Exception:
        mtime = None
    cached = _SCENARIO_PARTICIPANT_CACHE.get(abs_path)
    if cached and cached.get('mtime') == mtime:
        return dict(cached.get('data') or {})
    mapping: dict[str, str] = {}
    try:
        tree = ET.parse(abs_path)
        root = tree.getroot()
    except Exception:
        _SCENARIO_PARTICIPANT_CACHE[abs_path] = {'mtime': mtime, 'data': mapping}
        return {}
    if root.tag == 'Scenarios':
        for scen_el in root.findall('Scenario'):
            name = scen_el.get('name') or os.path.splitext(os.path.basename(abs_path))[0]
            norm = _normalize_scenario_label(name)
            if not norm:
                continue
            editor = scen_el.find('ScenarioEditor')
            participant_url = _participant_url_from_editor(editor)
            if participant_url:
                mapping[norm] = participant_url
    elif root.tag == 'ScenarioEditor':
        name = root.get('name') or os.path.splitext(os.path.basename(abs_path))[0]
        norm = _normalize_scenario_label(name)
        if norm:
            participant_url = _participant_url_from_editor(root)
            if participant_url:
                mapping[norm] = participant_url
    _SCENARIO_PARTICIPANT_CACHE[abs_path] = {'mtime': mtime, 'data': mapping}
    return dict(mapping)


def _collect_scenario_participant_urls(
    scenario_paths: dict[str, set[str]],
    catalog_hints: Optional[Dict[str, Any]] = None,
) -> dict[str, str]:
    urls: dict[str, str] = {}
    if catalog_hints:
        for norm_key, raw_value in catalog_hints.items():
            norm = _normalize_scenario_label(norm_key)
            normalized_url = _normalize_participant_proxmox_url(raw_value)
            if not norm:
                continue
            if normalized_url:
                urls[norm] = normalized_url
            else:
                # Explicitly cleared hint overrides stale XML.
                urls.setdefault(norm, '')
    if not scenario_paths:
        return urls
    cache: dict[str, dict[str, str]] = {}
    for norm_key, candidates in scenario_paths.items():
        if not candidates:
            continue
        for candidate in candidates:
            try:
                abs_path = os.path.abspath(str(candidate))
            except Exception:
                abs_path = str(candidate)
            if not abs_path:
                continue
            if abs_path not in cache:
                cache[abs_path] = _participant_urls_from_xml(abs_path)
            url_value = cache[abs_path].get(norm_key)
            if url_value:
                # Prefer catalog hints (e.g., saved from editor snapshots) over XML values.
                # XML can lag behind the editor state when the user hasn't re-saved scenarios.xml.
                if norm_key not in urls:
                    urls[norm_key] = url_value
                break
    return urls


def _normalize_scenario_assignments(values: Iterable[Any] | None) -> list[str]:
    result: list[str] = []
    if not values:
        return result
    for value in values:
        norm = _normalize_scenario_label(value)
        if norm and norm not in result:
            result.append(norm)
    return result


def _nearest_gateway_address_for_scenario(
    scenario_norm: str,
    *,
    scenario_paths: dict[str, set[str]],
) -> str:
    """Best-effort HITL gateway address for a scenario.

    Uses the most recently saved CORE session XML for that scenario (from outputs/core_sessions.json
    + outputs/core-sessions/**/*.xml) and extracts the router-side IP of the HITL attachment.
    Returns an IPv4 string without CIDR (e.g., '10.12.34.1').
    """

    scenario_norm = _normalize_scenario_label(scenario_norm)
    if not scenario_norm:
        return ''

    try:
        store = _load_core_sessions_store()
    except Exception:
        store = {}

    best_path: str = ''
    best_mtime: float = 0.0
    for path, entry in (store or {}).items():
        if not path:
            continue
        stored_norm = _session_store_entry_scenario_norm(entry)
        if stored_norm and stored_norm != scenario_norm:
            continue
        if not stored_norm and not _path_matches_scenario(path, scenario_norm, scenario_paths):
            continue
        try:
            ap = os.path.abspath(str(path))
        except Exception:
            ap = str(path)
        if not ap or not os.path.exists(ap):
            continue
        try:
            mtime = os.path.getmtime(ap)
        except Exception:
            mtime = 0.0
        if not best_path or mtime > best_mtime:
            best_path = ap
            best_mtime = mtime

    if not best_path:
        return ''

    details = _hitl_details_from_path(best_path)
    if not details:
        return ''
    try:
        first = details[0] if details else None
        ips = first.get('ips') if isinstance(first, dict) else None
        if isinstance(ips, list) and ips:
            return str(ips[0]).split('/', 1)[0]
    except Exception:
        return ''
    return ''


def _participant_ui_state() -> Dict[str, Any]:
    if has_request_context():
        cached = getattr(g, '_participant_ui_state', _G_PARTICIPANT_STATE_SENTINEL)
        if cached is not _G_PARTICIPANT_STATE_SENTINEL:
            return cached
    user = _current_user()
    user_role = _normalize_role_value(user.get('role')) if user else ''
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(
        None,
        user=user,
    )
    display_by_norm: dict[str, str] = {}
    ordered_norms: list[str] = []
    for display in scenario_names:
        norm = _normalize_scenario_label(display)
        if not norm or norm in display_by_norm:
            continue
        display_by_norm[norm] = display
        ordered_norms.append(norm)
    mapping = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
    for norm_key in mapping.keys():
        if not norm_key or norm_key in display_by_norm:
            continue
        display_by_norm[norm_key] = norm_key
        ordered_norms.append(norm_key)
    assigned_norms = _current_user_assigned_scenarios()
    assigned_set = set(assigned_norms)
    has_assignments = bool(assigned_norms)
    restrict_to_assigned = user_role in {'participant', 'builder'}
    allowed_norms: Optional[set[str]] = set(assigned_norms) if restrict_to_assigned else None
    scenario_arg_norm = ''
    if has_request_context():
        try:
            scenario_arg_norm = _normalize_scenario_label(request.args.get('scenario'))
        except Exception:
            scenario_arg_norm = ''

    def _humanize_norm_text(value: str) -> str:
        if not value:
            return ''
        text = value.replace('_', ' ')
        text = re.sub(r'\s+', ' ', text).strip()
        return text.title() if text else ''

    def _pick_norm(candidates: Iterable[str]) -> str:
        for norm in candidates:
            if not norm:
                continue
            if allowed_norms is not None and norm not in allowed_norms:
                continue
            url_value = mapping.get(norm)
            if url_value:
                return norm
        return ''

    ordered_mapping_norms = [key for key in mapping.keys() if key]
    selected_norm = (
        _pick_norm([scenario_arg_norm])
        or _pick_norm(assigned_norms)
        or _pick_norm(ordered_norms)
        or _pick_norm(ordered_mapping_norms)
    )
    selected_label = display_by_norm.get(selected_norm, selected_norm)
    selected_url = mapping.get(selected_norm, '')
    selected_nearest_gateway = _nearest_gateway_address_for_scenario(selected_norm, scenario_paths=scenario_paths)

    listing: list[dict[str, Any]] = []
    for norm in ordered_norms:
        if not norm:
            continue
        entry = {
            'norm': norm,
            'display': display_by_norm.get(norm, norm),
            'url': mapping.get(norm, ''),
            'has_url': bool(mapping.get(norm)),
            'assigned': norm in assigned_set,
        }
        listing.append(entry)
    if restrict_to_assigned:
        listing = [row for row in listing if row['assigned']]
        present_norms = {row['norm'] for row in listing}
        missing_norms = [norm for norm in assigned_norms if norm and norm not in present_norms]
        for norm in missing_norms:
            display_value = display_by_norm.get(norm) or _humanize_norm_text(norm) or norm
            listing.append({
                'norm': norm,
                'display': display_value,
                'url': '',
                'has_url': False,
                'assigned': True,
                'placeholder': True,
            })
    for row in listing:
        row['active'] = bool(selected_norm) and row['norm'] == selected_norm
    heading = 'Your Assigned Scenarios' if restrict_to_assigned else 'Available Scenarios'
    if restrict_to_assigned:
        if has_assignments:
            empty_message = 'Your assigned scenarios are not available in the participant console yet. Ask an administrator to publish them.'
        else:
            empty_message = 'No scenarios have been assigned to your account yet. Ask an administrator for access.'
    else:
        empty_message = 'No scenarios with participant links are available yet.'
    hint = 'Load a scenario into the console or open its participant link in a new tab.'
    state = {
        'selected_norm': selected_norm,
        'selected_label': selected_label if selected_norm else '',
        'selected_url': selected_url or '',
        'selected_nearest_gateway': selected_nearest_gateway or '',
        'listing': listing,
        'listing_heading': heading,
        'listing_empty_message': empty_message,
        'listing_hint': hint,
        'restrict_to_assigned': restrict_to_assigned,
        'has_assignments': has_assignments,
    }
    if has_request_context():
        setattr(g, '_participant_ui_state', state)
    return state


def _resolve_participant_ui_target() -> tuple[str, str]:
    state = _participant_ui_state()
    return state.get('selected_url', ''), state.get('selected_label', '')


def _current_nav_participant_url() -> str:
    url_value, _label = _resolve_participant_ui_target()
    return url_value


def _resolve_ui_view_redirect_target(candidate: Optional[str]) -> str:
    fallback = url_for('index')
    if not candidate:
        return fallback
    try:
        parsed = urlparse(candidate)
    except Exception:
        return fallback
    if not parsed.scheme and candidate.startswith('/'):
        return candidate
    try:
        host_url = urlparse(request.host_url)
    except Exception:
        host_url = None
    if parsed.scheme and host_url and parsed.netloc == host_url.netloc:
        return candidate
    return fallback


def _select_single_scenario_path(candidates, scenario_norm: str) -> Optional[str]:
    """Prefer a path that contains exactly one matching scenario."""
    if not candidates:
        return None
    best_path: Optional[str] = None
    best_mtime = float('-inf')
    scen_norm = _normalize_scenario_label(scenario_norm)
    for candidate in candidates:
        if not candidate:
            continue
        try:
            ap = os.path.abspath(str(candidate))
        except Exception:
            ap = str(candidate)
        if not os.path.exists(ap):
            continue
        try:
            tree = ET.parse(ap)
            root = tree.getroot()
        except Exception:
            continue
        try:
            if root.tag == 'ScenarioEditor':
                name = str(root.get('name') or '').strip()
                if scen_norm and _normalize_scenario_label(name) != scen_norm:
                    continue
            elif root.tag == 'Scenarios':
                scen_elems = root.findall('Scenario')
                if len(scen_elems) != 1:
                    continue
                name = str(scen_elems[0].get('name') or '').strip()
                if scen_norm and _normalize_scenario_label(name) != scen_norm:
                    continue
            else:
                continue
        except Exception:
            continue
        try:
            mt = os.path.getmtime(ap)
        except Exception:
            mt = 0.0
        if best_path is None or mt > best_mtime:
            best_path = ap
            best_mtime = mt
    return best_path


def _select_existing_path(candidates) -> Optional[str]:
    """Return the newest existing file path from an iterable of candidates."""
    best_path: Optional[str] = None
    best_mtime = float('-inf')
    if not candidates:
        return None
    for candidate in candidates:
        if not candidate:
            continue
        try:
            ap = os.path.abspath(str(candidate))
        except Exception:
            ap = str(candidate)
        if not os.path.exists(ap):
            continue
        try:
            mt = os.path.getmtime(ap)
        except Exception:
            mt = 0.0
        if best_path is None or mt > best_mtime:
            best_path = ap
            best_mtime = mt
    return best_path


def _filter_history_by_scenario(history: List[dict], scenario_norm: str) -> List[dict]:
    if not scenario_norm:
        return history
    filtered: List[dict] = []
    for entry in history:
        names = entry.get('scenario_names') or []
        if not isinstance(names, list):
            continue
        if any(_normalize_scenario_label(name) == scenario_norm for name in names):
            filtered.append(entry)
    return filtered


def _resolve_scenario_display(scenario_norm: str, ordered_names: list[str], fallback: str = '') -> str:
    if not scenario_norm:
        return ''
    for name in ordered_names:
        if _normalize_scenario_label(name) == scenario_norm:
            return name
    return fallback.strip()


def _path_matches_scenario(path_value: Any, scenario_norm: str, scenario_paths: dict[str, set[str]]) -> bool:
    if not scenario_norm:
        return True
    if not path_value:
        return False
    try:
        ap = os.path.abspath(str(path_value))
    except Exception:
        ap = str(path_value)
    if ap in (scenario_paths.get(scenario_norm) or set()):
        return True

    # CORE internal session directories (pycore.<id>) often contain generic filenames
    # like Scenario_1.xml that do not encode the scenario. Even when the webapp runs
    # on the CORE VM and these paths exist locally, do not infer scenario membership
    # from the basename. Use session-meta or our mapping store instead.
    try:
        ap_norm = ap.replace('\\', '/')
        if '/tmp/pycore.' in ap_norm:
            return False
    except Exception:
        pass

    # If the path doesn't exist locally (common for CORE-reported remote paths like
    # /tmp/Scenario_1.xml), do not infer scenario membership from the basename.
    # Rely on explicit scenario_paths or our local mapping store instead.
    try:
        if not os.path.exists(ap):
            return False
    except Exception:
        return False
    base = os.path.splitext(os.path.basename(ap))[0]
    return _normalize_scenario_label(base) == scenario_norm


def _latest_session_owner_by_id(mapping: dict) -> dict[int, dict[str, Any]]:
    """Return session_id -> best mapping entry (newest updated_at).

    This prevents CORE session-id reuse from making a session appear under multiple scenarios.
    """
    owners: dict[int, dict[str, Any]] = {}
    best_ts: dict[int, float] = {}
    for _path, entry in (mapping or {}).items():
        sid = _session_store_entry_session_id(entry)
        if sid is None:
            continue
        ts = _session_store_entry_updated_at_epoch(entry)
        if ts is None:
            ts = 0.0
        prev = best_ts.get(sid)
        if prev is None or ts >= prev:
            best_ts[sid] = ts
            owners[sid] = entry if isinstance(entry, dict) else {'session_id': sid}
    return owners


def _scenario_label_from_path(
    path_value: Any,
    scenario_names: list[str],
    scenario_paths: dict[str, set[str]],
) -> str:
    if not path_value:
        return ''
    try:
        ap = os.path.abspath(str(path_value))
    except Exception:
        ap = str(path_value)

    # Never infer scenario names from CORE internal session paths. These often use
    # generic filenames like Scenario_1.xml that do not correspond to our scenarios.
    try:
        if '/tmp/pycore.' in ap.replace('\\', '/'):
            return ''
    except Exception:
        return ''
    for norm_key, paths in (scenario_paths or {}).items():
        if ap in paths:
            return _resolve_scenario_display(norm_key, scenario_names, norm_key)

    # If the path doesn't exist locally, don't try to infer the scenario by basename.
    # CORE often reports remote paths like /tmp/Scenario_1.xml which would otherwise
    # incorrectly label Scenario 2 sessions as Scenario 1.
    try:
        if not os.path.exists(ap):
            return ''
    except Exception:
        return ''
    base = os.path.splitext(os.path.basename(ap))[0]
    base_norm = _normalize_scenario_label(base)
    if base_norm:
        # Avoid showing CORE internal session directories like /tmp/pycore.* as
        # scenario names in the UI.
        if base_norm.startswith('pycore'):
            return ''
        # Only return a display name if it matches a known scenario.
        display = _resolve_scenario_display(base_norm, scenario_names, '')
        return display or ''
    return ''


def _session_ids_for_scenario(mapping: dict, scenario_norm: str, scenario_paths: dict[str, set[str]]) -> set[int]:
    ids: set[int] = set()
    if not scenario_norm:
        return ids
    # Session IDs can be reused across CORE restarts and across scenarios.
    # When that happens, only the *latest* mapping entry should own the session id.
    owners = _latest_session_owner_by_id(mapping)
    for sid, entry in owners.items():
        stored_norm = _session_store_entry_scenario_norm(entry)
        if stored_norm and stored_norm == scenario_norm:
            ids.add(sid)

    # Legacy fallback: if we have no owner for a session id (missing/invalid updated_at),
    # allow path-based inference based on the mapping key.
    for path, entry in (mapping or {}).items():
        sid = _session_store_entry_session_id(entry)
        if sid is None:
            continue
        if sid in owners:
            continue
        stored_norm = _session_store_entry_scenario_norm(entry)
        if stored_norm and stored_norm == scenario_norm:
            ids.add(sid)
            continue
        if _path_matches_scenario(path, scenario_norm, scenario_paths):
            ids.add(sid)
    return ids


def _filter_sessions_by_scenario(
    sessions: list[dict],
    scenario_norm: str,
    scenario_paths: dict[str, set[str]],
    scenario_session_ids: set[int],
) -> tuple[list[dict], bool]:
    if not scenario_norm:
        return sessions, True
    filtered: list[dict] = []
    matched = False
    for session in sessions:
        sid = session.get('id')
        sid_int: Optional[int]
        try:
            sid_int = int(sid) if sid is not None else None
        except Exception:
            sid_int = None
        label = _normalize_scenario_label(session.get('scenario_name')) if session.get('scenario_name') else ''
        if label and label == scenario_norm:
            filtered.append(session)
            matched = True
            continue
        if _path_matches_scenario(session.get('file'), scenario_norm, scenario_paths):
            filtered.append(session)
            matched = True
            continue
        if _path_matches_scenario(session.get('dir'), scenario_norm, scenario_paths):
            filtered.append(session)
            matched = True
            continue
        # Session IDs can be reused across CORE restarts; only fall back to ID-based
        # matching when we have no path information.
        if (not session.get('file')) and (not session.get('dir')):
            if sid_int is not None and sid_int in scenario_session_ids:
                filtered.append(session)
                matched = True
                continue
    return filtered, matched


def _xml_matches_scenario(
    path_value: Any,
    scenario_norm: str,
    scenario_paths: dict[str, set[str]],
    mapping: dict,
) -> bool:
    if not scenario_norm:
        return True
    if _path_matches_scenario(path_value, scenario_norm, scenario_paths):
        return True
    if not path_value:
        return False
    try:
        abs_path = os.path.abspath(str(path_value))
    except Exception:
        abs_path = str(path_value)
    entry = mapping.get(abs_path)
    if not entry and abs_path:
        entry = mapping.get(abs_path.rstrip('/'))
    if not entry:
        return False
    stored_norm = _session_store_entry_scenario_norm(entry)
    return bool(stored_norm and stored_norm == scenario_norm)


def _filter_xmls_by_scenario(
    xmls: list[dict],
    scenario_norm: str,
    scenario_paths: dict[str, set[str]],
    mapping: dict,
) -> tuple[list[dict], bool]:
    if not scenario_norm:
        return xmls, True
    filtered: list[dict] = []
    matched = False
    for entry in xmls:
        if _xml_matches_scenario(entry.get('path'), scenario_norm, scenario_paths, mapping):
            filtered.append(entry)
            matched = True
    return filtered, matched


def _build_session_scenario_labels(
    mapping: dict,
    scenario_names: list[str],
    scenario_paths: dict[str, set[str]],
) -> dict[int, str]:
    labels: dict[int, str] = {}
    best_ts: dict[int, float] = {}
    for path, entry in (mapping or {}).items():
        sid = _session_store_entry_session_id(entry)
        if sid is None:
            continue
        label = ''
        if isinstance(entry, dict):
            label = str(entry.get('scenario_name') or entry.get('scenario') or '').strip()
        if not label:
            norm = _session_store_entry_scenario_norm(entry)
            if norm:
                label = _resolve_scenario_display(norm, scenario_names, norm)
        if not label:
            label = _scenario_label_from_path(path, scenario_names, scenario_paths)
        if not label:
            continue

        ts = _session_store_entry_updated_at_epoch(entry)
        if ts is None:
            ts = _safe_path_mtime_epoch(path)
        if ts is None:
            ts = 0.0

        prev = best_ts.get(sid)
        if prev is None or ts >= prev:
            best_ts[sid] = ts
            labels[sid] = label
    return labels


def _annotate_sessions_with_scenarios(
    sessions: list[dict],
    session_labels: dict[int, str],
    scenario_norm: str,
    scenario_names: list[str],
    scenario_paths: dict[str, set[str]],
    scenario_query: str = '',
    scenario_session_ids: Optional[set[int]] = None,
) -> None:
    active_display = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query or '') if scenario_norm else ''
    matched_ids = set(scenario_session_ids or [])
    for session in sessions:
        label: Optional[str] = None
        # If the session dict already carries a scenario name, preserve it (when it
        # matches a known scenario) rather than clobbering it with empty inference.
        try:
            existing = (session.get('scenario_name') or '').strip()
        except Exception:
            existing = ''
        if existing:
            existing_norm = _normalize_scenario_label(existing)
            display = _resolve_scenario_display(existing_norm, scenario_names, existing)
            if display:
                label = display
        sid = session.get('id')
        sid_int: Optional[int]
        try:
            sid_int = int(sid) if sid is not None else None
        except Exception:
            sid_int = None
        # Prefer the live session path over any cached session-id mapping.
        candidate_file = session.get('file') or ''
        candidate_dir = session.get('dir') or ''
        if not label and candidate_file:
            label = _scenario_label_from_path(candidate_file, scenario_names, scenario_paths)
        if not label and candidate_dir:
            label = _scenario_label_from_path(candidate_dir, scenario_names, scenario_paths)
        if not label and sid_int is not None:
            label = session_labels.get(sid_int)
        if not label and sid is not None:
            try:
                label = session_labels.get(int(str(sid)))
            except Exception:
                label = None
        if not label and scenario_norm:
            if sid_int is not None and sid_int in matched_ids:
                label = active_display or scenario_query
            elif _path_matches_scenario(session.get('file'), scenario_norm, scenario_paths) or _path_matches_scenario(session.get('dir'), scenario_norm, scenario_paths):
                label = active_display or scenario_query
        session['scenario_name'] = label or ''


def _augment_core_config_from_secret(core_cfg: Dict[str, Any]) -> Dict[str, Any]:
    if not isinstance(core_cfg, dict):
        return core_cfg
    secret_id_raw = core_cfg.get('core_secret_id')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    if not secret_id:
        return core_cfg
    needs_password = not core_cfg.get('ssh_password')
    missing_fields = [
        key for key in (
            'vm_key',
            'vm_name',
            'vm_node',
            'vmid',
            'proxmox_secret_id',
            'proxmox_target',
        ) if not core_cfg.get(key)
    ]
    missing_transport = [
        key for key in ('host', 'port', 'ssh_host', 'ssh_port', 'ssh_username', 'venv_bin')
        if not core_cfg.get(key)
    ]
    if not (needs_password or missing_fields or missing_transport):
        return core_cfg
    try:
        secret_record = _load_core_credentials(secret_id)
    except RuntimeError:
        secret_record = None
    if not secret_record:
        return core_cfg
    augmented = dict(core_cfg)
    if needs_password:
        augmented['ssh_password'] = secret_record.get('ssh_password_plain') or secret_record.get('password_plain') or augmented.get('ssh_password') or ''
    for field in missing_transport:
        if field == 'host':
            value = secret_record.get('host') or secret_record.get('grpc_host')
        elif field == 'port':
            value = secret_record.get('port') or secret_record.get('grpc_port')
        else:
            value = secret_record.get(field)
        if value not in (None, ''):
            augmented[field] = value
    for field in missing_fields:
        value = secret_record.get(field)
        if value not in (None, ''):
            augmented[field] = value
    return augmented


def _ensure_core_vm_metadata(core_cfg: Dict[str, Any]) -> Dict[str, Any]:
    if not isinstance(core_cfg, dict):
        return core_cfg
    vm_key = str(core_cfg.get('vm_key') or '').strip()
    secret_id = str(core_cfg.get('core_secret_id') or '').strip()
    if vm_key or not secret_id:
        return core_cfg
    try:
        secret_record = _load_core_credentials(secret_id)
    except RuntimeError:
        secret_record = None
    if not secret_record:
        return core_cfg
    enriched = dict(core_cfg)
    for field in ('vm_key', 'vm_name', 'vm_node', 'vmid', 'proxmox_secret_id', 'proxmox_target'):
        if enriched.get(field) in (None, '', {}):
            value = secret_record.get(field)
            if value not in (None, ''):
                enriched[field] = value
    return enriched


def _apply_core_secret_to_config(core_cfg: Dict[str, Any], scenario_norm: str) -> Dict[str, Any]:
    if not isinstance(core_cfg, dict):
        return core_cfg
    try:
        secret_record = _select_latest_core_secret_record(scenario_norm or None)
    except Exception:
        secret_record = None
    if not secret_record:
        return core_cfg
    enriched = dict(core_cfg)
    secret_password = secret_record.get('ssh_password_plain') or secret_record.get('password_plain') or ''
    if secret_password and not enriched.get('ssh_password'):
        enriched['ssh_password'] = secret_password
    if enriched.get('ssh_username') in (None, ''):
        value = secret_record.get('ssh_username')
        if value not in (None, ''):
            enriched['ssh_username'] = value
    if enriched.get('ssh_host') in (None, ''):
        value = secret_record.get('ssh_host') or secret_record.get('host') or secret_record.get('grpc_host')
        if value not in (None, ''):
            enriched['ssh_host'] = value
    if enriched.get('ssh_port') in (None, '', 0):
        value = secret_record.get('ssh_port')
        if value not in (None, ''):
            enriched['ssh_port'] = value
    if enriched.get('venv_bin') in (None, ''):
        value = secret_record.get('venv_bin')
        if value not in (None, ''):
            enriched['venv_bin'] = value
    if enriched.get('core_secret_id') in (None, ''):
        value = secret_record.get('identifier')
        if value not in (None, ''):
            enriched['core_secret_id'] = value
    if enriched.get('validated') in (None, '') and 'validated' in secret_record:
        enriched['validated'] = secret_record.get('validated')
    if enriched.get('last_tested_status') in (None, '') and 'last_tested_status' in secret_record:
        enriched['last_tested_status'] = secret_record.get('last_tested_status')
    for meta_field in ('vm_key', 'vm_name', 'vm_node', 'vmid', 'proxmox_secret_id', 'proxmox_target'):
        if enriched.get(meta_field) in (None, '', {}):
            value = secret_record.get(meta_field)
            if value not in (None, ''):
                enriched[meta_field] = value
    return _normalize_core_config(enriched, include_password=True)


def _build_core_vm_summary(core_cfg: Dict[str, Any]) -> tuple[bool, Optional[Dict[str, Any]]]:
    cfg = _ensure_core_vm_metadata(core_cfg)
    vm_key = str(cfg.get('vm_key') or '').strip()
    if not vm_key:
        return False, None
    host = str(cfg.get('host') or cfg.get('grpc_host') or CORE_HOST)
    try:
        port = int(cfg.get('port') or cfg.get('grpc_port') or CORE_PORT)
    except Exception:
        port = CORE_PORT
    ssh_host = str(cfg.get('ssh_host') or host)
    try:
        ssh_port = int(cfg.get('ssh_port') or 22)
    except Exception:
        ssh_port = 22
    ssh_username = str(cfg.get('ssh_username') or '').strip()
    vm_summary = {
        'label': cfg.get('vm_name') or vm_key,
        'node': cfg.get('vm_node') or '',
        'host': host,
        'port': port,
        'ssh_host': ssh_host,
        'ssh_port': ssh_port,
        'ssh_username': ssh_username,
    }
    return True, vm_summary


def _select_core_config_for_page(
    scenario_norm: str,
    history: Optional[List[dict]] = None,
    *,
    include_password: bool = True,
) -> Dict[str, Any]:
    """Pick the most relevant CORE config (with SSH creds) for the CORE page/data views."""

    try:
        hitl_map = _load_scenario_hitl_config_from_disk()
    except Exception:
        hitl_map = {}
    try:
        hitl_validation = _load_scenario_hitl_validation_from_disk()
    except Exception:
        hitl_validation = {}

    def _hitl_lookup(hmap: dict[str, Any], norm: str) -> dict[str, Any] | None:
        if not norm or not isinstance(hmap, dict):
            return None
        if norm in hmap:
            val = hmap.get(norm)
            return val if isinstance(val, dict) else None
        try:
            key = _scenario_match_key(norm)
        except Exception:
            key = ''
        if not key:
            return None
        for k, v in hmap.items():
            try:
                if _scenario_match_key(k) == key and isinstance(v, dict):
                    return v
            except Exception:
                continue
        return None

    if scenario_norm and isinstance(hitl_map, dict):
        scenario_hitl = _hitl_lookup(hitl_map, scenario_norm)
        if isinstance(scenario_hitl, dict):
            scenario_core_raw = scenario_hitl.get('core') if isinstance(scenario_hitl.get('core'), dict) else None
            if isinstance(scenario_core_raw, dict) and scenario_core_raw:
                merged = _merge_core_configs(None, scenario_core_raw, include_password=include_password)
                validation_hitl = _hitl_lookup(hitl_validation, scenario_norm)
                validation_core = validation_hitl.get('core') if isinstance(validation_hitl, dict) else None
                if isinstance(validation_core, dict) and validation_core:
                    merged = _merge_core_configs(merged, validation_core, include_password=include_password)
                return _ensure_core_vm_metadata(_augment_core_config_from_secret(merged))
        # Fall back to the latest VM/Access secret record for this scenario.
        try:
            secret_record = _select_latest_core_secret_record(scenario_norm or None)
        except Exception:
            secret_record = None
        if secret_record:
            secret_password = secret_record.get('ssh_password_plain') or secret_record.get('password_plain') or ''
            # Key-based auth may have empty password; do not filter out.
            if True:
                secret_cfg = {
                    'host': secret_record.get('host') or secret_record.get('grpc_host') or CORE_HOST,
                    'port': secret_record.get('port') or secret_record.get('grpc_port') or CORE_PORT,
                    'ssh_host': secret_record.get('ssh_host') or secret_record.get('host') or secret_record.get('grpc_host'),
                    'ssh_port': secret_record.get('ssh_port') or 22,
                    'ssh_username': secret_record.get('ssh_username') or '',
                    'ssh_password': secret_password,
                    'venv_bin': secret_record.get('venv_bin') or DEFAULT_CORE_VENV_BIN,
                    'ssh_enabled': True,
                    'core_secret_id': secret_record.get('identifier'),
                    'vm_key': secret_record.get('vm_key'),
                    'vm_name': secret_record.get('vm_name'),
                    'vm_node': secret_record.get('vm_node'),
                    'vmid': secret_record.get('vmid'),
                    'proxmox_secret_id': secret_record.get('proxmox_secret_id'),
                    'proxmox_target': secret_record.get('proxmox_target'),
                    'validated': secret_record.get('validated') if 'validated' in secret_record else None,
                    'last_tested_status': secret_record.get('last_tested_status') if 'last_tested_status' in secret_record else None,
                }
                merged = _merge_core_configs(secret_cfg, include_password=include_password)
                try:
                    logger = logging.getLogger(__name__)
                    logger.info("Selected default CORE config from secret record: %s (host=%s)", secret_record.get('identifier'), secret_cfg.get('host'))
                except Exception:
                    pass
                return _ensure_core_vm_metadata(_augment_core_config_from_secret(merged))
        # Enforce VM/Access as the only source of truth.
        logger = getattr(app, 'logger', logging.getLogger(__name__))
        logger.info("No default CORE config found in secrets (scenario=%s). Falling back to empty/localhost.", scenario_norm)
        return {}

    entries = list(history or _load_run_history())

    def _entry_matches(entry: dict) -> bool:
        if not scenario_norm:
            return True
        names = entry.get('scenario_names') or []
        if not isinstance(names, list):
            return False
        return any(_normalize_scenario_label(name) == scenario_norm for name in names)

    def _combine(entry: dict) -> Dict[str, Any]:
        core_raw = entry.get('core') if isinstance(entry.get('core'), dict) else None
        scenario_core_raw = entry.get('scenario_core') if isinstance(entry.get('scenario_core'), dict) else None
        return _merge_core_configs(core_raw, scenario_core_raw, include_password=include_password)

    def _has_ssh(creds: Dict[str, Any]) -> bool:
        username = str(creds.get('ssh_username') or '').strip()
        password_raw = creds.get('ssh_password')
        if password_raw is None:
            password = ''
        elif isinstance(password_raw, str):
            password = password_raw.strip()
        else:
            password = str(password_raw).strip()
        return bool(username and password)

    scenario_fallback: Dict[str, Any] | None = None
    for entry in reversed(entries):
        if not _entry_matches(entry):
            continue
        combined = _combine(entry)
        if _has_ssh(combined):
            return _ensure_core_vm_metadata(_augment_core_config_from_secret(combined))
        if scenario_fallback is None:
            scenario_fallback = combined
    if scenario_fallback:
        return _ensure_core_vm_metadata(_augment_core_config_from_secret(scenario_fallback))

    general_fallback: Dict[str, Any] | None = None
    for entry in reversed(entries):
        combined = _combine(entry)
        if _has_ssh(combined):
            return _ensure_core_vm_metadata(_augment_core_config_from_secret(combined))
        if general_fallback is None:
            general_fallback = combined
    if general_fallback:
        return _ensure_core_vm_metadata(_augment_core_config_from_secret(general_fallback))

    secret_record = _select_latest_core_secret_record(scenario_norm or None)
    secret_password = None
    if secret_record:
        secret_password = secret_record.get('ssh_password_plain') or secret_record.get('password_plain') or ''
    if secret_record and secret_password:
        secret_cfg = {
            'host': secret_record.get('host') or secret_record.get('grpc_host') or CORE_HOST,
            'port': secret_record.get('port') or secret_record.get('grpc_port') or CORE_PORT,
            'ssh_host': secret_record.get('ssh_host') or secret_record.get('host') or secret_record.get('grpc_host'),
            'ssh_port': secret_record.get('ssh_port') or 22,
            'ssh_username': secret_record.get('ssh_username') or '',
            'ssh_password': secret_password,
            'venv_bin': secret_record.get('venv_bin') or DEFAULT_CORE_VENV_BIN,
            'ssh_enabled': True,
            'core_secret_id': secret_record.get('identifier'),
            'vm_key': secret_record.get('vm_key'),
            'vm_name': secret_record.get('vm_name'),
            'vm_node': secret_record.get('vm_node'),
            'vmid': secret_record.get('vmid'),
            'proxmox_secret_id': secret_record.get('proxmox_secret_id'),
            'proxmox_target': secret_record.get('proxmox_target'),
        }
        merged = _merge_core_configs(secret_cfg, include_password=True)
        return _ensure_core_vm_metadata(_augment_core_config_from_secret(merged))

    defaults = _core_backend_defaults(include_password=include_password)
    return _ensure_core_vm_metadata(_augment_core_config_from_secret(defaults))


def _core_config_for_request(*, include_password: bool = True) -> Dict[str, Any]:
    """Return the best CORE config for the current request context (respecting scenario filter)."""

    scenario_raw = ''
    history: Optional[List[dict]] = None
    if has_request_context():
        try:
            scenario_raw = (request.values.get('scenario') or '').strip()
        except Exception:
            scenario_raw = ''
    scenario_norm = _normalize_scenario_label(scenario_raw)
    try:
        history = _load_run_history()
    except Exception:
        history = []
    cfg = _select_core_config_for_page(scenario_norm, history, include_password=include_password)

    def _has_required_creds(c: Dict[str, Any]) -> bool:
        username = str(c.get('ssh_username') or '').strip()
        if not username:
            return False
        if not include_password:
            return True
        password = c.get('ssh_password')
        if isinstance(password, str):
            return bool(password.strip())
        return bool(password)

    if not _has_required_creds(cfg):
        fallback = _core_backend_defaults(include_password=include_password)
        cfg = _merge_core_configs(cfg, fallback, include_password=include_password)
    return cfg

def _extract_report_path_from_text(text: str, *, require_exists: bool = True) -> str | None:
    """Parse CLI output to extract the most recent report path reference."""

    if not text:
        return None
    matches = list(re.finditer(r"Scenario report written to\s+(.+)", text))
    for m in reversed(matches):
        path = m.group(1).strip().rstrip(' .')
        if not os.path.isabs(path):
            repo_root = _get_repo_root()
            path = os.path.abspath(os.path.join(repo_root, path))
        if not require_exists or os.path.exists(path):
            return path
    return None

def _find_latest_report_path(since_ts: float | None = None) -> str | None:
    """Find the most recent scenario_report_*.md under the repo reports directory.

    If since_ts is provided (epoch seconds), prefer files modified after this time.
    """
    try:
        report_dir = _reports_dir()
        if not os.path.isdir(report_dir):
            return None
        cands = []
        for name in os.listdir(report_dir):
            if not name.endswith('.md'):
                continue
            if not name.startswith('scenario_report_'):
                continue
            p = os.path.join(report_dir, name)
            try:
                st = os.stat(p)
                if since_ts is None or st.st_mtime >= max(0.0, float(since_ts) - 5.0):
                    cands.append((st.st_mtime, p))
            except Exception:
                continue
        if not cands:
            return None
        cands.sort(key=lambda x: x[0], reverse=True)
        return cands[0][1]
    except Exception:
        return None


def _derive_summary_from_report(report_path: str | None) -> str | None:
    try:
        if not report_path:
            return None
        candidate = os.path.splitext(report_path)[0] + '.json'
        if os.path.exists(candidate):
            return candidate
    except Exception:
        pass
    return None


def _extract_summary_path_from_text(text: str, *, require_exists: bool = True) -> str | None:
    """Parse CLI output to extract the most recent summary path reference."""

    if not text:
        return None
    try:
        matches = list(re.finditer(r"Scenario summary written to\s+(.+)", text))
        for m in reversed(matches):
            path = m.group(1).strip().rstrip(' .')
            if not os.path.isabs(path):
                repo_root = _get_repo_root()
                path = os.path.abspath(os.path.join(repo_root, path))
            if not require_exists or os.path.exists(path):
                return path
    except Exception:
        pass
    return None


def _extract_validation_summary_from_text(text: str) -> dict | None:
    if not text:
        return None
    try:
        import re as _re
        matches = _re.findall(r"VALIDATION_SUMMARY_JSON:\s*(\{.*?\})\s*$", text, flags=_re.MULTILINE)
        if not matches:
            return None
        raw = matches[-1]
        return json.loads(raw)
    except Exception:
        return None


def _extract_docker_conflicts_from_text(text: str) -> dict | None:
    """Parse CLI logs for machine-readable Docker conflict info.

    Expected line emitted by core_topo_gen.cli:
        DOCKER_CONFLICTS_JSON: {"containers": [...], "images": [...]} 
    """

    if not text:
        return None
    try:
        matches = list(re.finditer(r"DOCKER_CONFLICTS_JSON:\s*(\{.*\})\s*$", text, flags=re.MULTILINE))
        if not matches:
            return None
        raw = matches[-1].group(1)
        obj = json.loads(raw)
        if not isinstance(obj, dict):
            return None
        containers = obj.get('containers')
        images = obj.get('images')
        if not isinstance(containers, list):
            containers = []
        if not isinstance(images, list):
            images = []
        containers = [str(x) for x in containers if str(x).strip()]
        images = [str(x) for x in images if str(x).strip()]
        containers = list(dict.fromkeys(containers))
        images = list(dict.fromkeys(images))
        if not containers and not images:
            return None
        return {'containers': containers, 'images': images}
    except Exception:
        return None


def _find_latest_summary_path(since_ts: float | None = None) -> str | None:
    try:
        report_dir = _reports_dir()
        if not os.path.isdir(report_dir):
            return None
        cands = []
        for name in os.listdir(report_dir):
            if not name.endswith('.json'):
                continue
            if not name.startswith('scenario_report_'):
                continue
            p = os.path.join(report_dir, name)
            try:
                st = os.stat(p)
                if since_ts is None or st.st_mtime >= max(0.0, float(since_ts) - 5.0):
                    cands.append((st.st_mtime, p))
            except Exception:
                continue
        if not cands:
            return None
        cands.sort(key=lambda x: x[0], reverse=True)
        return cands[0][1]
    except Exception:
        return None

def _extract_session_id_from_text(text: str) -> str | None:
    """Parse CLI logs for the session id marker emitted by core_topo_gen.cli.

    Expected line:
        CORE_SESSION_ID: <id>
    """
    try:
        if not text:
            return None
        m = re.search(r"CORE_SESSION_ID:\s*(\S+)", text)
        if m:
            return m.group(1)
    except Exception:
        pass
    return None


def _extract_session_id_from_core_path(text: str) -> int | None:
    """Best-effort: extract a CORE session id from a CORE VM path or filename.

    Examples:
      - /tmp/pycore.17/Scenario_1.xml -> 17
      - /tmp/pycore.17 -> 17
      - core-session-17-20251227-153012.xml -> 17
      - session-17.xml -> 17
    """
    if not text:
        return None
    s = str(text)
    try:
        m = re.search(r"(?:^|/|\\)pycore\.(\d+)(?:/|\\|$)", s)
        if m:
            return int(m.group(1))
    except Exception:
        pass
    try:
        m = re.search(r"\bcore-session-(\d+)\b", s)
        if m:
            return int(m.group(1))
    except Exception:
        pass
    try:
        m = re.search(r"\bsession-(\d+)\b", s)
        if m:
            return int(m.group(1))
    except Exception:
        pass
    return None


def _session_store_scenario_for_session_id(store: dict, session_id: int, *, host: str, port: int) -> str | None:
    """Return the newest known scenario_name for a given session id (scoped by CORE host/port)."""
    if not isinstance(store, dict) or not store:
        return None
    best_label: str | None = None
    best_ts: float | None = None
    for _path, entry in store.items():
        try:
            sid = _session_store_entry_session_id(entry)
            if sid is None or int(sid) != int(session_id):
                continue
            if not _session_store_entry_matches_core(entry, host, port):
                continue
            ts = _session_store_entry_updated_at_epoch(entry)
            if ts is None:
                ts = 0.0
            label = (entry.get('scenario_name') or '').strip() if isinstance(entry, dict) else ''
            if not label:
                continue
            if best_ts is None or ts >= best_ts:
                best_ts = ts
                best_label = label
        except Exception:
            continue
    return best_label


def _remote_write_session_scenario_meta_script(
    session_id: int,
    scenario_name: str | None,
    scenario_norm: str | None,
    scenario_xml_basename: str | None,
) -> str:
    sid_lit = json.dumps(int(session_id))
    name_lit = json.dumps((scenario_name or '').strip())
    norm_lit = json.dumps((scenario_norm or '').strip())
    base_lit = json.dumps((scenario_xml_basename or '').strip())
    template = textwrap.dedent(
        """
import json
import os
import time
import traceback


def main():
    payload = {}
    try:
        sid = __SID__
        meta_dir = '/tmp/core-topo-gen/session-meta'
        os.makedirs(meta_dir, exist_ok=True)
        meta_path = os.path.join(meta_dir, f"{sid}.json")
        meta = {
            'session_id': sid,
            'scenario_name': __SCEN_NAME__,
            'scenario_norm': __SCEN_NORM__,
            'scenario_xml_basename': __SCEN_BASE__,
            'written_at_epoch': time.time(),
        }
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(meta, f, indent=2, sort_keys=True)
        payload['ok'] = True
        payload['meta_path'] = meta_path
    except Exception as exc:
        payload['ok'] = False
        payload['error'] = str(exc)
        payload['traceback'] = traceback.format_exc()
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    )
    script = template.replace('__SID__', sid_lit)
    script = script.replace('__SCEN_NAME__', name_lit)
    script = script.replace('__SCEN_NORM__', norm_lit)
    script = script.replace('__SCEN_BASE__', base_lit)
    return script


def _remote_read_session_scenario_meta_script(session_id: int) -> str:
    sid_lit = json.dumps(int(session_id))
    template = textwrap.dedent(
        """
import json
import os
import traceback


def main():
    payload = {}
    try:
        sid = __SID__
        meta_path = os.path.join('/tmp/core-topo-gen/session-meta', f"{sid}.json")
        if not os.path.exists(meta_path):
            raise FileNotFoundError(meta_path)
        with open(meta_path, 'r', encoding='utf-8') as f:
            meta = json.load(f)
        payload['ok'] = True
        payload['meta'] = meta
        payload['meta_path'] = meta_path
    except Exception as exc:
        payload['ok'] = False
        payload['error'] = str(exc)
        payload['traceback'] = traceback.format_exc()
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    )
    return template.replace('__SID__', sid_lit)


def _remote_read_session_scenario_meta_bulk_script(session_ids: list[int]) -> str:
    ids_lit = json.dumps([int(x) for x in (session_ids or [])])
    template = textwrap.dedent(
        """
import json
import os
import traceback


def main():
    payload = {}
    try:
        sids = __SIDS__
        out = {}
        for sid in sids:
            try:
                meta_path = os.path.join('/tmp/core-topo-gen/session-meta', f"{int(sid)}.json")
                if not os.path.exists(meta_path):
                    continue
                with open(meta_path, 'r', encoding='utf-8') as f:
                    meta = json.load(f)
                if isinstance(meta, dict):
                    out[int(sid)] = meta
            except Exception:
                continue
        payload['ok'] = True
        payload['meta_by_sid'] = out
    except Exception as exc:
        payload['ok'] = False
        payload['error'] = str(exc)
        payload['traceback'] = traceback.format_exc()
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    )
    return template.replace('__SIDS__', ids_lit)


def _write_remote_session_scenario_meta(
    core_cfg: Dict[str, Any],
    *,
    session_id: int,
    scenario_name: str | None,
    scenario_xml_basename: str | None = None,
    logger: Optional[logging.Logger] = None,
) -> None:
    """Write a small session->scenario mapping file on the CORE VM.

    This makes it possible to map a *session XML path on the CORE VM* back to the
    scenario by extracting the session id from the path and reading this file.
    """
    log = logger or getattr(app, 'logger', logging.getLogger(__name__))
    scenario_label = (scenario_name or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_label) if scenario_label else ''
    script = _remote_write_session_scenario_meta_script(
        int(session_id),
        scenario_label,
        scenario_norm,
        scenario_xml_basename,
    )
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    command_desc = f"remote ssh {ssh_user}@{ssh_host} -> write /tmp/core-topo-gen/session-meta/{int(session_id)}.json"
    try:
        payload = _run_remote_python_json(
            cfg,
            script,
            logger=log,
            label='core.write_session_meta',
            command_desc=command_desc,
            timeout=30.0,
        )
        if payload.get('error'):
            log.debug('[core.session_meta] remote error: %s', payload.get('error'))
    except Exception as exc:
        log.debug('[core.session_meta] write failed: %s', exc)


def _read_remote_session_scenario_meta(
    core_cfg: Dict[str, Any],
    *,
    session_id: int,
    logger: Optional[logging.Logger] = None,
) -> dict[str, Any] | None:
    log = logger or getattr(app, 'logger', logging.getLogger(__name__))
    script = _remote_read_session_scenario_meta_script(int(session_id))
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    command_desc = f"remote ssh {ssh_user}@{ssh_host} -> read /tmp/core-topo-gen/session-meta/{int(session_id)}.json"
    try:
        payload = _run_remote_python_json(
            cfg,
            script,
            logger=log,
            label='core.read_session_meta',
            command_desc=command_desc,
            timeout=30.0,
        )
        if payload.get('ok') and isinstance(payload.get('meta'), dict):
            return payload.get('meta')
    except Exception as exc:
        log.debug('[core.session_meta] read failed: %s', exc)
    return None


def _read_local_session_scenario_meta_bulk(session_ids: list[int]) -> dict[int, dict[str, Any]]:
    """Best-effort local read of session-meta files.

    When the webapp runs on the CORE VM, the session-meta directory is directly
    accessible and SSH may be disabled/unavailable.
    """
    ids = [int(x) for x in (session_ids or []) if x not in (None, '')]
    if not ids:
        return {}
    out: dict[int, dict[str, Any]] = {}
    meta_dir = '/tmp/core-topo-gen/session-meta'
    try:
        if not os.path.isdir(meta_dir):
            return {}
    except Exception:
        return {}
    for sid in ids:
        try:
            meta_path = os.path.join(meta_dir, f"{int(sid)}.json")
            if not os.path.exists(meta_path):
                continue
            with open(meta_path, 'r', encoding='utf-8') as f:
                meta = json.load(f)
            if isinstance(meta, dict):
                out[int(sid)] = meta
        except Exception:
            continue
    return out


def _read_remote_session_scenario_meta_bulk(
    core_cfg: Dict[str, Any],
    *,
    session_ids: list[int],
    logger: Optional[logging.Logger] = None,
) -> dict[int, dict[str, Any]]:
    """Read multiple session meta files from the CORE VM in one remote call."""
    log = logger or getattr(app, 'logger', logging.getLogger(__name__))
    ids = [int(x) for x in (session_ids or []) if x not in (None, '')]
    if not ids:
        return {}

    # Fast-path: if we are running on the CORE VM, read directly.
    local = _read_local_session_scenario_meta_bulk(ids)
    if local and len(local) == len(set(ids)):
        return local
    script = _remote_read_session_scenario_meta_bulk_script(ids)
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    command_desc = f"remote ssh {ssh_user}@{ssh_host} -> read /tmp/core-topo-gen/session-meta/*.json (bulk)"
    try:
        payload = _run_remote_python_json(
            cfg,
            script,
            logger=log,
            label='core.read_session_meta_bulk',
            command_desc=command_desc,
            timeout=30.0,
        )
        if payload.get('ok') and isinstance(payload.get('meta_by_sid'), dict):
            out: dict[int, dict[str, Any]] = {}
            for k, v in payload.get('meta_by_sid', {}).items():
                try:
                    sid = int(k)
                except Exception:
                    continue
                if isinstance(v, dict):
                    out[sid] = v
            if local:
                merged = dict(local)
                merged.update(out)
                return merged
            return out
    except Exception as exc:
        log.debug('[core.session_meta] bulk read failed: %s', exc)
    return local or {}


def _session_store_updated_at_for_session_id(store: dict, session_id: int, *, host: str, port: int) -> float | None:
    """Return the newest known updated_at epoch for a session id (scoped by CORE host/port)."""
    if not isinstance(store, dict) or not store:
        return None
    best_ts: float | None = None
    for _path, entry in store.items():
        try:
            sid = _session_store_entry_session_id(entry)
            if sid is None or int(sid) != int(session_id):
                continue
            if not _session_store_entry_matches_core(entry, host, port):
                continue
            ts = _session_store_entry_updated_at_epoch(entry)
            if ts is None:
                continue
            if best_ts is None or ts >= best_ts:
                best_ts = ts
        except Exception:
            continue
    return best_ts


def _scenario_timestamped_filename(scenario_name: str | None, ts_epoch: float | None) -> str:
    """Build <scenario-name><timestamp>.xml (timestamp includes leading '-') for UI display."""
    try:
        stem = secure_filename((scenario_name or 'scenario')).strip('_-.') or 'scenario'
    except Exception:
        stem = 'scenario'
    try:
        epoch = float(ts_epoch) if ts_epoch is not None else None
    except Exception:
        epoch = None
    if epoch is None or epoch <= 0:
        try:
            epoch = time.time()
        except Exception:
            epoch = 0.0
    try:
        ts = datetime.datetime.fromtimestamp(epoch, tz=datetime.timezone.utc).strftime('-%Y%m%d-%H%M%S')
    except Exception:
        ts = '-unknown'
    return f"{stem}{ts}.xml"

def _safe_add_to_zip(zf: zipfile.ZipFile, abs_path: str, arcname: str) -> None:
    try:
        if abs_path and os.path.exists(abs_path):
            zf.write(abs_path, arcname)
    except Exception:
        pass

def _gather_scripts_into_zip(zf: zipfile.ZipFile, scenario_dir: str | None = None) -> int:
    """Collect generated traffic and segmentation artifacts into the provided zip file.

    This now walks both persistent output directories and the runtime `/tmp`
    locations used by the CLI so that *all* generated scripts and supporting
    files (JSON summaries, helper assets, custom plugin payloads, etc.) are
    included in the bundle. Returns the count of files added.
    """
    added = 0
    seen: set[str] = set()

    def _collect(label: str, dir_candidates: list[str]) -> None:
        nonlocal added
        for base in dir_candidates:
            if not base:
                continue
            try:
                base_abs = os.path.abspath(base)
            except Exception:
                base_abs = base
            if not base_abs or not os.path.isdir(base_abs):
                continue
            for root, dirs, files in os.walk(base_abs):
                # Skip hidden directories to avoid noise like .cache/.DS_Store
                dirs[:] = [d for d in dirs if not d.startswith('.')]
                for fname in files:
                    if fname.startswith('.'):
                        continue
                    try:
                        path = os.path.join(root, fname)
                        if not os.path.isfile(path) or os.path.islink(path):
                            continue
                        rel = os.path.relpath(path, base_abs)
                        # Defensive: ignore paths that navigate upwards
                        if rel.startswith('..'):
                            continue
                        rel = rel.replace('\\', '/')
                        arcname = f"{label}/{rel}".lstrip('/')
                        key = arcname.lower()
                        if key in seen:
                            continue
                        _safe_add_to_zip(zf, path, arcname)
                        seen.add(key)
                        added += 1
                    except Exception:
                        continue

    traffic_dirs = []
    try:
        traffic_dirs.append(_traffic_dir())
    except Exception:
        pass
    # Runtime traffic scripts live under /tmp/traffic by default
    traffic_dirs.extend(filter(None, [
        os.path.join(_outputs_dir(), 'traffic'),
        '/tmp/traffic',
        os.path.join(scenario_dir, 'traffic') if scenario_dir else None,
    ]))
    # Preserve order but drop duplicates
    traffic_dirs = list(dict.fromkeys(traffic_dirs))

    segmentation_dirs = []
    try:
        segmentation_dirs.append(_segmentation_dir())
    except Exception:
        pass
    segmentation_dirs.extend(filter(None, [
        os.path.join(_outputs_dir(), 'segmentation'),
        '/tmp/segmentation',
        os.path.join(scenario_dir, 'segmentation') if scenario_dir else None,
    ]))
    segmentation_dirs = list(dict.fromkeys(segmentation_dirs))

    _collect('traffic', traffic_dirs)
    _collect('segmentation', segmentation_dirs)
    return added

def _normalize_core_device_types(xml_path: str) -> None:
    """Normalize device 'type' attributes in a saved CORE session XML.

    - Docker/podman devices (class='docker'/'podman' or with compose attrs) -> type='docker'
    - Devices with routing services (zebra/BGP/OSPF*/RIP*/Xpimd) -> type='router'
    - Otherwise -> type='PC'
    """
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        devices = root.find('devices')
        if devices is None:
            return
        routing_like = {"zebra", "BGP", "Babel", "OSPFv2", "OSPFv3", "OSPFv3MDR", "RIP", "RIPNG", "Xpimd"}
        changed = False
        for dev in list(devices):
            if not isinstance(dev.tag, str) or dev.tag != 'device':
                continue
            clazz = (dev.get('class') or '').strip().lower()
            compose = (dev.get('compose') or '').strip()
            compose_name = (dev.get('compose_name') or '').strip()
            dtype = dev.get('type') or ''
            # collect services
            svc_names = set()
            try:
                services_el = dev.find('services') or dev.find('configservices')
                if services_el is not None:
                    for s in list(services_el):
                        nm = s.get('name')
                        if nm:
                            svc_names.add(nm)
            except Exception:
                pass
            new_type = None
            if clazz in ('docker', 'podman') or compose or compose_name:
                new_type = 'docker'
            elif any(s in routing_like for s in svc_names):
                new_type = 'router'
            else:
                new_type = 'PC'
            if new_type and new_type != dtype:
                dev.set('type', new_type)
                changed = True
        if changed:
            try:
                raw = ET.tostring(root, encoding='utf-8')
                lroot = LET.fromstring(raw)
                pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
                with open(xml_path, 'wb') as f:
                    f.write(pretty)
            except Exception:
                tree.write(xml_path, encoding='utf-8', xml_declaration=True)
    except Exception:
        pass

def _write_single_scenario_xml(src_xml_path: str, scenario_name: str | None, out_dir: str | None = None) -> str | None:
    """Create a new XML file containing only the selected Scenario from a Scenarios XML.

    - If `scenario_name` is None, selects the first Scenario present.
    - Returns the path to the new XML written under `out_dir` (or next to the source file) or None on failure.
    """
    try:
        if not (src_xml_path and os.path.exists(src_xml_path)):
            return None
        tree = ET.parse(src_xml_path)
        root = tree.getroot()
        # Normalize: if file is a single ScenarioEditor root, just copy it under Scenarios/Scenario
        chosen_se = None
        chosen_name = scenario_name
        if root.tag == 'Scenarios':
            # find Scenario child with matching name, else use first
            scenarios = [c for c in list(root) if isinstance(c.tag, str) and c.tag == 'Scenario']
            target = None
            if chosen_name:
                for s in scenarios:
                    if (s.get('name') or '') == chosen_name:
                        target = s
                        break
            if target is None and scenarios:
                target = scenarios[0]
                chosen_name = target.get('name') or 'Scenario'
            if target is None:
                return None
            se = target.find('ScenarioEditor')
            if se is None:
                # allow copying entire Scenario element if no ScenarioEditor child
                chosen_se = target
            else:
                chosen_se = se
        elif root.tag == 'ScenarioEditor':
            chosen_se = root
            if not chosen_name:
                # attempt to infer from nested metadata (not guaranteed)
                chosen_name = 'Scenario'
        else:
            # if root is Scenario, accept it
            if root.tag == 'Scenario':
                chosen_se = root.find('ScenarioEditor') or root
                chosen_name = chosen_name or (root.get('name') or 'Scenario')
            else:
                return None
        # Build new XML
        new_root = ET.Element('Scenarios')
        scen_el = ET.SubElement(new_root, 'Scenario')
        scen_el.set('name', chosen_name or 'Scenario')
        if chosen_se.tag == 'ScenarioEditor':
            # deep copy ScenarioEditor
            scen_el.append(ET.fromstring(ET.tostring(chosen_se)))
        else:
            # chosen_se was Scenario; append its contents
            scen_el.append(ET.fromstring(ET.tostring(chosen_se.find('ScenarioEditor'))) if chosen_se.find('ScenarioEditor') is not None else ET.Element('ScenarioEditor'))
        new_tree = ET.ElementTree(new_root)
        # Determine output path
        base_dir = out_dir or os.path.dirname(os.path.abspath(src_xml_path))
        os.makedirs(base_dir, exist_ok=True)
        stem = secure_filename((chosen_name or 'scenario')).strip('_-.') or 'scenario'
        out_path = os.path.join(base_dir, f"{stem}.xml")
        try:
            raw = ET.tostring(new_tree.getroot(), encoding='utf-8')
            lroot = LET.fromstring(raw)
            pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
            with open(out_path, 'wb') as f:
                f.write(pretty)
        except Exception:
            new_tree.write(out_path, encoding='utf-8', xml_declaration=True)
        return out_path if os.path.exists(out_path) else None
    except Exception:
        return None

def _build_full_scenario_archive(out_dir: str, scenario_xml_path: str | None, report_path: str | None, pre_xml_path: str | None, post_xml_path: str | None, *, summary_path: str | None = None, run_id: str | None = None) -> str | None:
    """Create a zip bundle that includes the scenario XML, pre/post session XML, report, and any generated scripts.

    Returns the path to the created zip, or None on failure.
    """
    try:
        os.makedirs(out_dir, exist_ok=True)
        stem = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S')
        if run_id:
            stem = f"{stem}-{run_id[:8]}"
        zip_path = os.path.join(out_dir, f"full_scenario_{stem}.zip")
        scenario_dir = os.path.dirname(os.path.abspath(scenario_xml_path)) if scenario_xml_path else None
        with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as zf:
            # Add top-level artifacts if present
            if scenario_xml_path and os.path.exists(scenario_xml_path):
                _safe_add_to_zip(zf, scenario_xml_path, "scenario.xml")
            if report_path and os.path.exists(report_path):
                _safe_add_to_zip(zf, report_path, os.path.join("report", os.path.basename(report_path)))
            if summary_path and os.path.exists(summary_path):
                _safe_add_to_zip(zf, summary_path, os.path.join("report", os.path.basename(summary_path)))
            csv_candidate = f"{report_path}.connectivity.csv" if report_path else None
            if csv_candidate and os.path.exists(csv_candidate):
                _safe_add_to_zip(zf, csv_candidate, os.path.join("report", os.path.basename(csv_candidate)))
            if pre_xml_path and os.path.exists(pre_xml_path):
                _safe_add_to_zip(zf, pre_xml_path, os.path.join("core-session", os.path.basename(pre_xml_path)))
            if post_xml_path and os.path.exists(post_xml_path):
                _safe_add_to_zip(zf, post_xml_path, os.path.join("core-session", os.path.basename(post_xml_path)))
            # Add generated scripts and summaries
            _gather_scripts_into_zip(zf, scenario_dir)
        return zip_path if os.path.exists(zip_path) else None
    except Exception:
        return None

# Reserved artifact keys for the Generator Builder. Populated from the fact ontology.
_RESERVED_ARTIFACTS: dict[str, dict[str, Any]] = {}


def _load_fact_reserved_artifacts() -> dict[str, dict[str, Any]]:
    """Build reserved artifacts from the fact ontology reference (best-effort)."""
    out: dict[str, dict[str, Any]] = {}
    try:
        from core_topo_gen.sequencer.facts import parse_fact_ref
    except Exception:
        parse_fact_ref = None  # type: ignore

    def _fact_meta(sig: str) -> dict[str, Any]:
        name = sig
        try:
            parsed = parse_fact_ref(sig) if parse_fact_ref else None
        except Exception:
            parsed = None
        if parsed:
            name = parsed[0]
        base = str(name or '').strip().lower()
        tp = 'text'
        sensitive = False
        if base in {'file', 'binary', 'pcap', 'backuparchive', 'sourcecode', 'encryptedblob', 'decryptionkey'}:
            tp = 'file'
        elif base in {'directory'}:
            tp = 'dir'
        elif base in {'flag', 'partialflag'}:
            tp = 'flag'
            sensitive = True
        elif base in {'credential'}:
            tp = 'credential'
            sensitive = True
        elif base in {'token', 'apikey', 'exposedsecret'}:
            tp = 'text'
            sensitive = True
        return {'type': tp, 'description': f"Fact: {sig}", **({'sensitive': True} if sensitive else {})}

    try:
        repo_root = _get_repo_root()
        path = os.path.join(repo_root, 'new-schema', 'fact_ontology_reference.yaml')
        if not os.path.isfile(path):
            return out
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                raw = line.split('#', 1)[0].strip()
                if not raw:
                    continue
                out[raw] = _fact_meta(raw)
    except Exception:
        return {}
    return out


try:
    _FACT_RESERVED_ARTIFACTS = _load_fact_reserved_artifacts()
    if _FACT_RESERVED_ARTIFACTS:
        _RESERVED_ARTIFACTS = _FACT_RESERVED_ARTIFACTS
except Exception:
    pass


def _coerce_flaggen_io_type(name: str) -> str:
    raw = (name or '').strip()
    n = raw.lower()
    if not raw:
        return 'text'
    try:
        meta = _RESERVED_ARTIFACTS.get(raw) or _RESERVED_ARTIFACTS.get(n)
        if isinstance(meta, dict):
            tp0 = str(meta.get('type') or '').strip()
            if tp0:
                return tp0
    except Exception:
        pass
    try:
        from core_topo_gen.sequencer.facts import parse_fact_ref
        parsed = parse_fact_ref(raw)
    except Exception:
        parsed = None
    if parsed:
        base = parsed[0].strip().lower()
        if base in {'file', 'binary', 'pcap', 'backuparchive', 'sourcecode', 'encryptedblob', 'decryptionkey'}:
            return 'file'
        if base in {'directory'}:
            return 'dir'
        if base in {'flag', 'partialflag'}:
            return 'flag'
        if base in {'credential'}:
            return 'credential'
        return 'text'
    if n.startswith('filesystem.'):
        if n.endswith('.dir'):
            return 'dir'
        if n.endswith('.file'):
            return 'file'
        if n.endswith('.path'):
            return 'path'
        return 'file'
    if n.startswith('network.'):
        if n.endswith('.port') or n == 'port':
            return 'port'
        if 'subnet' in n or 'cidr' in n:
            return 'subnet'
        if n.endswith('.ip') or n.endswith('.ipv4') or n.endswith('.ipv6') or '.ip' in n:
            return 'ip'
        if n.endswith('.host') or n.endswith('.hostname'):
            return 'host'
    if n.endswith('.url') or 'url' in n or n.startswith('web.') or n.startswith('http.'):
        return 'url'
    if n.startswith('credential.'):
        return 'credential'
    if 'private_key' in n or n.endswith('.key'):
        return 'text'
    if 'password' in n or 'secret' in n:
        return 'text'
    if 'token' in n:
        return 'text'
    if 'flag' in n:
        return 'flag'
    return 'text'


def _coerce_flaggen_io_sensitive(name: str, tp: str) -> bool:
    raw = (name or '').strip()
    n = raw.lower()
    t = (tp or '').strip().lower()
    try:
        meta = _RESERVED_ARTIFACTS.get(raw) or _RESERVED_ARTIFACTS.get(n)
        if isinstance(meta, dict) and meta.get('sensitive') is True:
            return True
    except Exception:
        pass
    try:
        from core_topo_gen.sequencer.facts import parse_fact_ref
        parsed = parse_fact_ref(raw)
    except Exception:
        parsed = None
    if parsed:
        base = parsed[0].strip().lower()
        if base in {'flag', 'partialflag', 'credential', 'token', 'apikey', 'exposedsecret', 'decryptionkey'}:
            return True
    if t == 'flag':
        return True
    if any(k in n for k in ('password', 'secret', 'token', 'private_key', 'ssh.private_key')):
        return True
    return False


def _generator_defs_from_flag_catalog_items(items: list[dict]) -> list[dict]:
    """Deprecated: no longer converting flag-catalog templates into generators."""
    return []


def _coerce_bool(val, default: bool = False) -> bool:
    try:
        if isinstance(val, bool):
            return val
        if isinstance(val, (int, float)):
            return bool(val)
        if isinstance(val, str):
            v = val.strip().lower()
            if v in {'true', '1', 'yes', 'y', 'on'}:
                return True
            if v in {'false', '0', 'no', 'n', 'off'}:
                return False
    except Exception:
        pass
    return default


def _normalize_generator_id(val: str) -> str:
    raw = (val or '').strip().lower()
    if not raw:
        return ''
    # Keep it simple and stable: allow a-z0-9._-
    out = []
    for ch in raw:
        if ch.isalnum() or ch in {'.', '_', '-'}:
            out.append(ch)
        elif ch.isspace():
            out.append('-')
    return ''.join(out).strip('-')


def _coerce_str(val: object, default: str = '') -> str:
    try:
        if val is None:
            return default
        s = str(val)
        return s
    except Exception:
        return default


def _coerce_stripped(val: object, default: str = '') -> str:
    return _coerce_str(val, default=default).strip()
def _flag_base_dir() -> str:
    """Base directory for downloaded flag compose assets."""
    try:
        return os.path.abspath(os.path.join(_outputs_dir(), 'flags'))
    except Exception:
        return os.path.abspath(os.path.join('outputs', 'flags'))


def _default_scenarios_payload():
    return _default_scenarios_payload_for_names(["Scenario 1"])


def _default_scenario_payload(name: str) -> Dict[str, Any]:
    # Single default scenario with empty sections mirroring PyQt structure
    sections = [
        "Node Information", "Routing", "Services", "Traffic",
        "Events", "Vulnerabilities", "Segmentation", "HITL"
    ]
    display_name = str(name or '').strip() or "Scenario"
    sections_dict: Dict[str, Any] = {}
    for sec_name in sections:
        entry: Dict[str, Any] = {
            "density": 0.5 if sec_name not in ("Node Information", "HITL") else None,
            "total_nodes": 1 if sec_name == "Node Information" else None,
            "items": [],
        }
        if sec_name == "Vulnerabilities":
            entry["flag_type"] = "text"
        sections_dict[sec_name] = entry
    return {
        "name": display_name,
        "base": {"filepath": ""},
        "hitl": {"enabled": False, "interfaces": [], "core": None},
        "sections": sections_dict,
        "notes": ""
    }


def _default_scenarios_payload_for_names(names: Iterable[Any] | None) -> Dict[str, Any]:
    scenario_names: list[str] = []
    for entry in names or []:
        try:
            text = str(entry or '').strip()
        except Exception:
            text = ''
        if text:
            scenario_names.append(text)
    if not scenario_names:
        scenario_names = ["Scenario 1"]

    return {
        "scenarios": [_default_scenario_payload(name) for name in scenario_names],
        "result_path": None,
        "core": _default_core_dict(),
        "host_interfaces": _enumerate_host_interfaces(),
    }


def _filter_scenarios_by_norms(
    scenarios: Iterable[Any],
    allowed_norms: set[str],
) -> List[Dict[str, Any]]:
    filtered: List[Dict[str, Any]] = []
    if not allowed_norms:
        return filtered
    allowed_keys = {key for key in (_scenario_match_key(v) for v in allowed_norms) if key}
    if not allowed_keys:
        return filtered
    for scen in scenarios or []:
        if not isinstance(scen, dict):
            continue
        if _scenario_match_key(scen.get('name')) in allowed_keys:
            filtered.append(scen)
    return filtered


def _collect_scenario_norms(scenarios: Iterable[Any]) -> set[str]:
    norms: set[str] = set()
    for scen in scenarios or []:
        if not isinstance(scen, dict):
            continue
        norm = _normalize_scenario_label(scen.get('name'))
        if norm:
            norms.add(norm)
    return norms


def _prepare_payload_for_index(payload: Optional[Dict[str, Any]], *, user: Optional[dict] = None) -> Dict[str, Any]:
    """Normalize payload data before rendering the index page."""
    if not isinstance(payload, dict):
        payload = {}
    else:
        payload = dict(payload)

    try:
        force_empty = _scenario_catalog_force_empty()
        payload['scenario_catalog_force_empty'] = bool(force_empty)
        if force_empty:
            payload['scenario_catalog_names'] = []
            payload['scenario_catalog_empty'] = True
        else:
            scenario_names_live, _scenario_paths_live, _scenario_url_hints_live = _scenario_catalog_for_user(None, user=user)
            payload['scenario_catalog_names'] = list(scenario_names_live or [])
            payload['scenario_catalog_empty'] = not bool(scenario_names_live)
    except Exception:
        payload['scenario_catalog_names'] = []
        payload['scenario_catalog_empty'] = True
        payload['scenario_catalog_force_empty'] = False

    project_hint_raw = payload.get('project_key_hint') if isinstance(payload.get('project_key_hint'), str) else None
    project_hint = project_hint_raw.strip() if isinstance(project_hint_raw, str) else None
    scenario_hint_raw = payload.get('scenario_query') if isinstance(payload.get('scenario_query'), str) else None
    scenario_hint = scenario_hint_raw.strip() if isinstance(scenario_hint_raw, str) else None
    allowed_norms = _builder_allowed_norms(user)
    builder_assignment_order: Optional[list[str]] = None
    if allowed_norms is not None:
        builder_assignment_order = _assigned_scenarios_for_user(user)

    defaults = _default_scenarios_payload()

    # --- Core connection defaults ---
    core_meta = _normalize_core_config(payload.get('core'), include_password=True)
    core_defaults = defaults['core']
    if not core_meta.get('host'):
        core_meta['host'] = core_defaults.get('host')
    if not core_meta.get('port'):
        core_meta['port'] = core_defaults.get('port')
    # Ensure ssh_host defaults to current host to simplify UI when enabling later
    if not core_meta.get('ssh_host'):
        core_meta['ssh_host'] = core_meta.get('host') or core_defaults.get('host')
    if not core_meta.get('ssh_port'):
        core_meta['ssh_port'] = 22
    payload['core'] = core_meta

    # --- Scenarios ---
    scenarios_raw = payload.get('scenarios')
    if not isinstance(scenarios_raw, list) or not scenarios_raw:
        # For actual restricted roles (builder/participant), an empty assignment list
        # should result in an empty scenario list ("all or none" visibility).
        scenarios_raw = defaults['scenarios'] if allowed_norms is None else []
    # For restricted roles, always build scenarios from catalog + assignments so
    # Scenarios/CORE/Reports are consistent (including ordering).
    if allowed_norms is not None and allowed_norms:
        seeded = _builder_catalog_seed_scenarios(allowed_norms, builder_assignment_order, user=user)
        if seeded:
            scenarios_raw = seeded

    required_sections = {
        'Node Information': {'density': None, 'total_nodes': None},
        'Routing': {'density': 0.5},
        'Services': {'density': 0.5},
        'Traffic': {'density': 0.5},
        'Events': {'density': 0.5},
        'Vulnerabilities': {'density': 0.5, 'flag_type': 'text'},
        'Segmentation': {'density': 0.5},
        'HITL': {},
    }

    normalized_scenarios: List[Dict[str, Any]] = []
    for idx, scen in enumerate(scenarios_raw, start=1):
        if not isinstance(scen, dict):
            continue
        scen_norm = dict(scen)
        scen_norm['name'] = scen_norm.get('name') or f"Scenario {idx}"

        base_meta = scen_norm.get('base')
        if not isinstance(base_meta, dict):
            base_meta = {}
        base_meta = dict(base_meta)
        filepath = base_meta.get('filepath')
        if not isinstance(filepath, str):
            filepath = '' if filepath is None else str(filepath)
        base_meta['filepath'] = filepath
        if filepath and not base_meta.get('display_name'):
            base_meta['display_name'] = os.path.basename(filepath)
        scen_norm['base'] = base_meta

        if 'density_count' not in scen_norm:
            scen_norm['density_count'] = 10

        sections_meta = scen_norm.get('sections')
        if not isinstance(sections_meta, dict):
            sections_meta = {}
        sections_out: Dict[str, Any] = {}
        for section_name, defaults_map in required_sections.items():
            sec_val = sections_meta.get(section_name)
            if isinstance(sec_val, dict):
                sec_norm = dict(sec_val)
            else:
                sec_norm = {}
            items = sec_norm.get('items')
            if isinstance(items, list):
                sec_norm['items'] = [item for item in items if isinstance(item, dict)]
            else:
                sec_norm['items'] = []
            for key, val in defaults_map.items():
                sec_norm.setdefault(key, val)
            sections_out[section_name] = sec_norm
        for extra_name, extra_val in sections_meta.items():
            if extra_name not in sections_out:
                sections_out[extra_name] = extra_val
        scen_norm['sections'] = sections_out

        hitl_meta = scen_norm.get('hitl')
        if isinstance(hitl_meta, dict):
            hitl_norm = dict(hitl_meta)
        else:
            hitl_norm = {}
        hitl_norm['enabled'] = bool(hitl_norm.get('enabled'))
        interfaces_raw = hitl_norm.get('interfaces')
        interfaces_norm: List[Dict[str, Any]] = []
        if isinstance(interfaces_raw, list):
            for iface in interfaces_raw:
                if not isinstance(iface, dict):
                    continue
                iface_norm = dict(iface)
                name = iface_norm.get('name')
                if not isinstance(name, str):
                    name = '' if name is None else str(name)
                name = name.strip()
                if not name:
                    continue
                iface_norm['name'] = name
                iface_norm['attachment'] = _normalize_hitl_attachment(iface_norm.get('attachment'))
                for addr_key in ('ipv4', 'ipv6'):
                    vals = iface_norm.get(addr_key)
                    if isinstance(vals, list):
                        iface_norm[addr_key] = [str(v).strip() for v in vals if v is not None and str(v).strip()]
                interfaces_norm.append(iface_norm)
        hitl_norm['interfaces'] = interfaces_norm
        hitl_norm['core'] = _extract_optional_core_config(hitl_norm.get('core'), include_password=True)
        scen_norm['hitl'] = hitl_norm

        normalized_scenarios.append(scen_norm)

    # Only fall back to default scenarios for unrestricted users.
    if not normalized_scenarios and allowed_norms is None:
        normalized_scenarios = defaults['scenarios']

    # If the UI is in builder mode (even for admin users previewing builder view),
    # merge admin-managed HITL validation hints into scenarios so builder cannot
    # appear "unassigned" when admin has already selected a CORE VM.
    try:
        view_mode = getattr(g, 'ui_view_mode', _UI_VIEW_DEFAULT)
    except Exception:
        view_mode = _UI_VIEW_DEFAULT
    if view_mode == 'builder':
        try:
            hitl_validation_hints = _load_scenario_hitl_validation_from_disk()
            builder_hitl_fallback = _select_builder_hitl_fallback(hitl_validation_hints)
            hitl_config_hints = _load_scenario_hitl_config_from_disk()
            builder_hitl_config_fallback = _select_builder_hitl_fallback(hitl_config_hints) if hitl_config_hints else None
            try:
                _, _, participant_urls_by_norm = _load_scenario_catalog_from_disk()
            except Exception:
                participant_urls_by_norm = {}
        except Exception:
            hitl_validation_hints = {}
            builder_hitl_fallback = None
            hitl_config_hints = {}
            builder_hitl_config_fallback = None
            participant_urls_by_norm = {}

        for scen in normalized_scenarios:
            if not isinstance(scen, dict):
                continue
            norm = _normalize_scenario_label(scen.get('name'))
            validation_hint = hitl_validation_hints.get(norm) if (norm and isinstance(hitl_validation_hints, dict)) else None
            effective_hint = validation_hint if isinstance(validation_hint, dict) and validation_hint else builder_hitl_fallback
            config_hint = hitl_config_hints.get(norm) if (norm and isinstance(hitl_config_hints, dict)) else None
            effective_cfg = config_hint if isinstance(config_hint, dict) and config_hint else builder_hitl_config_fallback
            if not (isinstance(effective_hint, dict) and effective_hint) and not (isinstance(effective_cfg, dict) and effective_cfg):
                continue

            effective_hint_dict: Dict[str, Any] = effective_hint if isinstance(effective_hint, dict) else {}

            hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else {}
            hitl_meta = dict(hitl_meta)

            # Merge participant URL hints (non-secret) so builder view shows Step 5 as configured.
            try:
                participant_hint = participant_urls_by_norm.get(norm) if (norm and isinstance(participant_urls_by_norm, dict)) else ''
                if participant_hint:
                    for k in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                        if k not in hitl_meta or hitl_meta.get(k) in (None, ''):
                            hitl_meta[k] = participant_hint
            except Exception:
                pass

            prox_hint = effective_hint_dict.get('proxmox') if isinstance(effective_hint_dict.get('proxmox'), dict) else None
            if isinstance(prox_hint, dict) and prox_hint:
                prox_state = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else {}
                prox_state = dict(prox_state)
                for k, v in prox_hint.items():
                    if k not in prox_state or prox_state.get(k) in (None, '', False):
                        prox_state[k] = v
                hitl_meta['proxmox'] = prox_state

            core_hint = effective_hint_dict.get('core') if isinstance(effective_hint_dict.get('core'), dict) else None
            if isinstance(core_hint, dict) and core_hint:
                core_state = hitl_meta.get('core') if isinstance(hitl_meta.get('core'), dict) else {}
                core_state = dict(core_state)
                for k, v in core_hint.items():
                    if k not in core_state or core_state.get(k) in (None, '', False):
                        core_state[k] = v
                hitl_meta['core'] = core_state

            # Apply HITL configuration hints (enabled/interfaces/mappings)
            if isinstance(effective_cfg, dict) and effective_cfg:
                try:
                    participant_cfg = _normalize_participant_proxmox_url(effective_cfg.get('participant_proxmox_url'))
                    if participant_cfg:
                        for k in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                            if k not in hitl_meta or hitl_meta.get(k) in (None, ''):
                                hitl_meta[k] = participant_cfg
                except Exception:
                    pass
                if 'enabled' in effective_cfg:
                    hitl_meta['enabled'] = bool(effective_cfg.get('enabled'))
                if isinstance(effective_cfg.get('interfaces'), list):
                    hitl_meta['interfaces'] = effective_cfg.get('interfaces')
                cfg_prox = effective_cfg.get('proxmox') if isinstance(effective_cfg.get('proxmox'), dict) else None
                if isinstance(cfg_prox, dict) and cfg_prox:
                    prox_state = hitl_meta.get('proxmox') if isinstance(hitl_meta.get('proxmox'), dict) else {}
                    prox_state = dict(prox_state)
                    prox_state.update(cfg_prox)
                    hitl_meta['proxmox'] = prox_state
                cfg_core = effective_cfg.get('core') if isinstance(effective_cfg.get('core'), dict) else None
                if isinstance(cfg_core, dict) and cfg_core:
                    core_state = hitl_meta.get('core') if isinstance(hitl_meta.get('core'), dict) else {}
                    core_state = dict(core_state)
                    core_state.update(cfg_core)
                    hitl_meta['core'] = core_state

            scen['hitl'] = hitl_meta
    if allowed_norms is not None:
        # If there are assignments, filter to them.
        if allowed_norms:
            normalized_scenarios = _filter_scenarios_by_norms(normalized_scenarios, allowed_norms)
        else:
            normalized_scenarios = []
        # Always mark as restricted when allowed_norms is not None.
        payload['builder_restricted_scenarios'] = True
        payload['builder_assigned_scenarios'] = sorted(allowed_norms)
        payload['builder_no_assignments'] = not bool(allowed_norms)
    else:
        payload.pop('builder_restricted_scenarios', None)
        payload.pop('builder_assigned_scenarios', None)
        payload.pop('builder_no_assignments', None)
        payload.pop('builder_assigned_scenarios', None)
        payload.pop('builder_no_assignments', None)
    payload['scenarios'] = normalized_scenarios

    # --- Base upload metadata ---
    base_upload = payload.get('base_upload')
    if isinstance(base_upload, dict):
        base_norm = dict(base_upload)
        path = base_norm.get('path')
        if isinstance(path, str):
            base_norm['path'] = path
            base_norm.setdefault('display_name', os.path.basename(path) if path else '')
        else:
            base_norm['path'] = ''
        if 'valid' in base_norm:
            base_norm['valid'] = bool(base_norm['valid'])
        payload['base_upload'] = base_norm

    # --- Host interfaces ---
    host_ifaces = payload.get('host_interfaces')
    if not isinstance(host_ifaces, list):
        host_ifaces = []
    sanitized_ifaces: List[Dict[str, Any]] = []
    adaptor_names: set[str] = set()
    for iface in host_ifaces:
        if not isinstance(iface, dict):
            continue
        entry = dict(iface)
        name = entry.get('name')
        if isinstance(name, str):
            name = name.strip()
        elif name is not None:
            name = str(name)
        else:
            name = ''
        entry['name'] = name
        if name:
            adaptor_names.add(name)
        for arr_key in ('ipv4', 'ipv6', 'flags'):
            vals = entry.get(arr_key)
            if isinstance(vals, list):
                entry[arr_key] = [v for v in vals if v not in (None, '')]
        sanitized_ifaces.append(entry)
    payload['host_interfaces'] = sanitized_ifaces
    payload['hitl_adaptors'] = sorted(adaptor_names)

    payload.setdefault('result_path', defaults['result_path'])

    try:
        scen_names_for_catalog = [scen.get('name') for scen in normalized_scenarios if isinstance(scen, dict)]
        # Build a per-scenario participant URL hint map. Include explicit empty values
        # so we can clear stale catalog hints when a user removes a URL in the editor.
        participant_hints: Dict[str, Any] = {}
        for scen in normalized_scenarios:
            if not isinstance(scen, dict):
                continue
            name = scen.get('name')
            norm = _normalize_scenario_label(name)
            if not norm:
                continue
            hitl_meta = scen.get('hitl') if isinstance(scen.get('hitl'), dict) else None
            participant_url_value = ''
            if hitl_meta:
                for key in ('participant_proxmox_url', 'participant_ui_url', 'participant_url', 'participant'):
                    candidate = hitl_meta.get(key)
                    normalized = _normalize_participant_proxmox_url(candidate)
                    if normalized:
                        participant_url_value = normalized
                        break
            participant_hints[norm] = participant_url_value or ''

        if allowed_norms is None:
            result_path = payload.get('result_path') if isinstance(payload.get('result_path'), str) else None
            should_persist = bool(result_path)
            if not should_persist:
                try:
                    should_persist = not os.path.exists(_scenario_catalog_file())
                except Exception:
                    should_persist = False
            if should_persist:
                _persist_scenario_catalog(scen_names_for_catalog, result_path, participant_urls=participant_hints)
            else:
                if participant_hints:
                    _merge_participant_urls_into_scenario_catalog(participant_hints)
        else:
            if participant_hints:
                _merge_participant_urls_into_scenario_catalog(participant_hints)
    except Exception:
        pass

    if project_hint:
        payload['project_key_hint'] = project_hint
    elif payload.get('result_path') and not payload.get('project_key_hint'):
        payload['project_key_hint'] = payload['result_path']
    if scenario_hint:
        payload['scenario_query'] = scenario_hint

    return payload


@app.route('/api/editor_snapshot', methods=['POST'])
def api_editor_snapshot():
    user = _current_user()
    if not user or not user.get('username'):
        return jsonify({'success': False, 'error': 'Authentication required'}), 401
    payload = request.get_json(silent=True)
    if not isinstance(payload, dict):
        return jsonify({'success': False, 'error': 'Invalid snapshot payload'}), 400
    snapshot = _build_editor_snapshot_payload(payload)
    if not snapshot:
        return jsonify({'success': False, 'error': 'Snapshot rejected'}), 400
    try:
        _write_editor_state_snapshot(snapshot, user=user)
    except Exception as exc:
        try:
            app.logger.exception('[editor_snapshot] persist failed: %s', exc)
        except Exception:
            pass
        return jsonify({'success': False, 'error': 'Unable to persist snapshot'}), 500
    return jsonify({'success': True})


# Hardware in the Loop utilities
@app.route('/api/host_interfaces', methods=['GET', 'POST'])
def api_host_interfaces():
    if request.method == 'POST':
        payload = request.get_json(silent=True) or {}
        secret_id_raw = payload.get('core_secret_id') or payload.get('secret_id')
        secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
        include_down = _coerce_bool(payload.get('include_down'))
        core_vm_payload = payload.get('core_vm') if isinstance(payload.get('core_vm'), dict) else {}
        prox_interfaces_raw = core_vm_payload.get('interfaces') if isinstance(core_vm_payload, dict) else None
        prox_interfaces = [entry for entry in prox_interfaces_raw if isinstance(entry, dict)] if isinstance(prox_interfaces_raw, list) else None
        vm_context = {
            'vm_key': core_vm_payload.get('vm_key'),
            'vm_name': core_vm_payload.get('vm_name'),
            'vm_node': core_vm_payload.get('vm_node'),
            'vmid': core_vm_payload.get('vmid'),
        }
        if not secret_id:
            # Try to fetch fresh data from Proxmox directly if we know the VMID
            fresh_prox_interfaces = None
            if vm_context.get('vmid'):
                fresh_prox_interfaces = _find_proxmox_vm_config(
                    vm_context.get('vm_node'),
                    vm_context.get('vmid'),
                )
            
            if fresh_prox_interfaces:
                # Use fresh data!
                prox_interfaces = fresh_prox_interfaces
                # Also fall back to this if SSH fails (handled below)
            else:
                return jsonify({'success': False, 'error': 'CORE credentials are required to enumerate interfaces from the CORE VM'}), 400

        def _fallback_from_proxmox() -> Optional[Dict[str, Any]]:
            if not prox_interfaces:
                return None
            synthesized: List[Dict[str, Any]] = []
            for idx, vmif in enumerate(prox_interfaces):
                if not isinstance(vmif, dict):
                    continue
                name = str(vmif.get('name') or vmif.get('id') or vmif.get('label') or f'interface-{idx}')
                # For fallback, if we have 'id' (net0), use it as name because that's what user expects
                if vmif.get('id'):
                    name = vmif.get('id')
                
                mac = vmif.get('macaddr') or vmif.get('mac') or vmif.get('hwaddr') or ''
                entry: Dict[str, Any] = {
                    'name': name,
                    'display': name,
                    'mac': mac,
                    'ipv4': [],
                    'ipv6': [],
                    'mtu': None,
                    'speed': None,
                    'is_up': None,
                    'flags': [],
                    'proxmox': {
                        'id': vmif.get('id') or vmif.get('name') or vmif.get('label') or name,
                        'macaddr': mac,
                        'bridge': vmif.get('bridge'),
                        'model': vmif.get('model'),
                        'raw': vmif,
                        'vm_key': vm_context.get('vm_key'),
                        'vm_name': vm_context.get('vm_name'),
                        'vm_node': vm_context.get('vm_node'),
                        'vmid': vm_context.get('vmid'),
                    },
                }
                if vmif.get('bridge'):
                    entry['bridge'] = vmif.get('bridge')
                synthesized.append(entry)
            meta = {k: v for k, v in vm_context.items() if v not in (None, '')}
            return {
                'success': True,
                'source': 'proxmox_inventory_fallback',
                'interfaces': synthesized,
                'metadata': meta,
                'fetched_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
                'note': 'Using Proxmox inventory as a fallback; CORE VM SSH enumeration unavailable.'
            }

        try:
            # If we have secret_id, we can try SSH.
            # But we should still try to get fresh Proxmox data if possible, 
            # because the frontend payload might be stale (MAC address mismatch).
            fresh_prox_interfaces = None
            if vm_context.get('vmid'):
                try:
                    fresh_prox_interfaces = _find_proxmox_vm_config(
                        vm_context.get('vm_node'),
                        vm_context.get('vmid'),
                    )
                except Exception:
                    pass
            
            # Use fresh data if available, otherwise use payload
            use_prox_interfaces = fresh_prox_interfaces if fresh_prox_interfaces else prox_interfaces

            if secret_id:
                interfaces = _enumerate_core_vm_interfaces_from_secret(
                    secret_id,
                    prox_interfaces=use_prox_interfaces,
                    include_down=include_down,
                    vm_context=vm_context,
                )
            else:
                # No secret ID. if we have fresh data, stick to fallback.
                # If we don't have fresh data and no secret ID, we errored out above (lines 25414)
                # UNLESS we are in the path where we just have payload prox_interfaces but no secret_id
                # (which strictly speaking lines 25414 covers)
                # But let's be safe:
                if use_prox_interfaces:
                     # This will trigger the fallback check below
                     interfaces = []
                else:
                     raise ValueError("CORE credentials are required")

            if (not interfaces) and use_prox_interfaces:
                # No interfaces returned from CORE VM; fall back to Proxmox inventory snapshot
                # We need to temporarily point 'prox_interfaces' to 'use_prox_interfaces' for the closure to work?
                # Actually _fallback_from_proxmox uses 'prox_interfaces' from outer scope.
                # We should update that variable or pass it. 
                # Inner function uses outer scope. Let's update outer scope variable 'prox_interfaces'.
                prox_interfaces = use_prox_interfaces
                fb = _fallback_from_proxmox()
                if fb is not None:
                    return jsonify(fb)
        except ValueError:
            # If creds are missing/invalid, try falling back to Proxmox inventory if available.
            if fresh_prox_interfaces:
                 prox_interfaces = fresh_prox_interfaces
            fb = _fallback_from_proxmox()
            if fb is not None:
                return jsonify(fb)
            return jsonify({'success': False, 'error': 'CORE credentials are required (and no Proxmox fallback available)'}), 400
        except _SSHTunnelError as exc:
            fb = _fallback_from_proxmox()
            if fb is not None:
                return jsonify(fb)
            return jsonify({'success': False, 'error': str(exc)}), 502
        except RuntimeError as exc:
            fb = _fallback_from_proxmox()
            if fb is not None:
                return jsonify(fb)
            return jsonify({'success': False, 'error': str(exc)}), 500
        except Exception as exc:  # pragma: no cover - defensive logging
            app.logger.exception('[hitl] unexpected failure retrieving CORE VM interfaces: %s', exc)
            return jsonify({'success': False, 'error': 'Unexpected error retrieving CORE VM interfaces'}), 500
        response_data = {
            'success': True,
            'source': 'core_vm',
            'interfaces': interfaces,
            'metadata': {k: v for k, v in vm_context.items() if v not in (None, '')},
            'fetched_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
        }
        return jsonify(response_data)
    try:
        interfaces = _enumerate_host_interfaces()
        return jsonify({'success': True, 'interfaces': interfaces})
    except Exception as exc:  # pragma: no cover - defensive logging for unexpected psutil errors
        app.logger.exception('[hitl] failed to enumerate host interfaces via GET: %s', exc)
        return jsonify({'success': False, 'error': 'Failed to enumerate host interfaces'}), 500


@app.route('/api/proxmox/validate', methods=['POST'])
def api_proxmox_validate():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    if ProxmoxAPI is None:
        return jsonify({'success': False, 'error': 'Proxmox integration unavailable: install proxmoxer package'}), 500
    payload = request.get_json(silent=True) or {}
    url_raw = str(payload.get('url') or '').strip()
    if not url_raw:
        return jsonify({'success': False, 'error': 'URL is required'}), 400
    parsed = urlparse(url_raw)
    if parsed.scheme not in {'http', 'https'}:
        return jsonify({'success': False, 'error': 'URL must start with http:// or https://'}), 400
    host = parsed.hostname
    if not host:
        return jsonify({'success': False, 'error': 'Unable to determine host from URL'}), 400
    try:
        port = int(payload.get('port') or (parsed.port or 8006))
    except Exception:
        port = parsed.port or 8006
    if port < 1 or port > 65535:
        return jsonify({'success': False, 'error': 'Port must be between 1 and 65535'}), 400
    username = str(payload.get('username') or '').strip()
    if not username:
        return jsonify({'success': False, 'error': 'Username is required'}), 400
    password = payload.get('password')
    if password is None:
        password = ''
    if not isinstance(password, str):
        password = str(password)
    verify_ssl_raw = payload.get('verify_ssl')
    reuse_secret_raw = payload.get('reuse_secret_id')
    reuse_secret_id = reuse_secret_raw.strip() if isinstance(reuse_secret_raw, str) else ''
    stored_record: Optional[Dict[str, Any]] = None
    if not password and reuse_secret_id:
        stored_record = _load_proxmox_credentials(reuse_secret_id)
        if not stored_record:
            return jsonify({'success': False, 'error': 'Stored credentials unavailable. Re-enter the password.'}), 400
        stored_password = stored_record.get('password_plain') or ''
        if not stored_password:
            return jsonify({'success': False, 'error': 'Stored credentials are missing password material. Re-enter the password.'}), 400
        stored_url = (stored_record.get('url') or '').strip()
        stored_username = (stored_record.get('username') or '').strip()
        if stored_url and stored_url != url_raw:
            return jsonify({'success': False, 'error': 'URL changed since the last validation. Re-enter the password.'}), 400
        if stored_username and stored_username != username:
            return jsonify({'success': False, 'error': 'Username changed since the last validation. Re-enter the password.'}), 400
        password = stored_password
        if stored_record.get('verify_ssl') is not None and verify_ssl_raw is None:
            verify_ssl_raw = bool(stored_record.get('verify_ssl'))
    if not isinstance(password, str):
        password = str(password)
    if not password:
        return jsonify({'success': False, 'error': 'Password is required'}), 400
    if verify_ssl_raw is None:
        verify_ssl = (parsed.scheme == 'https')
    else:
        verify_ssl = bool(verify_ssl_raw)
    timeout_val = payload.get('timeout', 5.0)
    try:
        timeout = float(timeout_val)
    except Exception:
        timeout = 5.0
    timeout = max(1.0, min(timeout, 30.0))
    # Persist credentials only if explicitly requested.
    remember_credentials = bool(payload.get('remember_credentials', True))
    prox_kwargs = {
        'host': host,
        'user': username,
        'password': password,
        'port': port,
        'verify_ssl': verify_ssl,
        'timeout': timeout,
        'backend': 'https' if parsed.scheme == 'https' else 'http',
    }
    try:
        app.logger.debug('[proxmox] attempting auth for %s@%s:%s (verify_ssl=%s)', username, host, port, verify_ssl)
    except Exception:
        pass
    try:
        prox = ProxmoxAPI(**prox_kwargs)  # type: ignore[arg-type]
        prox.version.get()
    except Exception as exc:
        try:
            app.logger.warning('[proxmox] authentication failed: %s', exc)
        except Exception:
            pass
        msg = str(exc)
        lowered = msg.lower()
        # In docker-compose deployments, common failures are reachability/DNS/TLS/proxy.
        # If we see strong signals of a network/proxy error, report 502 (Bad Gateway)
        # instead of 401 (credentials).
        if any(tok in lowered for tok in (
            'bad gateway', '502', 'connection refused', 'name or service not known',
            'temporary failure in name resolution', 'timed out', 'timeout',
            'max retries exceeded', 'connection error', 'proxyerror',
            'ssLError'.lower(), 'certificate verify failed',
        )):
            detail = (
                'Unable to reach Proxmox API from this server (network/proxy/TLS issue). '
                f'Detail: {msg}'
            )
            return jsonify({'success': False, 'error': detail}), 502
        return jsonify({'success': False, 'error': f'Authentication failed: {msg}'}), 401
    try:
        app.logger.info('[proxmox] authentication succeeded for %s@%s:%s', username, host, port)
    except Exception:
        pass
    scenario_index = payload.get('scenario_index')
    scenario_name = str(payload.get('scenario_name') or '').strip()
    summary: Dict[str, Any] = {
        'url': url_raw,
        'port': port,
        'username': username,
        'verify_ssl': verify_ssl,
    }
    secret_identifier: Optional[str] = None
    stored_at_val: Optional[str] = None
    secret_payload = {
        'scenario_name': scenario_name,
        'scenario_index': scenario_index,
        'url': url_raw,
        'username': username,
        'password': password,
        'port': port,
        'verify_ssl': verify_ssl,
    }
    if remember_credentials:
        try:
            stored_meta = _save_proxmox_credentials(secret_payload)
        except RuntimeError as exc:
            return jsonify({'success': False, 'error': str(exc)}), 500
        except Exception as exc:
            app.logger.exception('[proxmox] failed to persist credentials: %s', exc)
            return jsonify({'success': False, 'error': 'Credentials validated but could not be stored'}), 500
        stored_at_val = stored_meta.get('stored_at')
        summary = {
            'url': stored_meta['url'],
            'port': stored_meta['port'],
            'username': stored_meta['username'],
            'verify_ssl': stored_meta['verify_ssl'],
            'stored_at': stored_at_val,
        }
        secret_identifier = stored_meta['identifier']
    else:
        summary = {
            'url': url_raw,
            'port': port,
            'username': username,
            'verify_ssl': verify_ssl,
            'stored_at': None,
        }
    message = f"Validated Proxmox access for {username} at {host}:{port}"
    # Persist a safe shared hint so builders/participants can see the validated state.
    # This is the non-secret subset (no password/token material).
    try:
        if scenario_name:
            _merge_hitl_validation_into_scenario_catalog(
                scenario_name,
                proxmox={
                    'url': summary.get('url'),
                    'port': summary.get('port'),
                    'verify_ssl': summary.get('verify_ssl'),
                    'secret_id': secret_identifier if remember_credentials else None,
                    'validated': bool(secret_identifier) if remember_credentials else False,
                    'last_validated_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
                    'stored_at': summary.get('stored_at'),
                    'last_message': message,
                },
            )
    except Exception:
        pass
    return jsonify({
        'success': True,
        'message': message,
        'summary': summary,
        'secret_id': secret_identifier if remember_credentials else None,
        'scenario_index': scenario_index,
        'scenario_name': scenario_name,
    })


@app.route('/api/proxmox/clear', methods=['POST'])
def api_proxmox_clear():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    secret_id_raw = payload.get('secret_id')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    scenario_index = payload.get('scenario_index')
    scenario_name = str(payload.get('scenario_name') or '').strip()
    removed = False
    try:
        if secret_id:
            removed = _delete_proxmox_credentials(secret_id)
    except Exception:
        app.logger.exception('[proxmox] failed to clear credentials for %s (scenario %s)', secret_id or 'unknown', scenario_name or scenario_index)
        return jsonify({'success': False, 'error': 'Failed to clear stored Proxmox credentials'}), 500
    try:
        app.logger.info('[proxmox] cleared credentials request for %s (scenario_index=%s, removed=%s)', scenario_name or 'unnamed', scenario_index, removed)
    except Exception:
        pass
    try:
        if scenario_name:
            _clear_hitl_validation_in_scenario_catalog(scenario_name, proxmox=True)
    except Exception:
        pass
    return jsonify({
        'success': True,
        'secret_removed': removed,
        'scenario_index': scenario_index,
        'scenario_name': scenario_name,
    })


@app.route('/api/proxmox/credentials/get', methods=['POST'])
def api_proxmox_credentials_get():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    secret_id_raw = payload.get('secret_id')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    if not secret_id:
        return jsonify({'success': False, 'error': 'secret_id is required'}), 400
    try:
        record = _load_proxmox_credentials(secret_id)
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 500
    if not record:
        return jsonify({'success': False, 'error': 'Stored credentials not found'}), 404
    credentials = {
        'identifier': record.get('identifier') or secret_id,
        'scenario_name': record.get('scenario_name') or '',
        'scenario_index': record.get('scenario_index'),
        'url': record.get('url') or '',
        'port': int(record.get('port') or 8006),
        'username': record.get('username') or '',
        'password': record.get('password_plain') or '',
        'verify_ssl': bool(record.get('verify_ssl', True)),
        'stored_at': record.get('stored_at'),
    }
    return jsonify({'success': True, 'credentials': credentials})


@app.route('/api/core/credentials/clear', methods=['POST'])
def api_core_credentials_clear():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    secret_id_raw = payload.get('core_secret_id') or payload.get('secret_id')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    scenario_index = payload.get('scenario_index')
    scenario_name = str(payload.get('scenario_name') or '').strip()
    removed = False
    try:
        if secret_id:
            removed = _delete_core_credentials(secret_id)
    except Exception:
        app.logger.exception('[core] failed to clear credentials for %s (scenario %s)', secret_id or 'unknown', scenario_name or scenario_index)
        return jsonify({'success': False, 'error': 'Failed to clear stored CORE credentials'}), 500
    try:
        app.logger.info('[core] cleared credentials request for %s (scenario_index=%s, removed=%s)', scenario_name or 'unnamed', scenario_index, removed)
    except Exception:
        pass
    try:
        if scenario_name:
            _clear_hitl_validation_in_scenario_catalog(scenario_name, core=True)
    except Exception:
        pass
    return jsonify({
        'success': True,
        'secret_removed': removed,
        'scenario_index': scenario_index,
        'scenario_name': scenario_name,
    })


@app.route('/api/core/credentials/get', methods=['POST'])
def api_core_credentials_get():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    secret_id_raw = payload.get('core_secret_id') or payload.get('secret_id')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    if not secret_id:
        return jsonify({'success': False, 'error': 'core_secret_id is required'}), 400
    try:
        record = _load_core_credentials(secret_id)
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 500
    if not record:
        return jsonify({'success': False, 'error': 'Stored credentials not found'}), 404
    credentials = {
        'identifier': record.get('identifier') or secret_id,
        'scenario_name': record.get('scenario_name') or '',
        'scenario_index': record.get('scenario_index'),
        'host': record.get('host') or record.get('grpc_host') or '',
        'port': int(record.get('port') or record.get('grpc_port') or 50051),
        'grpc_host': record.get('grpc_host') or record.get('host') or '',
        'grpc_port': int(record.get('grpc_port') or record.get('port') or 50051),
        'ssh_host': record.get('ssh_host') or '',
        'ssh_port': int(record.get('ssh_port') or 22),
        'ssh_username': record.get('ssh_username') or '',
        'ssh_password': record.get('ssh_password_plain') or '',
        'ssh_enabled': bool(record.get('ssh_enabled', True)),
        'venv_bin': record.get('venv_bin') or DEFAULT_CORE_VENV_BIN,
        'vm_key': record.get('vm_key') or '',
        'vm_name': record.get('vm_name') or '',
        'vm_node': record.get('vm_node') or '',
        'vmid': record.get('vmid'),
        'proxmox_secret_id': record.get('proxmox_secret_id'),
        'proxmox_target': record.get('proxmox_target'),
        'stored_at': record.get('stored_at'),
    }
    return jsonify({'success': True, 'credentials': credentials})


@app.route('/api/hitl/core_vm/clear', methods=['POST'])
def api_hitl_core_vm_clear():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    scenario_name = str(payload.get('scenario_name') or payload.get('scenario') or '').strip()
    scenario_index = payload.get('scenario_index')
    if not scenario_name:
        return jsonify({'success': False, 'error': 'scenario_name is required'}), 400
    try:
        _clear_hitl_config_in_scenario_catalog(scenario_name, clear_core_vm=True)
    except Exception:
        app.logger.exception('[hitl] failed clearing CORE VM selection for %s', scenario_name)
        return jsonify({'success': False, 'error': 'Failed to clear CORE VM selection'}), 500
    return jsonify({'success': True, 'scenario_name': scenario_name, 'scenario_index': scenario_index})


@app.route('/api/hitl/config/clear', methods=['POST'])
def api_hitl_config_clear():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    scenario_name = str(payload.get('scenario_name') or payload.get('scenario') or '').strip()
    scenario_index = payload.get('scenario_index')
    if not scenario_name:
        return jsonify({'success': False, 'error': 'scenario_name is required'}), 400
    try:
        _clear_hitl_config_in_scenario_catalog(scenario_name, clear_config=True)
    except Exception:
        app.logger.exception('[hitl] failed clearing HITL config for %s', scenario_name)
        return jsonify({'success': False, 'error': 'Failed to clear HITL config'}), 500
    return jsonify({'success': True, 'scenario_name': scenario_name, 'scenario_index': scenario_index})


@app.route('/api/proxmox/vms', methods=['POST'])
def api_proxmox_vms():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403
    payload = request.get_json(silent=True) or {}
    secret_id_raw = payload.get('secret_id')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    if not secret_id:
        return jsonify({'success': False, 'error': 'secret_id is required'}), 400
    try:
        inventory = _enumerate_proxmox_vms(secret_id)
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 502
    except Exception as exc:  # pragma: no cover - unexpected failure path
        app.logger.exception('[proxmox] unexpected error fetching VM inventory: %s', exc)
        return jsonify({'success': False, 'error': 'Failed to fetch Proxmox VM inventory'}), 500
    return jsonify({'success': True, 'inventory': inventory})


@app.route('/api/hitl/apply_bridge', methods=['POST'])
def api_hitl_apply_bridge():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403

    payload = request.get_json(silent=True) or {}
    bridge_raw = payload.get('bridge_name') or payload.get('internal_bridge') or payload.get('bridge')
    if bridge_raw in (None, ''):
        return jsonify({'success': False, 'error': 'Bridge name is required'}), 400
    try:
        bridge_name = _normalize_internal_bridge_name(bridge_raw)
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400

    scenario_name = str(payload.get('scenario_name') or payload.get('scenario') or '').strip()
    scenario_index_raw = payload.get('scenario_index')
    try:
        scenario_index = int(scenario_index_raw)
    except Exception:
        scenario_index = None

    hitl_payload = payload.get('hitl') or payload.get('scenario_hitl') or payload.get('hitl_config')
    if not isinstance(hitl_payload, dict):
        return jsonify({'success': False, 'error': 'HITL configuration is required to apply bridge changes'}), 400

    prox_state = hitl_payload.get('proxmox') or {}
    secret_id_raw = prox_state.get('secret_id') or prox_state.get('secretId') or prox_state.get('identifier')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    if not secret_id:
        return jsonify({'success': False, 'error': 'Validate and store Proxmox credentials before applying bridge changes'}), 400

    core_state = hitl_payload.get('core') or {}
    vm_key_raw = core_state.get('vm_key') or core_state.get('vmKey')
    vm_key = str(vm_key_raw or '').strip()
    if not vm_key:
        return jsonify({'success': False, 'error': 'Select a CORE VM before applying bridge changes'}), 400
    try:
        core_node, core_vmid = _parse_proxmox_vm_key(vm_key)
    except ValueError as exc:
        return jsonify({'success': False, 'error': f'CORE VM selection invalid: {exc}'}), 400
    core_vm_name = str(core_state.get('vm_name') or core_state.get('vmName') or '').strip()

    interfaces_payload = hitl_payload.get('interfaces')
    if not isinstance(interfaces_payload, list) or not interfaces_payload:
        return jsonify({'success': False, 'error': 'Add at least one HITL interface before applying bridge changes'}), 400

    validation_errors: List[str] = []
    assignments: List[Dict[str, Any]] = []
    for idx, iface in enumerate(interfaces_payload):
        if not isinstance(iface, dict):
            continue
        iface_name = str(iface.get('name') or f'Interface {idx + 1}').strip() or f'Interface {idx + 1}'
        prox_target = iface.get('proxmox_target')
        if not isinstance(prox_target, dict):
            validation_errors.append(f'{iface_name}: Map the interface to a CORE VM adapter in Step 3.')
            continue
        external = iface.get('external_vm')
        attachment = _normalize_hitl_attachment(iface.get('attachment'))
        if attachment != 'proxmox_vm':
            if isinstance(external, dict):
                attachment = 'proxmox_vm'
            else:
                continue
        core_iface_id = str(prox_target.get('interface_id') or '').strip()
        if not core_iface_id:
            validation_errors.append(f'{iface_name}: Select the CORE VM interface to use for HITL connectivity.')
            continue
        target_node = str(prox_target.get('node') or '').strip() or core_node
        try:
            target_vmid = int(prox_target.get('vmid') or core_vmid)
        except Exception:
            validation_errors.append(f'{iface_name}: CORE VM identifier is invalid.')
            continue
        if target_node != core_node or target_vmid != core_vmid:
            validation_errors.append(f'{iface_name}: CORE interface must belong to the selected CORE VM on node {core_node}.')
            continue
        if not isinstance(external, dict):
            validation_errors.append(f'{iface_name}: Select an external Proxmox VM in Step 4.')
            continue
        external_vm_key = str(external.get('vm_key') or external.get('vmKey') or '').strip()
        if not external_vm_key:
            validation_errors.append(f'{iface_name}: Select an external Proxmox VM in Step 4.')
            continue
        try:
            external_node, external_vmid = _parse_proxmox_vm_key(external_vm_key)
        except ValueError as exc:
            validation_errors.append(f'{iface_name}: External VM invalid: {exc}')
            continue
        if external_node != core_node:
            validation_errors.append(f'{iface_name}: External VM must be hosted on node {core_node}.')
            continue
        external_iface_id = str(external.get('interface_id') or '').strip()
        if not external_iface_id:
            validation_errors.append(f'{iface_name}: Select the external VM interface to connect through the bridge.')
            continue
        assignments.append({
            'name': iface_name,
            'core': {
                'node': core_node,
                'vmid': core_vmid,
                'vm_name': core_vm_name,
                'interface_id': core_iface_id,
            },
            'external': {
                'node': external_node,
                'vmid': external_vmid,
                'vm_name': str(external.get('vm_name') or '').strip(),
                'interface_id': external_iface_id,
            },
        })

    if validation_errors:
        message = ' ; '.join(validation_errors[:3])
        if len(validation_errors) > 3:
            message += f' (and {len(validation_errors) - 3} more issue(s))'
        return jsonify({'success': False, 'error': message, 'details': validation_errors}), 400
    if not assignments:
        return jsonify({'success': False, 'error': 'No eligible HITL interface mappings found. Map at least one interface to a CORE VM and external VM before applying.'}), 400

    try:
        client, _record = _connect_proxmox_from_secret(secret_id)
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 502
    except Exception as exc:  # pragma: no cover - defensive logging
        app.logger.exception('[hitl] unexpected failure connecting to Proxmox: %s', exc)
        return jsonify({'success': False, 'error': 'Unexpected error connecting to Proxmox'}), 500

    owner_raw = payload.get('bridge_owner') or core_state.get('internal_bridge_owner') or payload.get('username')
    bridge_owner = str(owner_raw or '').strip()
    comment_bits = ['core-topo-gen HITL bridge']
    if scenario_name:
        comment_bits.append(f'scenario={scenario_name}')
    if bridge_owner:
        comment_bits.append(f'owner={bridge_owner}')
    bridge_comment = ' '.join(comment_bits)
    try:
        bridge_meta = _ensure_proxmox_bridge(client, core_node, bridge_name, comment=bridge_comment)
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 502

    vm_config_cache: Dict[tuple[str, int], Dict[str, Any]] = {}

    def _get_vm_config(node: str, vmid: int) -> Dict[str, Any]:
        key = (node, vmid)
        if key not in vm_config_cache:
            try:
                vm_config_cache[key] = client.nodes(node).qemu(vmid).config.get()
            except Exception as exc:
                raise RuntimeError(f'Failed to fetch configuration for VM {vmid} on node {node}: {exc}') from exc
        return vm_config_cache[key]

    vm_updates: Dict[tuple[str, int], Dict[str, str]] = {}
    change_details: List[Dict[str, Any]] = []
    try:
        for assignment in assignments:
            for role in ('core', 'external'):
                vm_info = assignment[role]
                node = vm_info['node']
                vmid = vm_info['vmid']
                interface_id = vm_info['interface_id']
                vm_name = vm_info.get('vm_name') or ''
                config = _get_vm_config(node, vmid)
                net_config = config.get(interface_id)
                if not isinstance(net_config, str) or not net_config.strip():
                    raise ValueError(f'{role.title()} VM {vm_name or vmid} is missing Proxmox interface {interface_id}.')
                new_config, changed, previous_bridge = _rewrite_bridge_in_net_config(net_config, bridge_name)
                change_details.append({
                    'role': role,
                    'scenario_interface': assignment['name'],
                    'node': node,
                    'vmid': vmid,
                    'vm_name': vm_name,
                    'interface_id': interface_id,
                    'previous_bridge': previous_bridge,
                    'new_bridge': bridge_name,
                    'changed': changed,
                })
                if changed:
                    vm_updates.setdefault((node, vmid), {})[interface_id] = new_config
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 502

    updated_vms: List[Dict[str, Any]] = []
    for (node, vmid), updates in vm_updates.items():
        if not updates:
            continue
        try:
            client.nodes(node).qemu(vmid).config.post(**updates)
        except Exception as exc:  # pragma: no cover - defensive logging
            app.logger.exception('[hitl] failed updating Proxmox VM config: %s', exc)
            return jsonify({'success': False, 'error': f'Failed to update Proxmox VM {vmid} on node {node}: {exc}'}), 502
        updated_vms.append({
            'node': node,
            'vmid': vmid,
            'interfaces': list(updates.keys()),
        })

    changed_interfaces = sum(1 for change in change_details if change.get('changed'))
    unchanged_interfaces = len(change_details) - changed_interfaces
    assignment_count = len(assignments)

    parts = [f'Bridge {bridge_name} applied to {assignment_count} HITL link{"s" if assignment_count != 1 else ""}.']
    if changed_interfaces:
        parts.append(f'{changed_interfaces} Proxmox interface{"s" if changed_interfaces != 1 else ""} updated.')
    if unchanged_interfaces:
        parts.append(f'{unchanged_interfaces} already on the requested bridge.')
    message = ' '.join(parts)

    warnings: List[str] = []
    if bridge_meta.get('created') and not bridge_meta.get('reload_ok'):
        warnings.append('Bridge created but Proxmox did not confirm network reload; apply pending changes manually if required.')

    response: Dict[str, Any] = {
        'success': True,
        'message': message,
        'bridge_name': bridge_name,
        'bridge_created': bool(bridge_meta.get('created')),
        'bridge_already_exists': bool(bridge_meta.get('already_exists')),
        'bridge_reload_ok': bool(bridge_meta.get('reload_ok')),
        'bridge_reload_error': bridge_meta.get('reload_error'),
        'scenario_index': scenario_index,
        'scenario_name': scenario_name,
        'assignments': assignment_count,
        'changed_interfaces': changed_interfaces,
        'unchanged_interfaces': unchanged_interfaces,
        'changes': change_details,
        'updated_vms': updated_vms,
        'proxmox_node': core_node,
    }
    if warnings:
        response['warnings'] = warnings
    if bridge_owner:
        response['bridge_owner'] = bridge_owner

    # Persist the applied HITL config for builder view (non-secret fields only).
    try:
        if scenario_name:
            hitl_to_store = dict(hitl_payload)
            core_store = hitl_to_store.get('core') if isinstance(hitl_to_store.get('core'), dict) else {}
            core_store = dict(core_store)
            core_store['internal_bridge'] = bridge_name
            if bridge_owner:
                core_store['internal_bridge_owner'] = bridge_owner
            hitl_to_store['core'] = core_store
            _merge_hitl_config_into_scenario_catalog(scenario_name, hitl_to_store)
    except Exception:
        pass

    app.logger.info(
        '[hitl] applied internal bridge %s on node %s (%d assignment(s), %d interface change(s))',
        bridge_name,
        core_node,
        assignment_count,
        changed_interfaces,
    )
    return jsonify(response)


@app.route('/api/hitl/validate_bridge', methods=['POST'])
def api_hitl_validate_bridge():
    """Validate HITL bridge configuration without applying any changes."""

    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'success': False, 'error': 'Admin privileges required'}), 403

    payload = request.get_json(silent=True) or {}
    bridge_raw = payload.get('bridge_name') or payload.get('internal_bridge') or payload.get('bridge')
    if bridge_raw in (None, ''):
        return jsonify({'success': False, 'error': 'Bridge name is required'}), 400
    try:
        bridge_name = _normalize_internal_bridge_name(bridge_raw)
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400

    scenario_name = str(payload.get('scenario_name') or payload.get('scenario') or '').strip()
    scenario_index_raw = payload.get('scenario_index')
    try:
        scenario_index = int(scenario_index_raw)
    except Exception:
        scenario_index = None

    hitl_payload = payload.get('hitl') or payload.get('scenario_hitl') or payload.get('hitl_config')
    if not isinstance(hitl_payload, dict):
        return jsonify({'success': False, 'error': 'HITL configuration is required to validate bridge settings'}), 400

    prox_state = hitl_payload.get('proxmox') or {}
    secret_id_raw = prox_state.get('secret_id') or prox_state.get('secretId') or prox_state.get('identifier')
    secret_id = secret_id_raw.strip() if isinstance(secret_id_raw, str) else ''
    if not secret_id:
        return jsonify({'success': False, 'error': 'Validate and store Proxmox credentials before verifying HITL bridge settings'}), 400

    core_state = hitl_payload.get('core') or {}
    vm_key_raw = core_state.get('vm_key') or core_state.get('vmKey')
    vm_key = str(vm_key_raw or '').strip()
    if not vm_key:
        return jsonify({'success': False, 'error': 'Select a CORE VM before verifying HITL bridge settings'}), 400
    try:
        core_node, core_vmid = _parse_proxmox_vm_key(vm_key)
    except ValueError as exc:
        return jsonify({'success': False, 'error': f'CORE VM selection invalid: {exc}'}), 400
    core_vm_name = str(core_state.get('vm_name') or core_state.get('vmName') or '').strip()

    interfaces_payload = hitl_payload.get('interfaces')
    if not isinstance(interfaces_payload, list) or not interfaces_payload:
        return jsonify({'success': False, 'error': 'Add at least one HITL interface before verifying bridge settings'}), 400

    validation_errors: List[str] = []
    assignments: List[Dict[str, Any]] = []
    for idx, iface in enumerate(interfaces_payload):
        if not isinstance(iface, dict):
            continue
        iface_name = str(iface.get('name') or f'Interface {idx + 1}').strip() or f'Interface {idx + 1}'
        prox_target = iface.get('proxmox_target')
        if not isinstance(prox_target, dict):
            validation_errors.append(f'{iface_name}: Map the interface to a CORE VM adapter in Step 3.')
            continue
        external = iface.get('external_vm')
        attachment = _normalize_hitl_attachment(iface.get('attachment'))
        if attachment != 'proxmox_vm':
            if isinstance(external, dict):
                attachment = 'proxmox_vm'
            else:
                continue
        core_iface_id = str(prox_target.get('interface_id') or '').strip()
        if not core_iface_id:
            validation_errors.append(f'{iface_name}: Select the CORE VM interface to use for HITL connectivity.')
            continue
        target_node = str(prox_target.get('node') or '').strip() or core_node
        try:
            target_vmid = int(prox_target.get('vmid') or core_vmid)
        except Exception:
            validation_errors.append(f'{iface_name}: CORE VM identifier is invalid.')
            continue
        if target_node != core_node or target_vmid != core_vmid:
            validation_errors.append(f'{iface_name}: CORE interface must belong to the selected CORE VM on node {core_node}.')
            continue
        if not isinstance(external, dict):
            validation_errors.append(f'{iface_name}: Select an external Proxmox VM in Step 4.')
            continue
        external_vm_key = str(external.get('vm_key') or external.get('vmKey') or '').strip()
        if not external_vm_key:
            validation_errors.append(f'{iface_name}: Select an external Proxmox VM in Step 4.')
            continue
        try:
            external_node, external_vmid = _parse_proxmox_vm_key(external_vm_key)
        except ValueError as exc:
            validation_errors.append(f'{iface_name}: External VM invalid: {exc}')
            continue
        if external_node != core_node:
            validation_errors.append(f'{iface_name}: External VM must be hosted on node {core_node}.')
            continue
        external_iface_id = str(external.get('interface_id') or '').strip()
        if not external_iface_id:
            validation_errors.append(f'{iface_name}: Select the external VM interface to connect through the bridge.')
            continue
        assignments.append({
            'name': iface_name,
            'core': {
                'node': core_node,
                'vmid': core_vmid,
                'vm_name': core_vm_name,
                'interface_id': core_iface_id,
            },
            'external': {
                'node': external_node,
                'vmid': external_vmid,
                'vm_name': str(external.get('vm_name') or '').strip(),
                'interface_id': external_iface_id,
            },
        })

    if validation_errors:
        message = ' ; '.join(validation_errors[:3])
        if len(validation_errors) > 3:
            message += f' (and {len(validation_errors) - 3} more issue(s))'
        return jsonify({'success': False, 'error': message, 'details': validation_errors}), 400
    if not assignments:
        return jsonify({'success': False, 'error': 'No eligible HITL interface mappings found. Map at least one interface to a CORE VM and external VM before verifying.'}), 400

    try:
        client, _record = _connect_proxmox_from_secret(secret_id)
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 502
    except Exception as exc:  # pragma: no cover
        app.logger.exception('[hitl] unexpected failure connecting to Proxmox: %s', exc)
        return jsonify({'success': False, 'error': 'Unexpected error connecting to Proxmox'}), 500

    try:
        bridge_meta = _ensure_proxmox_bridge(client, core_node, bridge_name)
    except RuntimeError as exc:
        msg = str(exc)
        status = 400 if 'not found on node' in msg.lower() else 502
        return jsonify({'success': False, 'error': msg}), status

    vm_config_cache: Dict[tuple[str, int], Dict[str, Any]] = {}

    def _get_vm_config(node: str, vmid: int) -> Dict[str, Any]:
        key = (node, vmid)
        if key not in vm_config_cache:
            try:
                vm_config_cache[key] = client.nodes(node).qemu(vmid).config.get()
            except Exception as exc:
                raise RuntimeError(f'Failed to fetch configuration for VM {vmid} on node {node}: {exc}') from exc
        return vm_config_cache[key]

    change_details: List[Dict[str, Any]] = []
    try:
        for assignment in assignments:
            for role in ('core', 'external'):
                vm_info = assignment[role]
                node = vm_info['node']
                vmid = vm_info['vmid']
                interface_id = vm_info['interface_id']
                vm_name = vm_info.get('vm_name') or ''
                config = _get_vm_config(node, vmid)
                net_config = config.get(interface_id)
                if not isinstance(net_config, str) or not net_config.strip():
                    raise ValueError(f'{role.title()} VM {vm_name or vmid} is missing Proxmox interface {interface_id}.')
                _new_config, changed, previous_bridge = _rewrite_bridge_in_net_config(net_config, bridge_name)
                change_details.append({
                    'role': role,
                    'scenario_interface': assignment['name'],
                    'node': node,
                    'vmid': vmid,
                    'vm_name': vm_name,
                    'interface_id': interface_id,
                    'previous_bridge': previous_bridge,
                    'new_bridge': bridge_name,
                    'changed': changed,
                })
    except ValueError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 400
    except RuntimeError as exc:
        return jsonify({'success': False, 'error': str(exc)}), 502

    changed_interfaces = sum(1 for change in change_details if change.get('changed'))
    unchanged_interfaces = len(change_details) - changed_interfaces
    assignment_count = len(assignments)

    parts = [f'Bridge {bridge_name} validation succeeded for {assignment_count} HITL link{"s" if assignment_count != 1 else ""}.']
    if changed_interfaces:
        parts.append(f'{changed_interfaces} Proxmox interface{"s" if changed_interfaces != 1 else ""} would be updated.')
    if unchanged_interfaces:
        parts.append(f'{unchanged_interfaces} already on the requested bridge.')
    message = ' '.join(parts)

    response: Dict[str, Any] = {
        'success': True,
        'message': message,
        'bridge_name': bridge_name,
        'bridge_meta': bridge_meta,
        'scenario_index': scenario_index,
        'scenario_name': scenario_name,
        'assignments': assignment_count,
        'changed_interfaces': changed_interfaces,
        'unchanged_interfaces': unchanged_interfaces,
        'changes': change_details,
        'proxmox_node': core_node,
    }

    # Persist verified HITL config for builder view (non-secret fields only).
    try:
        if scenario_name:
            hitl_to_store = dict(hitl_payload)
            core_store = hitl_to_store.get('core') if isinstance(hitl_to_store.get('core'), dict) else {}
            core_store = dict(core_store)
            core_store['internal_bridge'] = bridge_name
            hitl_to_store['core'] = core_store
            _merge_hitl_config_into_scenario_catalog(scenario_name, hitl_to_store)
    except Exception:
        pass
    return jsonify(response)

# ---------------- Docker (per-node) status & cleanup ----------------
def _compose_assignments_path() -> str:
    return os.path.join(_vuln_base_dir() or "/tmp/vulns", "compose_assignments.json")


def _load_compose_assignments() -> dict:
    p = _compose_assignments_path()
    try:
        if os.path.exists(p):
            with open(p, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        app.logger.debug("compose assignments read failed: %s", e)
    return {}


def _compose_file_for_node(node_name: str) -> str:
    base = _vuln_base_dir() or "/tmp/vulns"
    return os.path.join(base, f"docker-compose-{node_name}.yml")


def _docker_container_exists(name: str) -> tuple[bool, bool]:
    try:
        proc = subprocess.run(["docker", "ps", "-a", "--format", "{{.Names}}"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        if proc.returncode != 0:
            return (False, False)
        names = set(ln.strip() for ln in (proc.stdout or '').splitlines() if ln.strip())
        if name not in names:
            return (False, False)
        proc2 = subprocess.run(["docker", "inspect", "-f", "{{.State.Running}}", name], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        running = (proc2.returncode == 0 and (proc2.stdout or '').strip().lower() == 'true')
        return (True, running)
    except Exception:
        return (False, False)


def _images_pulled_for_compose_safe(yml_path: str) -> bool:
    try:
        from core_topo_gen.utils.vuln_process import _images_pulled_for_compose as _pulled  # type: ignore
        return bool(_pulled(yml_path))
    except Exception as e:
        try: app.logger.debug("pull check failed for %s: %s", yml_path, e)
        except Exception: pass
        return False


def _remote_docker_status_script(sudo_password: str | None = None) -> str:
    """Remote script to read compose assignments + docker state on the CORE VM.

    This intentionally runs on the remote CORE host (via SSH) so the Docker Compose
    card reflects remote state, not the local Flask machine.
    """

    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    return (
        """
import json, os, time, subprocess

SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__


def _run_docker(cmd, timeout=20, capture=True):
    # Run docker on the CORE VM via sudo.
    # - Try non-interactive sudo first (fast fail if password is required).
    # - If a sudo password is available, retry with sudo -S.
    stdout = subprocess.PIPE if capture else subprocess.DEVNULL
    stderr = subprocess.STDOUT if capture else subprocess.DEVNULL
    try:
        p = subprocess.run(['sudo', '-n', 'docker'] + list(cmd), stdout=stdout, stderr=stderr, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            # -k forces sudo to prompt for password (so -S matters even if a cached ticket exists).
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', '', 'docker'] + list(cmd),
                input=str(SUDO_PASSWORD) + "\\n",
                stdout=stdout,
                stderr=stderr,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        class _P:
            returncode = 1
            stdout = str(e)
        return _P()


def _first_existing(path_candidates):
    for p in path_candidates:
        if not p:
            continue
        try:
            if os.path.exists(p):
                return p
        except Exception:
            continue
    return None


def _read_json(path):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def _docker_names():
    try:
        p = _run_docker(['ps', '-a', '--format', '{{.Names}}'], timeout=15, capture=True)
        if p.returncode != 0:
            out = (p.stdout or '').strip()
            if out and 'permission denied' in out.lower():
                out = out + "\\nHint: add the SSH user to the 'docker' group or allow passwordless sudo for docker."
            return set(), out
        return set(ln.strip() for ln in (p.stdout or '').splitlines() if ln.strip()), ''
    except Exception as e:
        return set(), str(e)


def _container_running(name):
    try:
        p = _run_docker(['inspect', '-f', '{{.State.Running}}', name], timeout=10, capture=True)
        return (p.returncode == 0 and (p.stdout or '').strip().lower() == 'true')
    except Exception:
        return False


def _images_pulled_for_compose(yml_path):
    # Best-effort: list images referenced by the compose file and confirm they exist locally.
    try:
        p = _run_docker(['compose', '-f', yml_path, 'config', '--images'], timeout=20, capture=True)
        if p.returncode != 0:
            return False
        images = [ln.strip() for ln in (p.stdout or '').splitlines() if ln.strip()]
        if not images:
            return False
        for img in images:
            chk = _run_docker(['image', 'inspect', img], timeout=20, capture=False)
            if chk.returncode != 0:
                return False
        return True
    except Exception:
        return False


def main():
    base = os.environ.get('CORE_REMOTE_BASE_DIR', '/tmp/core-topo-gen')
    candidates = [
        os.path.join(base, 'vulns', 'compose_assignments.json'),
        os.path.join(base, 'outputs', 'vulns', 'compose_assignments.json'),
        os.path.join(base, 'compose_assignments.json'),
        '/tmp/vulns/compose_assignments.json',
    ]
    assignments_path = _first_existing(candidates)
    if not assignments_path:
        print(json.dumps({'items': [], 'timestamp': int(time.time()), 'error': 'compose_assignments.json not found on CORE VM'}))
        return

    try:
        data = _read_json(assignments_path)
    except Exception as e:
        print(json.dumps({'items': [], 'timestamp': int(time.time()), 'error': f'failed reading assignments: {e}'}))
        return

    assignments = data.get('assignments', {}) if isinstance(data, dict) else {}
    if not isinstance(assignments, dict):
        assignments = {}

    assign_dir = os.path.dirname(assignments_path)
    names, docker_err = _docker_names()
    items = []
    for node_name in sorted(assignments.keys()):
        yml = os.path.join(assign_dir, f'docker-compose-{node_name}.yml')
        exists = False
        try:
            exists = os.path.exists(yml)
        except Exception:
            exists = False
        pulled = _images_pulled_for_compose(yml) if exists else False
        c_exists = node_name in names
        running = _container_running(node_name) if c_exists else False
        items.append({
            'name': node_name,
            'compose': yml,
            'exists': bool(exists),
            'pulled': bool(pulled),
            'container_exists': bool(c_exists),
            'running': bool(running),
        })

    payload = {'items': items, 'timestamp': int(time.time())}
    if docker_err:
        payload['error'] = docker_err
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    ).replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal)


def _remote_copy_flow_artifacts_into_containers_script(sudo_password: str | None = None) -> str:
    """Remote script to copy Flow generator artifacts into vuln containers.

    This supports CORETG_FLOW_ARTIFACTS_MODE=copy where compose files contain
    labels describing src/dest paths instead of bind mounts.
    """
    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    return (
        r"""
import json, os, re, subprocess, time

SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__


def _run_docker(cmd, timeout=30, capture=True):
    stdout = subprocess.PIPE if capture else subprocess.DEVNULL
    stderr = subprocess.STDOUT if capture else subprocess.DEVNULL
    try:
        p = subprocess.run(['sudo', '-n', 'docker'] + list(cmd), stdout=stdout, stderr=stderr, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', '', 'docker'] + list(cmd),
                input=str(SUDO_PASSWORD) + "\n",
                stdout=stdout,
                stderr=stderr,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        class _P:
            returncode = 1
            stdout = str(e)
        return _P()


def _first_existing(path_candidates):
    for p in path_candidates:
        if not p:
            continue
        try:
            if os.path.exists(p):
                return p
        except Exception:
            continue
    return None


def _read_json(path):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def _docker_names():
    p = _run_docker(['ps', '-a', '--format', '{{.Names}}'], timeout=20, capture=True)
    if getattr(p, 'returncode', 1) != 0:
        return set(), (getattr(p, 'stdout', '') or '').strip()
    return set(ln.strip() for ln in (p.stdout or '').splitlines() if ln.strip()), ''


def _parse_flow_labels(txt: str):
    # YAML-ish key:value, possibly quoted.
    src = None
    dest = None
    m1 = re.search(r"coretg\.flow_artifacts\.src\s*:\s*(['\"]?)([^\n'\"]+)\1", txt)
    if m1:
        src = (m1.group(2) or '').strip()
    m2 = re.search(r"coretg\.flow_artifacts\.dest\s*:\s*(['\"]?)([^\n'\"]+)\1", txt)
    if m2:
        dest = (m2.group(2) or '').strip()
    return src, dest


def _parse_flow_bind_mount(txt: str):
    # Fallback: parse a volumes entry like /tmp/vulns/flag_generators_runs/...:/flow_artifacts:ro
    m = re.search(r"(/tmp/vulns/(?:flag_generators_runs|flag_node_generators_runs)/[^:\s\"\']+)\s*:\s*([^:\s\"\']+)", txt)
    if not m:
        return None, None
    return (m.group(1) or '').strip(), (m.group(2) or '').strip()


def _extract_label_value(txt: str, key: str):
    patterns = [
        rf"{re.escape(key)}\s*:\s*(['\"])(.*?)\1",
        rf"{re.escape(key)}\s*=\s*(['\"])(.*?)\1",
        rf"{re.escape(key)}\s*:\s*([^\n]+)",
        rf"{re.escape(key)}\s*=\s*([^\n]+)",
    ]
    for pat in patterns:
        m = re.search(pat, txt, flags=re.S)
        if not m:
            continue
        val = m.group(m.lastindex or 1)
        if val is None:
            continue
        return str(val).strip().strip('"').strip("'")
    return None


def _parse_inject_labels(txt: str):
    source_dir = _extract_label_value(txt, 'coretg.inject.source_dir') or None
    raw_map = _extract_label_value(txt, 'coretg.inject.map') or None
    items = []
    if raw_map:
        try:
            parsed = json.loads(raw_map)
            if isinstance(parsed, list):
                items = [x for x in parsed if isinstance(x, dict)]
        except Exception:
            items = []
    return source_dir, items


def _compose_container_ids(project: str, yml: str):
    p = _run_docker(['compose', '-p', project, '-f', yml, 'ps', '-q'], timeout=25, capture=True)
    if getattr(p, 'returncode', 1) != 0:
        return []
    return [ln.strip() for ln in (p.stdout or '').splitlines() if ln.strip()]


def main():
    base = os.environ.get('CORE_REMOTE_BASE_DIR', '/tmp/core-topo-gen')
    candidates = [
        os.path.join(base, 'vulns', 'compose_assignments.json'),
        os.path.join(base, 'outputs', 'vulns', 'compose_assignments.json'),
        os.path.join(base, 'compose_assignments.json'),
        '/tmp/vulns/compose_assignments.json',
    ]
    assignments_path = _first_existing(candidates)
    if not assignments_path:
        print(json.dumps({'ok': False, 'items': [], 'error': 'compose_assignments.json not found on CORE VM'}))
        return

    try:
        data = _read_json(assignments_path)
    except Exception as e:
        print(json.dumps({'ok': False, 'items': [], 'error': f'failed reading assignments: {e}'}))
        return

    assignments = data.get('assignments', {}) if isinstance(data, dict) else {}
    if not isinstance(assignments, dict):
        assignments = {}

    assign_dir = os.path.dirname(assignments_path)
    items = []
    for node_name in sorted(assignments.keys()):
        yml = os.path.join(assign_dir, f'docker-compose-{node_name}.yml')
        if not os.path.exists(yml):
            items.append({'node': node_name, 'compose': yml, 'ok': False, 'error': 'compose file missing'})
            continue
        try:
            txt = open(yml, 'r', encoding='utf-8', errors='ignore').read()
        except Exception:
            txt = ''

        src, dest = _parse_flow_labels(txt)
        if not src or not dest:
            src2, dest2 = _parse_flow_bind_mount(txt)
            src = src or src2
            dest = dest or dest2

        inject_source, inject_items = _parse_inject_labels(txt)

        if not src or not dest:
            items.append({'node': node_name, 'compose': yml, 'src': src, 'dest': dest, 'ok': False, 'error': 'flow artifacts src/dest not found'})
            continue

        if not os.path.isdir(src):
            items.append({'node': node_name, 'compose': yml, 'src': src, 'dest': dest, 'ok': False, 'error': 'src dir missing'})
            continue

        # Find container targets: prefer container named node_name; else use compose project containers.
        targets = []
        last_err = ''
        for _ in range(6):
            names, err = _docker_names()
            last_err = err or last_err
            if node_name in names:
                targets = [node_name]
                break
            project = f"{node_name}conf" if node_name else 'coretg'
            ids = _compose_container_ids(project, yml)
            if ids:
                targets = ids
                break
            time.sleep(2)

        if not targets:
            items.append({'node': node_name, 'compose': yml, 'src': src, 'dest': dest, 'ok': False, 'error': 'container not found', 'docker_error': last_err})
            continue

        copied_any = False
        errs = []
        commands = []
        command_outputs = []

        # Prefer explicit inject mapping when available.
        if inject_items:
            source_dir = inject_source or src
            for t in targets:
                for entry in inject_items:
                    rel = str(entry.get('src') or '').strip()
                    dest_dir = str(entry.get('dest') or dest or '').strip()
                    if not rel or not dest_dir or not dest_dir.startswith('/'):
                        continue
                    if os.path.isabs(rel):
                        src_path = rel
                        rel_path = os.path.basename(rel)
                    else:
                        src_path = os.path.join(source_dir, rel.lstrip('/')) if source_dir else rel
                        rel_path = rel
                    if not src_path or not rel_path:
                        continue
                    dest_path = dest_dir.rstrip('/') + '/' + rel_path.lstrip('/')
                    rel_dir = os.path.dirname(rel_path)
                    if rel_dir:
                        mkdir_cmd = f"mkdir -p {dest_dir.rstrip('/')}/{rel_dir}"
                    else:
                        mkdir_cmd = f"mkdir -p {dest_dir.rstrip('/')}"
                    commands.append(f"docker exec {t} sh -lc {mkdir_cmd}")
                    _run_docker(['exec', t, 'sh', '-lc', mkdir_cmd], timeout=30, capture=True)
                    commands.append(f"docker cp {src_path} {t}:{dest_path}")
                    p = _run_docker(['cp', src_path, f"{t}:{dest_path}"], timeout=60, capture=True)
                    out = (getattr(p, 'stdout', '') or '').strip()
                    rc = int(getattr(p, 'returncode', 1) or 0)
                    command_outputs.append({'target': t, 'rc': rc, 'out': out, 'dest': dest_path})
                    if getattr(p, 'returncode', 1) == 0:
                        copied_any = True
                    else:
                        errs.append(out)

        # Fallback: copy entire artifacts directory into flow_artifacts mount path.
        if not inject_items:
            for t in targets:
                # Copy directory contents into dest.
                cp_src = src.rstrip('/') + '/.'
                cp_dest = f"{t}:{dest}"
                commands.append(f"docker cp {cp_src} {cp_dest}")
                p = _run_docker(['cp', cp_src, cp_dest], timeout=60, capture=True)
                out = (getattr(p, 'stdout', '') or '').strip()
                rc = int(getattr(p, 'returncode', 1) or 0)
                command_outputs.append({'target': t, 'rc': rc, 'out': out})
                if getattr(p, 'returncode', 1) == 0:
                    copied_any = True
                else:
                    errs.append(out)
        items.append({'node': node_name, 'compose': yml, 'src': src, 'dest': dest, 'targets': targets, 'ok': bool(copied_any), 'errors': errs, 'commands': commands, 'command_outputs': command_outputs})

    print(json.dumps({'ok': True, 'items': items, 'timestamp': int(time.time()), 'assignments_count': len(assignments), 'assignments_keys': sorted(assignments.keys())}))


if __name__ == '__main__':
    main()
"""
    ).replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal)


def _sync_local_vulns_to_remote(
    core_cfg: Dict[str, Any],
    *,
    local_dir: str = "/tmp/vulns",
    remote_dir: str = "/tmp/vulns",
    flow_plan_path: str | None = None,
    logger: Optional[logging.Logger] = None,
) -> bool:
    """Copy vulnerability compose artifacts created locally to the remote CORE host.

    The webapp runs the CLI locally (with a tunneled gRPC connection), so the CLI
    writes compose artifacts under local /tmp. The Docker Compose card and CORE
    DockerComposeService run on the CORE VM and expect the same artifacts on the
    remote filesystem.

    Returns True if any files were uploaded.
    """

    log = logger or getattr(app, 'logger', logging.getLogger(__name__))

    if not os.path.isdir(local_dir):
        return False

    assignments_path = os.path.join(local_dir, "compose_assignments.json")
    node_names: List[str] = []
    if os.path.exists(assignments_path):
        try:
            with open(assignments_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            assignments = data.get('assignments', {}) if isinstance(data, dict) else {}
            if isinstance(assignments, dict):
                node_names = [str(k) for k in assignments.keys()]
        except Exception:
            node_names = []

    local_files: List[str] = []
    if os.path.exists(assignments_path):
        local_files.append(assignments_path)

    compose_glob = []
    try:
        import glob as _glob
        compose_glob = _glob.glob(os.path.join(local_dir, 'docker-compose-*.yml'))
    except Exception:
        compose_glob = []

    if node_names:
        for nm in sorted(set(node_names)):
            yml = os.path.join(local_dir, f"docker-compose-{nm}.yml")
            if os.path.exists(yml):
                local_files.append(yml)
    else:
        for yml in sorted(set(compose_glob)):
            if os.path.exists(yml):
                local_files.append(yml)

    # Flow flag artifacts: bind-mounted into docker-compose services (e.g., to /flow_artifacts).
    # These directories live under /tmp/vulns/* on the webapp host and must be present on the
    # CORE VM too; otherwise the container sees an empty mount.
    local_dirs: List[str] = []
    try:
        import re

        for yml in [p for p in local_files if p.endswith('.yml') or p.endswith('.yaml')]:
            try:
                txt = ''
                with open(yml, 'r', encoding='utf-8', errors='ignore') as f:
                    txt = f.read()
                # Match the source side of a bind mount that points into /tmp/vulns.
                # Example: /tmp/vulns/flag_generators_runs/flow-...:/flow_artifacts:ro
                for m in re.findall(r'(/tmp/vulns/(?:flag_generators_runs|flag_node_generators_runs)/[^:\s\"\']+)', txt):
                    pth = str(m).strip().strip('"').strip("'")
                    if pth and os.path.isdir(pth):
                        local_dirs.append(pth)
            except Exception:
                continue
    except Exception:
        local_dirs = []

    # Fallback: include any Flow artifact dirs from the preview plan / FlowState (XML).
    if flow_plan_path:
        try:
            for pth in _extract_flow_artifact_dirs_from_plan(flow_plan_path, prefer_mount_dir=False):
                if pth and os.path.isdir(pth):
                    local_dirs.append(pth)
        except Exception:
            pass

    # Wrapper build contexts are required on the CORE VM when compose files reference /tmp/vulns/docker-wrap-*.
    wrapper_dirs: List[str] = []
    try:
        for entry in os.scandir(local_dir):
            if entry.is_dir() and entry.name.startswith('docker-wrap-'):
                wrapper_dirs.append(entry.path)
    except Exception:
        wrapper_dirs = []

    if not local_files and not wrapper_dirs and not local_dirs:
        return False

    client = None
    sftp = None
    uploaded = 0
    try:
        client = _open_ssh_client(core_cfg)
        sftp = client.open_sftp()
        remote_dir_resolved = _remote_expand_path(sftp, remote_dir)
        _remote_mkdirs(client, remote_dir_resolved)
        made_dirs: set[str] = set()
        for lp in local_files:
            rp = _remote_path_join(remote_dir_resolved, os.path.basename(lp))
            try:
                sftp.put(lp, rp)
                uploaded += 1
            except Exception:
                log.exception("[sync] Failed uploading %s -> %s", lp, rp)

        # Upload wrapper build contexts, preserving relative paths.
        for d in sorted(set(wrapper_dirs)):
            try:
                rel = os.path.relpath(d, local_dir)
            except Exception:
                rel = None
            if not rel or rel.startswith('..'):
                continue
            remote_d = _remote_path_join(remote_dir_resolved, rel)
            try:
                if remote_d not in made_dirs:
                    _remote_mkdirs(client, remote_d)
                    made_dirs.add(remote_d)
            except Exception:
                pass
            for root, dirs, files in os.walk(d):
                for dn in dirs:
                    lp_dir = os.path.join(root, dn)
                    try:
                        rel_dir = os.path.relpath(lp_dir, local_dir)
                    except Exception:
                        continue
                    if not rel_dir or rel_dir.startswith('..'):
                        continue
                    rp_dir = _remote_path_join(remote_dir_resolved, rel_dir)
                    if rp_dir in made_dirs:
                        continue
                    try:
                        _remote_mkdirs(client, rp_dir)
                        made_dirs.add(rp_dir)
                    except Exception:
                        pass
                for fn in files:
                    lp_file = os.path.join(root, fn)
                    try:
                        rel_file = os.path.relpath(lp_file, local_dir)
                    except Exception:
                        continue
                    if not rel_file or rel_file.startswith('..'):
                        continue
                    rp_file = _remote_path_join(remote_dir_resolved, rel_file)
                    rp_parent = os.path.dirname(rp_file)
                    try:
                        if rp_parent and rp_parent not in made_dirs:
                            _remote_mkdirs(client, rp_parent)
                            made_dirs.add(rp_parent)
                    except Exception:
                        pass
                    try:
                        sftp.put(lp_file, rp_file)
                        uploaded += 1
                    except Exception:
                        log.exception("[sync] Failed uploading %s -> %s", lp_file, rp_file)

        # Upload any referenced Flow generator run directories, preserving relative paths.
        # Example local:  /tmp/vulns/flag_generators_runs/flow-.../outputs.json
        # Example remote: /tmp/vulns/flag_generators_runs/flow-.../outputs.json
        for d in sorted(set(local_dirs)):
            try:
                rel = os.path.relpath(d, local_dir)
            except Exception:
                rel = None
            if not rel or rel.startswith('..'):
                continue
            remote_d = _remote_path_join(remote_dir_resolved, rel)
            try:
                if remote_d not in made_dirs:
                    _remote_mkdirs(client, remote_d)
                    made_dirs.add(remote_d)
            except Exception:
                pass
            for root, dirs, files in os.walk(d):
                # Create directories first.
                for dn in dirs:
                    lp_dir = os.path.join(root, dn)
                    try:
                        rel_dir = os.path.relpath(lp_dir, local_dir)
                    except Exception:
                        continue
                    if not rel_dir or rel_dir.startswith('..'):
                        continue
                    rp_dir = _remote_path_join(remote_dir_resolved, rel_dir)
                    if rp_dir in made_dirs:
                        continue
                    try:
                        _remote_mkdirs(client, rp_dir)
                        made_dirs.add(rp_dir)
                    except Exception:
                        pass
                for fn in files:
                    lp_file = os.path.join(root, fn)
                    try:
                        rel_file = os.path.relpath(lp_file, local_dir)
                    except Exception:
                        continue
                    if not rel_file or rel_file.startswith('..'):
                        continue
                    rp_file = _remote_path_join(remote_dir_resolved, rel_file)
                    rp_parent = os.path.dirname(rp_file)
                    try:
                        if rp_parent and rp_parent not in made_dirs:
                            _remote_mkdirs(client, rp_parent)
                            made_dirs.add(rp_parent)
                    except Exception:
                        pass
                    try:
                        sftp.put(lp_file, rp_file)
                        uploaded += 1
                    except Exception:
                        log.exception("[sync] Failed uploading %s -> %s", lp_file, rp_file)
        if uploaded:
            log.info("[sync] Uploaded %d vuln artifact(s) to CORE host dir=%s", uploaded, remote_dir_resolved)
        return uploaded > 0
    except Exception:
        log.exception("[sync] Vulnerability artifact upload skipped/failed")
        return False
    finally:
        try:
            if sftp:
                sftp.close()
        except Exception:
            pass
        try:
            if client:
                client.close()
        except Exception:
            pass


def _remote_docker_cleanup_script(names: list[str], sudo_password: str | None = None) -> str:
    names_literal = json.dumps([str(x) for x in (names or [])])
    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    return (
        """
import json, subprocess

NAMES = __NAMES_LITERAL__

SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__


def _run_docker(cmd, timeout=20):
    # Run docker on the CORE VM via sudo.
    # - Try non-interactive sudo first (fast fail if password is required).
    # - If a sudo password is available, retry with sudo -S.
    try:
        p = subprocess.run(['sudo', '-n', 'docker'] + list(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', '', 'docker'] + list(cmd),
                input=str(SUDO_PASSWORD) + "\\n",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        class _P:
            returncode = 1
            stdout = str(e)
        return _P()


def main():
    results = []
    for nm in (NAMES or []):
        stopped = removed = False
        try:
            p1 = _run_docker(['stop', nm], timeout=20)
            stopped = (p1.returncode == 0)
        except Exception:
            stopped = False
        try:
            p2 = _run_docker(['rm', nm], timeout=20)
            removed = (p2.returncode == 0)
        except Exception:
            removed = False
        results.append({'name': nm, 'stopped': bool(stopped), 'removed': bool(removed)})
    # Prune unused networks to avoid exhausting Docker's default address pools.
    # CORE docker nodes frequently create per-node compose networks; if sessions are
    # started/stopped repeatedly without network cleanup, docker can hit:
    # "all predefined address pools have been fully subnetted".
    net_prune_ok = False
    net_prune_out = ''
    try:
        p3 = _run_docker(['network', 'prune', '-f'], timeout=30)
        net_prune_ok = (getattr(p3, 'returncode', 1) == 0)
        net_prune_out = (getattr(p3, 'stdout', '') or '').strip()
    except Exception as e:
        net_prune_ok = False
        net_prune_out = str(e)
    print(json.dumps({'ok': True, 'results': results, 'network_prune': {'ok': bool(net_prune_ok), 'output': net_prune_out}}))


if __name__ == '__main__':
    main()
"""
    ).replace('__NAMES_LITERAL__', names_literal).replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal)


def _remote_docker_remove_wrapper_images_script(sudo_password: str | None = None) -> str:
    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    return (
        """
import json, subprocess

SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__


def _run_docker(cmd, timeout=30):
    try:
        p = subprocess.run(['sudo', '-n', 'docker'] + list(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', '', 'docker'] + list(cmd),
                input=str(SUDO_PASSWORD) + "\\n",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        class _P:
            returncode = 1
            stdout = str(e)
        return _P()


def main():
    # List all tool-generated wrapper images. We scope these under the "coretg" repo prefix.
    # Only remove images whose tag begins with "iproute2" (our wrapper tag).
    listed = []
    removed = []
    errors = []
    p = _run_docker(['images', '--format', '{{.Repository}}:{{.Tag}}', '--filter', 'reference=coretg/*'], timeout=30)
    out = (getattr(p, 'stdout', '') or '').strip()
    if out:
        for line in out.splitlines():
            ref = (line or '').strip()
            if not ref or ':' not in ref:
                continue
            listed.append(ref)
    targets = []
    for ref in listed:
        try:
            repo, tag = ref.rsplit(':', 1)
        except Exception:
            continue
        if tag.startswith('iproute2'):
            targets.append(ref)
    for ref in targets:
        try:
            pr = _run_docker(['image', 'rm', '-f', ref], timeout=60)
            if getattr(pr, 'returncode', 1) == 0:
                removed.append(ref)
            else:
                errors.append({'ref': ref, 'output': (getattr(pr, 'stdout', '') or '').strip()})
        except Exception as e:
            errors.append({'ref': ref, 'output': str(e)})
    print(json.dumps({'ok': True, 'listed': listed, 'targets': targets, 'removed': removed, 'errors': errors}))


if __name__ == '__main__':
    main()
"""
    ).replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal)


def _remote_docker_remove_all_containers_script(sudo_password: str | None = None) -> str:
    """Remote script to delete ALL docker containers on the CORE VM (does not remove images)."""

    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    return (
        """
import json, subprocess

SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__


def _run_docker(cmd, timeout=60):
    try:
        p = subprocess.run(['sudo', '-n', 'docker'] + list(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', '', 'docker'] + list(cmd),
                input=str(SUDO_PASSWORD) + "\n",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        class _P:
            returncode = 1
            stdout = str(e)
        return _P()


def _list_ids(args, timeout=30):
    p = _run_docker(args, timeout=timeout)
    if getattr(p, 'returncode', 1) != 0:
        return [], (getattr(p, 'stdout', '') or '').strip()
    out = (getattr(p, 'stdout', '') or '').strip()
    ids = [ln.strip() for ln in out.splitlines() if ln.strip()]
    return ids, ''


def _chunks(seq, n=40):
    for i in range(0, len(seq), n):
        yield seq[i:i + n]


def main():
    errors = []

    container_ids, cerr = _list_ids(['ps', '-aq'], timeout=30)
    if cerr:
        errors.append({'stage': 'list_containers', 'output': cerr})
    stopped_attempted = 0
    removed_attempted = 0
    if container_ids:
        for chunk in _chunks(container_ids, 25):
            p = _run_docker(['stop'] + list(chunk), timeout=120)
            if getattr(p, 'returncode', 1) != 0:
                out = (getattr(p, 'stdout', '') or '').strip()
                if out:
                    errors.append({'stage': 'stop_containers', 'output': out[:4000]})
            else:
                stopped_attempted += len(chunk)
        for chunk in _chunks(container_ids, 25):
            p = _run_docker(['rm', '-f'] + list(chunk), timeout=120)
            if getattr(p, 'returncode', 1) != 0:
                out = (getattr(p, 'stdout', '') or '').strip()
                if out:
                    errors.append({'stage': 'rm_containers', 'output': out[:4000]})
            else:
                removed_attempted += len(chunk)

    print(json.dumps({
        'ok': True,
        'containers': {
            'found': len(container_ids),
            'stopped_attempted': stopped_attempted,
            'removed_attempted': removed_attempted,
        },
        'images': {
            'removed_attempted': 0,
            'skipped': True,
        },
        'errors': errors,
    }))


if __name__ == '__main__':
    main()
"""
    ).replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal)


@app.route('/docker/status', methods=['GET'])
def docker_status():
    # Use the scenario-selected CORE VM (remote) to populate docker status.
    history = _load_run_history()
    current_user = _current_user()
    scenario_names, _scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
    scenario_query = request.args.get('scenario', '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    if scenario_names:
        if not scenario_norm or not any(_normalize_scenario_label(n) == scenario_norm for n in scenario_names):
            scenario_norm = _normalize_scenario_label(scenario_names[0])
    core_cfg = _select_core_config_for_page(scenario_norm, history, include_password=True)
    core_cfg = _ensure_core_vm_metadata(core_cfg)
    try:
        payload = _run_remote_python_json(
            core_cfg,
            _remote_docker_status_script(core_cfg.get('ssh_password')),
            logger=app.logger,
            label='docker.status',
            timeout=60.0,
        )
        if not isinstance(payload, dict):
            payload = {'items': [], 'timestamp': int(time.time()), 'error': 'invalid remote payload'}
        payload.setdefault('timestamp', int(time.time()))
        return jsonify(payload)
    except Exception as exc:
        return jsonify({'items': [], 'timestamp': int(time.time()), 'error': str(exc)}), 200


@app.route('/docker/cleanup', methods=['POST'])
def docker_cleanup():
    names: list[str] = []
    try:
        if request.is_json:
            body = request.get_json(silent=True) or {}
            if isinstance(body.get('names'), list):
                names = [str(x) for x in body.get('names')]
        else:
            raw = request.form.get('names')
            if raw:
                try:
                    arr = json.loads(raw)
                    if isinstance(arr, list):
                        names = [str(x) for x in arr]
                    else:
                        names = [str(raw)]
                except Exception:
                    names = [str(raw)]
        # Use the scenario-selected CORE VM (remote) to cleanup docker containers.
        history = _load_run_history()
        current_user = _current_user()
        scenario_names, _scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
        scenario_query = request.args.get('scenario', '').strip()
        scenario_norm = _normalize_scenario_label(scenario_query)
        if scenario_names:
            if not scenario_norm or not any(_normalize_scenario_label(n) == scenario_norm for n in scenario_names):
                scenario_norm = _normalize_scenario_label(scenario_names[0])
        core_cfg = _select_core_config_for_page(scenario_norm, history, include_password=True)
        core_cfg = _ensure_core_vm_metadata(core_cfg)

        if not names:
            # If no names supplied, reuse the remote status listing to derive all node names.
            try:
                status_payload = _run_remote_python_json(
                    core_cfg,
                    _remote_docker_status_script(core_cfg.get('ssh_password')),
                    logger=app.logger,
                    label='docker.status(for cleanup)',
                    timeout=60.0,
                )
                if isinstance(status_payload, dict) and isinstance(status_payload.get('items'), list):
                    names = [str(it.get('name')) for it in status_payload['items'] if isinstance(it, dict) and it.get('name')]
            except Exception:
                names = []

        payload = _run_remote_python_json(
            core_cfg,
            _remote_docker_cleanup_script(names, core_cfg.get('ssh_password')),
            logger=app.logger,
            label='docker.cleanup',
            timeout=120.0,
        )
        if not isinstance(payload, dict):
            payload = {'ok': False, 'error': 'invalid remote payload'}
        return jsonify(payload)
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)}), 500


def _find_latest_session_xml(base_dir: str, stem: str | None = None) -> str | None:
    """Best-effort helper to locate the newest session XML written previously.

    If `stem` is provided, prefer files beginning with that stem; otherwise
    return the most recent .xml file under the directory.
    """
    try:
        if not base_dir or not os.path.isdir(base_dir):
            return None
        candidates: list[tuple[float, str]] = []
        for name in os.listdir(base_dir):
            if not name.lower().endswith('.xml'):
                continue
            if stem:
                prefix = f"{stem}-"
                if not name.startswith(prefix):
                    continue
            path = os.path.join(base_dir, name)
            try:
                st = os.stat(path)
                candidates.append((st.st_mtime, path))
            except Exception:
                continue
        if not candidates and stem:
            # If no stem-specific match, allow fallback to any latest XML
            return _find_latest_session_xml(base_dir, stem=None)
        if not candidates:
            return None
        candidates.sort(key=lambda item: item[0], reverse=True)
        return candidates[0][1]
    except Exception:
        return None


def _grpc_save_current_session_xml_with_config(core_cfg: Dict[str, Any], out_dir: str, session_id: str | None = None) -> str | None:
    """Attempt to connect to CORE daemon via gRPC and save the active session XML.

    The save_xml call now executes on the remote CORE host via SSH. The resulting
    XML is downloaded locally into out_dir (and mirrored to outputs/core-sessions).
    """
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    address = f"{cfg.get('host') or CORE_HOST}:{cfg.get('port') or CORE_PORT}"

    # Desired local filename: <scenario-name><timestamp>.xml
    sid_int: int | None = None
    try:
        sid_int = int(session_id) if session_id not in (None, '') else None
    except Exception:
        sid_int = None
    scenario_label: str | None = None
    ts_epoch: float | None = None
    if sid_int is not None:
        try:
            meta_by_sid = _read_remote_session_scenario_meta_bulk(cfg, session_ids=[sid_int], logger=app.logger)
            meta = meta_by_sid.get(sid_int) if isinstance(meta_by_sid, dict) else None
            if isinstance(meta, dict):
                scenario_label = (meta.get('scenario_name') or '').strip() or None
                if meta.get('written_at_epoch') not in (None, ''):
                    try:
                        ts_epoch = float(meta.get('written_at_epoch'))
                    except Exception:
                        ts_epoch = None
        except Exception:
            pass
    if ts_epoch is None and sid_int is not None:
        try:
            store_map = _load_core_sessions_store()
            host = cfg.get('host') or CORE_HOST
            port = int(cfg.get('port') or CORE_PORT)
            ts_epoch = _session_store_updated_at_for_session_id(store_map, sid_int, host=host, port=port)
        except Exception:
            ts_epoch = None
    desired_name = _scenario_timestamped_filename(scenario_label or 'scenario', ts_epoch)
    # Use the scenario stem (not full desired_name) for remote uniqueness prefix.
    try:
        desired_stem = os.path.splitext(secure_filename(desired_name))[0]
    except Exception:
        desired_stem = 'scenario'
    stem = desired_stem.split('-')[0] or desired_stem or 'scenario'
    base_sessions_dir = os.path.join(_outputs_dir(), 'core-sessions')
    for directory in (out_dir, base_sessions_dir):
        try:
            os.makedirs(directory, exist_ok=True)
        except Exception:
            pass
    remote_dir = posixpath.join('/tmp', 'core-topo-gen', 'session_exports')
    remote_name = f"{stem}-{uuid.uuid4().hex}.xml"
    remote_path = posixpath.join(remote_dir, remote_name)
    script = _remote_core_save_xml_script(address, session_id, remote_path)
    command_desc = (
        f"remote ssh {ssh_user}@{ssh_host} -> CoreGrpcClient.save_xml {address} (session={session_id or 'first'})"
    )
    try:
        payload = _run_remote_python_json(
            cfg,
            script,
            logger=app.logger,
            label='core.save_xml',
            command_desc=command_desc,
            timeout=180.0,
        )
    except Exception as exc:
        app.logger.warning('Remote CORE save_xml failed: %s', exc)
        return _find_latest_session_xml(out_dir, stem)
    error_text = payload.get('error')
    if error_text:
        app.logger.warning('Remote CORE save_xml reported error: %s', error_text)
        tb = payload.get('traceback')
        if tb:
            app.logger.debug('save_xml traceback: %s', tb)
        return _find_latest_session_xml(out_dir, stem)
    remote_output = payload.get('output_path') or remote_path
    session_id_remote = payload.get('session_id') or session_id or 'core'
    local_path = os.path.join(out_dir, secure_filename(desired_name))
    try:
        _download_remote_file(cfg, remote_output, local_path)
    except Exception as exc:
        app.logger.warning('Failed downloading remote CORE XML %s: %s', remote_output, exc)
        return _find_latest_session_xml(out_dir, stem)
    finally:
        try:
            _remove_remote_file(cfg, remote_output)
        except Exception:
            pass
    try:
        ok, errs = _validate_core_xml(local_path)
    except Exception as exc:
        app.logger.warning('Validation raised exception for %s: %s', local_path, exc)
        ok = False
        errs = str(exc)
    if not ok:
        app.logger.warning('Downloaded CORE XML failed validation (%s); removing file %s', errs, local_path)
        try:
            os.remove(local_path)
        except Exception:
            pass
        return _find_latest_session_xml(out_dir, stem)
    try:
        _normalize_core_device_types(local_path)
    except Exception as exc:
        app.logger.debug('Device type normalization skipped for %s: %s', local_path, exc)
    try:
        size = os.path.getsize(local_path)
    except Exception:
        size = -1
    app.logger.info('Saved CORE session XML (%s bytes) to %s', size if size >= 0 else '?', local_path)
    try:
        dest = os.path.join(base_sessions_dir, os.path.basename(local_path))
        if os.path.abspath(dest) != os.path.abspath(local_path):
            if os.path.exists(dest):
                root, ext = os.path.splitext(dest)
                counter = 2
                while os.path.exists(f"{root}-{counter}{ext}"):
                    counter += 1
                dest = f"{root}-{counter}{ext}"
            shutil.copy2(local_path, dest)
    except Exception:
        pass
    return local_path

def _grpc_save_current_session_xml(core_cfg_or_host: Any, maybe_port: Any, out_dir: str | None = None, session_id: str | None = None) -> str | None:
    if isinstance(core_cfg_or_host, dict):
        core_cfg = core_cfg_or_host
        target_out_dir = maybe_port
    else:
        core_cfg = {'host': core_cfg_or_host, 'port': maybe_port}
        target_out_dir = out_dir
    if not isinstance(target_out_dir, str) or not target_out_dir:
        raise ValueError('out_dir is required for _grpc_save_current_session_xml')
    return _grpc_save_current_session_xml_with_config(core_cfg, target_out_dir, session_id=session_id)


def _parse_session_xml_for_compare(session_xml_path: str) -> Dict[int, Dict[str, Any]]:
    """Return node info map from a CORE session XML file: id -> {name, ips, services, type}."""
    nodes: Dict[int, Dict[str, Any]] = {}
    if not session_xml_path or not os.path.exists(session_xml_path):
        return nodes
    try:
        import xml.etree.ElementTree as _ET
        root = _ET.parse(session_xml_path).getroot()
    except Exception:
        return nodes

    def _norm_id(raw: Any) -> Optional[int]:
        try:
            if raw is None:
                return None
            return int(str(raw).strip())
        except Exception:
            return None

    candidates = []
    rj45_names: set[str] = set()
    rj45_ids: set[int] = set()
    try:
        candidates.extend(root.findall('.//node'))
    except Exception:
        pass
    try:
        candidates.extend(root.findall('.//device'))
    except Exception:
        pass
    try:
        for net in root.findall('.//network'):
            try:
                ntype = (net.get('type') or net.get('model') or '').strip().lower()
                nclass = (net.get('class') or '').strip().lower()
                name = (net.get('name') or '').strip()
                if ntype == 'rj45' or 'rj45' in nclass:
                    try:
                        nid = _norm_id(net.get('id') or net.findtext('id'))
                        if nid is not None:
                            rj45_ids.add(nid)
                    except Exception:
                        pass
                if name and (ntype == 'rj45' or 'rj45' in nclass):
                    rj45_names.add(name)
            except Exception:
                continue
    except Exception:
        pass

    for node in candidates:
        nid = _norm_id(node.get('id') or node.findtext('id'))
        if nid is None:
            continue
        name = (node.get('name') or node.findtext('name') or '').strip()
        ntype = (node.get('type') or node.get('model') or '').strip().lower()
        # Ignore infrastructure-only nodes (not part of expected topology compare).
        if ntype in {'rj45', 'switch', 'hub', 'bridge', 'wlan', 'wireless', 'ethernet', 'link'}:
            continue
        if name and name in rj45_names:
            continue
        services: List[str] = []
        try:
            for svc in node.findall('.//service'):
                sname = (svc.get('name') or (svc.text or '')).strip()
                if sname:
                    services.append(sname)
        except Exception:
            pass
        nodes.setdefault(nid, {'name': name, 'type': ntype, 'services': set(), 'ips': set()})
        nodes[nid]['services'].update(services)

    for link in root.findall('.//link'):
        node1 = _norm_id(link.get('node1') or link.get('node1_id') or link.get('src'))
        node2 = _norm_id(link.get('node2') or link.get('node2_id') or link.get('dst'))
        for tag, nid in (('iface1', node1), ('iface2', node2)):
            if nid is None:
                continue
            if nid in rj45_ids:
                continue
            iface = link.find(tag)
            if iface is None:
                continue
            ip4 = (iface.get('ip4') or '').strip()
            mask = (iface.get('ip4_mask') or '').strip()
            if not ip4:
                continue
            cidr = f"{ip4}/{mask}" if mask else ip4
            nodes.setdefault(nid, {'name': '', 'type': '', 'services': set(), 'ips': set()})
            nodes[nid]['ips'].add(cidr)
    return nodes


def _expected_from_plan_preview(scenario_xml_path: str, scenario_label: Optional[str]) -> Dict[int, Dict[str, Any]]:
    expected: Dict[int, Dict[str, Any]] = {}
    if not scenario_xml_path or not os.path.exists(scenario_xml_path):
        return expected
    payload = _load_plan_preview_from_xml(scenario_xml_path, scenario_label)
    if not isinstance(payload, dict):
        return expected
    full = payload.get('full_preview') if isinstance(payload.get('full_preview'), dict) else payload
    if not isinstance(full, dict):
        return expected
    services_preview = full.get('services_preview') if isinstance(full.get('services_preview'), dict) else {}

    def _add_node(entry: dict, role: str | None = None) -> None:
        try:
            nid = entry.get('node_id') if isinstance(entry, dict) else None
            if nid is None:
                nid = entry.get('id') if isinstance(entry, dict) else None
            nid_int = int(nid)
        except Exception:
            return
        name = str(entry.get('name') or '').strip() if isinstance(entry, dict) else ''
        ip4 = str(entry.get('ip4') or '').strip() if isinstance(entry, dict) else ''
        expected.setdefault(nid_int, {'name': name, 'ip4': ip4, 'role': role, 'services': None})

    for r in (full.get('routers') or []):
        if isinstance(r, dict):
            _add_node(r, role='router')
    for h in (full.get('hosts') or []):
        if isinstance(h, dict):
            _add_node(h, role=str(h.get('role') or 'host').strip().lower())
    for sid, svcs in (services_preview or {}).items():
        try:
            nid_int = int(str(sid).strip())
        except Exception:
            continue
        if not isinstance(svcs, list):
            continue
        entry = expected.setdefault(nid_int, {'name': '', 'ip4': '', 'role': None, 'services': None})
        entry['services'] = list({str(s).strip() for s in svcs if str(s).strip()})
    return expected


def _append_session_scenario_discrepancies(
    report_path: Optional[str],
    scenario_xml_path: Optional[str],
    session_xml_path: Optional[str],
    scenario_label: Optional[str] = None,
) -> None:
    if not report_path or not os.path.exists(report_path):
        return
    if not scenario_xml_path or not session_xml_path:
        return
    try:
        with open(report_path, 'r', encoding='utf-8', errors='ignore') as rf:
            existing = rf.read()
        if '## Scenario/Session Discrepancies' in existing:
            return
    except Exception:
        existing = None

    expected = _expected_from_plan_preview(scenario_xml_path, scenario_label)
    actual = _parse_session_xml_for_compare(session_xml_path)

    lines: List[str] = []
    lines.append('\n## Scenario/Session Discrepancies')
    if not expected:
        lines.append('PlanPreview missing in scenario XML; comparison skipped.')
        _append_report_lines(report_path, lines)
        return
    if not actual:
        lines.append('Session XML missing or unreadable; comparison skipped.')
        _append_report_lines(report_path, lines)
        return

    expected_ids = set(expected.keys())
    actual_ids = set(actual.keys())
    missing_nodes = sorted(expected_ids - actual_ids)
    extra_nodes = sorted(actual_ids - expected_ids)

    mismatch_rows: List[str] = []
    for nid in sorted(expected_ids & actual_ids):
        exp = expected.get(nid) or {}
        act = actual.get(nid) or {}
        exp_name = (exp.get('name') or '').strip()
        act_name = (act.get('name') or '').strip()
        exp_ip = (exp.get('ip4') or '').strip()
        act_ips = sorted(list(act.get('ips') or []))
        exp_services = exp.get('services')
        act_services = sorted(list(act.get('services') or []))

        name_mismatch = exp_name and act_name and exp_name != act_name
        ip_mismatch = bool(exp_ip) and (exp_ip not in act_ips)
        svc_missing: List[str] = []
        svc_extra: List[str] = []
        if isinstance(exp_services, list):
            exp_set = set(exp_services)
            act_set = set(act_services)
            svc_missing = sorted(list(exp_set - act_set))
            svc_extra = sorted(list(act_set - exp_set))

        if name_mismatch or ip_mismatch or svc_missing or svc_extra:
            mismatch_rows.append(
                f"| {nid} | {exp_name or 'n/a'} | {act_name or 'n/a'} | {exp_ip or 'n/a'} | {', '.join(act_ips) or 'n/a'} | {', '.join(svc_missing) or ''} | {', '.join(svc_extra) or ''} |"
            )

    if not missing_nodes and not extra_nodes and not mismatch_rows:
        lines.append('No discrepancies detected.')
        _append_report_lines(report_path, lines)
        return

    if missing_nodes:
        lines.append(f"- Missing nodes in session XML: {', '.join(str(n) for n in missing_nodes)}")
    if extra_nodes:
        lines.append(f"- Extra nodes in session XML: {', '.join(str(n) for n in extra_nodes)}")
    if mismatch_rows:
        lines.append('')
        lines.append('| Node ID | Expected Name | Actual Name | Expected IP | Actual IPs | Missing Services | Extra Services |')
        lines.append('| ---: | --- | --- | --- | --- | --- | --- |')
        lines.extend(mismatch_rows)
    _append_report_lines(report_path, lines)


def _append_report_lines(report_path: str, lines: List[str]) -> None:
    try:
        with open(report_path, 'a', encoding='utf-8') as wf:
            wf.write('\n'.join(lines).rstrip() + '\n')
    except Exception:
        pass

def _attach_base_upload(payload: Dict[str, Any]):
    """Ensure payload['base_upload'] is present if first scenario has a base filepath referencing an existing file.
    Performs validation to set valid flag. Does nothing if already present.
    """
    try:
        if payload.get('base_upload'):
            return
        scen_list = payload.get('scenarios') or []
        if not scen_list:
            return
        base_path = scen_list[0].get('base', {}).get('filepath') or ''
        if not base_path or not os.path.exists(base_path):
            return
        ok, _errs = _validate_core_xml(base_path)
        display_name = os.path.basename(base_path)
        payload['base_upload'] = {
            'path': base_path,
            'valid': bool(ok),
            'display_name': display_name,
        }
        try:
            scen_list[0].setdefault('base', {})['display_name'] = display_name
        except Exception:
            pass
    except Exception:
        pass


def _parse_scenarios_xml(path):
    data = {"scenarios": []}
    tree = ET.parse(path)
    root = tree.getroot()
    if root.tag != "Scenarios":
        # Fallback: if file is a single ScenarioEditor, wrap
        if root.tag == "ScenarioEditor":
            scen = _parse_scenario_editor(root)
            scen["name"] = os.path.splitext(os.path.basename(path))[0]
            data["scenarios"].append(scen)
            return data
        raise ValueError("Root element must be <Scenarios> or <ScenarioEditor>")
    core_el = root.find('CoreConnection')
    if core_el is not None:
        core_meta = {
            'host': core_el.get('host'),
            'port': core_el.get('port'),
            'ssh_enabled': core_el.get('ssh_enabled'),
            'ssh_host': core_el.get('ssh_host'),
            'ssh_port': core_el.get('ssh_port'),
            'ssh_username': core_el.get('ssh_username'),
        }
        data['core'] = _normalize_core_config(core_meta, include_password=False)
    else:
        data['core'] = _default_core_dict()

    for scen_el in root.findall("Scenario"):
        scen = {"name": scen_el.get("name", "Scenario")}
        # Capture scenario-level density_count attribute if present
        dc_attr = scen_el.get('density_count')
        if dc_attr is not None and dc_attr != '':
            try:
                scen['density_count'] = int(dc_attr)
            except Exception:
                pass
        total_nodes_attr = scen_el.get('scenario_total_nodes')
        if total_nodes_attr not in (None, ''):
            try:
                scen['scenario_total_nodes'] = int(total_nodes_attr)
            except Exception:
                scen['scenario_total_nodes'] = total_nodes_attr
        base_nodes_attr = scen_el.get('base_nodes')
        if base_nodes_attr not in (None, ''):
            try:
                scen['base_nodes'] = int(base_nodes_attr)
            except Exception:
                scen['base_nodes'] = base_nodes_attr
        se = scen_el.find("ScenarioEditor")
        if se is None:
            continue
        scen.update(_parse_scenario_editor(se))
        # If scenario-level density_count was absent but Node Information section provided one, keep existing.
        data["scenarios"].append(scen)
    return data


def _core_config_from_xml_path(
    xml_path: str | None,
    scenario_norm: str,
    *,
    include_password: bool = True,
) -> Dict[str, Any] | None:
    if not xml_path:
        return None
    try:
        xml_abs = os.path.abspath(xml_path)
    except Exception:
        return None
    if not os.path.exists(xml_abs):
        return None
    try:
        payload = _parse_scenarios_xml(xml_abs)
    except Exception:
        return None

    scenario_core: Dict[str, Any] | None = None
    scen_list = payload.get('scenarios') if isinstance(payload, dict) else None
    if isinstance(scen_list, list):
        for sc in scen_list:
            if not isinstance(sc, dict):
                continue
            nm = str(sc.get('name') or '').strip()
            if scenario_norm and _normalize_scenario_label(nm) != scenario_norm:
                continue
            hitl = sc.get('hitl') if isinstance(sc.get('hitl'), dict) else None
            if isinstance(hitl, dict) and isinstance(hitl.get('core'), dict):
                scenario_core = hitl.get('core')
            if scenario_norm:
                break
            if scenario_core:
                break

    global_core: Dict[str, Any] | None = None
    try:
        tree = ET.parse(xml_abs)
        root = tree.getroot()
        core_el = root.find('CoreConnection')
        if core_el is not None:
            global_core = _extract_optional_core_config(dict(core_el.attrib), include_password=include_password)
    except Exception:
        global_core = None

    if not scenario_core and not global_core:
        return None
    return _merge_core_configs(global_core, scenario_core, include_password=include_password)


def _flow_state_from_latest_xml(scenario_norm: str) -> dict[str, Any] | None:
    try:
        scen_norm = _normalize_scenario_label(scenario_norm)
        if not scen_norm:
            return None
        xml_path = _latest_xml_path_for_scenario(scen_norm)
        if not xml_path:
            return None
        parsed = _parse_scenarios_xml(xml_path)
        scen_list = parsed.get('scenarios') if isinstance(parsed, dict) else None
        if not isinstance(scen_list, list):
            return None
        for sc in scen_list:
            if not isinstance(sc, dict):
                continue
            nm = str(sc.get('name') or '').strip()
            if _normalize_scenario_label(nm) != scen_norm:
                continue
            fs = sc.get('flow_state')
            return fs if isinstance(fs, dict) else None
        return None
    except Exception:
        return None


def _flow_state_from_xml_path(xml_path: str, scenario_label: str | None) -> dict[str, Any] | None:
    try:
        if not xml_path:
            return None
        xml_abs = os.path.abspath(xml_path)
        if not os.path.exists(xml_abs):
            return None
        parsed = _parse_scenarios_xml(xml_abs)
        scen_list = parsed.get('scenarios') if isinstance(parsed, dict) else None
        if not isinstance(scen_list, list):
            return None
        target_norm = _normalize_scenario_label(scenario_label or '') if scenario_label else ''
        if target_norm:
            for sc in scen_list:
                if not isinstance(sc, dict):
                    continue
                nm = str(sc.get('name') or '').strip()
                if _normalize_scenario_label(nm) != target_norm:
                    continue
                fs = sc.get('flow_state')
                return fs if isinstance(fs, dict) else None
            return None
        # No scenario specified: return the first flow_state found.
        for sc in scen_list:
            if not isinstance(sc, dict):
                continue
            fs = sc.get('flow_state')
            if isinstance(fs, dict):
                return fs
        return None
    except Exception:
        return None


def _latest_xml_path_for_scenario(scenario_norm: str) -> str | None:
    try:
        scen_norm = _normalize_scenario_label(scenario_norm)
        if not scen_norm:
            return None
        current_user = _current_user()
        scenario_names, scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(None, user=current_user)
        if not scenario_names or not isinstance(scenario_paths, dict):
            return None
        path_candidates = scenario_paths.get(scen_norm) or scenario_paths.get(_resolve_scenario_display(scen_norm, scenario_names, scen_norm)) or None
        filtered = None
        if isinstance(path_candidates, (list, tuple, set)):
            filtered = [p for p in path_candidates if not _is_autosave_xml_path(str(p or ''))]
        preferred = _select_single_scenario_path(filtered or path_candidates, scen_norm)
        # Only return a single-scenario XML. Avoid aggregate files.
        return preferred
    except Exception:
        return None


def _is_autosave_xml_path(path_value: str) -> bool:
    try:
        if not path_value:
            return True
        norm = str(path_value).replace('\\', '/').lower()
        if '/outputs/autosave/' in norm or norm.endswith('/outputs/autosave'):
            return True
        base = os.path.basename(norm)
        return base.startswith('autosave-')
    except Exception:
        return True


@app.route('/api/scenario/latest_xml', methods=['GET'])
def api_latest_xml_for_scenario():
    scenario = (request.args.get('scenario') or '').strip()
    if not scenario:
        return jsonify({'ok': False, 'error': 'scenario required'}), 400
    try:
        scen_norm = _normalize_scenario_label(scenario)
        xml_path = _latest_xml_path_for_scenario(scen_norm)
        if not xml_path:
            return jsonify({'ok': False, 'error': 'No XML found'}), 404
        return jsonify({'ok': True, 'scenario': scenario, 'xml_path': xml_path})
    except Exception as exc:
        return jsonify({'ok': False, 'error': str(exc)}), 500


def _update_flow_state_in_xml(xml_path: str, scenario_label: str | None, flow_state: dict[str, Any]) -> tuple[bool, str]:
    """Update (or create) FlagSequencing/FlowState within a scenarios XML file."""
    if not xml_path or not os.path.exists(xml_path):
        return False, 'xml_path not found'
    if not isinstance(flow_state, dict) or not flow_state:
        return False, 'flow_state empty'
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
    except Exception as e:
        return False, f'failed to parse xml: {e}'

    scenario_norm = _normalize_scenario_label(scenario_label or '')
    se_target = None

    try:
        if root.tag == 'ScenarioEditor':
            se_target = root
        elif root.tag == 'Scenario':
            se_target = root.find('ScenarioEditor')
            if se_target is None:
                se_target = ET.SubElement(root, 'ScenarioEditor')
        elif root.tag == 'Scenarios':
            for scen_el in root.findall('Scenario'):
                nm = str(scen_el.get('name') or '').strip()
                if scenario_norm and _normalize_scenario_label(nm) != scenario_norm:
                    continue
                se_target = scen_el.find('ScenarioEditor')
                if se_target is None:
                    se_target = ET.SubElement(scen_el, 'ScenarioEditor')
                if se_target is not None:
                    break
    except Exception:
        se_target = None

    if se_target is None:
        return False, 'ScenarioEditor not found'

    try:
        fs_el = se_target.find('FlagSequencing')
        if fs_el is None:
            fs_el = ET.SubElement(se_target, 'FlagSequencing')
        # Clear existing FlowState children.
        for child in list(fs_el):
            if child.tag == 'FlowState':
                fs_el.remove(child)
        st_el = ET.SubElement(fs_el, 'FlowState')
        st_el.text = json.dumps(flow_state, separators=(',', ':'), ensure_ascii=False)
        tree.write(xml_path, encoding='utf-8', xml_declaration=True)
        return True, 'ok'
    except Exception as e:
        return False, f'failed to update xml: {e}'


def _clear_flow_state_in_xml(xml_path: str, scenario_label: str | None) -> tuple[bool, str]:
    """Remove FlagSequencing/FlowState from a scenarios XML file (best-effort)."""
    if not xml_path or not os.path.exists(xml_path):
        return False, 'xml_path not found'
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
    except Exception as e:
        return False, f'failed to parse xml: {e}'

    scenario_norm = _normalize_scenario_label(scenario_label or '')
    se_target = None
    try:
        if root.tag == 'ScenarioEditor':
            se_target = root
        elif root.tag == 'Scenario':
            se_target = root.find('ScenarioEditor')
        elif root.tag == 'Scenarios':
            for scen_el in root.findall('Scenario'):
                nm = str(scen_el.get('name') or '').strip()
                if scenario_norm and _normalize_scenario_label(nm) != scenario_norm:
                    continue
                se_target = scen_el.find('ScenarioEditor')
                if se_target is not None:
                    break
    except Exception:
        se_target = None

    if se_target is None:
        return False, 'ScenarioEditor not found'

    try:
        fs_el = se_target.find('FlagSequencing')
        if fs_el is None:
            return True, 'ok'
        removed = False
        for child in list(fs_el):
            if child.tag == 'FlowState':
                fs_el.remove(child)
                removed = True
        if removed:
            tree.write(xml_path, encoding='utf-8', xml_declaration=True)
        return True, 'ok'
    except Exception as e:
        return False, f'failed to update xml: {e}'


def _update_plan_preview_in_xml(
    xml_path: str,
    scenario_label: str | None,
    plan_payload: dict[str, Any],
) -> tuple[bool, str]:
    """Update (or create) ScenarioEditor/PlanPreview within a scenarios XML file."""
    if not xml_path or not os.path.exists(xml_path):
        return False, 'xml_path not found'
    if not isinstance(plan_payload, dict) or not plan_payload:
        return False, 'plan payload empty'
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
    except Exception as e:
        return False, f'failed to parse xml: {e}'

    scenario_norm = _normalize_scenario_label(scenario_label or '')
    se_target = None
    try:
        if root.tag == 'ScenarioEditor':
            se_target = root
        elif root.tag == 'Scenarios':
            for scen_el in root.findall('Scenario'):
                nm = str(scen_el.get('name') or '').strip()
                if scenario_norm and _normalize_scenario_label(nm) != scenario_norm:
                    continue
                se_target = scen_el.find('ScenarioEditor')
                if se_target is not None:
                    break
    except Exception:
        se_target = None

    if se_target is None:
        return False, 'ScenarioEditor not found'

    try:
        plan_el = se_target.find('PlanPreview')
        if plan_el is None:
            plan_el = ET.SubElement(se_target, 'PlanPreview')
        plan_el.text = json.dumps(plan_payload, separators=(',', ':'), ensure_ascii=False)
        tree.write(xml_path, encoding='utf-8', xml_declaration=True)
        return True, 'ok'
    except Exception as e:
        return False, f'failed to update xml: {e}'


def _load_plan_preview_from_xml(xml_path: str, scenario_label: str | None) -> dict[str, Any] | None:
    """Load embedded PlanPreview JSON payload from a scenarios XML file."""
    if not xml_path or not os.path.exists(xml_path):
        return None
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
    except Exception:
        return None

    scenario_norm = _normalize_scenario_label(scenario_label or '')
    se_target = None
    try:
        if root.tag == 'ScenarioEditor':
            se_target = root
        elif root.tag == 'Scenarios':
            for scen_el in root.findall('Scenario'):
                nm = str(scen_el.get('name') or '').strip()
                if scenario_norm and _normalize_scenario_label(nm) != scenario_norm:
                    continue
                se_target = scen_el.find('ScenarioEditor')
                if se_target is not None:
                    break
    except Exception:
        se_target = None

    if se_target is None:
        return None
    try:
        plan_el = se_target.find('PlanPreview')
        raw = (plan_el.text or '').strip() if plan_el is not None else ''
        if not raw:
            return None
        payload = json.loads(raw)
        return payload if isinstance(payload, dict) else None
    except Exception:
        return None


def _load_preview_payload_from_path(path: str, scenario_label: str | None = None) -> dict[str, Any] | None:
    """Load a preview payload from the universal XML file only.

    JSON plan files are intentionally ignored.
    """
    try:
        if not path or not isinstance(path, str):
            return None
        if path.lower().endswith('.xml'):
            return _load_plan_preview_from_xml(path, scenario_label)
    except Exception:
        return None
    return None


def _parse_scenario_editor(se):
    scen = {"base": {"filepath": ""}, "sections": {}, "notes": ""}
    # If parent <Scenario> carries scenario-level density_count attribute, capture it.
    try:
        parent = se.getparent()  # lxml style (if ever switched) - fallback below
    except Exception:
        parent = None
    # ElementTree doesn't support getparent; instead inspect tail by traversing immediate children of root in caller.
    # Simplest: look for density_count on any ancestor via attrib access on 'se' .attrib is only local, so rely on caller to have set scen_el attrib earlier.
    # We can recover by walking up using a cheap hack: se._parent if present (cpython impl detail) else ignore.
    try:
        scen_el = getattr(se, 'attrib', None)
    except Exception:
        scen_el = None
    # Instead of fragile parent access, during parsing of root we can read attribute directly from the sibling Scenario element (handled in outer loop); emulate by checking se.get('density_count') first.
    # For backward compatibility, allow density_count on Scenario or Node Information section.
    # We'll set scen['density_count'] here only if Scenario element attribute is available; Node Information section handled later.
    # Outer loop already hands us 'se'; its parent was processed to create scen dict. We'll modify outer function to inject attribute before calling this if needed.
    # Simpler: just check if 'density_count' exists on any ancestor by scanning se.iterfind('..') unsupported; fallback: pass.
    pass
    base = se.find("BaseScenario")
    if base is not None:
        scen["base"]["filepath"] = base.get("filepath", "")
    hitl_el = se.find("HardwareInLoop")
    hitl_info: Dict[str, Any] = {"enabled": False, "interfaces": [], "core": None}
    if hitl_el is not None:
        enabled_raw = (hitl_el.get("enabled") or "").strip().lower()
        hitl_info["enabled"] = enabled_raw in ("1", "true", "yes", "on")
        participant_attr = (
            hitl_el.get('participant_proxmox_url')
            or hitl_el.get('participant_url')
            or hitl_el.get('participant')
        )
        if participant_attr not in (None, ''):
            participant_clean = str(participant_attr).strip()
            if participant_clean:
                hitl_info['participant_proxmox_url'] = participant_clean
        interfaces: List[Dict[str, Any]] = []
        for iface_el in hitl_el.findall("Interface"):
            name = (iface_el.get("name") or "").strip()
            if not name:
                continue
            entry: Dict[str, Any] = {"name": name}
            alias = (iface_el.get("alias") or iface_el.get("display") or iface_el.get("description") or "").strip()
            if alias:
                entry["alias"] = alias
            mac_attr = iface_el.get("mac")
            if mac_attr:
                entry["mac"] = mac_attr
            attachment_attr = iface_el.get("attachment") or iface_el.get("attach")
            entry["attachment"] = _normalize_hitl_attachment(attachment_attr)
            ipv4_attr = iface_el.get("ipv4") or iface_el.get("ipv4_addresses")
            if ipv4_attr:
                entry["ipv4"] = [p.strip() for p in ipv4_attr.split(',') if p.strip()]
            ipv6_attr = iface_el.get("ipv6") or iface_el.get("ipv6_addresses")
            if ipv6_attr:
                entry["ipv6"] = [p.strip() for p in ipv6_attr.split(',') if p.strip()]
            interfaces.append(entry)
        hitl_info["interfaces"] = interfaces
        core_el = hitl_el.find("CoreConnection")
        if core_el is not None:
            core_cfg = _extract_optional_core_config(dict(core_el.attrib), include_password=False)
            if core_cfg:
                hitl_info["core"] = core_cfg
    scen["hitl"] = hitl_info

    # Flag Sequencing flow state (resolved values, chain selections).
    try:
        fs_el = se.find("FlagSequencing")
        if fs_el is not None:
            flow_el = fs_el.find("FlowState")
            raw = None
            if flow_el is not None and flow_el.text:
                raw = flow_el.text
            elif fs_el.text:
                raw = fs_el.text
            if raw:
                try:
                    scen["flow_state"] = json.loads(raw)
                except Exception:
                    scen["flow_state"] = {"raw": str(raw)}
    except Exception:
        pass
    # Plan preview payload (full preview + plan/breakdowns metadata) for UI round-trip.
    try:
        plan_el = se.find("PlanPreview")
        if plan_el is not None and plan_el.text:
            raw_plan = plan_el.text
            try:
                scen["plan_preview"] = json.loads(raw_plan)
            except Exception:
                scen["plan_preview"] = {"raw": str(raw_plan)}
    except Exception:
        pass
    # Sections
    for sec in se.findall("section"):
        name = sec.get("name", "")
        if not name:
            continue
        entry = {"density": None, "total_nodes": None, "items": []}
        if name == "Node Information":
            tn = sec.get("total_nodes")
            if tn is not None:
                try:
                    entry["total_nodes"] = int(tn)
                except Exception:
                    entry["total_nodes"] = 1
        else:
            dens = sec.get("density")
            entry["density"] = float(dens) if dens is not None else 0.5
        if name == "Vulnerabilities":
            entry["flag_type"] = (sec.get("flag_type") or "text").strip() or "text"
        for item in sec.findall("item"):
            d = {
                "selected": item.get("selected", "Random"),
                "factor": float(item.get("factor", "1.0")),
            }
            if name == "Routing":
                em = item.get('r2r_mode')
                if em is not None:
                    # Store under new key r2r_mode; keep legacy key for UI components still referencing it
                    d['r2r_mode'] = em
                ev = item.get('r2r_edges') or item.get('edges')
                if ev is not None and ev.strip() != '':
                    try:
                        d['r2r_edges'] = int(ev)
                    except Exception:
                        pass
                r2s_m = item.get('r2s_mode')
                if r2s_m is not None:
                    d['r2s_mode'] = r2s_m
                r2s_ev = item.get('r2s_edges')
                if r2s_ev is not None and r2s_ev.strip() != '':
                    try:
                        d['r2s_edges'] = int(r2s_ev)
                    except Exception:
                        pass
                # New per-item hosts-per-switch bounds
                hmin_attr = item.get('r2s_hosts_min')
                hmax_attr = item.get('r2s_hosts_max')
                try:
                    if hmin_attr is not None and hmin_attr.strip() != '':
                        d['r2s_hosts_min'] = int(hmin_attr)
                except Exception:
                    pass
                try:
                    if hmax_attr is not None and hmax_attr.strip() != '':
                        d['r2s_hosts_max'] = int(hmax_attr)
                except Exception:
                    pass
            if name == "Events":
                d["script_path"] = item.get("script_path", "")
            if name == "Traffic":
                d.update({
                    "pattern": item.get("pattern", "continuous"),
                    "rate_kbps": float(item.get("rate_kbps", "64.0")),
                    "period_s": float(item.get("period_s", "1.0")),
                    "jitter_pct": float(item.get("jitter_pct", "10.0")),
                    "content_type": (item.get("content_type") or item.get("content") or "Random"),
                })
            if name == "Vulnerabilities":
                # Extra attributes for Vulnerabilities section
                sel = (d.get("selected") or "").strip()
                # UI historically uses "Category" while XML schema uses "Type/Vector".
                # Normalize to UI label "Category" so round-trips are stable.
                if sel in ("Type/Vector", "Category"):
                    d["selected"] = "Category"
                    d["v_type"] = item.get("v_type", "Random")
                    d["v_vector"] = item.get("v_vector", "Random")
                elif sel == "Specific":
                    d["v_name"] = item.get("v_name", "")
                    d["v_path"] = item.get("v_path", "")
                    # Default count to 1 if missing/invalid
                    try:
                        d["v_count"] = int(item.get("v_count", "1"))
                    except Exception:
                        d["v_count"] = 1
                # Persist metric choice if present (Weight or Count)
                vm = item.get("v_metric")
                if vm:
                    d["v_metric"] = vm
            # Generic metric/count for all sections (including Vulnerabilities)
            try:
                vm_generic = item.get("v_metric")
                if vm_generic and vm_generic in ("Weight", "Count"):
                    d["v_metric"] = vm_generic
                vc_generic = item.get("v_count")
                if vc_generic is not None:
                    try:
                        d["v_count"] = int(vc_generic)
                    except Exception:
                        d["v_count"] = 1
            except Exception:
                pass
            entry["items"].append(d)
        scen["sections"][name] = entry
        # Capture scenario-level density_count if present on Scenario element once
        if 'density_count' not in scen and se is not None:
            # Attempt to access parent <Scenario> by scanning for attribute on sec's ancestors is not directly supported.
            # Instead, rely on convention: during writing we place density_count on <Scenario>. So parse root manually here.
            try:
                # Walk up by brute force: find the nearest ancestor named 'Scenario'
                # We don't have parent links; reconstruct by searching from current element root.
                root = sec
                while getattr(root, 'tag', None) and root.tag != 'Scenario':
                    # ElementTree lacks parent pointer; break to avoid infinite loop
                    break
            except Exception:
                root = None
        # Fallback: if Node Information section carries density_count/base_nodes and scenario-level missing, propagate to scen.
        if name == 'Node Information' and 'density_count' not in scen:
            dc_attr = sec.get('density_count') or sec.get('base_nodes') or sec.get('total_nodes')
            if dc_attr:
                try:
                    scen['density_count'] = int(dc_attr)
                except Exception:
                    pass
    # Notes
    notes_sec = None
    for sec in se.findall("section"):
        if sec.get("name") == "Notes":
            notes_sec = sec; break
    if notes_sec is not None:
        notes_el = notes_sec.find("notes")
        if notes_el is not None and notes_el.text:
            scen["notes"] = notes_el.text
    return scen


def _build_scenarios_xml(data_dict: dict) -> ET.ElementTree:
    root = ET.Element("Scenarios")
    core_cfg = _normalize_core_config(data_dict.get('core'), include_password=False)
    core_el = ET.SubElement(root, 'CoreConnection')
    core_el.set('host', str(core_cfg.get('host') or ''))
    core_el.set('port', str(core_cfg.get('port') or ''))
    core_el.set('ssh_enabled', 'true' if core_cfg.get('ssh_enabled') else 'false')
    core_el.set('ssh_host', str(core_cfg.get('ssh_host') or core_cfg.get('host') or ''))
    core_el.set('ssh_port', str(core_cfg.get('ssh_port') or ''))
    core_el.set('ssh_username', str(core_cfg.get('ssh_username') or ''))
    for scen in data_dict.get("scenarios", []):
        scen_el = ET.SubElement(root, "Scenario")
        scen_el.set("name", scen.get("name", "Scenario"))
        # Persist scenario-level density_count (Count for Density) so parser priority can pick it up on reload.
        try:
            if 'density_count' in scen and scen.get('density_count') is not None:
                scen_el.set('density_count', str(int(scen.get('density_count'))))
        except Exception:
            pass
        se = ET.SubElement(scen_el, "ScenarioEditor")
        base = ET.SubElement(se, "BaseScenario")
        base.set("filepath", scen.get("base", {}).get("filepath", ""))

        hitl = scen.get("hitl") or {}
        raw_ifaces = hitl.get("interfaces") if isinstance(hitl, dict) else None
        normalized_ifaces: List[Dict[str, Any]] = []
        if isinstance(raw_ifaces, list):
            for entry in raw_ifaces:
                if not entry:
                    continue
                if isinstance(entry, str):
                    normalized_ifaces.append({"name": entry})
                    continue
                if not isinstance(entry, dict):
                    continue
                name = entry.get("name") or entry.get("interface") or entry.get("iface")
                if not name:
                    continue
                clean: Dict[str, Any] = {"name": str(name)}
                for key in ("alias", "display", "description"):
                    val = entry.get(key)
                    if isinstance(val, str) and val.strip():
                        clean["alias"] = val.strip()
                        break
                mac_val = entry.get("mac")
                if isinstance(mac_val, str) and mac_val.strip():
                    clean["mac"] = mac_val.strip()
                clean["attachment"] = _normalize_hitl_attachment(entry.get("attachment"))
                for attr_key in ("ipv4", "ipv6"):
                    val = entry.get(attr_key)
                    if isinstance(val, str):
                        items = [p.strip() for p in val.split(',') if p.strip()]
                        if items:
                            clean[attr_key] = items
                    elif isinstance(val, (list, tuple, set)):
                        items = [str(p).strip() for p in val if str(p).strip()]
                        if items:
                            clean[attr_key] = items
                normalized_ifaces.append(clean)
        for iface in normalized_ifaces:
            if "attachment" not in iface:
                iface["attachment"] = _DEFAULT_HITL_ATTACHMENT

        participant_url_raw = hitl.get("participant_proxmox_url") if isinstance(hitl, dict) else None
        if participant_url_raw not in (None, ''):
            try:
                participant_url = str(participant_url_raw).strip()
            except Exception:
                participant_url = ''
        else:
            participant_url = ''

        if hitl and (hitl.get("enabled") or normalized_ifaces or hitl.get("core") or participant_url):
            hitl_el = ET.SubElement(se, "HardwareInLoop")
            hitl_el.set("enabled", "true" if hitl.get("enabled") else "false")
            if participant_url:
                hitl_el.set('participant_proxmox_url', participant_url)
            hitl_core_cfg = _extract_optional_core_config(hitl.get("core"), include_password=False)
            if hitl_core_cfg:
                hitl_core_el = ET.SubElement(hitl_el, "CoreConnection")
                hitl_core_el.set("host", str(hitl_core_cfg.get("host") or ""))
                hitl_core_el.set("port", str(hitl_core_cfg.get("port") or ""))
                hitl_core_el.set("ssh_enabled", "true" if hitl_core_cfg.get("ssh_enabled") else "false")
                hitl_core_el.set("ssh_host", str(hitl_core_cfg.get("ssh_host") or hitl_core_cfg.get("host") or ""))
                hitl_core_el.set("ssh_port", str(hitl_core_cfg.get("ssh_port") or ""))
                hitl_core_el.set("ssh_username", str(hitl_core_cfg.get("ssh_username") or ""))
                for extra_key, extra_val in hitl_core_cfg.items():
                    if extra_key in {"host", "port", "ssh_enabled", "ssh_host", "ssh_port", "ssh_username", "ssh_password"}:
                        continue
                    try:
                        hitl_core_el.set(str(extra_key), str(extra_val))
                    except Exception:
                        continue
            for iface in normalized_ifaces:
                name = iface.get("name")
                if not name:
                    continue
                iface_el = ET.SubElement(hitl_el, "Interface")
                iface_el.set("name", str(name))
                alias = iface.get("alias")
                if alias:
                    iface_el.set("alias", str(alias))
                if iface.get("mac"):
                    iface_el.set("mac", str(iface["mac"]))
                attachment = _normalize_hitl_attachment(iface.get("attachment"))
                if attachment:
                    iface_el.set("attachment", attachment)
                ipv4_vals = iface.get("ipv4")
                if isinstance(ipv4_vals, (list, tuple, set)):
                    joined = ",".join(str(p) for p in ipv4_vals if p)
                    if joined:
                        iface_el.set("ipv4", joined)
                ipv6_vals = iface.get("ipv6")
                if isinstance(ipv6_vals, (list, tuple, set)):
                    joined6 = ",".join(str(p) for p in ipv6_vals if p)
                    if joined6:
                        iface_el.set("ipv6", joined6)

        # Flag Sequencing flow state (resolved values, chain selections).
        try:
            flow_state = scen.get('flow_state')
            if isinstance(flow_state, dict) and flow_state:
                fs_el = ET.SubElement(se, "FlagSequencing")
                st_el = ET.SubElement(fs_el, "FlowState")
                st_el.text = json.dumps(flow_state, separators=(',', ':'), ensure_ascii=False)
        except Exception:
            pass

        # Plan preview payload (full preview + plan/breakdowns metadata) for UI round-trip.
        try:
            plan_preview = scen.get('plan_preview') or scen.get('planPreview')
            if isinstance(plan_preview, dict) and plan_preview:
                pp_el = ET.SubElement(se, "PlanPreview")
                pp_el.text = json.dumps(plan_preview, separators=(',', ':'), ensure_ascii=False)
        except Exception:
            pass

        order = [
            "Node Information", "Routing", "Services", "Traffic",
            "Events", "Vulnerabilities", "Segmentation", "Notes"
        ]
        combined_host_pool: int | None = None
        scenario_host_additive = 0
        scenario_routing_total = 0
        scenario_vuln_total = 0

        for name in order:
            if name == "Notes":
                sec_el = ET.SubElement(se, "section", name="Notes")
                ne = ET.SubElement(sec_el, "notes")
                ne.text = scen.get("notes", "") or ""
                continue
            sec = scen.get("sections", {}).get(name)
            if not sec:
                continue
            sec_el = ET.SubElement(se, "section", name=name)
            items_list = sec.get("items", []) or []
            if name == "Vulnerabilities":
                sec_el.set("flag_type", str(sec.get("flag_type") or "text"))
            weight_rows = [it for it in items_list if (it.get('v_metric') or (it.get('selected')=='Specific' and name=='Vulnerabilities') or 'Weight') == 'Weight']
            count_rows = [it for it in items_list if (it.get('v_metric') == 'Count') or (name == 'Vulnerabilities' and it.get('selected') == 'Specific')]
            weight_sum = sum(float(it.get('factor', 0) or 0) for it in weight_rows) if weight_rows else 0.0

            if name == "Node Information":
                # Determine if an explicit density_count was provided (scenario-level or legacy section field).
                explicit_density_raw = None
                scen_level_dc = scen.get('density_count')
                if scen_level_dc is not None:
                    explicit_density_raw = scen_level_dc
                else:
                    for legacy_key in ('density_count', 'total_nodes', 'base_nodes'):
                        if sec.get(legacy_key) not in (None, ""):
                            explicit_density_raw = sec.get(legacy_key)
                            break
                density_count: int | None = None
                if explicit_density_raw is not None:
                    try:
                        density_count = max(0, int(explicit_density_raw))
                    except Exception:
                        density_count = 0
                # additive Count rows always additive even if base omitted
                additive_nodes = sum(int(it.get('v_count') or 0) for it in count_rows)
                # For derived counts we only have a combined host pool if explicit density_count provided
                combined_nodes = (density_count or 0) + additive_nodes
                norm_sum = 0.0
                if weight_rows:
                    raw_sum = sum(float(it.get('factor') or 0) for it in weight_rows)
                    if raw_sum > 0:
                        for it in weight_rows:
                            try:
                                it['factor'] = float(it.get('factor') or 0) / raw_sum
                            except Exception:
                                it['factor'] = 0.0
                        norm_sum = 1.0
                    else:
                        weight_rows[0]['factor'] = 1.0
                        for it in weight_rows[1:]:
                            it['factor'] = 0.0
                        norm_sum = 1.0
                # Only persist base-related fields if an explicit density_count was supplied. Otherwise omit so parser can apply default.
                if density_count is not None:
                    sec_el.set("density_count", str(density_count))
                    sec_el.set("base_nodes", str(density_count))
                sec_el.set("additive_nodes", str(additive_nodes))
                if density_count is not None:
                    sec_el.set("combined_nodes", str(combined_nodes))
                sec_el.set("weight_rows", str(len(weight_rows)))
                sec_el.set("count_rows", str(len(count_rows)))
                sec_el.set("weight_sum", f"{weight_sum:.3f}")
                sec_el.set("normalized_weight_sum", f"{norm_sum:.3f}")
                combined_host_pool = combined_nodes if density_count is not None else None
                scenario_host_additive += combined_nodes if density_count is not None else additive_nodes
            else:
                dens = sec.get("density")
                if dens is not None:
                    try:
                        sec_el.set("density", f"{float(dens):.3f}")
                    except Exception:
                        sec_el.set("density", str(dens))
                if name in ("Routing", "Vulnerabilities"):
                    base_pool = combined_host_pool if isinstance(combined_host_pool, int) else None
                    explicit = sum(int(it.get('v_count') or 0) for it in count_rows)
                    derived = 0
                    try:
                        dens_val = float(dens or 0)
                    except Exception:
                        dens_val = 0.0
                    if weight_rows and base_pool and base_pool > 0:
                        if name == 'Routing':
                            if dens_val >= 1:
                                derived = int(round(dens_val))
                            elif dens_val > 0:
                                derived = int(round(base_pool * dens_val))
                        else:  # Vulnerabilities
                            if dens_val > 0:
                                dens_clip = min(1.0, dens_val)
                                derived = int(round(base_pool * dens_clip))
                    total_planned = explicit + derived
                    sec_el.set("explicit_count", str(explicit))
                    sec_el.set("derived_count", str(derived))
                    sec_el.set("total_planned", str(total_planned))
                    sec_el.set("weight_rows", str(len(weight_rows)))
                    sec_el.set("count_rows", str(len(count_rows)))
                    sec_el.set("weight_sum", f"{weight_sum:.3f}")
                    if name == 'Routing':
                        scenario_routing_total += total_planned
                    else:
                        scenario_vuln_total += total_planned
                elif name in ("Services", "Traffic", "Segmentation"):
                    explicit = sum(int(it.get('v_count') or 0) for it in count_rows)
                    sec_el.set("explicit_count", str(explicit))
                    sec_el.set("weight_rows", str(len(weight_rows)))
                    sec_el.set("count_rows", str(len(count_rows)))
                    sec_el.set("weight_sum", f"{weight_sum:.3f}")

            for item in items_list:
                it = ET.SubElement(sec_el, "item")
                sel_for_xml = str(item.get('selected', 'Random'))
                # UI label "Category" maps to schema label "Type/Vector".
                if name == 'Vulnerabilities' and sel_for_xml == 'Category':
                    sel_for_xml = 'Type/Vector'
                it.set("selected", sel_for_xml)
                try:
                    it.set("factor", f"{float(item.get('factor', 1.0)):.3f}")
                except Exception:
                    it.set("factor", "0.000")
                if name == 'Routing':
                    em = (item.get('r2r_mode') or '').strip()
                    r2s_mode = (item.get('r2s_mode') or '').strip()
                    if em:
                        it.set('r2r_mode', em)
                    if r2s_mode:
                        it.set('r2s_mode', r2s_mode)
                    # Persist edge budget hints when provided (including Uniform/NonUniform / aggregate modes)
                    try:
                        ev_raw = item.get('r2r_edges') or item.get('edges')
                        if em == 'Exact' and ev_raw is not None and str(ev_raw).strip() != '':
                            ev = int(ev_raw)
                            if ev > 0:  # only meaningful positive degrees
                                it.set('r2r_edges', str(ev))
                    except Exception:
                        pass
                    try:
                        r2s_raw = item.get('r2s_edges')
                        if r2s_raw is not None and str(r2s_raw).strip() != '':
                            ev2 = int(r2s_raw)
                            if ev2 >= 0:
                                it.set('r2s_edges', str(ev2))
                    except Exception:
                        pass
                    # Persist per-item host grouping bounds if provided (non-empty and >=0)
                    try:
                        hmin_raw = item.get('r2s_hosts_min')
                        if hmin_raw not in (None, ''):
                            hmin_val = int(hmin_raw)
                            if hmin_val >= 0:
                                it.set('r2s_hosts_min', str(hmin_val))
                    except Exception:
                        pass
                    try:
                        hmax_raw = item.get('r2s_hosts_max')
                        if hmax_raw not in (None, ''):
                            hmax_val = int(hmax_raw)
                            if hmax_val >= 0:
                                it.set('r2s_hosts_max', str(hmax_val))
                    except Exception:
                        pass
                    # If still absent, write explicit defaults (UI defaults 1 and 4) for deterministic round-trip
                    if 'r2s_hosts_min' not in it.attrib:
                        it.set('r2s_hosts_min', '1')
                    if 'r2s_hosts_max' not in it.attrib:
                        it.set('r2s_hosts_max', '4')
                if name == 'Events':
                    sp = item.get('script_path') or ''
                    if sp:
                        it.set('script_path', sp)
                if name == 'Traffic':
                    it.set('pattern', str(item.get('pattern', 'continuous')))
                    it.set('rate_kbps', f"{float(item.get('rate_kbps', 64.0)):.1f}")
                    it.set('period_s', f"{float(item.get('period_s', 1.0)):.1f}")
                    it.set('jitter_pct', f"{float(item.get('jitter_pct', 10.0)):.1f}")
                    ct = (item.get('content_type') or item.get('content') or '').strip()
                    if ct:
                        it.set('content_type', ct)
                if name == 'Vulnerabilities':
                    sel = sel_for_xml
                    if sel in ('Type/Vector', 'Category'):
                        vt = item.get('v_type')
                        vv = item.get('v_vector')
                        if vt:
                            it.set('v_type', str(vt))
                        if vv:
                            it.set('v_vector', str(vv))
                    elif sel == 'Specific':
                        vn = item.get('v_name')
                        vp = item.get('v_path')
                        if vn:
                            it.set('v_name', str(vn))
                        if vp:
                            it.set('v_path', str(vp))
                vm_any = item.get('v_metric')
                if vm_any:
                    it.set('v_metric', str(vm_any))
                if (item.get('v_metric') == 'Count') or (name == 'Vulnerabilities' and str(item.get('selected', '')) == 'Specific'):
                    vc_any = item.get('v_count')
                    try:
                        if vc_any is not None:
                            it.set('v_count', str(int(vc_any)))
                    except Exception:
                        pass

        # Final scenario-level aggregate
        try:
            total_nodes = scenario_host_additive + scenario_routing_total + scenario_vuln_total
            scen_el.set('scenario_total_nodes', str(total_nodes))
            scen_el.set('base_nodes', '0')
        except Exception:
            pass

    return ET.ElementTree(root)


def _validate_core_xml(xml_path: str):
    """Validate the scenario XML against the CORE XML XSD. Returns (ok, errors_text)."""
    try:
        # Derive project root relative to this file (../) then the validation directory
        here = os.path.abspath(os.path.dirname(__file__))
        repo_root = os.path.abspath(os.path.join(here, '..'))
        xsd_path = os.path.join(repo_root, 'validation', 'core-xml-syntax', 'corexml_codebased.xsd')
        # Fallback: if not found, try relative to current working directory (for unusual run contexts)
        if not os.path.exists(xsd_path):
            alt = os.path.abspath(os.path.join(os.getcwd(), 'validation', 'core-xml-syntax', 'corexml_codebased.xsd'))
            if os.path.exists(alt):
                xsd_path = alt
        if not os.path.exists(xsd_path):
            return False, f"Schema not found: {xsd_path}"
        with open(xsd_path, 'rb') as f:
            schema_doc = LET.parse(f)
        schema = LET.XMLSchema(schema_doc)
        # Read original XML; if it contains any <container> elements (session export artifacts),
        # strip them prior to validation so that user-provided or auto-exported session XML can
        # still be validated against the scenario schema. This addresses UI errors like:
        #   Element 'container': This element is not expected.
        # We purposefully do NOT mutate the source file on disk; sanitization is in-memory.
        try:
            raw_tree = LET.parse(xml_path)
            root = raw_tree.getroot()
            # Collect and remove any elements whose local-name is 'container'
            containers = root.xpath('.//*[local-name()="container"]')
            if containers:
                for el in containers:
                    parent = el.getparent()
                    if parent is not None:
                        parent.remove(el)
                # Validate sanitized tree
                try:
                    schema.assertValid(root)
                    return True, ''
                except LET.DocumentInvalid as e:
                    # Fall through to structured error collection below
                    err_log = e.error_log
                    lines = [f"{er.level_name} L{er.line}:C{er.column} - {er.message}" for er in err_log]
                    return False, "\n".join(lines) or str(e)
            else:
                # No <container>; validate normally using parser bound to schema for speed
                parser = LET.XMLParser(schema=schema)
                LET.parse(xml_path, parser)
                return True, ''
        except LET.XMLSyntaxError as e:  # low-level parse error before schema phase
            lines = []
            for err in e.error_log:
                lines.append(f"{err.level_name} L{err.line}:C{err.column} - {err.message}")
            return False, "\n".join(lines) or str(e)
    except LET.XMLSyntaxError as e:
        lines = []
        for err in e.error_log:
            lines.append(f"{err.level_name} L{err.line}:C{err.column} - {err.message}")
        return False, "\n".join(lines) or str(e)
    except Exception as e:
        return False, str(e)


def _analyze_core_xml(xml_path: str) -> Dict[str, Any]:
    """Extract a topology summary from a CORE session/scenario XML."""
    info: Dict[str, Any] = {}
    try:
        tree = LET.parse(xml_path)
        root = tree.getroot()

        def attrs(el, *names):
            return {n: el.get(n) for n in names if el.get(n) is not None}

        def local(tag: str) -> str:
            if not tag:
                return ''
            if '}' in tag:
                return tag.split('}', 1)[1]
            return tag

        def iter_by_local(el, lname: str):
            lname = lname.lower()
            for e in el.iter():
                if local(getattr(e, 'tag', '')).lower() == lname:
                    yield e

        # Combine device/node representations (session exports sometimes use <node>)
        candidates = list(iter_by_local(root, 'device')) + list(iter_by_local(root, 'node'))
        devices: list[Any] = []
        seen_ids: set[str] = set()
        for cand in candidates:
            ident = cand.get('id') or cand.get('name')
            key = str(ident).strip() if ident is not None else ''
            if key and key in seen_ids:
                continue
            if key:
                seen_ids.add(key)
            devices.append(cand)

        networks = list(iter_by_local(root, 'network'))
        links = list(iter_by_local(root, 'link'))
        services = list(iter_by_local(root, 'service'))

        routing_edge_policies: list[dict] = []
        try:
            for sec in root.findall('.//section'):
                if (sec.get('name') or '').strip() != 'Routing':
                    continue
                for item in sec.findall('./item'):
                    r2r_edges = item.get('r2r_edges') or item.get('edges')
                    r2s_edges = item.get('r2s_edges')
                    if any([item.get('r2r_mode'), r2r_edges, item.get('r2s_mode'), r2s_edges]):
                        routing_edge_policies.append({
                            'r2r_mode': item.get('r2r_mode') or '',
                            'r2r_edges': int(r2r_edges) if (r2r_edges and r2r_edges.isdigit()) else None,
                            'r2s_mode': item.get('r2s_mode') or '',
                            'r2s_edges': int(r2s_edges) if (r2s_edges and r2s_edges.isdigit()) else None,
                            'protocol': item.get('selected') or '',
                        })
        except Exception:
            routing_edge_policies = []

        interface_store: Dict[str, Dict[str, Dict[str, Any]]] = defaultdict(dict)

        def record_interface(node_ref: Any, iface_el: Any) -> None:
            node_key = str(node_ref or '').strip()
            if not node_key or iface_el is None:
                return
            try:
                attrs_iface = dict(getattr(iface_el, 'attrib', {}) or {})
            except Exception:
                attrs_iface = {}
            name_raw = (attrs_iface.get('name') or attrs_iface.get('id') or '').strip()
            if not name_raw:
                name_el = iface_el.find('./name') if hasattr(iface_el, 'find') else None
                if name_el is not None and getattr(name_el, 'text', None):
                    name_raw = name_el.text.strip()
            mac = (attrs_iface.get('mac') or '').strip()
            if not mac and hasattr(iface_el, 'find'):
                mac_el = iface_el.find('./mac')
                if mac_el is not None and getattr(mac_el, 'text', None):
                    mac = mac_el.text.strip()
            ip4 = (attrs_iface.get('ip4') or attrs_iface.get('ipv4') or '').strip()
            ip4_mask = (attrs_iface.get('ip4_mask') or attrs_iface.get('ipv4_mask') or '').strip()
            ip6 = (attrs_iface.get('ip6') or attrs_iface.get('ipv6') or '').strip()
            ip6_mask = (attrs_iface.get('ip6_mask') or attrs_iface.get('ipv6_mask') or '').strip()
            try:
                for addr in iface_el.findall('.//addr'):
                    addr_type = (addr.get('type') or addr.get('family') or '').lower()
                    addr_val = (addr.get('address') or addr.get('ip') or (addr.text or '')).strip()
                    mask_val = (addr.get('mask') or addr.get('prefix') or addr.get('netmask') or '').strip()
                    if not addr_val:
                        continue
                    if '6' in addr_type:
                        if not ip6:
                            ip6 = addr_val
                        if not ip6_mask:
                            ip6_mask = mask_val
                    else:
                        if not ip4:
                            ip4 = addr_val
                        if not ip4_mask:
                            ip4_mask = mask_val
                for addr in iface_el.findall('.//address'):
                    addr_val = (addr.get('value') or addr.get('address') or (addr.text or '')).strip()
                    mask_val = (addr.get('mask') or addr.get('prefix') or '').strip()
                    addr_type = (addr.get('type') or '').lower()
                    if not addr_val:
                        continue
                    if '6' in addr_type:
                        if not ip6:
                            ip6 = addr_val
                        if not ip6_mask:
                            ip6_mask = mask_val
                    else:
                        if not ip4:
                            ip4 = addr_val
                        if not ip4_mask:
                            ip4_mask = mask_val
            except Exception:
                pass
            if not any([name_raw, mac, ip4, ip6]):
                return
            stable_key = '|'.join([
                name_raw.lower(),
                ip4,
                ip6,
            ])
            existing = interface_store[node_key].get(stable_key)
            if existing:
                if mac and not existing.get('mac'):
                    existing['mac'] = mac
                if ip4_mask and not existing.get('ipv4_mask'):
                    existing['ipv4_mask'] = ip4_mask
                if ip6_mask and not existing.get('ipv6_mask'):
                    existing['ipv6_mask'] = ip6_mask
                if name_raw and not existing.get('name'):
                    existing['name'] = name_raw
                return
            interface_store[node_key][stable_key] = {
                'name': name_raw or None,
                'mac': mac or None,
                'ipv4': ip4 or None,
                'ipv4_mask': ip4_mask or None,
                'ipv6': ip6 or None,
                'ipv6_mask': ip6_mask or None,
            }

        id_to_name: Dict[str, str] = {}
        id_to_type: Dict[str, str] = {}
        id_to_services: Dict[str, list] = {}

        def coerce_device_id(raw_id: Any, fallback_index: int) -> str:
            value = str(raw_id).strip() if raw_id is not None else ''
            if value:
                return value
            return f"device_{fallback_index}"

        for idx, dev in enumerate(devices, start=1):
            did = coerce_device_id(dev.get('id') or dev.get('name'), idx)
            name_val = (dev.get('name') or '').strip()
            if not name_val and hasattr(dev, 'find'):
                name_el = dev.find('./name')
                if name_el is not None and getattr(name_el, 'text', None):
                    name_val = name_el.text.strip()
            type_val = (dev.get('type') or '').strip()
            if not type_val and hasattr(dev, 'find'):
                type_el = dev.find('./type') or dev.find('./model') or dev.find('./icon')
                if type_el is not None and getattr(type_el, 'text', None):
                    type_val = type_el.text.strip()
            id_to_name[did] = name_val or did
            id_to_type[did] = type_val or ''

            services_found: set[str] = set()
            try:
                for svc in dev.findall('./services/service'):
                    nm = (svc.get('name') or (svc.text or '')).strip()
                    if nm:
                        services_found.add(nm)
                for svc in dev.findall('./service'):
                    nm = (svc.get('name') or (svc.text or '')).strip()
                    if nm:
                        services_found.add(nm)
            except Exception:
                pass
            id_to_services[did] = sorted(services_found)

            try:
                for iface in dev.findall('.//interface'):
                    record_interface(did, iface)
                for iface in dev.findall('.//iface'):
                    record_interface(did, iface)
            except Exception:
                pass

        adj: Dict[str, set[str]] = defaultdict(set)

        def normalize_ref(value: Any) -> str:
            return str(value).strip() if value is not None else ''

        for link in links:
            n1 = normalize_ref(link.get('node1') or link.get('node1_id'))
            n2 = normalize_ref(link.get('node2') or link.get('node2_id'))
            if not n1 or not n2:
                try:
                    if not n1 and hasattr(link, 'find'):
                        iface1 = link.find('.//iface1')
                        if iface1 is not None:
                            n1 = normalize_ref(iface1.get('node') or iface1.get('device') or iface1.get('node_id'))
                    if not n2 and hasattr(link, 'find'):
                        iface2 = link.find('.//iface2')
                        if iface2 is not None:
                            n2 = normalize_ref(iface2.get('node') or iface2.get('device') or iface2.get('node_id'))
                except Exception:
                    pass
            if n1 and n2:
                adj[n1].add(n2)
                adj[n2].add(n1)
            try:
                for child in list(link):
                    tag = local(getattr(child, 'tag', '')).lower()
                    target = None
                    if tag in ('iface1', 'interface1'):
                        target = n1
                    elif tag in ('iface2', 'interface2'):
                        target = n2
                    elif tag in ('iface', 'interface'):
                        target = normalize_ref(child.get('node') or child.get('node_id') or child.get('device')) or n2
                    if target:
                        record_interface(target, child)
            except Exception:
                continue

        nodes: list[dict] = []
        for idx, dev in enumerate(devices, start=1):
            did = coerce_device_id(dev.get('id') or dev.get('name'), idx)
            raw_ifaces = interface_store.get(did, {})
            iface_entries = []
            for entry in raw_ifaces.values():
                cleaned = {k: v for k, v in entry.items() if v not in (None, '')}
                if cleaned:
                    iface_entries.append(cleaned)
            iface_entries.sort(key=lambda e: ((e.get('name') or '').lower(), e.get('ipv4') or '', e.get('ipv6') or ''))
            nodes.append({
                'id': did,
                'name': id_to_name.get(did, did),
                'type': id_to_type.get(did, ''),
                'services': id_to_services.get(did, []),
                'linked_nodes': [],
                'interfaces': iface_entries,
            })

        switches = [n for n in nodes if (n.get('type') or '').lower() == 'switch']
        extra_switch_nodes: list[dict] = []
        extra_hitl_network_nodes: list[dict] = []
        hitl_network_markers = ('rj45', 'rj-45', 'hitl', 'tap', 'bridge', 'ethernet', 'physical')
        try:
            for net in networks:
                ntype = (net.get('type') or '').lower()
                is_switch_candidate = 'switch' in ntype
                is_hitl_candidate = any(marker in ntype for marker in hitl_network_markers)
                if not is_switch_candidate and not is_hitl_candidate:
                    continue
                sw_id = normalize_ref(net.get('id') or net.get('name'))
                if not sw_id:
                    continue
                sw_name = (net.get('name') or sw_id).strip() or sw_id
                if is_switch_candidate and any(sw_id == sw.get('id') or sw_name == sw.get('name') for sw in switches):
                    continue
                if is_hitl_candidate and any(sw_id == n.get('id') or sw_name == n.get('name') for n in nodes):
                    continue
                raw_ifaces = interface_store.get(sw_id, {})
                iface_entries = []
                for entry in raw_ifaces.values():
                    cleaned = {k: v for k, v in entry.items() if v not in (None, '')}
                    if cleaned:
                        iface_entries.append(cleaned)
                iface_entries.sort(key=lambda e: ((e.get('name') or '').lower(), e.get('ipv4') or '', e.get('ipv6') or ''))
                if is_switch_candidate:
                    extra_switch = {
                        'id': sw_id,
                        'name': sw_name,
                        'type': 'switch',
                        'services': [],
                        'linked_nodes': [],
                        'interfaces': iface_entries,
                    }
                    switches.append(extra_switch)
                    extra_switch_nodes.append(extra_switch)
                    id_to_name.setdefault(sw_id, sw_name)
                    id_to_type.setdefault(sw_id, 'switch')
                elif is_hitl_candidate:
                    extra_hitl = {
                        'id': sw_id,
                        'name': sw_name,
                        'type': net.get('type') or 'rj45',
                        'services': [],
                        'linked_nodes': [],
                        'interfaces': iface_entries,
                    }
                    extra_hitl_network_nodes.append(extra_hitl)
                    id_to_name.setdefault(sw_id, sw_name)
                    id_to_type.setdefault(sw_id, extra_hitl['type'].lower())
        except Exception:
            pass

        valid_ids: set[str] = {n['id'] for n in nodes}
        valid_ids.update(sw['id'] for sw in extra_switch_nodes)
        valid_ids.update(hitl['id'] for hitl in extra_hitl_network_nodes)
        adj_clean: Dict[str, set[str]] = {}
        for nid, neighbors in adj.items():
            if nid not in valid_ids:
                continue
            adj_clean[nid] = {nbr for nbr in neighbors if nbr in valid_ids}
        for sw in extra_switch_nodes:
            adj_clean.setdefault(sw['id'], set())
        for hitl in extra_hitl_network_nodes:
            adj_clean.setdefault(hitl['id'], set())

        def _prune_neighbors(node_id: str, node_type: str) -> None:
            if node_type in ('router', 'switch'):
                return
            current_neighbors = sorted(adj_clean.get(node_id, set()), key=lambda vid: id_to_name.get(vid, vid).lower())
            routers = [vid for vid in current_neighbors if (id_to_type.get(vid, '').lower() == 'router')]
            switches_local = [vid for vid in current_neighbors if (id_to_type.get(vid, '').lower() == 'switch')]

            def _trim_group(group: list[str]) -> None:
                if len(group) <= 1:
                    return
                keep = group[0]
                for extra in group[1:]:
                    adj_clean.get(node_id, set()).discard(extra)
                    if extra in adj_clean:
                        adj_clean[extra].discard(node_id)

            _trim_group(routers)
            _trim_group(switches_local)

        for node in nodes:
            _prune_neighbors(node['id'], (node.get('type') or '').lower())
        for sw in extra_switch_nodes:
            _prune_neighbors(sw['id'], (sw.get('type') or '').lower())
        for hitl in extra_hitl_network_nodes:
            _prune_neighbors(hitl['id'], (hitl.get('type') or '').lower())

        def _neighbor_names(node_id: str) -> list[str]:
            neighbors = sorted(adj_clean.get(node_id, set()), key=lambda vid: id_to_name.get(vid, vid).lower())
            return [id_to_name.get(vid, vid) for vid in neighbors]

        filtered_nodes: list[dict] = []
        important_types = {'router', 'switch', 'rj45', 'rj-45', 'tap', 'bridge'}
        for node in nodes:
            nid = node['id']
            linked = adj_clean.get(nid, set())
            node_type = (node.get('type') or '').lower()
            if linked or (node.get('interfaces') and len(node.get('interfaces')) > 0) or node_type in important_types:
                node['linked_nodes'] = _neighbor_names(nid)
                filtered_nodes.append(node)
        nodes = filtered_nodes

        filtered_extra_switch_nodes: list[dict] = []
        for sw in extra_switch_nodes:
            nid = sw['id']
            linked = adj_clean.get(nid, set())
            if linked:
                sw['linked_nodes'] = _neighbor_names(nid)
                filtered_extra_switch_nodes.append(sw)
        extra_switch_nodes = filtered_extra_switch_nodes

        filtered_extra_hitl_nodes: list[dict] = []
        for hitl in extra_hitl_network_nodes:
            nid = hitl['id']
            linked = adj_clean.get(nid, set())
            if linked:
                hitl['linked_nodes'] = _neighbor_names(nid)
                filtered_extra_hitl_nodes.append(hitl)
        extra_hitl_network_nodes = filtered_extra_hitl_nodes

        valid_ids = {n['id'] for n in nodes}
        valid_ids.update(sw['id'] for sw in extra_switch_nodes)
        valid_ids.update(hitl['id'] for hitl in extra_hitl_network_nodes)
        adj_clean = {nid: {nbr for nbr in neighbors if nbr in valid_ids} for nid, neighbors in adj_clean.items() if nid in valid_ids}

        switches = [n for n in nodes if (n.get('type') or '').lower() == 'switch']
        hitl_markers = ('rj45', 'hitl', 'tap', 'bridge', 'ethernet', 'rj-45')

        def _is_hitl_node(node: dict) -> bool:
            node_type = (node.get('type') or '').lower()
            if node_type and any(marker in node_type for marker in hitl_markers):
                return True
            for svc in node.get('services') or []:
                svc_name = (svc or '').lower()
                if any(marker in svc_name for marker in hitl_markers):
                    return True
            node_name = (node.get('name') or '').lower()
            if node_name and any(marker in node_name for marker in ('rj45', 'hitl')):
                return True
            return False

        hitl_nodes: list[dict] = []
        for node in nodes:
            if _is_hitl_node(node):
                node['is_hitl'] = True
                hitl_nodes.append(node)
        for hitl in extra_hitl_network_nodes:
            hitl['is_hitl'] = True
            hitl_nodes.append(hitl)

        link_details: list[dict] = []
        seen_pairs: set[tuple[str, str]] = set()
        for src, neighbors in adj_clean.items():
            for dst in neighbors:
                if src == dst or dst not in valid_ids:
                    continue
                ordered = tuple(sorted((src, dst)))
                if ordered in seen_pairs:
                    continue
                seen_pairs.add(ordered)
                link_details.append({
                    'node1': ordered[0],
                    'node2': ordered[1],
                    'node1_name': id_to_name.get(ordered[0], ordered[0]),
                    'node2_name': id_to_name.get(ordered[1], ordered[1]),
                })

        info.update({
            'nodes_count': len(nodes),
            'networks_count': len(networks),
            'links_count': len(link_details),
            'services_count': len(services),
            'nodes': nodes,
            'switches_count': len(switches) + len(extra_switch_nodes),
            'switches_device_count': len(switches),
            'switches': [sw['name'] for sw in switches] + [sw['name'] for sw in extra_switch_nodes],
            'switch_nodes': extra_switch_nodes,
            'hitl_network_nodes': extra_hitl_network_nodes,
            'hitl_nodes': hitl_nodes,
            'hitl_nodes_count': len(hitl_nodes),
            'links_detail': link_details,
            'routing_edges_policies': routing_edge_policies,
        })
        info['devices'] = [attrs(d, 'id', 'name', 'type', 'class', 'image') for d in devices[:50]]
        info['networks'] = [attrs(n, 'id', 'name', 'type', 'model', 'mobility') for n in networks[:50]]
        info['links_sample'] = len(link_details)

        try:
            st = os.stat(xml_path)
            info['file_size_bytes'] = st.st_size
        except Exception:
            pass

        try:
            router_ids = [normalize_ref(dev.get('id')) for dev in devices if (dev.get('type') or '').lower().find('router') >= 0]
            degs = {rid: len(adj_clean.get(rid, set())) for rid in router_ids if rid}
            if degs:
                vals = list(degs.values())
                info['router_degree_stats'] = {
                    'min': min(vals),
                    'max': max(vals),
                    'avg': round(sum(vals) / len(vals), 2),
                    'per_router': degs,
                }
        except Exception:
            pass

        return info
    except Exception as e:
        return {'error': str(e)}


def _summarize_planner_scenarios(xml_path: str) -> Dict[str, Any]:
    """Create a lightweight summary for Scenario Editor bundles (root <Scenarios>)."""
    summary: Dict[str, Any] = {'__planner_bundle': True, 'scenarios': [], 'scenarios_count': 0}
    try:
        payload = _parse_scenarios_xml(xml_path)
    except Exception as exc:
        summary['error'] = str(exc)
        return summary
    scen_list = payload.get('scenarios') or []
    for scen in scen_list:
        if not isinstance(scen, dict):
            continue
        sections_meta = scen.get('sections') if isinstance(scen.get('sections'), dict) else {}
        sections_summary: List[Dict[str, Any]] = []
        for sec_name, sec_val in sections_meta.items():
            if not isinstance(sec_val, dict):
                continue
            sections_summary.append({
                'name': sec_name,
                'item_count': len(sec_val.get('items') or []),
                'density': sec_val.get('density'),
                'total_nodes': sec_val.get('total_nodes'),
            })
        summary['scenarios'].append({
            'name': scen.get('name') or 'Scenario',
            'density_count': scen.get('density_count'),
            'scenario_total_nodes': scen.get('scenario_total_nodes'),
            'base_nodes': scen.get('base_nodes'),
            'hitl_enabled': bool(((scen.get('hitl') or {}).get('enabled'))),
            'base_file': ((scen.get('base') or {}).get('filepath') or ''),
            'sections': sections_summary,
        })
    summary['scenarios_count'] = len(summary['scenarios'])
    core_meta = payload.get('core') if isinstance(payload.get('core'), dict) else None
    if core_meta:
        summary['core'] = core_meta
    return summary


@app.route('/', methods=['GET'])
def index():
    current = _current_user()
    scenario_query = ''
    try:
        scenario_query = (request.args.get('scenario') or '').strip()
    except Exception:
        scenario_query = ''
    if current and _is_participant_role(current.get('role')):
        target_args = {'scenario': scenario_query} if scenario_query else {}
        return redirect(url_for('participant_ui_page', **target_args))
    # Admin Scenarios editor should list the same scenarios as CORE/Reports.
    # Start from catalog names so the left-hand scenario list includes saved-but-not-executed
    # scenarios like "Scenario 1b".
    payload = _default_scenarios_payload()
    force_empty = False
    try:
        role = _normalize_role_value(current.get('role')) if current else ''
        force_empty = _scenario_catalog_force_empty()
        if role == 'admin' and not force_empty:
            scenario_names, _scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(None, user=current)
            if scenario_names:
                payload = _default_scenarios_payload_for_names(scenario_names)
                payload['_catalog_stub'] = True
    except Exception:
        payload = _default_scenarios_payload()
    # Reconstruct base_upload if base filepath already present
    _attach_base_upload(payload)
    _hydrate_base_upload_from_disk(payload)
    payload['host_interfaces'] = _enumerate_host_interfaces()
    if payload.get('base_upload'):
        _save_base_upload_state(payload['base_upload'])
    payload = _prepare_payload_for_index(payload, user=current)
    if scenario_query:
        payload['scenario_query'] = scenario_query
    if payload.get('result_path') and not payload.get('project_key_hint'):
        payload['project_key_hint'] = payload['result_path']
    if force_empty:
        _delete_editor_state_snapshot(current)
    else:
        snapshot = _load_editor_state_snapshot(current)
        if snapshot:
            payload['editor_snapshot'] = snapshot
            if snapshot.get('project_key_hint') and not payload.get('project_key_hint'):
                payload['project_key_hint'] = snapshot.get('project_key_hint')
            if snapshot.get('scenario_query') and not payload.get('scenario_query'):
                payload['scenario_query'] = snapshot.get('scenario_query')

    # Always prefer XML as the single source of truth for Topology state.
    # Load the latest scenario XML (non-autosave when available) and hydrate
    # the UI payload from it, regardless of any editor snapshot contents.
    try:
        def _scenario_is_skeletal(s: dict[str, Any]) -> bool:
            try:
                secs = s.get('sections') if isinstance(s, dict) else None
                if not isinstance(secs, dict) or not secs:
                    return True
                # If all sections lack items, treat as skeletal.
                for v in secs.values():
                    if not isinstance(v, dict):
                        continue
                    items = v.get('items') if isinstance(v.get('items'), list) else []
                    if items:
                        return False
                return True
            except Exception:
                return True

        def _best_xml_for_index() -> str:
            xml_hint = ''
            if scenario_query:
                xml_hint = _latest_xml_path_for_scenario(_normalize_scenario_label(scenario_query)) or ''
            if not xml_hint:
                snap = payload.get('editor_snapshot') if isinstance(payload.get('editor_snapshot'), dict) else {}
                idx = None
                try:
                    idx = snap.get('active_index')
                    if isinstance(idx, str) and idx.isdigit():
                        idx = int(idx)
                except Exception:
                    idx = None
                try:
                    by_index = snap.get('saved_xml_paths_by_index') if isinstance(snap.get('saved_xml_paths_by_index'), list) else []
                    if isinstance(idx, int) and 0 <= idx < len(by_index):
                        cand = str(by_index[idx] or '').strip()
                        if cand and cand.lower().endswith('.xml'):
                            xml_hint = cand
                except Exception:
                    xml_hint = ''
                if xml_hint and _is_autosave_xml_path(xml_hint):
                    try:
                        scenarios_list = payload.get('scenarios') if isinstance(payload.get('scenarios'), list) else []
                        if isinstance(idx, int) and 0 <= idx < len(scenarios_list) and isinstance(scenarios_list[idx], dict):
                            nm = str(scenarios_list[idx].get('name') or '').strip()
                            if nm:
                                preferred = _latest_xml_path_for_scenario(_normalize_scenario_label(nm)) or ''
                                if preferred and not _is_autosave_xml_path(preferred):
                                    xml_hint = preferred
                    except Exception:
                        pass
            if not xml_hint:
                try:
                    scenarios_list = payload.get('scenarios') if isinstance(payload.get('scenarios'), list) else []
                    first_name = ''
                    if scenarios_list and isinstance(scenarios_list[0], dict):
                        first_name = str(scenarios_list[0].get('name') or '').strip()
                    if first_name:
                        xml_hint = _latest_xml_path_for_scenario(_normalize_scenario_label(first_name)) or ''
                except Exception:
                    xml_hint = ''
            if xml_hint and _is_autosave_xml_path(xml_hint):
                # Prefer non-autosave XML when available.
                try:
                    if scenario_query:
                        preferred = _latest_xml_path_for_scenario(_normalize_scenario_label(scenario_query)) or ''
                        if preferred and not _is_autosave_xml_path(preferred):
                            xml_hint = preferred
                except Exception:
                    pass
            return xml_hint

        xml_hint = _best_xml_for_index()
        if xml_hint and os.path.exists(xml_hint):
            parsed = _parse_scenarios_xml(xml_hint)
            if isinstance(parsed, dict) and isinstance(parsed.get('scenarios'), list) and parsed.get('scenarios'):
                payload['scenarios'] = parsed.get('scenarios')
                if isinstance(parsed.get('core'), dict):
                    payload['core'] = parsed.get('core')
                payload['result_path'] = xml_hint
                if not payload.get('project_key_hint'):
                    payload['project_key_hint'] = xml_hint
                payload['_xml_forced'] = True
                try:
                    app.logger.info('[index] forced scenarios from xml: %s', xml_hint)
                except Exception:
                    pass
    except Exception:
        pass

    # If the payload has no scenarios (or only skeletal scenarios) after refresh,
    # attempt to hydrate from the latest saved XML path (non-autosave) so the
    # Topology editor is not blank.
    try:
        if not payload.get('_xml_forced'):
            scenarios_present = isinstance(payload.get('scenarios'), list) and bool(payload.get('scenarios'))
            scenarios_list = payload.get('scenarios') if isinstance(payload.get('scenarios'), list) else []
            has_only_skeletal = bool(scenarios_list) and all(_scenario_is_skeletal(s) for s in scenarios_list if isinstance(s, dict))
            catalog_stub = bool(payload.get('_catalog_stub'))
            if (not scenarios_present) or has_only_skeletal or catalog_stub:
                snap = payload.get('editor_snapshot') if isinstance(payload.get('editor_snapshot'), dict) else {}
                xml_hint = ''
                try:
                    idx = snap.get('active_index')
                    if isinstance(idx, str) and idx.isdigit():
                        idx = int(idx)
                except Exception:
                    idx = None
                try:
                    by_index = snap.get('saved_xml_paths_by_index') if isinstance(snap.get('saved_xml_paths_by_index'), list) else []
                    if isinstance(idx, int) and 0 <= idx < len(by_index):
                        cand = str(by_index[idx] or '').strip()
                        if cand and cand.lower().endswith('.xml') and not _is_autosave_xml_path(cand):
                            xml_hint = cand
                except Exception:
                    xml_hint = ''
                if not xml_hint:
                    try:
                        cand = str(snap.get('result_path') or '').strip()
                        if cand and cand.lower().endswith('.xml') and not _is_autosave_xml_path(cand):
                            xml_hint = cand
                    except Exception:
                        xml_hint = ''
                if not xml_hint:
                    try:
                        cand = str(payload.get('result_path') or '').strip()
                        if cand and cand.lower().endswith('.xml') and not _is_autosave_xml_path(cand):
                            xml_hint = cand
                    except Exception:
                        xml_hint = ''
                if not xml_hint:
                    try:
                        # Best-effort: fall back to latest catalog XML for the first scenario name.
                        first_name = ''
                        if scenarios_list and isinstance(scenarios_list[0], dict):
                            first_name = str(scenarios_list[0].get('name') or '').strip()
                        if first_name:
                            xml_hint = _latest_xml_path_for_scenario(_normalize_scenario_label(first_name)) or ''
                    except Exception:
                        xml_hint = ''

                if xml_hint and os.path.exists(xml_hint):
                    parsed = _parse_scenarios_xml(xml_hint)
                    if isinstance(parsed, dict) and isinstance(parsed.get('scenarios'), list) and parsed.get('scenarios'):
                        payload['scenarios'] = parsed.get('scenarios')
                        if isinstance(parsed.get('core'), dict) and not isinstance(payload.get('core'), dict):
                            payload['core'] = parsed.get('core')
                        payload['result_path'] = xml_hint
                        if not payload.get('project_key_hint'):
                            payload['project_key_hint'] = xml_hint
                        try:
                            app.logger.info('[index] hydrated scenarios from xml: %s', xml_hint)
                        except Exception:
                            pass
    except Exception:
        pass

    # Populate XML preview on initial load so the dock isn't blank after refresh.
    xml_text = ""
    try:
        # Prefer reading an existing persisted XML file if we have a path.
        result_path = payload.get('result_path') if isinstance(payload, dict) else None
        if isinstance(result_path, str) and result_path.strip() and result_path.lower().endswith('.xml'):
            rp = os.path.expanduser(result_path.strip())
            rp = os.path.normpath(rp)
            candidates = [rp]
            try:
                repo_root = _get_repo_root()
                if not os.path.isabs(rp):
                    candidates.append(os.path.abspath(os.path.join(repo_root, rp)))
                if rp.startswith('outputs' + os.sep):
                    candidates.append(os.path.abspath(os.path.join(_outputs_dir(), rp.split(os.sep, 1)[-1])))
            except Exception:
                pass
            chosen = next((p for p in candidates if p and os.path.exists(p)), None)
            if chosen:
                with open(chosen, 'r', encoding='utf-8', errors='ignore') as f:
                    xml_text = f.read()

        # Otherwise, generate a best-effort preview from the in-memory editor state.
        if not xml_text:
            scenarios = payload.get('scenarios') if isinstance(payload, dict) else None
            if isinstance(scenarios, list) and scenarios:
                core_meta = payload.get('core') if isinstance(payload.get('core'), dict) else None
                tree = _build_scenarios_xml({'scenarios': scenarios, 'core': core_meta})
                try:
                    from lxml import etree as LET  # type: ignore

                    raw = ET.tostring(tree.getroot(), encoding='utf-8')
                    lroot = LET.fromstring(raw)
                    pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
                    xml_text = pretty.decode('utf-8', errors='ignore')
                except Exception:
                    xml_text = ET.tostring(tree.getroot(), encoding='unicode')
    except Exception:
        xml_text = ""

    return render_template('index.html', payload=payload, logs="", xml_preview=xml_text)


@app.route('/load_xml', methods=['POST'])
def load_xml():
    user = _current_user()
    file = request.files.get('scenarios_xml')
    if not file or file.filename == '':
        flash('No file selected.')
        return redirect(url_for('index'))
    if not allowed_file(file.filename):
        flash('Invalid file type. Only XML allowed.')
        return redirect(url_for('index'))
    filename = secure_filename(file.filename)
    filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
    os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
    file.save(filepath)
    try:
        payload = _parse_scenarios_xml(filepath)
        # add default CORE connection parameters
        if "core" not in payload:
            payload["core"] = _default_core_dict()
        payload["result_path"] = filepath
        _attach_base_upload(payload)
        _hydrate_base_upload_from_disk(payload)
        payload['host_interfaces'] = _enumerate_host_interfaces()
        if payload.get('base_upload'):
            _save_base_upload_state(payload['base_upload'])
        xml_text = ""
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                xml_text = f.read()
        except Exception:
            xml_text = ""
        payload = _prepare_payload_for_index(payload, user=user)
        snapshot_source = dict(payload)
        snapshot_source['active_index'] = 0
        snapshot_source['project_key_hint'] = payload.get('result_path')
        _persist_editor_state_snapshot(snapshot_source, user=user)
        snapshot = _load_editor_state_snapshot(user)
        if snapshot:
            payload['editor_snapshot'] = snapshot
        return render_template('index.html', payload=payload, logs="", xml_preview=xml_text)
    except Exception as e:
        flash(f'Failed to parse XML: {e}')
        return redirect(url_for('index'))


@app.route('/save_xml', methods=['POST'])
def save_xml():
    data_str = request.form.get('scenarios_json')
    if not data_str:
        flash('No data received.')
        return redirect(url_for('index'))
    user = _current_user()
    try:
        data = json.loads(data_str)
    except Exception as e:
        flash(f'Invalid JSON: {e}')
        return redirect(url_for('index'))
    try:
        active_index = None
        try:
            active_index = int(data.get('active_index')) if 'active_index' in data else None
        except Exception:
            active_index = None
        core_meta = None
        try:
            core_str = request.form.get('core_json')
            if core_str:
                core_meta = json.loads(core_str)
        except Exception:
            core_meta = None
        client_project_hint = (request.form.get('project_key_hint') or '').strip()
        client_scenario_query = (request.form.get('scenario_query') or '').strip()
        normalized_core = _normalize_core_config(core_meta, include_password=True) if core_meta else None
        # Enforce unique scenario names (case-insensitive, trimmed) to prevent confusing overwrites.
        try:
            scenarios_list = data.get('scenarios') or []
            seen: set[str] = set()
            dupes: list[str] = []
            if isinstance(scenarios_list, list):
                for idx, sc in enumerate(scenarios_list):
                    if not isinstance(sc, dict):
                        continue
                    raw_name = (sc.get('name') or '').strip()
                    name = raw_name or f"Scenario {idx + 1}"
                    key = name.casefold()
                    if key in seen:
                        dupes.append(name)
                    else:
                        seen.add(key)
            if dupes:
                pretty = ', '.join(sorted(set(dupes)))
                flash(f'Duplicate scenario names are not allowed: {pretty}')
                return redirect(url_for('index'))
        except Exception:
            # If validation fails unexpectedly, fall through to existing error handling.
            pass
        scenario_count = len(data.get('scenarios') or []) if isinstance(data.get('scenarios'), list) else 0
        scenario_names_desc = []
        try:
            scenario_names_desc = [str((sc or {}).get('name') or '').strip() for sc in (data.get('scenarios') or []) if isinstance(sc, dict)]
        except Exception:
            scenario_names_desc = []
        username = (user or {}).get('username') if isinstance(user, dict) else None
        try:
            app.logger.info(
                '[save_xml] user=%s scen_count=%s active_index=%s project_hint=%s scenario_query=%s names=%s',
                username or 'anonymous',
                scenario_count,
                active_index if active_index is not None else 'none',
                client_project_hint or '<none>',
                client_scenario_query or '<none>',
                ', '.join(name for name in scenario_names_desc if name) or '<unnamed>'
            )
        except Exception:
            pass
        scenarios_list = data.get('scenarios') if isinstance(data.get('scenarios'), list) else []
        ts = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
        out_dir = os.path.join(_outputs_dir(), f'scenarios-{ts}')
        os.makedirs(out_dir, exist_ok=True)
        try:
            legacy_bundle = os.path.join(out_dir, 'scenarios.xml')
            if os.path.exists(legacy_bundle):
                os.remove(legacy_bundle)
        except Exception:
            pass
        scenario_paths_map: dict[str, str] = {}
        active_out_path = None
        if scenarios_list:
            for idx, scen in enumerate(scenarios_list):
                if not isinstance(scen, dict):
                    continue
                raw_name = (scen.get('name') or '').strip()
                display_name = raw_name or f"Scenario {idx + 1}"
                stem = secure_filename(display_name).strip('_-.') or f"Scenario_{idx + 1}"
                out_path = os.path.join(out_dir, f"{stem}.xml")
                # Ensure unique filename in this save directory.
                if os.path.exists(out_path):
                    suffix = 2
                    base = stem
                    while os.path.exists(out_path):
                        stem = f"{base}-{suffix}"
                        out_path = os.path.join(out_dir, f"{stem}.xml")
                        suffix += 1
                try:
                    tree = _build_scenarios_xml({ 'scenarios': [scen], 'core': normalized_core })
                    from lxml import etree as LET  # type: ignore
                    raw = ET.tostring(tree.getroot(), encoding='utf-8')
                    lroot = LET.fromstring(raw)
                    pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
                    with open(out_path, 'wb') as f:
                        f.write(pretty)
                except Exception:
                    try:
                        tree = _build_scenarios_xml({ 'scenarios': [scen], 'core': normalized_core })
                        tree.write(out_path, encoding='utf-8', xml_declaration=True)
                    except Exception:
                        continue
                scenario_paths_map[display_name] = out_path
                if active_index is not None and active_index == idx:
                    active_out_path = out_path
            if active_out_path is None and scenario_paths_map:
                # Default to first scenario when active_index is missing/invalid.
                active_out_path = next(iter(scenario_paths_map.values()))
        else:
            flash('No scenarios to save.')
            return redirect(url_for('index'))
        out_path = active_out_path
        try:
            app.logger.info('[save_xml] wrote %s scenario xml files under %s', len(scenario_paths_map) or 1, out_dir)
        except Exception:
            pass
        # Read back XML content for preview
        xml_text = ""
        try:
            with open(out_path, 'r', encoding='utf-8', errors='ignore') as f:
                xml_text = f.read()
        except Exception:
            xml_text = ""
        try:
            names_for_catalog = [name for name in scenario_names_desc if isinstance(name, str) and name.strip()]
            if names_for_catalog:
                _persist_scenario_catalog(names_for_catalog, source_path=scenario_paths_map or out_path)
        except Exception:
            pass
        if out_path:
            flash(f'Scenarios saved (per-scenario). Active XML: {os.path.basename(out_path)}')
        else:
            flash('Scenarios saved (per-scenario).')
        payload = {
            'scenarios': data.get('scenarios', []),
            'result_path': out_path,
            'core': _normalize_core_config(normalized_core or {}, include_password=False) if normalized_core else _default_core_dict(),
        }
        payload['host_interfaces'] = _enumerate_host_interfaces()
        _attach_base_upload(payload)
        _hydrate_base_upload_from_disk(payload)
        if payload.get('base_upload'):
            _save_base_upload_state(payload['base_upload'])
        payload = _prepare_payload_for_index(payload, user=user)
        if client_project_hint:
            payload['project_key_hint'] = client_project_hint
        if client_scenario_query:
            payload['scenario_query'] = client_scenario_query
        snapshot_source = dict(payload)
        try:
            snapshot_source['scenarios'] = copy.deepcopy(data.get('scenarios') or [])
        except Exception:
            snapshot_source['scenarios'] = data.get('scenarios') or []
        snapshot_source['active_index'] = active_index
        if client_project_hint:
            snapshot_source['project_key_hint'] = client_project_hint
        elif payload.get('project_key_hint'):
            snapshot_source['project_key_hint'] = payload.get('project_key_hint')
        else:
            snapshot_source['project_key_hint'] = payload.get('result_path')
        if client_scenario_query:
            snapshot_source['scenario_query'] = client_scenario_query
        elif payload.get('scenario_query'):
            snapshot_source['scenario_query'] = payload.get('scenario_query')
        _persist_editor_state_snapshot(snapshot_source, user=user)
        snapshot = _load_editor_state_snapshot(user)
        if snapshot:
            payload['editor_snapshot'] = snapshot
        try:
            app.logger.info('[save_xml] success user=%s xml=%s scen_count=%s', username or 'anonymous', out_path, scenario_count)
        except Exception:
            pass
        return render_template('index.html', payload=payload, logs="", xml_preview=xml_text)
    except Exception as e:
        flash(f'Failed to save XML: {e}')
        return redirect(url_for('index'))


@app.route('/save_xml_api', methods=['POST'])
def save_xml_api():
    try:
        user = _current_user()
        data = request.get_json(silent=True) or {}
        scenarios = data.get('scenarios')
        core_meta = data.get('core')
        normalized_core = _normalize_core_config(core_meta, include_password=True) if isinstance(core_meta, (dict, list)) or core_meta else None
        raw_project_hint = data.get('project_key_hint') if isinstance(data, dict) else None
        project_key_hint = raw_project_hint.strip() if isinstance(raw_project_hint, str) else ''
        raw_scenario_query = data.get('scenario_query') if isinstance(data, dict) else None
        scenario_query_hint = raw_scenario_query.strip() if isinstance(raw_scenario_query, str) else ''
        active_index = None
        try:
            active_index = int(data.get('active_index')) if 'active_index' in data else None
        except Exception:
            active_index = None
        if not isinstance(scenarios, list):
            return jsonify({ 'ok': False, 'error': 'Invalid payload (scenarios list required)' }), 400
        # Enforce unique scenario names (case-insensitive, trimmed).
        try:
            seen: set[str] = set()
            dupes: list[str] = []
            for idx, sc in enumerate(scenarios):
                if not isinstance(sc, dict):
                    continue
                raw_name = (sc.get('name') or '').strip()
                name = raw_name or f"Scenario {idx + 1}"
                key = name.casefold()
                if key in seen:
                    dupes.append(name)
                else:
                    seen.add(key)
            if dupes:
                pretty = ', '.join(sorted(set(dupes)))
                return jsonify({ 'ok': False, 'error': f'Duplicate scenario names are not allowed: {pretty}' }), 400
        except Exception:
            # Best-effort validation; if it fails, continue with existing behavior.
            pass
        scenario_names: list[str] = []
        try:
            scenario_names = [str((s or {}).get('name') or '').strip() for s in scenarios if isinstance(s, dict)]
        except Exception:
            scenario_names = []
        username = (user or {}).get('username') if isinstance(user, dict) else None
        try:
            app.logger.info(
                '[save_xml_api] user=%s scen_count=%s active_index=%s project_hint=%s scenario_query=%s names=%s',
                username or 'anonymous',
                len(scenarios),
                active_index if active_index is not None else 'none',
                project_key_hint or '<none>',
                scenario_query_hint or '<none>',
                ', '.join(name for name in scenario_names if name) or '<unnamed>'
            )
        except Exception:
            pass
        autosave = _coerce_bool(data.get('autosave'))
        ts = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
        if autosave:
            out_dir = os.path.join(_outputs_dir(), 'autosave')
        else:
            out_dir = os.path.join(_outputs_dir(), f'scenarios-{ts}')
        os.makedirs(out_dir, exist_ok=True)
        try:
            legacy_bundle = os.path.join(out_dir, 'scenarios.xml' if not autosave else 'autosave.xml')
            if os.path.exists(legacy_bundle):
                os.remove(legacy_bundle)
        except Exception:
            pass
        scenario_paths_map: dict[str, str] = {}
        scenario_paths_by_index: list[str | None] = []
        active_out_path = None
        if scenarios:
            for idx, scen in enumerate(scenarios):
                if not isinstance(scen, dict):
                    scenario_paths_by_index.append(None)
                    continue
                raw_name = (scen.get('name') or '').strip()
                display_name = raw_name or f"Scenario {idx + 1}"
                stem_raw = display_name
                if autosave:
                    # Always include index and scenario name so autosave never collapses
                    # multiple scenarios into a single file.
                    stem_raw = f"autosave-{idx + 1}-{display_name or f'Scenario_{idx + 1}'}"
                stem = secure_filename(stem_raw).strip('_-.') or (f"autosave-{idx + 1}" if autosave else f"Scenario_{idx + 1}")
                out_path = os.path.join(out_dir, f"{stem}.xml")
                if os.path.exists(out_path):
                    suffix = 2
                    base = stem
                    while os.path.exists(out_path):
                        stem = f"{base}-{suffix}"
                        out_path = os.path.join(out_dir, f"{stem}.xml")
                        suffix += 1
                try:
                    tree = _build_scenarios_xml({ 'scenarios': [scen], 'core': normalized_core })
                    raw = ET.tostring(tree.getroot(), encoding='utf-8')
                    lroot = LET.fromstring(raw)
                    pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
                    with open(out_path, 'wb') as f:
                        f.write(pretty)
                except Exception:
                    try:
                        tree = _build_scenarios_xml({ 'scenarios': [scen], 'core': normalized_core })
                        tree.write(out_path, encoding='utf-8', xml_declaration=True)
                    except Exception:
                        continue
                # Defensive: ensure each file contains exactly one <Scenario>.
                try:
                    parsed = ET.parse(out_path)
                    root = parsed.getroot()
                    scenario_count = len(root.findall('Scenario'))
                    if scenario_count != 1:
                        tree = _build_scenarios_xml({ 'scenarios': [scen], 'core': normalized_core })
                        tree.write(out_path, encoding='utf-8', xml_declaration=True)
                except Exception:
                    pass
                scenario_paths_map[display_name] = out_path
                scenario_paths_by_index.append(out_path)
                if active_index is not None and active_index == idx:
                    active_out_path = out_path
            if active_out_path is None and scenario_paths_map:
                active_out_path = next(iter(scenario_paths_map.values()))
        else:
            return jsonify({ 'ok': False, 'error': 'No scenarios to save' }), 400
        out_path = active_out_path
        try:
            app.logger.info('[save_xml_api] wrote %s scenario xml files under %s', len(scenario_paths_map) or 1, out_dir)
        except Exception:
            pass
        resp_core = _normalize_core_config(normalized_core or core_meta or {}, include_password=False) if (normalized_core or core_meta) else _default_core_dict()
        snapshot_source = {
            'scenarios': scenarios,
            'core': resp_core,
            'result_path': out_path,
            'active_index': active_index,
            'project_key_hint': project_key_hint or out_path,
        }
        try:
            if scenario_paths_by_index:
                snapshot_source['saved_xml_paths_by_index'] = scenario_paths_by_index
        except Exception:
            pass
        if scenario_query_hint:
            snapshot_source['scenario_query'] = scenario_query_hint
        try:
            if active_index is not None and 0 <= active_index < len(scenarios):
                active_name = str((scenarios[active_index] or {}).get('name') or '').strip()
                if active_name:
                    snapshot_source['result_path_scenario'] = active_name
        except Exception:
            pass
        _persist_editor_state_snapshot(snapshot_source, user=user)
        try:
            app.logger.info('[save_xml_api] success user=%s xml=%s scen_count=%s', username or 'anonymous', out_path, len(scenarios))
        except Exception:
            pass
        if not autosave:
            try:
                names_for_catalog = [name for name in scenario_names if isinstance(name, str) and name.strip()]
                if names_for_catalog:
                    _persist_scenario_catalog(names_for_catalog, source_path=scenario_paths_map or out_path)
            except Exception:
                pass
        response_payload = { 'ok': True, 'result_path': out_path, 'core': resp_core }
        if scenario_paths_map:
            response_payload['scenario_paths'] = scenario_paths_map
        if scenario_paths_by_index:
            response_payload['scenario_paths_by_index'] = scenario_paths_by_index
        if active_index is not None and 0 <= active_index < len(scenarios):
            try:
                active_name = str((scenarios[active_index] or {}).get('name') or '').strip()
            except Exception:
                active_name = ''
            if active_name:
                response_payload['active_scenario'] = active_name
        return jsonify(response_payload)
    except Exception as e:
        try:
            app.logger.exception("[save_xml_api] failed: %s", e)
        except Exception:
            pass
        return jsonify({ 'ok': False, 'error': str(e) }), 500


@app.route('/render_xml_api', methods=['POST'])
def render_xml_api():
    """Render scenario XML for preview without persisting to disk."""
    try:
        data = request.get_json(silent=True) or {}
        scenarios = data.get('scenarios')
        core_meta = data.get('core')
        normalized_core = _normalize_core_config(core_meta, include_password=True) if isinstance(core_meta, (dict, list)) or core_meta else None
        if not isinstance(scenarios, list):
            return jsonify({ 'ok': False, 'error': 'Invalid payload (scenarios list required)' }), 400
        tree = _build_scenarios_xml({ 'scenarios': scenarios, 'core': normalized_core })
        # Pretty print when possible
        try:
            raw = ET.tostring(tree.getroot(), encoding='utf-8')
            lroot = LET.fromstring(raw)
            pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
            return Response(pretty, mimetype='application/xml')
        except Exception:
            out = ET.tostring(tree.getroot(), encoding='utf-8', xml_declaration=True)
            return Response(out, mimetype='application/xml')
    except Exception as e:
        try:
            app.logger.exception('[render_xml_api] failed: %s', e)
        except Exception:
            pass
        return jsonify({ 'ok': False, 'error': str(e) }), 500


@app.route('/run_cli', methods=['POST'])
def run_cli():
    user = _current_user()
    xml_path = request.form.get('xml_path')
    if not xml_path:
        flash('XML path missing. Save XML first.')
        return redirect(url_for('index'))
    # Always resolve to absolute path
    xml_path = os.path.abspath(xml_path)
    # Path fallback: if user supplied /app/outputs but actual saved path lives under /app/webapp/outputs (volume mapping difference)
    if not os.path.exists(xml_path) and '/outputs/' in xml_path:
        try:
            # Replace first occurrence of '/app/outputs' with '/app/webapp/outputs'
            alt = xml_path.replace('/app/outputs', '/app/webapp/outputs')
            if alt != xml_path and os.path.exists(alt):
                app.logger.info("[sync] Remapped XML path %s -> %s", xml_path, alt)
                xml_path = alt
        except Exception:
            pass
    if not os.path.exists(xml_path):
        try:
            recovered = _try_resolve_latest_outputs_xml(xml_path)
            if recovered and os.path.exists(recovered):
                app.logger.warning('[sync] XML path missing; recovered to newest match: %s -> %s', xml_path, recovered)
                xml_path = recovered
        except Exception:
            pass
    if not os.path.exists(xml_path):
        flash(f'XML path not found: {xml_path}')
        return redirect(url_for('index'))
    # Skip schema validation: format differs from CORE XML
    # Determine CORE connection settings (including SSH details) from saved payload and defaults
    core_override = None
    try:
        core_json = request.form.get('core_json')
        if core_json:
            core_override = json.loads(core_json)
    except Exception:
        core_override = None
    scenario_core_override = None
    try:
        hitl_core_json = request.form.get('hitl_core_json')
        if hitl_core_json:
            scenario_core_override = json.loads(hitl_core_json)
    except Exception:
        scenario_core_override = None
    scenario_name_hint = request.form.get('scenario') or request.form.get('scenario_name') or None
    docker_cleanup_before_run = _coerce_bool(request.form.get('docker_cleanup_before_run'))
    docker_remove_all_containers = _coerce_bool(
        request.form.get('docker_remove_all_containers')
    ) or _coerce_bool(request.form.get('docker_nuke_all'))
    scenario_index_hint: Optional[int] = None
    try:
        raw_index = request.form.get('scenario_index')
        if raw_index not in (None, ''):
            scenario_index_hint = int(raw_index)
    except Exception:
        scenario_index_hint = None
    payload_for_core: Dict[str, Any] | None = None
    try:
        payload_for_core = _parse_scenarios_xml(xml_path)
    except Exception:
        payload_for_core = None
    scenario_payload: Dict[str, Any] | None = None
    if payload_for_core:
        scen_list = payload_for_core.get('scenarios') or []
        if isinstance(scen_list, list) and scen_list:
            if scenario_name_hint:
                for scen_entry in scen_list:
                    if not isinstance(scen_entry, dict):
                        continue
                    if str(scen_entry.get('name') or '').strip() == str(scenario_name_hint).strip():
                        scenario_payload = scen_entry
                        break
            if scenario_payload is None and scenario_index_hint is not None:
                if 0 <= scenario_index_hint < len(scen_list):
                    candidate = scen_list[scenario_index_hint]
                    if isinstance(candidate, dict):
                        scenario_payload = candidate
            if scenario_payload is None:
                for scen_entry in scen_list:
                    if isinstance(scen_entry, dict):
                        scenario_payload = scen_entry
                        break
    scenario_core_saved = None
    if scenario_payload and isinstance(scenario_payload.get('hitl'), dict):
        scenario_core_saved = scenario_payload['hitl'].get('core')
    global_core_saved = payload_for_core.get('core') if (payload_for_core and isinstance(payload_for_core.get('core'), dict)) else None
    scenario_core_public: Dict[str, Any] | None = None
    candidate_scenario_core = scenario_core_override if isinstance(scenario_core_override, dict) else None
    if not candidate_scenario_core and isinstance(scenario_core_saved, dict):
        candidate_scenario_core = scenario_core_saved
    if candidate_scenario_core:
        scenario_core_public = _scrub_scenario_core_config(candidate_scenario_core)
    core_cfg = _merge_core_configs(
        global_core_saved,
        scenario_core_saved,
        core_override,
        scenario_core_override,
        include_password=True,
    )
    try:
        core_cfg = _require_core_ssh_credentials(core_cfg)
    except _SSHTunnelError as exc:
        flash(str(exc))
        return redirect(url_for('index'))
    core_host = core_cfg.get('host', '127.0.0.1')
    try:
        core_port = int(core_cfg.get('port', 50051))
    except Exception:
        core_port = 50051
    remote_desc = f"{core_host}:{core_port}"
    app.logger.info("[sync] Preparing CLI run against CORE remote=%s (ssh_enabled=%s), xml=%s", remote_desc, core_cfg.get('ssh_enabled'), xml_path)
    preferred_cli_venv = _sanitize_venv_bin_path(core_cfg.get('venv_bin'))
    venv_is_explicit = _venv_is_explicit(core_cfg, preferred_cli_venv)
    if venv_is_explicit and preferred_cli_venv:
        cli_venv_bin = _resolve_cli_venv_bin(preferred_cli_venv, allow_fallback=False)
        if not cli_venv_bin:
            flash(
                f"Remote CORE venv bin '{preferred_cli_venv}' is not accessible from this host. "
                "Mount that directory or adjust the path before running the CLI.",
            )
            return redirect(url_for('index'))
    else:
        cli_venv_bin = _resolve_cli_venv_bin(preferred_cli_venv, allow_fallback=True)
    # Run gRPC CLI script (config2scen_core_grpc.py) instead of internal module
    try:
        # Pre-save any existing active CORE session XML (best-effort) using derived config
        try:
            pre_dir = os.path.join(os.path.dirname(xml_path) or _outputs_dir(), 'core-pre')
            pre_saved = _grpc_save_current_session_xml_with_config(core_cfg, pre_dir)
            if pre_saved:
                flash(f'Captured current CORE session XML: {os.path.basename(pre_saved)}')
                app.logger.debug("[sync] Pre-run session XML saved to %s", pre_saved)
        except Exception:
            pre_saved = None
        repo_root = _get_repo_root()
        # Invoke package CLI so it can generate reports under repo_root/reports
        py_exec = _select_python_interpreter(cli_venv_bin)
        cli_env = _prepare_cli_env(preferred_venv_bin=cli_venv_bin)
        cli_env.setdefault('PYTHONUNBUFFERED', '1')
        # Deliver Flow generator artifacts into vuln containers by copying files in,
        # rather than bind-mounting directories from the host.
        cli_env.setdefault('CORETG_FLOW_ARTIFACTS_MODE', 'copy')
        app.logger.info("[sync] Using python interpreter: %s", py_exec)
        # Determine active scenario name (prefer explicit hint, fallback to first in XML)
        active_scenario_name = None
        if scenario_name_hint:
            active_scenario_name = scenario_name_hint
        elif scenario_payload and isinstance(scenario_payload.get('name'), str):
            active_scenario_name = scenario_payload.get('name')
        if not active_scenario_name:
            try:
                names_for_cli = _scenario_names_from_xml(xml_path)
                if names_for_cli:
                    active_scenario_name = names_for_cli[0]
            except Exception:
                active_scenario_name = None

        # Scope tool-generated wrapper images by upload/scenario to prevent cross-scenario image reuse.
        try:
            out_dir_for_tag = os.path.dirname(xml_path) if xml_path else ''
            upload_base = os.path.basename(out_dir_for_tag) if out_dir_for_tag else ''
            parts = []
            if upload_base:
                parts.append(upload_base)
            if active_scenario_name:
                parts.append(active_scenario_name)
            scenario_tag = _safe_name('-'.join(parts) if parts else (active_scenario_name or 'scenario'))
            cli_env.setdefault('CORETG_SCENARIO_TAG', scenario_tag)
        except Exception:
            pass
        with _core_connection(core_cfg) as (conn_host, conn_port):
            forwarded_desc = f"{conn_host}:{conn_port}"
            app.logger.info(
                "[sync] Running CLI with CORE remote=%s via=%s, xml=%s",
                remote_desc,
                forwarded_desc,
                xml_path,
            )

            # Optional: best-effort cleanup of tool-managed Docker state before execution.
            # This is conservative: it only targets vuln docker-compose node containers derived
            # from compose_assignments.json plus tool-generated wrapper images under coretg/*.

            # Optional: very destructive cleanup.
            # Stops/removes ALL containers on the remote CORE VM.
            if docker_remove_all_containers:
                try:
                    app.logger.warning('[sync] Pre-run: docker remove-all-containers requested')
                    _run_remote_python_json(
                        core_cfg,
                        _remote_docker_remove_all_containers_script(core_cfg.get('ssh_password')),
                        logger=app.logger,
                        label='docker.remove_all_containers(prerun)',
                        timeout=900.0,
                    )
                except Exception as exc:
                    try:
                        app.logger.warning('[sync] Pre-run docker remove-all-containers skipped/failed: %s', exc)
                    except Exception:
                        pass
            if docker_cleanup_before_run:
                try:
                    app.logger.info('[sync] Pre-run: docker cleanup requested (containers + wrapper images)')
                    status_payload = _run_remote_python_json(
                        core_cfg,
                        _remote_docker_status_script(core_cfg.get('ssh_password')),
                        logger=app.logger,
                        label='docker.status(for prerun cleanup)',
                        timeout=60.0,
                    )
                    names: list[str] = []
                    if isinstance(status_payload, dict) and isinstance(status_payload.get('items'), list):
                        for it in status_payload.get('items') or []:
                            if isinstance(it, dict) and it.get('name'):
                                names.append(str(it.get('name')))
                    if names:
                        _run_remote_python_json(
                            core_cfg,
                            _remote_docker_cleanup_script(names, core_cfg.get('ssh_password')),
                            logger=app.logger,
                            label='docker.cleanup(prerun)',
                            timeout=120.0,
                        )
                    _run_remote_python_json(
                        core_cfg,
                        _remote_docker_remove_wrapper_images_script(core_cfg.get('ssh_password')),
                        logger=app.logger,
                        label='docker.wrapper_images.cleanup(prerun)',
                        timeout=180.0,
                    )
                except Exception as exc:
                    try:
                        app.logger.warning('[sync] Pre-run docker cleanup skipped/failed: %s', exc)
                    except Exception:
                        pass

            cli_args = [
                py_exec,
                '-m',
                'core_topo_gen.cli',
                '--xml',
                xml_path,
                '--host',
                conn_host,
                '--port',
                str(conn_port),
                '--verbose',
            ]
            if active_scenario_name:
                cli_args.extend(['--scenario', active_scenario_name])
            proc = subprocess.run(cli_args, cwd=repo_root, check=False, capture_output=True, text=True, env=cli_env)
        logs = (proc.stdout or '') + ('\n' + proc.stderr if proc.stderr else '')
        app.logger.debug("[sync] CLI return code: %s", proc.returncode)

        # If docker nodes are expected, ensure per-node compose artifacts exist locally.
        try:
            docker_expected = False
            if preview_plan_path and scenario_for_plan:
                plan_payload = _load_preview_payload_from_path(preview_plan_path, scenario_for_plan)
                if isinstance(plan_payload, dict):
                    full_preview = plan_payload.get('full_preview') if isinstance(plan_payload.get('full_preview'), dict) else None
                    if isinstance(full_preview, dict):
                        role_counts = full_preview.get('role_counts') if isinstance(full_preview.get('role_counts'), dict) else None
                        if isinstance(role_counts, dict):
                            try:
                                docker_expected = int(role_counts.get('Docker') or 0) > 0
                            except Exception:
                                docker_expected = False
                        if not docker_expected:
                            hosts = full_preview.get('hosts') if isinstance(full_preview.get('hosts'), list) else []
                            for h in hosts or []:
                                if not isinstance(h, dict):
                                    continue
                                role = str(h.get('role') or '').strip().lower()
                                if role == 'docker':
                                    docker_expected = True
                                    break
            if docker_expected:
                try:
                    import glob as _glob
                    compose_files = _glob.glob('/tmp/vulns/docker-compose-*.yml')
                except Exception:
                    compose_files = []
                if not compose_files:
                    msg = 'No per-node docker-compose files were generated under /tmp/vulns despite Docker nodes in the plan.'
                    try:
                        log_f.write(f"[sync] ERROR: {msg}\n")
                    except Exception:
                        pass
                    try:
                        log_f.close()
                    except Exception:
                        pass
                    try:
                        os.remove(log_path)
                    except Exception:
                        pass
                    return jsonify({"error": msg}), 500
        except Exception:
            pass

        # If the CLI generated vulnerability compose artifacts locally, copy them to the CORE VM
        # so the remote Docker Compose card and DockerComposeService can access them.
        uploaded_vuln_artifacts = False
        try:
            uploaded_vuln_artifacts = bool(
                _sync_local_vulns_to_remote(
                    core_cfg,
                    flow_plan_path=xml_path,
                    logger=app.logger,
                )
            )
        except Exception as exc:
            uploaded_vuln_artifacts = False
            try:
                app.logger.warning('[sync] Vuln artifact upload failed: %s', exc)
            except Exception:
                pass
        try:
            app.logger.info('[sync] Vuln artifact upload complete uploaded=%s', bool(uploaded_vuln_artifacts))
        except Exception:
            pass

        # Flow flag artifacts: copy them into running/stopped vuln containers on the CORE VM.
        # This is used when CORETG_FLOW_ARTIFACTS_MODE=copy (no bind mounts).
        try:
            payload = _run_remote_python_json(
                core_cfg,
                _remote_copy_flow_artifacts_into_containers_script(core_cfg.get('ssh_password')),
                logger=app.logger,
                label='docker.copy_flow_artifacts(postrun)',
                timeout=180.0,
            )
            try:
                copied_ok = 0
                copied_total = 0
                if isinstance(payload, dict) and isinstance(payload.get('items'), list):
                    copied_total = len(payload.get('items') or [])
                    copied_ok = sum(1 for it in (payload.get('items') or []) if isinstance(it, dict) and it.get('ok'))
                app.logger.info('[sync] docker cp flow artifacts results ok=%s total=%s', int(copied_ok), int(copied_total))
            except Exception:
                pass
        except Exception as exc:
            try:
                app.logger.warning('[sync] docker cp flow artifacts skipped/failed: %s', exc)
            except Exception:
                pass

        # Report path (if generated by CLI): parse logs or fallback to latest under reports/
        report_md = _extract_report_path_from_text(logs) or _find_latest_report_path()
        if report_md:
            app.logger.info("[sync] Detected report path: %s", report_md)
        summary_json = _extract_summary_path_from_text(logs)
        if not summary_json:
            summary_json = _derive_summary_from_report(report_md)
        if not summary_json and not report_md:
            summary_json = _find_latest_summary_path()
        if summary_json and not os.path.exists(summary_json):
            summary_json = None
        if summary_json:
            app.logger.info("[sync] Detected summary path: %s", summary_json)
        # Try to capture the exact session id from logs for precise post-run save
        session_id = _extract_session_id_from_text(logs)
        if session_id:
            app.logger.info("[sync] Detected CORE session id: %s", session_id)
            _record_session_mapping(
                xml_path,
                session_id,
                scenario_name=active_scenario_name or scenario_name_hint or None,
            )
            try:
                sid_int = int(str(session_id).strip())
                _write_remote_session_scenario_meta(
                    core_cfg,
                    session_id=sid_int,
                    scenario_name=active_scenario_name or scenario_name_hint or None,
                    scenario_xml_basename=os.path.basename(xml_path),
                    logger=app.logger,
                )
            except Exception:
                pass
        # Read XML for preview
        xml_text = ""
        try:
            with open(xml_path, 'r', encoding='utf-8', errors='ignore') as f:
                xml_text = f.read()
        except Exception:
            xml_text = ""
        run_success = (proc.returncode == 0)
        post_saved = None
        # Inform user
        if run_success:
            if report_md and os.path.exists(report_md):
                flash('CLI completed. Report ready to download.')
            else:
                flash('CLI completed. No report found.')
        else:
            flash('CLI finished with errors. See logs.')
        # Best-effort: save the active CORE session XML after run (try even on failures)
        try:
            post_dir = os.path.join(os.path.dirname(xml_path), 'core-post')
            post_saved = _grpc_save_current_session_xml_with_config(core_cfg, post_dir, session_id=session_id)
            if post_saved:
                flash(f'Captured post-run CORE session XML: {os.path.basename(post_saved)}')
                app.logger.debug("[sync] Post-run session XML saved to %s", post_saved)
        except Exception:
            post_saved = None
        # Append session/scenario discrepancies to the report (best-effort).
        try:
            _append_session_scenario_discrepancies(
                report_md,
                xml_path,
                post_saved,
                scenario_label=active_scenario_name or scenario_name_hint,
            )
        except Exception:
            pass
        payload = payload_for_core or {}
        if not payload:
            try:
                payload = _parse_scenarios_xml(xml_path)
            except Exception:
                payload = {}
        if "core" not in payload:
            payload["core"] = _default_core_dict()
        try:
            payload['core'] = _normalize_core_config(core_cfg, include_password=False)
        except Exception:
            pass
        _attach_base_upload(payload)
        # Always use absolute xml_path for result_path fallback
        payload["result_path"] = report_md if (report_md and os.path.exists(report_md)) else xml_path
        # Append run history entry regardless of intermediate failures; log details
        scen_names = []
        try:
            scen_names = _scenario_names_from_xml(xml_path)
        except Exception as e_names:
            app.logger.exception("[sync] failed extracting scenario names from %s: %s", xml_path, e_names)
        full_bundle_path = None
        single_scen_xml = None
        try:
            # Build a single-scenario XML containing only the active scenario to satisfy bundling constraint
            try:
                single_scen_xml = _write_single_scenario_xml(xml_path, (active_scenario_name or (scen_names[0] if scen_names else None)), out_dir=os.path.dirname(xml_path))
            except Exception:
                single_scen_xml = None
            bundle_xml = single_scen_xml or xml_path
            app.logger.info("[sync] Building full scenario archive (xml=%s, report=%s, pre=%s, post=%s)", bundle_xml, report_md, (pre_saved if 'pre_saved' in locals() else None), post_saved)
            full_bundle_path = _build_full_scenario_archive(
                os.path.dirname(bundle_xml),
                bundle_xml,
                (report_md if (report_md and os.path.exists(report_md)) else None),
                (pre_saved if 'pre_saved' in locals() else None),
                post_saved,
                summary_path=summary_json,
                run_id=None,
            )
        except Exception as e_bundle:
            app.logger.exception("[sync] failed building full scenario bundle: %s", e_bundle)
        try:
            session_xml_path = post_saved if (post_saved and os.path.exists(post_saved)) else None
            core_public = dict(core_cfg)
            core_public.pop('ssh_password', None)
            _append_run_history({
                'timestamp': datetime.datetime.utcnow().isoformat() + 'Z',
                'mode': 'sync',
                'xml_path': xml_path,
                'post_xml_path': session_xml_path,
                'session_xml_path': session_xml_path,
                'scenario_xml_path': xml_path,
                'report_path': report_md if (report_md and os.path.exists(report_md)) else None,
                'summary_path': summary_json if (summary_json and os.path.exists(summary_json)) else None,
                'pre_xml_path': pre_saved if 'pre_saved' in locals() else None,
                'full_scenario_path': full_bundle_path,
                'single_scenario_xml_path': single_scen_xml,
                'returncode': proc.returncode,
                'scenario_names': scen_names,
                'scenario_name': active_scenario_name,
                'core': _normalize_core_config(core_cfg, include_password=False),
                'core_cfg_public': core_public,
                'scenario_core': scenario_core_public,
            })
        except Exception as e_hist:
            app.logger.exception("[sync] failed appending run history: %s", e_hist)
        payload = _prepare_payload_for_index(payload, user=user)
        return render_template('index.html', payload=payload, logs=logs, xml_preview=xml_text, run_success=run_success)
    except Exception as e:
        flash(f'Error running core-topo-gen: {e}')
        return redirect(url_for('index'))


# ----------------------- Planning (Preview / Run) -----------------------


@app.route('/api/seed_hints', methods=['POST'])
def api_seed_hints():
    """Return deterministic seed hints for one or more scenarios.

    Request JSON: { xml_path: "/abs/scenarios.xml", scenarios: ["Scenario 1", ...] }
    Response JSON: { ok: true, xml_path, xml_hash, seeds: { "scenario 1": 123, ... } }
    """
    try:
        payload = request.get_json(silent=True) or {}
        xml_path = (payload.get('xml_path') or '').strip()
        scenarios = payload.get('scenarios') or []
        if not xml_path:
            return jsonify({'ok': False, 'error': 'xml_path missing'}), 400
        xml_path = os.path.abspath(xml_path)
        if not os.path.exists(xml_path):
            return jsonify({'ok': False, 'error': f'XML not found: {xml_path}'}), 404

        try:
            from core_topo_gen.planning.plan_cache import hash_xml_file
            xml_hash = hash_xml_file(xml_path)
        except Exception as exc:
            return jsonify({'ok': False, 'error': f'Failed to hash XML: {exc}'}), 500

        seeds: dict[str, int] = {}
        if isinstance(scenarios, list):
            for raw in scenarios:
                try:
                    name = (str(raw) if raw is not None else '').strip()
                    if not name:
                        continue
                    key = name.lower()
                    if key in seeds:
                        continue
                    seeds[key] = _derive_seed_for_scenario(xml_hash, name)
                except Exception:
                    continue
        return jsonify({'ok': True, 'xml_path': xml_path, 'xml_hash': xml_hash, 'seeds': seeds})
    except Exception as e:
        try:
            app.logger.exception('[seed_hints] error: %s', e)
        except Exception:
            pass
        return jsonify({'ok': False, 'error': str(e)}), 500



@app.route('/api/plan/preview_full', methods=['POST'])
def api_plan_preview_full():
    """Compute a full dry-run plan (no CORE session) including routers, hosts, IPs, services,
    vulnerabilities, segmentation slot preview and connectivity policies.

    Request JSON: { xml_path: "/abs/scenarios.xml", scenario: optionalName }
    Response: { ok, full_preview: {...} }
    """
    try:
        payload = request.get_json(silent=True) or {}
        xml_path = payload.get('xml_path')
        scenarios_inline = payload.get('scenarios')
        core_inline = payload.get('core')
        scenario = payload.get('scenario') or None
        seed = payload.get('seed')
        r2s_hosts_min_list = payload.get('r2s_hosts_min_list') or []
        r2s_hosts_max_list = payload.get('r2s_hosts_max_list') or []
        try:
            if seed is not None:
                seed = int(seed)
        except Exception:
            seed = None
        if not xml_path:
            # Builder/participant roles may preview without saving by posting scenarios/core.
            if isinstance(scenarios_inline, list):
                try:
                    normalized_core = _normalize_core_config(core_inline, include_password=True) if isinstance(core_inline, dict) else None
                    tree = _build_scenarios_xml({ 'scenarios': scenarios_inline, 'core': normalized_core })
                    ts = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
                    tag = str(uuid.uuid4())[:8]
                    out_dir = os.path.join(_outputs_dir(), f'tmp-preview-{ts}-{tag}')
                    os.makedirs(out_dir, exist_ok=True)
                    stem_raw = scenario or None
                    if not stem_raw:
                        try:
                            first_name = None
                            for sc in scenarios_inline:
                                if isinstance(sc, dict) and sc.get('name'):
                                    first_name = sc.get('name')
                                    break
                            stem_raw = first_name or 'scenarios'
                        except Exception:
                            stem_raw = 'scenarios'
                    stem = secure_filename(str(stem_raw)).strip('_-.') or 'scenarios'
                    xml_path = os.path.join(out_dir, f"{stem}.xml")
                    try:
                        from lxml import etree as LET  # type: ignore
                        raw = ET.tostring(tree.getroot(), encoding='utf-8')
                        lroot = LET.fromstring(raw)
                        pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
                        with open(xml_path, 'wb') as f:
                            f.write(pretty)
                    except Exception:
                        tree.write(xml_path, encoding='utf-8', xml_declaration=True)
                except Exception as exc:
                    return jsonify({'ok': False, 'error': f'Failed to render XML for preview: {exc}'}), 400
            else:
                return jsonify({'ok': False, 'error': 'xml_path missing'}), 400
        xml_path = os.path.abspath(xml_path)
        if not os.path.exists(xml_path):
            return jsonify({'ok': False, 'error': f'XML not found: {xml_path}'}), 404
        from core_topo_gen.planning.orchestrator import compute_full_plan
        from core_topo_gen.planning.plan_cache import hash_xml_file
        xml_hash = hash_xml_file(xml_path)
        plan = compute_full_plan(xml_path, scenario=scenario, seed=seed, include_breakdowns=True)
        if seed is None:
            seed = plan.get('seed') or _derive_default_seed(xml_hash)
        full_prev = _build_full_preview_from_plan(plan, seed, r2s_hosts_min_list, r2s_hosts_max_list)
        xml_basename = os.path.splitext(os.path.basename(xml_path))[0]
        try:
            raw_hitl_config = parse_hitl_info(xml_path, scenario)
        except Exception as hitl_exc:
            try:
                app.logger.debug('[plan.preview_full] hitl parse failed: %s', hitl_exc)
            except Exception:
                pass
            raw_hitl_config = {"enabled": False, "interfaces": []}
        hitl_config = _sanitize_hitl_config(raw_hitl_config, scenario, xml_basename)
        try:
            full_prev['hitl_interfaces'] = hitl_config.get('interfaces', [])
            full_prev['hitl_enabled'] = bool(hitl_config.get('enabled'))
            full_prev['hitl_scenario_key'] = hitl_config.get('scenario_key')
            if hitl_config.get('core'):
                full_prev['hitl_core'] = hitl_config.get('core')
        except Exception:
            pass
        try:
            _merge_hitl_preview_with_full_preview(full_prev, hitl_config)
        except Exception:
            pass
        flow_meta = None
        try:
            flow_meta = _attach_latest_flow_into_full_preview(full_prev, scenario)
        except Exception:
            flow_meta = None
        return jsonify({'ok': True, 'full_preview': full_prev, 'plan': plan, 'breakdowns': plan.get('breakdowns'), 'flow_meta': flow_meta or {}})
    except Exception as e:
        app.logger.exception('[plan.preview_full] error: %s', e)
        return jsonify({'ok': False, 'error': str(e) }), 500


@app.route('/api/plan/persist_flow_plan', methods=['POST'])
def api_plan_persist_flow_plan():
    """Compute a full preview and persist it into the XML (PlanPreview + FlowState).

    Request JSON: { xml_path: "/abs/path.xml", scenario: "name" (optional), seed: int (optional) }
    Response JSON: { ok, xml_path, scenario, seed, preview_plan_path }
    """
    try:
        payload = request.get_json(silent=True) or {}
        xml_path = (payload.get('xml_path') or '').strip()
        scenario = (payload.get('scenario') or '').strip() or None
        seed = payload.get('seed')
        try:
            if seed is not None:
                seed = int(seed)
        except Exception:
            seed = None

        if not xml_path:
            return jsonify({'ok': False, 'error': 'xml_path missing'}), 400

        result = _planner_persist_flow_plan(xml_path=xml_path, scenario=scenario, seed=seed)

        return jsonify({
            'ok': True,
            'xml_path': result.get('xml_path'),
            'scenario': result.get('scenario'),
            'seed': result.get('seed'),
            'preview_plan_path': result.get('preview_plan_path'),
        })
    except Exception as e:
        try:
            app.logger.exception('[plan.persist_flow_plan] error: %s', e)
        except Exception:
            pass
        return jsonify({'ok': False, 'error': str(e)}), 500


def _planner_persist_flow_plan(*, xml_path: str, scenario: str | None, seed: int | None, persist_plan_file: bool = False) -> dict[str, Any]:
    """Planner-owned plan creation (XML-only, no cache/files beyond XML)."""
    if not xml_path:
        raise ValueError('xml_path missing')
    xml_path = os.path.abspath(xml_path)
    if not os.path.exists(xml_path):
        raise FileNotFoundError(f'XML not found: {xml_path}')

    from core_topo_gen.planning.orchestrator import compute_full_plan
    from core_topo_gen.planning.plan_cache import hash_xml_file

    plan = compute_full_plan(xml_path, scenario=scenario, seed=seed, include_breakdowns=True)

    if seed is None:
        try:
            seed = plan.get('seed') or _derive_default_seed(hash_xml_file(xml_path))
        except Exception:
            seed = None

    full_prev = _build_full_preview_from_plan(plan, seed, [], [])
    try:
        flow_meta_from_plan = _attach_latest_flow_into_full_preview(full_prev, scenario)
    except Exception:
        flow_meta_from_plan = None

    try:
        xml_basename = os.path.basename(xml_path)
    except Exception:
        xml_basename = None
    scenario_name = scenario or None
    if not scenario_name:
        try:
            names_for_cli = _scenario_names_from_xml(xml_path)
            if names_for_cli:
                scenario_name = names_for_cli[0]
        except Exception:
            scenario_name = None
    try:
        raw_hitl_config = parse_hitl_info(xml_path, scenario_name)
    except Exception:
        raw_hitl_config = {"enabled": False, "interfaces": []}
    try:
        hitl_config = _sanitize_hitl_config(raw_hitl_config, scenario_name, xml_basename)
    except Exception:
        hitl_config = {"enabled": False, "interfaces": []}
    try:
        full_prev['hitl_interfaces'] = hitl_config.get('interfaces', [])
        full_prev['hitl_enabled'] = bool(hitl_config.get('enabled'))
        full_prev['hitl_scenario_key'] = hitl_config.get('scenario_key')
        if hitl_config.get('core'):
            full_prev['hitl_core'] = hitl_config.get('core')
    except Exception:
        pass
    try:
        _merge_hitl_preview_with_full_preview(full_prev, hitl_config)
    except Exception:
        pass

    preview_plan_path = xml_path
    existing_meta: dict[str, Any] = {}
    if persist_plan_file:
        preview_plan_path = xml_path
        existing_meta = {}
    flow_meta = flow_meta_from_plan
    if not isinstance(flow_meta, dict):
        flow_meta = existing_meta.get('flow') if isinstance(existing_meta.get('flow'), dict) else {}
    created_at = existing_meta.get('created_at') if isinstance(existing_meta, dict) else None
    if not isinstance(created_at, str) or not created_at.strip():
        created_at = datetime.datetime.now(datetime.timezone.utc).isoformat().replace('+00:00', 'Z')
    plan_payload = {
        'full_preview': full_prev,
        'metadata': {
            'xml_path': xml_path,
            'scenario': scenario_name or scenario,
            'seed': full_prev.get('seed'),
            'origin': 'planner',
            'flow': flow_meta or {},
            'created_at': created_at,
            'updated_at': datetime.datetime.now(datetime.timezone.utc).isoformat().replace('+00:00', 'Z'),
        },
    }
    try:
        _update_plan_preview_in_xml(xml_path, scenario_name or scenario, plan_payload)
        if isinstance(flow_meta, dict) and flow_meta:
            _update_flow_state_in_xml(xml_path, scenario_name or scenario, flow_meta)
    except Exception:
        pass
    if persist_plan_file:
        # JSON plan files are disabled; XML is the universal source.
        pass

    return {
        'xml_path': xml_path,
        'scenario': scenario_name or scenario,
        'seed': full_prev.get('seed'),
        'preview_plan_path': xml_path,
        'full_preview': full_prev,
        'plan': plan,
    }

@app.route('/plan/full_preview_page', methods=['POST'])
def plan_full_preview_page():
    """Generate a full preview and render a dedicated page similar to core_details but without CORE.

    Form fields: xml_path, optional scenario, seed
    """
    try:
        embed_raw = request.args.get('embed') or request.form.get('embed') or ''
        embed = str(embed_raw).strip().lower() in ['1', 'true', 'yes', 'y', 'on']
        xml_path = request.form.get('xml_path')
        scenario = request.form.get('scenario') or None
        force_raw = request.form.get('force') or request.form.get('force_recompute') or ''
        force_recompute = str(force_raw).strip().lower() in ['1', 'true', 'yes', 'y', 'on']
        seed_raw = request.form.get('seed') or ''
        seed = None
        try:
            if seed_raw:
                s = int(seed_raw)
                if s>0: seed = s
        except Exception:
            seed = None
        # Render from the requested XML when provided; otherwise fall back to latest scenario plan.
        try:
            xml_path_abs = os.path.abspath(xml_path) if xml_path else ''
        except Exception:
            xml_path_abs = ''
        if xml_path_abs and os.path.exists(xml_path_abs):
            plan_path = xml_path_abs
        else:
            plan_path = None
        try:
            scen_norm = _normalize_scenario_label(scenario or '')
        except Exception:
            scen_norm = ''
        if (not plan_path) and scen_norm:
            try:
                plan_path = _latest_flow_plan_for_scenario_norm(scen_norm)
            except Exception:
                plan_path = plan_path
        if plan_path:
            try:
                payload = _load_preview_payload_from_path(plan_path, scenario)
                if isinstance(payload, dict):
                    full_prev = payload.get('full_preview') if isinstance(payload, dict) else None
                    meta = payload.get('metadata') if isinstance(payload, dict) else None
                    if isinstance(full_prev, dict):
                        if not isinstance(meta, dict):
                            meta = {}
                        xml_path0 = str(meta.get('xml_path') or '')
                        scenario0 = str(meta.get('scenario') or '') or (scenario or None)
                        seed0 = meta.get('seed')
                        try:
                            seed0 = int(seed0) if seed0 is not None else full_prev.get('seed')
                        except Exception:
                            seed0 = full_prev.get('seed')

                    flow_meta = None
                    try:
                        xml_for_flow = xml_path0 or (plan_path if plan_path and str(plan_path).lower().endswith('.xml') else '')
                        if xml_for_flow:
                            flow_meta = _flow_state_from_xml_path(xml_for_flow, scenario0 or scenario)
                    except Exception:
                        flow_meta = None
                    if not isinstance(flow_meta, dict):
                        try:
                            meta_flow = meta.get('flow') if isinstance(meta, dict) else None
                            if isinstance(meta_flow, dict):
                                flow_meta = meta_flow
                        except Exception:
                            flow_meta = None

                    display_artifacts = full_prev.get('display_artifacts')
                    if not display_artifacts:
                        try:
                            display_artifacts = _attach_display_artifacts(full_prev)
                        except Exception:
                            display_artifacts = {
                                'segmentation': {
                                    'rows': [],
                                    'table_rows': [],
                                    'tableRows': [],
                                    'json': {'rules_count': 0, 'types_summary': {}, 'rules': [], 'metadata': None},
                                },
                                '__version': FULL_PREVIEW_ARTIFACT_VERSION,
                            }
                    segmentation_artifacts = (display_artifacts or {}).get('segmentation')

                    # Reconstruct hitl_config for template display from embedded full_preview fields.
                    hitl_config = {
                        'enabled': bool(full_prev.get('hitl_enabled')),
                        'interfaces': full_prev.get('hitl_interfaces') or [],
                        'scenario_key': full_prev.get('hitl_scenario_key') or (scenario0 or None),
                    }
                    try:
                        if full_prev.get('hitl_core'):
                            hitl_config['core'] = full_prev.get('hitl_core')
                    except Exception:
                        pass

                    import json as _json
                    preview_json_str = _json.dumps(full_prev, indent=2, default=str)
                    xml_basename = None
                    try:
                        if xml_path0:
                            xml_basename = os.path.basename(xml_path0)
                    except Exception:
                        xml_basename = None

                    try:
                        plan_label = os.path.basename(plan_path)
                    except Exception:
                        plan_label = str(plan_path)
                    app.logger.info('[plan.full_preview_page] using plan=%s scenario=%s', plan_label, scen_norm or (scenario or ''))
                    return render_template(
                        'full_preview.html',
                        full_preview=full_prev,
                        preview_json=preview_json_str,
                        xml_path=xml_path0,
                        scenario=scenario0,
                        seed=seed0,
                        flow_meta=flow_meta or {},
                        preview_plan_path=plan_path,
                        display_artifacts=display_artifacts,
                        segmentation_artifacts=segmentation_artifacts,
                        hitl_config=hitl_config,
                        xml_basename=xml_basename,
                        hide_chrome=embed,
                    )
            except Exception:
                pass

            if embed:
                return Response(
                    '<div style="font-family: system-ui; padding: 16px; color: #6c757d;">'
                    'Generate a Preview/Flow first to keep Preview in sync.'
                    '</div>',
                    mimetype='text/html',
                )
            flash('Generate a Preview/Flow first to keep Preview in sync.')
            return redirect(url_for('scenarios_preview'))

        if embed:
            return Response(
                '<div style="font-family: system-ui; padding: 16px; color: #6c757d;">'
                'Select a scenario to load its Preview.'
                '</div>',
                mimetype='text/html',
            )
        flash('Select a scenario to load its Preview.')
        return redirect(url_for('scenarios_preview'))

        if not force_recompute:
            if embed:
                return Response(
                    '<div style="font-family: system-ui; padding: 16px; color: #6c757d;">'
                    'Preview renders saved XML only. Generate a Preview first.'
                    '</div>',
                    mimetype='text/html',
                )
            flash('Preview renders saved XML only. Generate a Preview first.')
            return redirect(url_for('scenarios_preview'))

        if not xml_path:
            if embed:
                return Response(
                    '<div style="font-family: system-ui; padding: 16px; color: #6c757d;">'
                    'Save XML first to preview (missing xml_path).'
                    '</div>',
                    mimetype='text/html',
                )
            flash('xml_path missing (full preview page)')
            return redirect(url_for('index'))
        xml_path = os.path.abspath(xml_path)
        xml_basename = os.path.splitext(os.path.basename(xml_path))[0] if xml_path else ''
        # Path fallback: handle container/volume mapping differences (e.g., /app/outputs vs /app/webapp/outputs)
        if not os.path.exists(xml_path) and '/outputs/' in xml_path:
            try:
                alt = xml_path.replace('/app/outputs', '/app/webapp/outputs')
                if alt != xml_path and os.path.exists(alt):
                    app.logger.info('[full_preview] remapped xml_path %s -> %s', xml_path, alt)
                    xml_path = alt
            except Exception:
                pass
        if not os.path.exists(xml_path) and '/outputs/' in xml_path:
            try:
                alt = xml_path.replace('/app/webapp/outputs', '/app/outputs')
                if alt != xml_path and os.path.exists(alt):
                    app.logger.info('[full_preview] remapped xml_path %s -> %s', xml_path, alt)
                    xml_path = alt
            except Exception:
                pass
        if not os.path.exists(xml_path):
            if embed:
                safe = (xml_path or '').replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
                return Response(
                    '<div style="font-family: system-ui; padding: 16px; color: #6c757d;">'
                    'Save XML first to preview (XML not found):<br><code style="color:#495057;">'
                    + safe +
                    '</code></div>',
                    mimetype='text/html',
                )
            flash(f'XML not found: {xml_path}')
            return redirect(url_for('index'))
        from core_topo_gen.planning.orchestrator import compute_full_plan
        from core_topo_gen.planning.plan_cache import hash_xml_file

        xml_hash = None
        try:
            xml_hash = hash_xml_file(xml_path)
        except Exception:
            xml_hash = None

        plan = compute_full_plan(xml_path, scenario=scenario, seed=seed, include_breakdowns=True)
        if seed is None:
            seed = plan.get('seed') or _derive_default_seed(xml_hash or hash_xml_file(xml_path))
        full_prev = _build_full_preview_from_plan(plan, seed, [], [])
        display_artifacts = full_prev.get('display_artifacts')
        if not display_artifacts:
            try:
                display_artifacts = _attach_display_artifacts(full_prev)
            except Exception:
                display_artifacts = {
                    'segmentation': {
                        'rows': [],
                        'table_rows': [],
                        'tableRows': [],
                        'json': {'rules_count': 0, 'types_summary': {}, 'rules': [], 'metadata': None},
                    },
                    '__version': FULL_PREVIEW_ARTIFACT_VERSION,
                }
        segmentation_artifacts = (display_artifacts or {}).get('segmentation')
        # Annotate & enforce enumerated host roles (Server, Workstation, PC) in preview
        # Full preview already receives normalized roles from planning layer
        # Attempt scenario name
        scenario_name = scenario or None
        if not scenario_name:
            try:
                names_for_cli = _scenario_names_from_xml(xml_path)
                if names_for_cli: scenario_name = names_for_cli[0]
            except Exception:
                pass
        try:
            raw_hitl_config = parse_hitl_info(xml_path, scenario_name)
        except Exception as hitl_exc:
            try:
                app.logger.debug('[plan.full_preview_page] hitl parse failed: %s', hitl_exc)
            except Exception:
                pass
            raw_hitl_config = {"enabled": False, "interfaces": []}
        hitl_config = _sanitize_hitl_config(raw_hitl_config, scenario_name, xml_basename)
        try:
            full_prev['hitl_interfaces'] = hitl_config.get('interfaces', [])
            full_prev['hitl_enabled'] = bool(hitl_config.get('enabled'))
            full_prev['hitl_scenario_key'] = hitl_config.get('scenario_key')
            if hitl_config.get('core'):
                full_prev['hitl_core'] = hitl_config.get('core')
        except Exception:
            pass
        try:
            _merge_hitl_preview_with_full_preview(full_prev, hitl_config)
        except Exception:
            pass
        flow_meta = None
        try:
            meta_flow = meta.get('flow') if isinstance(meta, dict) else None
            if isinstance(meta_flow, dict):
                flow_meta = meta_flow
        except Exception:
            flow_meta = None
        if flow_meta is None:
            try:
                flow_meta = _attach_latest_flow_into_full_preview(full_prev, scenario_name, repair=False)
            except Exception:
                flow_meta = None
        # Do not persist preview plan JSON; use XML as the universal source.
        preview_plan_path = xml_path
        # Provide JSON string for embedding (stringify smaller subset for safety)
        import json as _json
        preview_json_str = _json.dumps(full_prev, indent=2, default=str)
        return render_template(
            'full_preview.html',
            full_preview=full_prev,
            preview_json=preview_json_str,
            xml_path=xml_path,
            scenario=scenario_name,
            seed=full_prev.get('seed'),
            flow_meta=flow_meta or {},
            preview_plan_path=preview_plan_path,
            display_artifacts=display_artifacts,
            segmentation_artifacts=segmentation_artifacts,
            hitl_config=hitl_config,
            xml_basename=xml_basename,
            hide_chrome=embed,
        )
    except Exception as e:
        app.logger.exception('[plan.full_preview_page] error: %s', e)
        flash(f'Full preview page error: {e}')
        return redirect(url_for('index'))


@app.route('/plan/full_preview_from_plan', methods=['POST'])
def plan_full_preview_from_plan():
    """Render Full Preview from an existing persisted preview plan under outputs/plans.

    Form fields: preview_plan (absolute or relative path), optional embed=1
    """
    try:
        embed_raw = request.args.get('embed') or request.form.get('embed') or ''
        embed = str(embed_raw).strip().lower() in ['1', 'true', 'yes', 'y', 'on']
        plan_path = (request.form.get('preview_plan') or '').strip()
        if not plan_path:
            return redirect(url_for('index'))
        try:
            plan_path = os.path.abspath(plan_path)
            plans_dir = os.path.abspath(os.path.join(_outputs_dir(), 'plans'))
            if os.path.commonpath([plan_path, plans_dir]) != plans_dir:
                flash('Invalid preview plan path')
                return redirect(url_for('index'))
            if not os.path.exists(plan_path):
                flash('Preview plan not found')
                return redirect(url_for('index'))
        except Exception:
            flash('Invalid preview plan path')
            return redirect(url_for('index'))

        flash('Preview plans are embedded in XML now; use the XML preview endpoint.')
        return redirect(url_for('index'))

        full_prev = None
        meta = {}

        xml_path = str(meta.get('xml_path') or '')
        scenario_name = str(meta.get('scenario') or '') or None
        seed_val = meta.get('seed')
        try:
            seed_val = int(seed_val) if seed_val is not None else full_prev.get('seed')
        except Exception:
            seed_val = full_prev.get('seed')

        flow_meta = None
        try:
            meta_flow = meta.get('flow') if isinstance(meta, dict) else None
            if isinstance(meta_flow, dict):
                flow_meta = meta_flow
        except Exception:
            flow_meta = None
        if flow_meta is None:
            try:
                flow_meta = _attach_latest_flow_into_full_preview(full_prev, scenario_name, repair=False)
            except Exception:
                flow_meta = None

        # Reconstruct hitl_config for template display from embedded full_preview fields.
        hitl_config = {
            'enabled': bool(full_prev.get('hitl_enabled')),
            'interfaces': full_prev.get('hitl_interfaces') or [],
            'scenario_key': full_prev.get('hitl_scenario_key') or (scenario_name or None),
        }
        try:
            if full_prev.get('hitl_core'):
                hitl_config['core'] = full_prev.get('hitl_core')
        except Exception:
            pass

        display_artifacts = full_prev.get('display_artifacts')
        if not display_artifacts:
            try:
                display_artifacts = _attach_display_artifacts(full_prev)
            except Exception:
                display_artifacts = {
                    'segmentation': {
                        'rows': [],
                        'table_rows': [],
                        'tableRows': [],
                        'json': {'rules_count': 0, 'types_summary': {}, 'rules': [], 'metadata': None},
                    },
                    '__version': FULL_PREVIEW_ARTIFACT_VERSION,
                }
        segmentation_artifacts = (display_artifacts or {}).get('segmentation')

        import json as _json
        preview_json_str = _json.dumps(full_prev, indent=2, default=str)
        xml_basename = None
        try:
            if xml_path:
                xml_basename = os.path.basename(xml_path)
        except Exception:
            xml_basename = None

        try:
            app.logger.info(
                '[plan.full_preview_from_plan] plan=%s scenario=%s seed=%s xml=%s',
                os.path.basename(plan_path),
                scenario_name or '',
                seed_val,
                os.path.basename(xml_path) if xml_path else '',
            )
        except Exception:
            pass
        return render_template(
            'full_preview.html',
            full_preview=full_prev,
            preview_json=preview_json_str,
            xml_path=xml_path,
            scenario=scenario_name,
            seed=seed_val,
            flow_meta=flow_meta or {},
            preview_plan_path=plan_path,
            display_artifacts=display_artifacts,
            segmentation_artifacts=segmentation_artifacts,
            hitl_config=hitl_config,
            xml_basename=xml_basename,
            hide_chrome=embed,
        )
    except Exception as e:
        app.logger.exception('[plan.full_preview_from_plan] error: %s', e)
        flash(f'Full preview error: {e}')
        return redirect(url_for('index'))

@app.route('/plan/full_preview_from_xml', methods=['POST'])
def plan_full_preview_from_xml():
    """Render Full Preview from PlanPreview embedded in a scenarios XML file.

    Form fields: xml_path, optional scenario, optional embed=1
    """
    try:
        start_ts = time.time()
        embed_raw = request.args.get('embed') or request.form.get('embed') or ''
        embed = str(embed_raw).strip().lower() in ['1', 'true', 'yes', 'y', 'on']
        xml_path = (request.form.get('xml_path') or '').strip()
        scenario = (request.form.get('scenario') or '').strip() or None
        if not xml_path:
            return redirect(url_for('index'))
        try:
            xml_path = os.path.abspath(xml_path)
            repo_root = os.path.abspath(_get_repo_root())
            if os.path.commonpath([xml_path, repo_root]) != repo_root:
                flash('Invalid XML path')
                return redirect(url_for('index'))
            if not os.path.exists(xml_path):
                flash('XML not found')
                return redirect(url_for('index'))
        except Exception:
            flash('Invalid XML path')
            return redirect(url_for('index'))

        payload = _load_plan_preview_from_xml(xml_path, scenario)
        if not payload:
            flash('PlanPreview not found in XML')
            return redirect(url_for('index'))

        full_prev = payload.get('full_preview') if isinstance(payload, dict) else None
        if not isinstance(full_prev, dict):
            flash('PlanPreview missing full_preview')
            return redirect(url_for('index'))
        meta = payload.get('metadata') if isinstance(payload, dict) else None
        if not isinstance(meta, dict):
            meta = {}

        # Avoid auto-recomputing on refresh; only recompute when explicitly forced.
        force_refresh = str(request.form.get('force') or '').strip().lower() in ['1', 'true', 'yes', 'y', 'on']
        if force_refresh:
            try:
                plan_ts = None
                updated_at = str(meta.get('updated_at') or meta.get('created_at') or '').strip()
                if updated_at:
                    try:
                        if updated_at.endswith('Z'):
                            updated_at = updated_at[:-1] + '+00:00'
                        plan_ts = datetime.datetime.fromisoformat(updated_at).timestamp()
                    except Exception:
                        plan_ts = None
                if plan_ts is not None:
                    xml_mtime = os.path.getmtime(xml_path)
                    if xml_mtime > plan_ts:
                        try:
                            app.logger.info('[plan.full_preview_from_xml] PlanPreview stale; recomputing for %s', xml_path)
                        except Exception:
                            pass
                        try:
                            recomputed = _planner_persist_flow_plan(
                                xml_path=xml_path,
                                scenario=scenario,
                                seed=meta.get('seed'),
                                persist_plan_file=False,
                            )
                            if isinstance(recomputed, dict) and isinstance(recomputed.get('full_preview'), dict):
                                full_prev = recomputed.get('full_preview')
                            payload = _load_plan_preview_from_xml(xml_path, scenario) or payload
                            meta = payload.get('metadata') if isinstance(payload, dict) else meta
                            if not isinstance(meta, dict):
                                meta = {}
                        except Exception:
                            pass
            except Exception:
                pass

        scenario_name = str(meta.get('scenario') or '') or scenario or None
        seed_val = meta.get('seed')
        try:
            seed_val = int(seed_val) if seed_val is not None else full_prev.get('seed')
        except Exception:
            seed_val = full_prev.get('seed')

        flow_meta = None
        try:
            scen_norm = _normalize_scenario_label(scenario_name or '')
            if scen_norm:
                parsed = _parse_scenarios_xml(xml_path)
                scen_list = parsed.get('scenarios') if isinstance(parsed, dict) else None
                if isinstance(scen_list, list):
                    for sc in scen_list:
                        if not isinstance(sc, dict):
                            continue
                        nm = str(sc.get('name') or '').strip()
                        if _normalize_scenario_label(nm) != scen_norm:
                            continue
                        fs = sc.get('flow_state')
                        if isinstance(fs, dict) and fs:
                            flow_meta = fs
                            break
        except Exception:
            flow_meta = None
        if not isinstance(flow_meta, dict) or not flow_meta:
            flow_meta = None

        hitl_config = {
            'enabled': bool(full_prev.get('hitl_enabled')),
            'interfaces': full_prev.get('hitl_interfaces') or [],
            'scenario_key': full_prev.get('hitl_scenario_key') or (scenario_name or None),
        }
        try:
            if full_prev.get('hitl_core'):
                hitl_config['core'] = full_prev.get('hitl_core')
        except Exception:
            pass

        display_artifacts = full_prev.get('display_artifacts')
        if not display_artifacts:
            try:
                display_artifacts = _attach_display_artifacts(full_prev)
            except Exception:
                display_artifacts = {
                    'segmentation': {
                        'rows': [],
                        'table_rows': [],
                        'tableRows': [],
                        'json': {'rules_count': 0, 'types_summary': {}, 'rules': [], 'metadata': None},
                    },
                    '__version': FULL_PREVIEW_ARTIFACT_VERSION,
                }
        segmentation_artifacts = (display_artifacts or {}).get('segmentation')

        import json as _json
        preview_json_str = _json.dumps(full_prev, indent=2, default=str)
        xml_basename = None
        try:
            xml_basename = os.path.basename(xml_path)
        except Exception:
            xml_basename = None

        try:
            elapsed_ms = int((time.time() - start_ts) * 1000)
            app.logger.info('[plan.full_preview_from_xml] ok in %sms xml=%s scenario=%s', elapsed_ms, xml_path, scenario_name or '')
        except Exception:
            pass
        return render_template(
            'full_preview.html',
            full_preview=full_prev,
            preview_json=preview_json_str,
            xml_path=xml_path,
            scenario=scenario_name,
            seed=seed_val,
            flow_meta=flow_meta or {},
            preview_plan_path=xml_path,
            display_artifacts=display_artifacts,
            segmentation_artifacts=segmentation_artifacts,
            hitl_config=hitl_config,
            xml_basename=xml_basename,
            hide_chrome=embed,
        )
    except Exception as e:
        try:
            elapsed_ms = int((time.time() - start_ts) * 1000) if 'start_ts' in locals() else None
            app.logger.exception('[plan.full_preview_from_xml] error after %sms: %s', elapsed_ms, e)
        except Exception:
            app.logger.exception('[plan.full_preview_from_xml] error: %s', e)
        flash(f'Full preview error: {e}')
        return redirect(url_for('index'))

def _plan_summary_from_full_preview(full_prev: dict) -> dict:
    try:
        role_counts = full_prev.get('role_counts') or {}
    except Exception:
        role_counts = {}
    hosts_total = len(full_prev.get('hosts') or [])
    routers_planned = len(full_prev.get('routers') or [])
    switches = full_prev.get('switches_detail') or []
    services_plan = full_prev.get('services_plan') or full_prev.get('services_preview') or {}
    vuln_plan = full_prev.get('vulnerabilities_plan') or {}
    if not vuln_plan:
        # Backwards/compat: some payloads only have assignments-by-host.
        # Derive planned counts by counting occurrences.
        try:
            vp = full_prev.get('vulnerabilities_preview') or {}
            if isinstance(vp, dict):
                counts: dict[str, int] = {}
                for _hid, vv in vp.items():
                    if isinstance(vv, list):
                        for name in vv:
                            s = str(name or '').strip()
                            if s:
                                counts[s] = counts.get(s, 0) + 1
                if counts:
                    vuln_plan = counts
        except Exception:
            pass
    r2r_policy = full_prev.get('r2r_policy_preview') or {}
    r2s_policy = full_prev.get('r2s_policy_preview') or {}
    summary = {
        'hosts_total': hosts_total,
        'routers_planned': routers_planned,
        'hosts_allocated': 0,
        'routers_allocated': 0,
        'role_counts': role_counts,
        'services_plan': services_plan,
        'services_assigned': {},
        'vulnerabilities_plan': vuln_plan,
        'vulnerabilities_assigned': 0,
        'r2r_policy': r2r_policy,
        'r2s_policy': r2s_policy,
        'switches_allocated': len(switches),
        'notes': ['generated_from_full_preview'],
        'full_preview_seed': full_prev.get('seed'),
    }
    return summary


def _normalize_plan_count_dict(raw: Any) -> dict[str, int]:
    if isinstance(raw, dict):
        normalized: dict[str, int] = {}
        for k, v in raw.items():
            key = str(k or '').strip()
            if not key:
                continue
            try:
                normalized[key] = int(v) if v is not None else 0
            except Exception:
                try:
                    normalized[key] = int(float(v))
                except Exception:
                    normalized[key] = 0
        return normalized
    if isinstance(raw, list):
        counts: dict[str, int] = {}
        for item in raw:
            key = str(item or '').strip()
            if not key:
                continue
            counts[key] = counts.get(key, 0) + 1
        return counts
    return {}


def _normalize_plan_summary(summary: dict[str, Any] | None) -> dict[str, Any]:
    if not isinstance(summary, dict):
        return {}
    normalized = dict(summary)
    normalized['role_counts'] = _normalize_plan_count_dict(summary.get('role_counts'))
    normalized['services_plan'] = _normalize_plan_count_dict(summary.get('services_plan'))
    normalized['vulnerabilities_plan'] = _normalize_plan_count_dict(summary.get('vulnerabilities_plan'))
    try:
        normalized['r2r_policy'] = _json_ready(summary.get('r2r_policy'))
    except Exception:
        normalized['r2r_policy'] = summary.get('r2r_policy')
    try:
        normalized['r2s_policy'] = _json_ready(summary.get('r2s_policy'))
    except Exception:
        normalized['r2s_policy'] = summary.get('r2s_policy')
    return normalized


def _diff_plan_summaries(flow_summary: dict[str, Any], xml_summary: dict[str, Any]) -> list[dict[str, Any]]:
    diffs: list[dict[str, Any]] = []
    flow_norm = _normalize_plan_summary(flow_summary)
    xml_norm = _normalize_plan_summary(xml_summary)

    simple_fields = ['hosts_total', 'routers_planned', 'switches_allocated']
    for key in simple_fields:
        a = flow_norm.get(key)
        b = xml_norm.get(key)
        if a != b:
            diffs.append({'field': key, 'flow': a, 'xml': b})

    dict_fields = ['role_counts', 'services_plan', 'vulnerabilities_plan']
    for key in dict_fields:
        a = flow_norm.get(key) if isinstance(flow_norm.get(key), dict) else {}
        b = xml_norm.get(key) if isinstance(xml_norm.get(key), dict) else {}
        keys = sorted(set(a.keys()) | set(b.keys()))
        for subkey in keys:
            av = a.get(subkey, 0)
            bv = b.get(subkey, 0)
            if av != bv:
                diffs.append({'field': f"{key}.{subkey}", 'flow': av, 'xml': bv})

    policy_fields = ['r2r_policy', 'r2s_policy']
    for key in policy_fields:
        a = flow_norm.get(key)
        b = xml_norm.get(key)
        if a != b:
            diffs.append({'field': key, 'flow': a, 'xml': b})

    return diffs


def _summary_from_preview_plan_path(plan_path: str, scenario_label: str | None = None) -> tuple[dict[str, Any], dict[str, Any]]:
    payload = _load_preview_payload_from_path(plan_path, scenario_label)
    if not isinstance(payload, dict):
        raise ValueError('preview plan not embedded in XML')
    meta = payload.get('metadata') if isinstance(payload, dict) else {}
    full_prev = payload.get('full_preview') if isinstance(payload, dict) else None
    if not isinstance(full_prev, dict):
        raise ValueError('preview plan missing full_preview')
    summary = _plan_summary_from_full_preview(full_prev)
    return summary, (meta if isinstance(meta, dict) else {})


def _summary_from_xml_plan(xml_path: str, scenario: str | None, seed: int | None) -> tuple[dict[str, Any], int | None]:
    # Prefer embedded PlanPreview in XML if available to keep flow/preview aligned.
    try:
        if xml_path and os.path.exists(xml_path):
            tree = ET.parse(xml_path)
            root = tree.getroot()
            scenario_norm = _normalize_scenario_label(scenario or '')
            se_target = None
            if root.tag == 'ScenarioEditor':
                se_target = root
            elif root.tag == 'Scenarios':
                for scen_el in root.findall('Scenario'):
                    nm = str(scen_el.get('name') or '').strip()
                    if scenario_norm and _normalize_scenario_label(nm) != scenario_norm:
                        continue
                    se_target = scen_el.find('ScenarioEditor')
                    if se_target is not None:
                        break
            if se_target is not None:
                plan_el = se_target.find('PlanPreview')
                if plan_el is not None and (plan_el.text or '').strip():
                    plan_payload = json.loads(plan_el.text) or {}
                    full_prev = plan_payload.get('full_preview') if isinstance(plan_payload, dict) else None
                    meta = plan_payload.get('metadata') if isinstance(plan_payload, dict) else None
                    if isinstance(full_prev, dict):
                        summary = _plan_summary_from_full_preview(full_prev)
                        effective_seed = seed
                        if effective_seed is None and isinstance(meta, dict):
                            try:
                                effective_seed = int(meta.get('seed')) if meta.get('seed') is not None else full_prev.get('seed')
                            except Exception:
                                effective_seed = full_prev.get('seed')
                        return summary, effective_seed
    except Exception:
        pass

    from core_topo_gen.planning.orchestrator import compute_full_plan
    from core_topo_gen.planning.plan_cache import hash_xml_file

    xml_hash = hash_xml_file(xml_path)
    plan = compute_full_plan(xml_path, scenario=scenario, seed=seed, include_breakdowns=True)
    effective_seed = seed
    if effective_seed is None:
        try:
            effective_seed = int(plan.get('seed')) if plan.get('seed') is not None else _derive_default_seed(xml_hash)
        except Exception:
            effective_seed = None
    full_prev = _build_full_preview_from_plan(plan, effective_seed, [], [])
    summary = _plan_summary_from_full_preview(full_prev)
    return summary, effective_seed

# --- Unified Preview Helpers (ensure modal JSON preview == full page preview) ---
def _derive_routing_policies(routing_items):
    """Derive R2R and R2S policies from routing items (first item wins)."""
    r2r_policy_plan = None
    r2s_policy_plan = None
    try:
        first_r2r = next((ri for ri in (routing_items or []) if getattr(ri,'r2r_mode',None)), None)  # type: ignore
        if first_r2r:
            m = getattr(first_r2r, 'r2r_mode', '')
            if m == 'Exact' and getattr(first_r2r, 'r2r_edges', 0) > 0:
                r2r_policy_plan = { 'mode': 'Exact', 'target_degree': int(getattr(first_r2r,'r2r_edges',0)) }
            elif m:
                r2r_policy_plan = { 'mode': m }
        first_r2s = next((ri for ri in (routing_items or []) if getattr(ri,'r2s_mode',None)), None)  # type: ignore
        if first_r2s:
            m2 = getattr(first_r2s, 'r2s_mode', '')
            if m2 == 'Exact' and getattr(first_r2s, 'r2s_edges', 0) > 0:
                r2s_policy_plan = { 'mode': 'Exact', 'target_per_router': int(getattr(first_r2s,'r2s_edges',0)) }
            elif m2:
                r2s_policy_plan = { 'mode': m2 }
    except Exception:
        pass
    return r2r_policy_plan, r2s_policy_plan

def _json_ready(value):
    """Convert nested objects into JSON-friendly primitives (recursively)."""
    if isinstance(value, dict):
        return {k: _json_ready(v) for k, v in value.items()}
    if isinstance(value, (list, tuple, set)):
        return [_json_ready(v) for v in value]
    if isinstance(value, (str, int, float, bool)) or value is None:
        return value
    if hasattr(value, '__dict__'):
        try:
            return {k: _json_ready(v) for k, v in vars(value).items() if not k.startswith('_')}
        except Exception:
            pass
    try:
        return str(value)
    except Exception:
        return repr(value)


def _summarize_seg_rule(rule_dict: dict) -> str:
    if not isinstance(rule_dict, dict):
        return ''
    type_raw = rule_dict.get('type') or rule_dict.get('action') or ''
    type_str = str(type_raw).strip()
    if not type_str:
        return ''
    lower = type_str.lower()
    if lower == 'nat':
        mode = str(rule_dict.get('mode')).strip() if rule_dict.get('mode') not in (None, '') else ''
        internal = rule_dict.get('internal') or rule_dict.get('internal_subnet') or ''
        external = rule_dict.get('external') or rule_dict.get('external_subnet') or ''
        parts = []
        if mode:
            parts.append(mode)
        if internal:
            parts.append(str(internal))
        summary = ' '.join(parts)
        if internal and external:
            summary = f"{summary} -> {external}" if summary else f"{internal} -> {external}"
        elif external:
            summary = f"{summary} {external}".strip()
        return summary.strip()
    if lower == 'host_block':
        src = rule_dict.get('src') or rule_dict.get('source') or ''
        dst = rule_dict.get('dst') or rule_dict.get('destination') or ''
        return f"{src} X {dst}".strip()
    if lower == 'custom':
        desc = rule_dict.get('description') or rule_dict.get('summary') or ''
        desc_str = str(desc).strip()
        return desc_str or 'custom'
    src = rule_dict.get('src') or rule_dict.get('source')
    dst = rule_dict.get('dst') or rule_dict.get('destination')
    if src or dst:
        return f"{type_str}: {src or '*'} -> {dst or '*'}"
    return type_str


def _build_segmentation_display_artifacts(full_preview: dict) -> dict:
    seg_preview = {}
    try:
        seg_preview = _json_ready((full_preview or {}).get('segmentation_preview') or {})
    except Exception:
        seg_preview = {}
    raw_rules = []
    if isinstance(seg_preview, dict):
        raw_rules = seg_preview.get('rules') or []
    if not isinstance(raw_rules, list):
        raw_rules = []
    entries = []
    type_counts = {}
    for raw_entry in raw_rules:
        entry = _json_ready(raw_entry)
        if not isinstance(entry, dict):
            continue
        rule_dict = entry.get('rule')
        if rule_dict is None:
            rule_dict = entry
        rule_dict = _json_ready(rule_dict)
        if not isinstance(rule_dict, dict):
            continue
        node_id = entry.get('node_id', rule_dict.get('node_id'))
        rule_type = rule_dict.get('type') or rule_dict.get('action')
        rule_type_str = str(rule_type) if rule_type not in (None, '') else None
        summary = _summarize_seg_rule(rule_dict)
        script_path = entry.get('script') or rule_dict.get('script')
        if not isinstance(script_path, str):
            script_path = None
        script_name = os.path.basename(script_path) if script_path else None
        table_row = {
            'node_id': node_id,
            'type': rule_type_str,
            'summary': summary,
            'src': rule_dict.get('src') or rule_dict.get('source'),
            'dst': rule_dict.get('dst') or rule_dict.get('destination'),
            'subnet': rule_dict.get('subnet'),
            'internal': rule_dict.get('internal') or rule_dict.get('internal_subnet'),
            'external': rule_dict.get('external') or rule_dict.get('external_subnet'),
            'proto': rule_dict.get('proto') or rule_dict.get('protocol'),
            'port': rule_dict.get('port'),
            'script_path': script_path,
            'script_name': script_name,
            'detail': rule_dict,
        }
        entries.append(table_row)
        key = rule_type_str or 'unknown'
        type_counts[key] = type_counts.get(key, 0) + 1
    metadata = None
    if isinstance(seg_preview, dict):
        metadata = {k: v for k, v in seg_preview.items() if k != 'rules'}
        if not metadata:
            metadata = None
    result = {
        'rows': [{'node_id': e['node_id'], 'type': e['type'], 'summary': e['summary']} for e in entries],
        'table_rows': entries,
        'tableRows': entries,
        'json': {
            'rules_count': len(entries),
            'types_summary': type_counts,
            'rules': [
                {
                    'node_id': e['node_id'],
                    'type': e['type'],
                    'summary': e['summary'],
                    'detail': e['detail'],
                }
                for e in entries
            ],
            'metadata': metadata,
        },
    }
    return result


def _attach_display_artifacts(full_preview: dict) -> dict:
    artifacts = {
        'segmentation': _build_segmentation_display_artifacts(full_preview),
        '__version': FULL_PREVIEW_ARTIFACT_VERSION,
    }
    if isinstance(full_preview, dict):
        full_preview['display_artifacts'] = artifacts
        full_preview['display_artifacts_version'] = FULL_PREVIEW_ARTIFACT_VERSION
    return artifacts


def _build_full_preview_from_plan(plan: dict, seed, r2s_hosts_min_list=None, r2s_hosts_max_list=None):
    """Single source of truth to invoke build_full_preview using a compute_full_plan result."""
    try:
        from core_topo_gen.planning.full_preview import build_full_preview  # lazy import
    except ModuleNotFoundError:
        if _ensure_full_preview_module():
            from core_topo_gen.planning.full_preview import build_full_preview  # type: ignore
        else:
            raise
    role_counts = plan['role_counts']
    prelim_router_count = plan['routers_planned']
    routing_items = plan.get('routing_items') or []
    service_plan = plan.get('service_plan') or {}
    vplan = plan.get('vulnerability_plan') or {}
    seg_items_serial = plan.get('breakdowns', {}).get('segmentation', {}).get('raw_items_serialized') or []
    seg_density = plan.get('breakdowns', {}).get('segmentation', {}).get('density')
    r2r_policy_plan, r2s_policy_plan = _derive_routing_policies(routing_items)
    ip4_prefix = plan.get('ip4_prefix') or plan.get('ip_prefix') or '10.0.0.0/16'
    ip_mode = plan.get('ip_mode') or None
    ip_region = plan.get('ip_region') or None
    fp = build_full_preview(
        role_counts=role_counts,
        routers_planned=prelim_router_count,
        services_plan=service_plan,
        vulnerabilities_plan=vplan,
        r2r_policy=r2r_policy_plan,
        r2s_policy=r2s_policy_plan,
        routing_items=routing_items,
        routing_plan=plan.get('breakdowns', {}).get('router', {}).get('simple_plan', {}),
        segmentation_density=seg_density,
        segmentation_items=seg_items_serial,
        traffic_plan=plan.get('traffic_plan'),
        seed=seed,
        ip4_prefix=ip4_prefix,
        ip_mode=ip_mode,
        ip_region=ip_region,
        r2s_hosts_min_list=r2s_hosts_min_list,
        r2s_hosts_max_list=r2s_hosts_max_list,
        base_scenario=plan.get('base_scenario'),
    )
    fp['router_plan'] = plan.get('breakdowns', {}).get('router', {})
    try:
        _attach_display_artifacts(fp)
    except Exception:
        pass
    return fp


@app.route('/api/open_scripts', methods=['GET'])
def api_open_scripts():
    """Return a listing of traffic or segmentation script directory contents.

    Query params: kind=traffic|segmentation
    """
    kind = request.args.get('kind','traffic').lower()
    scope = request.args.get('scope','runtime').lower()  # runtime|preview
    if kind not in ('traffic','segmentation'):
        return jsonify({'ok': False, 'error': 'invalid kind'}), 400
    if scope == 'preview':
        # Look for latest preview dir (deterministic naming core-topo-preview-*)
        import tempfile, glob
        base = tempfile.gettempdir()
        pattern = 'core-topo-preview-traffic-*' if kind=='traffic' else 'core-topo-preview-seg-*'
        candidates = sorted(glob.glob(os.path.join(base, pattern)), key=lambda p: os.path.getmtime(p), reverse=True)
        path = candidates[0] if candidates else None
        if not path:
            return jsonify({'ok': False, 'error': 'no preview dir found for kind', 'pattern': pattern}), 404
    else:
        path = '/tmp/traffic' if kind == 'traffic' else '/tmp/segmentation'
    if not os.path.isdir(path):
        return jsonify({'ok': False, 'error': 'directory does not exist', 'path': path}), 404
    files = []
    try:
        for name in sorted(os.listdir(path)):
            fp = os.path.join(path, name)
            if not os.path.isfile(fp):
                continue
            try:
                sz = os.path.getsize(fp)
            except Exception:
                sz = 0
            files.append({'file': name, 'size': sz})
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)}), 500
    return jsonify({'ok': True, 'kind': kind, 'path': path, 'files': files})

@app.route('/api/open_script_file', methods=['GET'])
def api_open_script_file():
    """Return (truncated) contents of a requested script file.

    Query params: kind=traffic|segmentation, scope=runtime|preview, file=<filename>
    """
    kind = request.args.get('kind','traffic').lower()
    scope = request.args.get('scope','runtime').lower()
    fname = request.args.get('file') or ''
    if kind not in ('traffic','segmentation'):
        return jsonify({'ok': False, 'error': 'invalid kind'}), 400
    if not fname or '/' in fname or '..' in fname:
        return jsonify({'ok': False, 'error': 'invalid filename'}), 400
    if scope == 'preview':
        import tempfile, glob
        base = tempfile.gettempdir()
        pattern = 'core-topo-preview-traffic-*' if kind=='traffic' else 'core-topo-preview-seg-*'
        candidates = sorted(glob.glob(os.path.join(base, pattern)), key=lambda p: os.path.getmtime(p), reverse=True)
        path = candidates[0] if candidates else None
    else:
        path = '/tmp/traffic' if kind == 'traffic' else '/tmp/segmentation'
    if not path or not os.path.isdir(path):
        return jsonify({'ok': False, 'error': 'dir not found', 'path': path}), 404
    fp = os.path.join(path, fname)
    if not os.path.isfile(fp):
        return jsonify({'ok': False, 'error': 'file not found', 'file': fname}), 404
    try:
        with open(fp, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read(8000)
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)}), 500
    return jsonify({'ok': True, 'file': fname, 'path': path, 'content': content, 'truncated': len(content)==8000})

@app.route('/api/download_scripts', methods=['GET'])
def api_download_scripts():
    """Download a zip of segmentation or traffic scripts (preview or runtime).

    Query: kind=traffic|segmentation scope=runtime|preview
    """
    kind = request.args.get('kind','traffic').lower()
    scope = request.args.get('scope','runtime').lower()
    if kind not in ('traffic','segmentation'):
        return jsonify({'ok': False, 'error': 'invalid kind'}), 400
    if scope not in ('runtime','preview'):
        return jsonify({'ok': False, 'error': 'invalid scope'}), 400
    # Resolve directory
    if scope == 'runtime':
        base_dir = '/tmp/traffic' if kind=='traffic' else '/tmp/segmentation'
    else:
        import tempfile, glob
        pattern = 'core-topo-preview-traffic-*' if kind=='traffic' else 'core-topo-preview-seg-*'
        cands = sorted(glob.glob(os.path.join(tempfile.gettempdir(), pattern)), key=lambda p: os.path.getmtime(p), reverse=True)
        base_dir = cands[0] if cands else None
    if not base_dir or not os.path.isdir(base_dir):
        return jsonify({'ok': False, 'error': 'directory not found'}), 404
    import io, zipfile
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, 'w', zipfile.ZIP_DEFLATED) as zf:
        for root, _dirs, files in os.walk(base_dir):
            for f in files:
                fp = os.path.join(root, f)
                # avoid huge non-script artifacts except summary json
                if not (f.endswith('.py') or f.endswith('.json')):
                    continue
                arc = os.path.relpath(fp, base_dir)
                try:
                    zf.write(fp, arc)
                except Exception:
                    continue
    buf.seek(0)
    from flask import send_file as _send_file
    filename = f"{kind}_{scope}_scripts.zip"
    return _send_file(buf, mimetype='application/zip', as_attachment=True, download_name=filename)

@app.route('/download_report')
def download_report():
    result_path = request.args.get('path')
    # Normalize incoming value: strip quotes, decode percent-encoding, handle file://, expand ~
    try:
        if result_path:
            # strip surrounding quotes if present
            if (result_path.startswith('"') and result_path.endswith('"')) or (result_path.startswith("'") and result_path.endswith("'")):
                result_path = result_path[1:-1]
            # convert file:// URIs
            if result_path.startswith('file://'):
                result_path = result_path[len('file://'):]
            # percent-decode
            try:
                from urllib.parse import unquote
                result_path = unquote(result_path)
            except Exception:
                pass
            # expand ~ and normalize slashes
            result_path = os.path.expanduser(result_path)
            result_path = os.path.normpath(result_path)
    except Exception:
        pass
    # Attempt to resolve common path variants to absolute existing file
    candidates = []
    if result_path:
        candidates.append(result_path)
        # Absolute from repo root if provided as repo-relative
        try:
            repo_root = _get_repo_root()
            if not os.path.isabs(result_path):
                candidates.append(os.path.abspath(os.path.join(repo_root, result_path)))
            # Also try if client included an extra 'webapp/' segment
            if result_path.startswith('webapp' + os.sep):
                candidates.append(os.path.abspath(os.path.join(repo_root, result_path)))
                # Strip 'webapp/' and try from repo root
                candidates.append(os.path.abspath(os.path.join(repo_root, result_path.split(os.sep, 1)[-1])))
            # If path looks like outputs/<...>, join with configured outputs dir
            if result_path.startswith('outputs' + os.sep):
                candidates.append(os.path.abspath(os.path.join(_outputs_dir(), result_path.split(os.sep, 1)[-1])))
            # If absolute path contains '/webapp/outputs/...', remap to configured outputs dir
            rp_norm = os.path.normpath(result_path)
            parts = rp_norm.strip(os.sep).split(os.sep)
            if os.path.isabs(result_path) and 'outputs' in parts:
                try:
                    idx = parts.index('outputs')
                    tail = os.path.join(*parts[idx+1:]) if idx+1 < len(parts) else ''
                    candidates.append(os.path.join(_outputs_dir(), tail))
                except Exception:
                    pass
            if os.path.isabs(result_path) and 'webapp' in parts:
                # Remove the 'webapp' segment entirely
                parts_wo = [p for p in parts if p != 'webapp']
                candidates.append(os.path.sep + os.path.join(*parts_wo))
            # If the path already lives under our configured outputs dir but with different root, try direct mapping
            try:
                outputs_dir = os.path.abspath(_outputs_dir())
                if os.path.isabs(result_path) and 'core-sessions' in parts and not result_path.startswith(outputs_dir):
                    # replace everything up to 'core-sessions' with outputs_dir/core-sessions
                    idx = parts.index('core-sessions')
                    tail = os.path.join(*parts[idx+1:]) if idx+1 < len(parts) else ''
                    candidates.append(os.path.join(outputs_dir, 'core-sessions', tail))
            except Exception:
                pass
        except Exception:
            pass
    # Pick the first existing path
    chosen = None
    for p in candidates:
        if p and os.path.exists(p):
            chosen = p
            break
    if chosen:
        try:
            app.logger.info("[download] serving file: %s", os.path.abspath(chosen))
        except Exception:
            pass
        return send_file(chosen, as_attachment=True)
    # Fallback: try to match by basename within outputs/core-sessions and outputs/scenarios-*
    try:
        # Log diagnostics about missing primary candidates
        app.logger.warning("[download] file not found via direct candidates; requested=%s; candidates=%s", result_path, candidates)
    except Exception:
        pass
    try:
        base_name = None
        try:
            base_name = os.path.basename(result_path) if result_path else None
        except Exception:
            base_name = None
        if base_name and base_name.lower().endswith('.xml'):
            candidates_found = []
            # Search core-sessions
            root_dir = os.path.join(_outputs_dir(), 'core-sessions')
            if os.path.exists(root_dir):
                for dp, _dn, files in os.walk(root_dir):
                    for fn in files:
                        if fn == base_name:
                            alt = os.path.join(dp, fn)
                            if os.path.exists(alt):
                                candidates_found.append(alt)
            # Search scenarios-* (Scenario Editor saves)
            out_dir = _outputs_dir()
            if os.path.exists(out_dir):
                try:
                    for name in os.listdir(out_dir):
                        if not name.startswith('scenarios-'):
                            continue
                        p = os.path.join(out_dir, name)
                        if not os.path.isdir(p):
                            continue
                        for dp, _dn, files in os.walk(p):
                            for fn in files:
                                if fn == base_name:
                                    alt = os.path.join(dp, fn)
                                    if os.path.exists(alt):
                                        candidates_found.append(alt)
                except Exception:
                    pass
            if candidates_found:
                # Prefer the newest by mtime
                try:
                    candidates_found.sort(key=lambda p: os.stat(p).st_mtime, reverse=True)
                except Exception:
                    pass
                chosen_alt = candidates_found[0]
                app.logger.info("[download] basename match: %s -> %s", base_name, chosen_alt)
                return send_file(chosen_alt, as_attachment=True)
    except Exception:
        pass
    app.logger.warning("[download] file not found: %s (candidates=%s)", result_path, candidates)
    return "File not found", 404


@app.route('/schemas/node_authoring.yaml')
def node_schema_authoring_yaml():
    path = _node_schema_authoring_path()
    if not path or not os.path.isfile(path):
        return jsonify({'ok': False, 'error': 'schema not found'}), 404
    return send_file(
        path,
        mimetype='text/yaml; charset=utf-8',
        as_attachment=False,
        download_name='node_schema_authoring.yaml',
    )

@app.route('/reports')
def reports_page():
    raw = _load_run_history()
    enriched = []
    for entry in raw:
        e = dict(entry)
        # Per-scenario: always show exactly one scenario name.
        if not (isinstance(e.get('scenario_names'), list) and e.get('scenario_names')):
            scen = (e.get('scenario_name') or '').strip() if isinstance(e.get('scenario_name'), str) else ''
            if scen:
                e['scenario_names'] = [scen]
            else:
                src_xml = e.get('single_scenario_xml_path') or e.get('scenario_xml_path') or e.get('xml_path')
                names = _scenario_names_from_xml(src_xml) if src_xml else []
                e['scenario_names'] = [names[0]] if isinstance(names, list) and names else []
        # Normalize session xml pointer for UI compatibility
        session_xml = e.get('session_xml_path') or e.get('post_xml_path')
        if session_xml:
            e['session_xml_path'] = session_xml
        if not e.get('summary_path'):
            derived_summary = _derive_summary_from_report(e.get('report_path'))
            if derived_summary:
                e['summary_path'] = derived_summary
        # Hardening: ensure scenario_names is always a list (and truncate to 1)
        sn = e.get('scenario_names')
        if not isinstance(sn, list):
            if sn is None:
                e['scenario_names'] = []
            elif isinstance(sn, str):
                # Split comma or pipe delimited legacy forms
                if '||' in sn:
                    e['scenario_names'] = [s for s in sn.split('||') if s]
                else:
                    e['scenario_names'] = [s.strip() for s in sn.split(',') if s.strip()]
            else:
                e['scenario_names'] = []
        if isinstance(e.get('scenario_names'), list) and len(e['scenario_names']) > 1:
            e['scenario_names'] = [e['scenario_names'][0]]
        enriched.append(e)
    enriched = sorted(enriched, key=lambda x: x.get('timestamp',''), reverse=True)
    user = _current_user()
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(
        enriched,
        user=user,
    )
    scenario_participant_urls = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
    participant_url_flags = {
        norm: bool(url)
        for norm, url in scenario_participant_urls.items()
        if isinstance(norm, str) and norm
    }
    scenario_query = request.args.get('scenario', '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    scenario_names, scenario_norm, _allowed_norms = _builder_filter_report_scenarios(
        scenario_names,
        scenario_norm,
        user=user,
    )
    # Enforce per-scenario view: default to the first available scenario when unset.
    if scenario_names and not scenario_norm:
        scenario_norm = _normalize_scenario_label(scenario_names[0])
    if scenario_norm:
        enriched = _filter_history_by_scenario(enriched, scenario_norm)
    for entry in enriched:
        try:
            counts = _load_summary_counts(entry.get('summary_path'))
            entry['summary_output'] = _summary_text_from_counts(counts)
        except Exception:
            entry['summary_output'] = ''
    scenario_display = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query)
    return render_template(
        'reports.html',
        history=enriched,
        scenarios=scenario_names,
        active_scenario=scenario_display,
        participant_url_flags=participant_url_flags,
    )

@app.route('/reports_data')
def reports_data():
    raw = _load_run_history()
    enriched = []
    for entry in raw:
        e = dict(entry)
        # Per-scenario: always show exactly one scenario name.
        if not (isinstance(e.get('scenario_names'), list) and e.get('scenario_names')):
            scen = (e.get('scenario_name') or '').strip() if isinstance(e.get('scenario_name'), str) else ''
            if scen:
                e['scenario_names'] = [scen]
            else:
                src_xml = e.get('single_scenario_xml_path') or e.get('scenario_xml_path') or e.get('xml_path')
                names = _scenario_names_from_xml(src_xml) if src_xml else []
                e['scenario_names'] = [names[0]] if isinstance(names, list) and names else []
        session_xml = e.get('session_xml_path') or e.get('post_xml_path')
        if session_xml:
            e['session_xml_path'] = session_xml
        if not e.get('summary_path'):
            derived_summary = _derive_summary_from_report(e.get('report_path'))
            if derived_summary:
                e['summary_path'] = derived_summary
        # Hardening: normalize scenario_names to list (and truncate to 1)
        sn = e.get('scenario_names')
        if not isinstance(sn, list):
            if sn is None:
                e['scenario_names'] = []
            elif isinstance(sn, str):
                if '||' in sn:
                    e['scenario_names'] = [s for s in sn.split('||') if s]
                else:
                    e['scenario_names'] = [s.strip() for s in sn.split(',') if s.strip()]
            else:
                e['scenario_names'] = []
        if isinstance(e.get('scenario_names'), list) and len(e['scenario_names']) > 1:
            e['scenario_names'] = [e['scenario_names'][0]]
        try:
            counts = _load_summary_counts(e.get('summary_path'))
            e['summary_output'] = _summary_text_from_counts(counts)
        except Exception:
            e['summary_output'] = ''
        enriched.append(e)
    enriched = sorted(enriched, key=lambda x: x.get('timestamp',''), reverse=True)
    user = _current_user()
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(
        enriched,
        user=user,
    )
    scenario_participant_urls = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
    participant_url_flags = {
        norm: bool(url)
        for norm, url in scenario_participant_urls.items()
        if isinstance(norm, str) and norm
    }
    scenario_query = request.args.get('scenario', '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    scenario_names, scenario_norm, _allowed_norms = _builder_filter_report_scenarios(
        scenario_names,
        scenario_norm,
        user=user,
    )
    # Enforce per-scenario view: default to the first available scenario when unset.
    if scenario_names and not scenario_norm:
        scenario_norm = _normalize_scenario_label(scenario_names[0])
    if scenario_norm:
        enriched = _filter_history_by_scenario(enriched, scenario_norm)
    scenario_display = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query)
    return jsonify({
        'history': enriched,
        'scenarios': scenario_names,
        'active_scenario': scenario_display,
        'participant_url_flags': participant_url_flags,
    })

@app.route('/reports/delete', methods=['POST'])
def reports_delete():
    """Delete run history entries by run_id and remove associated artifacts under outputs/.
    Does not delete files under ./reports (reports are preserved by policy).
    Body: { "run_ids": ["...", ...] }
    """
    try:
        payload = request.get_json(force=True, silent=True) or {}
        run_ids = payload.get('run_ids') or []
        if not isinstance(run_ids, list):
            return jsonify({ 'error': 'run_ids must be a list' }), 400
        run_ids_set = set([str(x) for x in run_ids if x])
        if not run_ids_set:
            return jsonify({ 'deleted': 0 })
        history = _load_run_history()
        kept = []
        deleted_count = 0
        outputs_dir = _outputs_dir()
        for entry in history:
            rid = str(entry.get('run_id') or '')
            # fallback composite id to support entries without run_id
            rid_fallback = "|".join([
                str(entry.get('timestamp') or ''),
                str(entry.get('scenario_xml_path') or entry.get('xml_path') or ''),
                str(entry.get('report_path') or ''),
                str(entry.get('full_scenario_path') or ''),
            ])
            if (rid and rid in run_ids_set) or (rid_fallback and rid_fallback in run_ids_set):
                # Delete artifacts scoped to outputs/ only
                for key in ('full_scenario_path','scenario_xml_path','pre_xml_path','post_xml_path','xml_path','single_scenario_xml_path'):
                    p = entry.get(key)
                    if not p: continue
                    try:
                        ap = os.path.abspath(p)
                        if ap.startswith(os.path.abspath(outputs_dir)) and os.path.exists(ap):
                            try:
                                os.remove(ap)
                                app.logger.info("[reports.delete] removed %s", ap)
                            except IsADirectoryError:
                                # just in case, do not remove directories recursively here
                                app.logger.warning("[reports.delete] skipping directory %s", ap)
                    except Exception as e:
                        app.logger.warning("[reports.delete] error removing %s: %s", p, e)
                deleted_count += 1
            else:
                kept.append(entry)
        # Persist pruned history
        os.makedirs(os.path.dirname(RUN_HISTORY_PATH), exist_ok=True)
        tmp = RUN_HISTORY_PATH + '.tmp'
        with open(tmp, 'w', encoding='utf-8') as f:
            json.dump(kept, f, indent=2)
        os.replace(tmp, RUN_HISTORY_PATH)
        return jsonify({ 'deleted': deleted_count })
    except Exception as e:
        app.logger.exception("[reports.delete] failed: %s", e)
        return jsonify({ 'error': 'internal error' }), 500


def _close_async_run_tunnel(meta: Dict[str, Any]) -> None:
    tunnel = meta.pop('ssh_tunnel', None)
    if tunnel:
        try:
            tunnel.close()
        except Exception:
            pass


def _append_async_run_log_line(meta: Dict[str, Any] | None, line: str) -> None:
    if not meta:
        return
    lp = meta.get('log_path')
    if not lp:
        return
    try:
        with open(lp, 'a', encoding='utf-8', buffering=1) as _f:
            _f.write(line.rstrip('\n') + "\n")
    except Exception:
        pass


def _session_docker_nodes_from_xml(session_xml_path: str) -> List[str]:
    names: List[str] = []
    if not session_xml_path or not os.path.exists(session_xml_path):
        return names
    try:
        import xml.etree.ElementTree as _ET
        root = _ET.parse(session_xml_path).getroot()
    except Exception:
        return names
    candidates = []
    try:
        candidates.extend(root.findall('.//node'))
    except Exception:
        pass
    try:
        candidates.extend(root.findall('.//device'))
    except Exception:
        pass
    for node in candidates:
        try:
            ntype = (node.get('type') or node.get('model') or '').strip().lower()
            nclass = (node.get('class') or '').strip().lower()
            compose = (node.get('compose') or '').strip()
            compose_name = (node.get('compose_name') or '').strip()
        except Exception:
            ntype = ''
            nclass = ''
            compose = ''
            compose_name = ''
        is_docker = False
        if ntype in ('docker', 'docker-compose', 'docker_node', 'docker-node') or 'docker' in ntype:
            is_docker = True
        if nclass and 'docker' in nclass:
            is_docker = True
        if compose or compose_name:
            is_docker = True
        if not is_docker:
            continue
        nm = (node.get('name') or node.findtext('name') or '').strip()
        if nm:
            names.append(nm)
    # Deduplicate while preserving order
    seen: set[str] = set()
    out: List[str] = []
    for n in names:
        if n in seen:
            continue
        seen.add(n)
        out.append(n)
    return out


def _remote_docker_injects_status_script(
    *,
    containers: List[str],
    sudo_password: str | None = None,
    inject_dirs: List[str] | None = None,
    max_find: int = 200,
) -> str:
    containers_literal = json.dumps([str(x) for x in (containers or [])])
    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    max_find_literal = json.dumps(int(max_find))
    inject_dirs_literal = json.dumps([str(d) for d in (inject_dirs or ['/flow_injects', '/flow_artifacts'])])
    return (
        r"""
import json, subprocess

CONTAINERS = __CONTAINERS_LITERAL__
SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__
MAX_FIND = __MAX_FIND_LITERAL__
INJECT_DIRS = __INJECT_DIRS_LITERAL__


def _run(cmd, timeout=30):
    try:
        p = subprocess.run(['sudo', '-n'] + list(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', ''] + list(cmd),
                input=str(SUDO_PASSWORD) + "\n",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        return subprocess.CompletedProcess(cmd, 127, stdout=str(e))


def main():
    items = []
    for c in CONTAINERS:
        c = str(c or '').strip()
        if not c:
            continue
        exists = False
        running = False
        try:
            p = _run(['docker', 'inspect', '--format', '{{.State.Running}}', c], timeout=20)
            if p.returncode == 0:
                exists = True
                running = (str(p.stdout or '').strip().lower() == 'true')
        except Exception:
            exists = False
            running = False
        inject_count = 0
        inject_samples = []
        inject_dirs_found = []
        debug_logs = []
        if exists:
            for d in INJECT_DIRS:
                if not d:
                    continue
                shell = (
                    'set -e; '
                    'mesg n >/dev/null 2>&1 || true; '
                    f"if [ -d {d} ]; then "
                    f"find {d} -maxdepth 4 -type f -print 2>/dev/null | head -n {MAX_FIND}; "
                    f"fi; "
                )
                p2 = _run(['docker', 'exec', c, 'sh', '-c', shell], timeout=25)
                out = (p2.stdout or '').strip()
                if out:
                    lines = [ln for ln in out.splitlines() if ln.strip()]
                    if lines:
                        inject_dirs_found.append(d)
                        inject_samples = (inject_samples + lines)[:5]
                        inject_count += len(lines)
                else:
                    if d:
                        shell_dbg = (
                            'set -e; '
                            'mesg n >/dev/null 2>&1 || true; '
                            f"if [ -d {d} ]; then ls -la {d} 2>&1 | head -n 10; else echo 'MISSING_DIR:{d}'; fi; "
                        )
                        p3 = _run(['docker', 'exec', c, 'sh', '-c', shell_dbg], timeout=20)
                        dbg = (p3.stdout or '').strip()
                        if dbg:
                            debug_logs.append(dbg)
        items.append({
            'container': c,
            'exists': exists,
            'running': running,
            'inject_count': inject_count,
            'inject_samples': inject_samples,
            'inject_dirs_found': inject_dirs_found,
            'debug_logs': debug_logs,
        })
    print(json.dumps({'ok': True, 'items': items}))


if __name__ == '__main__':
    main()
"""
    ).replace('__CONTAINERS_LITERAL__', containers_literal) \
     .replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal) \
     .replace('__MAX_FIND_LITERAL__', max_find_literal) \
     .replace('__INJECT_DIRS_LITERAL__', inject_dirs_literal)


def _remote_flow_artifacts_validation_script(assignments: List[Dict[str, Any]]) -> str:
    assignments_literal = json.dumps(assignments or [])
    return (
        r"""
import json, os

ASSIGNMENTS = __ASSIGNMENTS_LITERAL__


def _safe_exists(p: str) -> bool:
    try:
        return bool(p and os.path.exists(p))
    except Exception:
        return False


def _looks_like_path(key, val) -> bool:
    try:
        if not isinstance(val, str):
            return False
        v = val.strip()
        if not v:
            return False
        k = str(key or '').lower()
        if 'path' in k or 'file' in k:
            return True
        if v.startswith('/') or v.startswith('artifacts/'):
            return True
        if '/' in v:
            return True
        return False
    except Exception:
        return False


def _resolve_output_path(val: str, run_dir: str, artifacts_dir: str) -> str:
    v = str(val or '').strip()
    if not v:
        return ''
    if os.path.isabs(v):
        return v
    if v.startswith('artifacts/') and artifacts_dir:
        return os.path.join(artifacts_dir, v.split('artifacts/', 1)[1].lstrip('/'))
    if run_dir:
        return os.path.join(run_dir, v.lstrip('/'))
    if artifacts_dir:
        return os.path.join(artifacts_dir, v.lstrip('/'))
    return v


def _normalize_inject_src_for_copy(raw_src: str, source_dir: str) -> str:
    src = str(raw_src or '').strip()
    if not src:
        return ''
    if not os.path.isabs(src):
        return src
    try:
        if source_dir:
            src_abs = os.path.abspath(src)
            base_abs = os.path.abspath(source_dir)
            if os.path.commonpath([src_abs, base_abs]) == base_abs:
                return os.path.relpath(src_abs, base_abs)
    except Exception:
        pass
    for marker in ('/artifacts/', '/flow_artifacts/'):
        if marker in src:
            return src.split(marker, 1)[1].lstrip('/')
    return ''


def _normalize_inject_spec_for_copy(raw: str, source_dir: str) -> str:
    text = str(raw or '').strip()
    if not text:
        return ''
    sep = '->' if '->' in text else '=>' if '=>' in text else ''
    if sep:
        left, _right = text.split(sep, 1)
        src_norm = _normalize_inject_src_for_copy(left.strip(), source_dir)
        return src_norm
    return _normalize_inject_src_for_copy(text, source_dir)


def _inject_files_for_copy_from_detail(detail_list, source_dir: str) -> list[str]:
    if not isinstance(detail_list, list):
        return []
    out = []
    for entry in detail_list:
        if not isinstance(entry, dict):
            continue
        resolved = str(entry.get('resolved') or '').strip()
        path = str(entry.get('path') or '').strip()
        if not path:
            continue
        dest_dir = os.path.dirname(path) if path.startswith('/') else ''
        if dest_dir == '/' and path.startswith('/'):
            dest_dir = path.rstrip('/') or '/'
        if not dest_dir or dest_dir == '/':
            continue
        src_norm = _normalize_inject_src_for_copy(resolved, source_dir)
        if not src_norm:
            continue
        out.append(src_norm)
    return out


def main():
    items = []
    for a in (ASSIGNMENTS or []):
        if not isinstance(a, dict):
            continue
        node_id = str(a.get('node_id') or '')
        gen_id = str(a.get('id') or a.get('generator_id') or '')
        gen_name = str(a.get('generator_name') or a.get('name') or '')
        gen_type = str(a.get('generator_type') or a.get('type') or '')
        run_dir = str(a.get('run_dir') or a.get('artifacts_dir') or '')
        artifacts_dir = str(a.get('mount_dir') or a.get('artifacts_dir') or '')
        outputs_manifest = str(a.get('outputs_manifest') or '')
        outputs_missing = []
        outputs_checked = []
        manifest_expected = bool(outputs_manifest)
        manifest_exists = _safe_exists(outputs_manifest)
        outs = None
        if manifest_expected and not manifest_exists:
            outputs_missing.append(f"missing manifest: {outputs_manifest}")
        if manifest_exists:
            try:
                with open(outputs_manifest, 'r', encoding='utf-8') as f:
                    m = json.load(f) or {}
                outs = m.get('outputs') if isinstance(m, dict) else None
            except Exception:
                outs = None
        if manifest_exists and not (isinstance(outs, dict) and outs):
            outputs_missing.append(f"empty manifest: {outputs_manifest}")
        if isinstance(outs, dict):
            for k, v in outs.items():
                if not _looks_like_path(k, v):
                    continue
                p = _resolve_output_path(str(v), run_dir, artifacts_dir)
                if not p:
                    continue
                outputs_checked.append(p)
                if not _safe_exists(p):
                    outputs_missing.append(p)

        source_dir = artifacts_dir or run_dir
        inject_sources = _inject_files_for_copy_from_detail(a.get('inject_files_detail'), source_dir)
        if not inject_sources:
            inj = a.get('inject_files') if isinstance(a.get('inject_files'), list) else []
            for raw in inj:
                src_norm = _normalize_inject_spec_for_copy(raw, source_dir)
                if src_norm:
                    inject_sources.append(src_norm)
        inject_missing = []
        inject_checked = []
        for src in inject_sources:
            if os.path.isabs(src):
                p = src
            elif source_dir:
                p = os.path.join(source_dir, src.lstrip('/'))
            else:
                p = src
            inject_checked.append(p)
            if not _safe_exists(p):
                inject_missing.append(p)

        items.append({
            'node_id': node_id,
            'generator_id': gen_id,
            'generator_name': gen_name,
            'generator_type': gen_type,
            'run_dir': run_dir,
            'artifacts_dir': artifacts_dir,
            'outputs_manifest': outputs_manifest,
            'outputs_manifest_exists': bool(manifest_exists),
            'outputs_checked': outputs_checked,
            'outputs_missing': outputs_missing,
            'inject_checked': inject_checked,
            'inject_missing': inject_missing,
        })

    print(json.dumps({'ok': True, 'items': items}))


if __name__ == '__main__':
    main()
"""
    ).replace('__ASSIGNMENTS_LITERAL__', assignments_literal)


def _remote_vuln_assignments_script(assignments_path: str = '/tmp/vulns/compose_assignments.json') -> str:
    assignments_literal = json.dumps(str(assignments_path))
    return (
        r"""
import json, os

ASSIGNMENTS_PATH = __ASSIGNMENTS_PATH_LITERAL__

def main():
    if not ASSIGNMENTS_PATH or not os.path.exists(ASSIGNMENTS_PATH):
        print(json.dumps({'ok': False, 'error': 'missing compose_assignments', 'nodes': []}))
        return
    try:
        with open(ASSIGNMENTS_PATH, 'r', encoding='utf-8') as f:
            payload = json.load(f)
    except Exception as exc:
        print(json.dumps({'ok': False, 'error': f'read_failed: {exc}', 'nodes': []}))
        return
    assignments = payload.get('assignments') if isinstance(payload, dict) else None
    nodes = []
    if isinstance(assignments, dict):
        nodes = [str(k) for k in assignments.keys() if str(k).strip()]
    print(json.dumps({'ok': True, 'nodes': nodes}))

if __name__ == '__main__':
    main()
"""
    ).replace('__ASSIGNMENTS_PATH_LITERAL__', assignments_literal)


def _validate_session_nodes_and_injects(
    *,
    scenario_xml_path: str | None,
    session_xml_path: str | None,
    core_cfg: Dict[str, Any] | None,
    preview_plan_path: str | None = None,
    scenario_label: str | None = None,
) -> Dict[str, Any]:
    summary: Dict[str, Any] = {
        'ok': True,
        'missing_nodes': [],
        'extra_nodes': [],
        'missing_node_ids': [],
        'extra_node_ids': [],
        'expected_nodes': [],
        'actual_nodes': [],
        'actual_nodes_detail': [],
        'docker_nodes': [],
        'expected_docker_nodes': [],
        'missing_docker_nodes': [],
        'extra_docker_nodes': [],
        'expected_vuln_nodes': [],
        'missing_vuln_nodes': [],
        'actual_vuln_nodes': [],
        'docker_missing': [],
        'docker_not_running': [],
        'docker_running': [],
        'injects_missing': [],
        'injects_detail': [],
        'inject_dirs_expected': [],
        'inject_nodes_expected': [],
        'inject_files_expected_by_node': {},
        'generator_outputs_missing': [],
        'generator_injects_missing': [],
        'generator_validation_detail': [],
    }
    if not scenario_xml_path or not session_xml_path:
        summary['ok'] = False
        summary['error'] = 'missing scenario or session xml'
        return summary

    try:
        expected = _expected_from_plan_preview(scenario_xml_path, scenario_label)
        actual = _parse_session_xml_for_compare(session_xml_path)
        expected_ids = set(expected.keys())
        actual_ids = set(actual.keys())
        missing_ids = sorted(expected_ids - actual_ids)
        extra_ids = sorted(actual_ids - expected_ids)
        summary['missing_node_ids'] = missing_ids
        summary['extra_node_ids'] = extra_ids
        missing_names = []
        for nid in missing_ids:
            exp = expected.get(nid) or {}
            nm = (exp.get('name') or '').strip()
            missing_names.append(nm or f"node-{nid}")
        extra_names = []
        for nid in extra_ids:
            act = actual.get(nid) or {}
            nm = (act.get('name') or '').strip()
            extra_names.append(nm or f"node-{nid}")
        summary['missing_nodes'] = missing_names
        summary['extra_nodes'] = extra_names
        try:
            expected_names = []
            for nid in sorted(expected_ids):
                exp = expected.get(nid) or {}
                nm = (exp.get('name') or '').strip()
                expected_names.append(nm or f"node-{nid}")
            actual_names = []
            actual_detail = []
            for nid in sorted(actual_ids):
                act = actual.get(nid) or {}
                nm = (act.get('name') or '').strip()
                ntype = (act.get('type') or '').strip()
                label = nm or f"node-{nid}"
                actual_names.append(label)
                if ntype:
                    actual_detail.append(f"{label} ({ntype})")
                else:
                    actual_detail.append(f"{label} (unknown)")
            summary['expected_nodes'] = expected_names
            summary['actual_nodes'] = actual_names
            summary['actual_nodes_detail'] = actual_detail
        except Exception:
            pass
    except Exception:
        pass

    flow_inject_specs = _extract_inject_specs_from_flow_state(scenario_xml_path, scenario_label)
    inject_expected_by_node = _extract_inject_expected_by_node(scenario_xml_path, scenario_label)
    if flow_inject_specs:
        flow_payload = {'inject_files': flow_inject_specs}
        expected_inject_dirs = _extract_inject_dirs_from_payload(flow_payload)
        expected_inject_files = _extract_inject_files_from_payload(flow_payload)
    else:
        expected_inject_dirs = _extract_inject_dirs_from_plan_xml(scenario_xml_path, scenario_label)
        expected_inject_files = _extract_inject_files_from_plan_xml(scenario_xml_path, scenario_label)
    if preview_plan_path and not flow_inject_specs:
        try:
            payload = _load_plan_preview_payload_from_path(preview_plan_path, scenario_label)
            extra_dirs = _extract_inject_dirs_from_payload(payload) if isinstance(payload, dict) else []
            if extra_dirs:
                seen_dirs = set(expected_inject_dirs)
                for d in extra_dirs:
                    if d in seen_dirs:
                        continue
                    seen_dirs.add(d)
                    expected_inject_dirs.append(d)
        except Exception:
            pass
        try:
            payload = _load_plan_preview_payload_from_path(preview_plan_path, scenario_label)
            extra_files = _extract_inject_files_from_payload(payload) if isinstance(payload, dict) else []
            if extra_files:
                seen_files = set(expected_inject_files)
                for f in extra_files:
                    if f in seen_files:
                        continue
                    seen_files.add(f)
                    expected_inject_files.append(f)
        except Exception:
            pass
    summary['inject_dirs_expected'] = expected_inject_dirs
    summary['inject_files_expected'] = expected_inject_files
    summary['inject_files_expected_by_node'] = inject_expected_by_node

    expected_docker_nodes, expected_vuln_nodes = _extract_expected_docker_and_vuln_nodes_from_plan_xml(
        scenario_xml_path,
        scenario_label,
    )
    summary['expected_docker_nodes'] = expected_docker_nodes
    summary['expected_vuln_nodes'] = expected_vuln_nodes

    docker_nodes = _session_docker_nodes_from_xml(session_xml_path)
    summary['docker_nodes'] = docker_nodes
    inject_node_ids = _extract_inject_node_ids_from_flow_state(scenario_xml_path, scenario_label)
    inject_node_names: List[str] = []
    if inject_node_ids:
        try:
            expected = _expected_from_plan_preview(scenario_xml_path, scenario_label)
            for nid in sorted(inject_node_ids):
                exp = expected.get(nid) or {}
                nm = str(exp.get('name') or '').strip()
                if nm:
                    inject_node_names.append(nm)
        except Exception:
            inject_node_names = []
    summary['inject_nodes_expected'] = inject_node_names
    actual_vuln_nodes: List[str] = []
    if docker_nodes and isinstance(core_cfg, dict):
        try:
            payload = _run_remote_python_json(
                core_cfg,
                _remote_vuln_assignments_script(),
                logger=app.logger,
                label='docker.compose.assignments',
                timeout=45.0,
            )
            nodes_raw = payload.get('nodes') if isinstance(payload, dict) else None
            if isinstance(nodes_raw, list):
                node_set = {str(n).strip() for n in nodes_raw if str(n).strip()}
                if node_set:
                    actual_vuln_nodes = [n for n in docker_nodes if n in node_set]
                    summary['actual_vuln_nodes'] = actual_vuln_nodes
        except Exception:
            pass
    if expected_docker_nodes:
        exp_set = set(expected_docker_nodes)
        act_set = set(docker_nodes)
        summary['missing_docker_nodes'] = sorted(exp_set - act_set)
        summary['extra_docker_nodes'] = sorted(act_set - exp_set)
    if expected_vuln_nodes:
        exp_vuln = set(expected_vuln_nodes)
        if actual_vuln_nodes:
            act_set = set(actual_vuln_nodes)
        else:
            act_set = set(docker_nodes)
            summary['actual_vuln_nodes'] = sorted(exp_vuln & act_set)
        summary['missing_vuln_nodes'] = sorted(exp_vuln - act_set)
    if docker_nodes and isinstance(core_cfg, dict):
        containers_for_injects = docker_nodes
        if inject_node_names:
            containers_for_injects = [n for n in docker_nodes if n in set(inject_node_names)]
        def _safe_inject_dir(path: str) -> str | None:
            p = str(path or '').strip()
            if not p or p == '/':
                return None
            if p.endswith('/'):
                p = p.rstrip('/') or '/'
            if not p or p == '/':
                return None
            try:
                d = os.path.dirname(p)
            except Exception:
                d = ''
            if d == '/' and p.startswith('/'):
                # Single-segment absolute path like /tmp should be its own dir.
                return p
            return d or p

        if inject_expected_by_node:
            inject_dirs = sorted({d for paths in inject_expected_by_node.values() for p in paths if p.startswith('/') for d in (_safe_inject_dir(p),) if d})
        else:
            inject_dirs = expected_inject_dirs if expected_inject_dirs else None
        try:
            payload = _run_remote_python_json(
                core_cfg,
                _remote_docker_injects_status_script(
                    containers=containers_for_injects,
                    sudo_password=core_cfg.get('ssh_password'),
                    inject_dirs=inject_dirs,
                ),
                logger=app.logger,
                label='docker.exec.injects_status',
                timeout=120.0,
            )
            items = payload.get('items') if isinstance(payload, dict) else None
            if isinstance(items, list):
                for it in items:
                    if not isinstance(it, dict):
                        continue
                    name = str(it.get('container') or '').strip()
                    if not name:
                        continue
                    if not it.get('exists'):
                        summary['docker_missing'].append(name)
                        summary['injects_detail'].append(f"{name}: missing")
                        continue
                    if not it.get('running'):
                        summary['docker_not_running'].append(name)
                        summary['injects_detail'].append(f"{name}: not running")
                    else:
                        summary['docker_running'].append(name)
                    try:
                        samples = it.get('inject_samples') if isinstance(it.get('inject_samples'), list) else []
                        inject_count = int(it.get('inject_count') or 0)
                        inject_dirs_found = it.get('inject_dirs_found') if isinstance(it.get('inject_dirs_found'), list) else []
                        debug_logs = it.get('debug_logs') if isinstance(it.get('debug_logs'), list) else []
                        expected_paths = []
                        if inject_expected_by_node:
                            expected_paths = inject_expected_by_node.get(name) or []
                        if expected_inject_dirs or inject_count > 0 or samples:
                            if inject_dirs_found:
                                dirs_text = ', '.join(str(d) for d in inject_dirs_found)
                            elif expected_inject_dirs:
                                dirs_text = ', '.join(str(d) for d in expected_inject_dirs)
                            else:
                                dirs_text = ''
                            detail = f"{name}: {inject_count} file(s)"
                            if dirs_text:
                                detail += f" in {dirs_text}"
                            if samples:
                                detail += f" (e.g., {', '.join(str(s) for s in samples)})"
                            if expected_paths:
                                exp_preview = ', '.join(str(p) for p in expected_paths[:3])
                                more = f" (+{len(expected_paths)-3} more)" if len(expected_paths) > 3 else ''
                                detail += f"; expected {exp_preview}{more}"
                            if debug_logs:
                                dbg = ' | '.join(str(x) for x in debug_logs[:2])
                                detail += f"; debug: {dbg}"
                            summary['injects_detail'].append(detail)
                    except Exception:
                        pass
                    if inject_dirs and int(it.get('inject_count') or 0) <= 0:
                        summary['injects_missing'].append(name)
        except Exception as exc:
            summary['ok'] = False
            summary['error'] = f'failed docker injects validation: {exc}'

    # Validate Flow generator outputs/inject sources on the CORE VM (remote).
    gen_validation_error: str | None = None
    assignments_list: List[Dict[str, Any]] | None = None
    try:
        flow_state = _flow_state_from_xml_path(scenario_xml_path or '', scenario_label)
        assignments = flow_state.get('flag_assignments') if isinstance(flow_state, dict) else None
        if isinstance(assignments, list) and assignments:
            assignments_list = [a for a in assignments if isinstance(a, dict)]
        if isinstance(assignments_list, list) and assignments_list and isinstance(core_cfg, dict):
            check_items: List[Dict[str, Any]] = []
            for fa in assignments_list:
                check_items.append({
                    'node_id': fa.get('node_id'),
                    'generator_id': fa.get('id') or fa.get('generator_id'),
                    'generator_name': fa.get('name'),
                    'generator_type': fa.get('type') or fa.get('generator_type'),
                    'run_dir': fa.get('run_dir') or fa.get('artifacts_dir'),
                    'artifacts_dir': fa.get('artifacts_dir'),
                    'mount_dir': fa.get('mount_dir'),
                    'outputs_manifest': fa.get('outputs_manifest'),
                    'inject_files_detail': fa.get('inject_files_detail'),
                    'inject_files': fa.get('inject_files'),
                })
            try:
                payload = _run_remote_python_json(
                    core_cfg,
                    _remote_flow_artifacts_validation_script(check_items),
                    logger=app.logger,
                    label='flow.artifacts.validate',
                    timeout=60.0,
                )
                items = payload.get('items') if isinstance(payload, dict) else None
                if isinstance(items, list):
                    for it in items:
                        if not isinstance(it, dict):
                            continue
                        node_id = str(it.get('node_id') or '').strip()
                        gen_id = str(it.get('generator_id') or '').strip()
                        miss_out = it.get('outputs_missing') if isinstance(it.get('outputs_missing'), list) else []
                        miss_inj = it.get('inject_missing') if isinstance(it.get('inject_missing'), list) else []
                        if miss_out:
                            for p in miss_out:
                                summary['generator_outputs_missing'].append(f"{node_id or gen_id}: {p}")
                        if miss_inj:
                            for p in miss_inj:
                                summary['generator_injects_missing'].append(f"{node_id or gen_id}: {p}")
                        summary['generator_validation_detail'].append(it)
                else:
                    gen_validation_error = 'no generator validation items returned'
            except Exception as exc:
                gen_validation_error = f"generator validation failed: {exc}"
        elif isinstance(assignments_list, list) and assignments_list and not isinstance(core_cfg, dict):
            gen_validation_error = 'generator validation skipped (no core config)'
    except Exception as exc:
        gen_validation_error = f"generator validation exception: {exc}"

    def _looks_like_path_local(val: Any) -> bool:
        try:
            s = str(val or '').strip()
        except Exception:
            return False
        if not s:
            return False
        return '/' in s or s.startswith('artifacts/') or s.startswith('flow_artifacts/')

    def _resolve_output_path_local(val: str, run_dir: str, artifacts_dir: str) -> str:
        v = str(val or '').strip()
        if not v:
            return ''
        if os.path.isabs(v):
            return v
        if v.startswith('artifacts/') and artifacts_dir:
            return os.path.join(artifacts_dir, v.split('artifacts/', 1)[1].lstrip('/'))
        if run_dir:
            return os.path.join(run_dir, v.lstrip('/'))
        if artifacts_dir:
            return os.path.join(artifacts_dir, v.lstrip('/'))
        return v

    def _normalize_inject_source_local(raw: str, run_dir: str, artifacts_dir: str) -> str:
        text = str(raw or '').strip()
        if not text:
            return ''
        for sep in ('->', '=>'):
            if sep in text:
                text = text.split(sep, 1)[0].strip()
                break
        if not text:
            return ''
        if os.path.isabs(text):
            for marker in ('/artifacts/', '/flow_artifacts/'):
                if marker in text:
                    rel = text.split(marker, 1)[1].lstrip('/')
                    base = artifacts_dir or run_dir
                    return os.path.join(base, rel) if base else text
            return text
        if text.startswith('artifacts/'):
            base = artifacts_dir or run_dir
            return os.path.join(base, text.split('artifacts/', 1)[1].lstrip('/')) if base else text
        base = artifacts_dir or run_dir
        return os.path.join(base, text.lstrip('/')) if base else text

    if isinstance(assignments_list, list) and assignments_list and not summary['generator_validation_detail']:
        for fa in assignments_list:
            run_dir = str(fa.get('run_dir') or fa.get('artifacts_dir') or '')
            artifacts_dir = str(fa.get('artifacts_dir') or fa.get('mount_dir') or '')
            outputs_checked: List[str] = []
            inject_checked: List[str] = []
            try:
                resolved_detail = fa.get('resolved_outputs_detail') if isinstance(fa.get('resolved_outputs_detail'), list) else []
                for item in resolved_detail:
                    if not isinstance(item, dict):
                        continue
                    val = item.get('resolved') or item.get('path')
                    if _looks_like_path_local(val):
                        p = _resolve_output_path_local(str(val), run_dir, artifacts_dir)
                        if p:
                            outputs_checked.append(p)
            except Exception:
                pass
            try:
                resolved_outputs = fa.get('resolved_outputs') if isinstance(fa.get('resolved_outputs'), dict) else None
                if isinstance(resolved_outputs, dict):
                    for v in resolved_outputs.values():
                        if _looks_like_path_local(v):
                            p = _resolve_output_path_local(str(v), run_dir, artifacts_dir)
                            if p:
                                outputs_checked.append(p)
            except Exception:
                pass
            try:
                inject_detail = fa.get('inject_files_detail') if isinstance(fa.get('inject_files_detail'), list) else []
                for item in inject_detail:
                    if not isinstance(item, dict):
                        continue
                    src = item.get('resolved') or item.get('path')
                    p = _normalize_inject_source_local(str(src), run_dir, artifacts_dir)
                    if p:
                        inject_checked.append(p)
            except Exception:
                pass
            try:
                inject_files = fa.get('inject_files') if isinstance(fa.get('inject_files'), list) else []
                for raw in inject_files:
                    p = _normalize_inject_source_local(str(raw), run_dir, artifacts_dir)
                    if p:
                        inject_checked.append(p)
            except Exception:
                pass
            summary['generator_validation_detail'].append({
                'node_id': fa.get('node_id'),
                'generator_id': fa.get('id') or fa.get('generator_id'),
                'generator_name': fa.get('name'),
                'generator_type': fa.get('type') or fa.get('generator_type'),
                'run_dir': run_dir,
                'artifacts_dir': artifacts_dir,
                'outputs_manifest': fa.get('outputs_manifest'),
                'outputs_checked': sorted(set(outputs_checked)),
                'outputs_missing': [],
                'inject_checked': sorted(set(inject_checked)),
                'inject_missing': [],
                'validation_error': gen_validation_error,
            })
        if gen_validation_error:
            summary['generator_outputs_missing'].append(f"generator validation error: {gen_validation_error}")

    if (
        summary['missing_nodes']
        or summary['docker_missing']
        or summary['docker_not_running']
        or summary['injects_missing']
        or summary['missing_docker_nodes']
        or summary['extra_docker_nodes']
        or summary['missing_vuln_nodes']
        or summary['generator_outputs_missing']
        or summary['generator_injects_missing']
    ):
        summary['ok'] = False
    return summary


def _maybe_copy_flow_artifacts_into_containers(meta: Dict[str, Any] | None, *, stage: str, log_prefix: str = '[remote] ') -> None:
    if not meta:
        return
    if not meta.get('remote'):
        return
    if meta.get('flow_artifacts_copied'):
        return
    cfg = meta.get('core_cfg')
    if not isinstance(cfg, dict):
        return

    try:
        _log_remote_vulns_inventory(meta, stage=f'before_copy.{stage}', log_prefix=log_prefix)
    except Exception:
        pass

    _append_async_run_log_line(meta, f"{log_prefix}=== docker.copy_flow_artifacts({stage}) ===")
    try:
        payload = _run_remote_python_json(
            cfg,
            _remote_copy_flow_artifacts_into_containers_script(cfg.get('ssh_password')),
            logger=app.logger,
            label=f'docker.copy_flow_artifacts({stage})',
            timeout=180.0,
        )
        try:
            if isinstance(payload, dict) and payload.get('error'):
                _append_async_run_log_line(meta, f"{log_prefix}docker.copy_flow_artifacts({stage}) error={payload.get('error')}")
            if isinstance(payload, dict) and 'assignments_count' in payload:
                _append_async_run_log_line(
                    meta,
                    f"{log_prefix}docker.copy_flow_artifacts({stage}) assignments={payload.get('assignments_count')} keys={payload.get('assignments_keys')}",
                )
        except Exception:
            pass
        items = payload.get('items') if isinstance(payload, dict) else None
        copied_total = len(items or []) if isinstance(items, list) else 0
        copied_ok = (
            sum(1 for it in (items or []) if isinstance(it, dict) and it.get('ok'))
            if isinstance(items, list)
            else 0
        )
        _append_async_run_log_line(
            meta,
            f"{log_prefix}docker.copy_flow_artifacts({stage}) complete ok={int(copied_ok)} total={int(copied_total)}",
        )
        if isinstance(items, list) and items:
            for it in items[:25]:
                if not isinstance(it, dict):
                    continue
                node = it.get('node')
                ok = bool(it.get('ok'))
                src = it.get('src')
                dest = it.get('dest')
                targets = it.get('targets')
                err = it.get('error') or ''
                errs = it.get('errors') or []
                commands = it.get('commands') or []
                cmd_outs = it.get('command_outputs') if isinstance(it.get('command_outputs'), list) else []
                err_tail = ''
                try:
                    if err:
                        err_tail = str(err)
                    elif isinstance(errs, list) and errs:
                        err_tail = _summarize_for_log(str(errs[0]))
                except Exception:
                    err_tail = ''
                target_desc = None
                try:
                    if isinstance(targets, list) and targets:
                        target_desc = ','.join(str(x) for x in targets[:3])
                except Exception:
                    target_desc = None
                detail = f"node={node} ok={ok} src={src} dest={dest}"
                if target_desc:
                    detail += f" targets={target_desc}"
                if err_tail:
                    detail += f" error={err_tail}"
                _append_async_run_log_line(meta, f"{log_prefix}{detail}")
                try:
                    if isinstance(commands, list):
                        for cmd in commands[:8]:
                            cmd_str = str(cmd or '').strip()
                            if cmd_str:
                                _append_async_run_log_line(meta, f"{log_prefix}cmd: {cmd_str}")
                    if cmd_outs:
                        for entry in cmd_outs[:8]:
                            if not isinstance(entry, dict):
                                continue
                            tgt = str(entry.get('target') or '').strip()
                            rc = entry.get('rc')
                            out = str(entry.get('out') or '').strip()
                            _append_async_run_log_line(meta, f"{log_prefix}cmd: docker cp target={tgt or 'unknown'} rc={rc}")
                            if out:
                                for line in out.splitlines()[:10]:
                                    _append_async_run_log_line(meta, f"{log_prefix}cmd: {line}")
                except Exception:
                    pass

        # Verify: list /flow_artifacts inside target container(s) so logs prove the copy worked.
        try:
            verify_targets: List[str] = []
            if isinstance(items, list):
                for it in items:
                    if not isinstance(it, dict) or not it.get('ok'):
                        continue
                    targets = it.get('targets')
                    if isinstance(targets, list):
                        for t in targets:
                            name = str(t or '').strip()
                            if name:
                                verify_targets.append(name)
            seen: set[str] = set()
            uniq: List[str] = []
            for t in verify_targets:
                if t in seen:
                    continue
                seen.add(t)
                uniq.append(t)
                if len(uniq) >= 5:
                    break

            if uniq:
                _append_async_run_log_line(meta, f"{log_prefix}=== docker.exec.verify_flow_artifacts({stage}) ===")
                verify_payload = _run_remote_python_json(
                    cfg,
                    _remote_docker_exec_flow_artifacts_listing_script(
                        containers=uniq,
                        sudo_password=cfg.get('ssh_password'),
                        max_find=200,
                    ),
                    logger=app.logger,
                    label=f'docker.exec.verify_flow_artifacts({stage})',
                    timeout=90.0,
                )
                vitems = verify_payload.get('items') if isinstance(verify_payload, dict) else None
                if isinstance(vitems, list):
                    for vit in vitems[:5]:
                        if not isinstance(vit, dict):
                            continue
                        container = vit.get('container')
                        ok2 = bool(vit.get('ok'))
                        rc2 = vit.get('rc')
                        out2 = vit.get('output') or ''
                        _append_async_run_log_line(meta, f"{log_prefix}docker.exec.verify container={container} ok={ok2} rc={rc2}")
                        for line in str(out2).splitlines()[:60]:
                            _append_async_run_log_line(meta, f"{log_prefix}{line}")
        except Exception as exc_verify:
            _append_async_run_log_line(meta, f"{log_prefix}docker.exec.verify_flow_artifacts({stage}) failed: {exc_verify}")
        meta['flow_artifacts_copied'] = True
    except Exception as exc:
        _append_async_run_log_line(meta, f"{log_prefix}docker.copy_flow_artifacts({stage}) failed: {exc}")


def _remote_vulns_inventory_script(base_dir: str | None) -> str:
    base_dir_literal = json.dumps(str(base_dir) if base_dir else '')
    return (
        r"""
import glob, json, os

BASE_DIR = __BASE_DIR_LITERAL__


def _safe_exists(p: str) -> bool:
    try:
        return bool(p) and os.path.exists(p)
    except Exception:
        return False


def _first_existing(paths):
    for p in paths:
        if not p:
            continue
        if _safe_exists(p):
            return p
    return None


def _list_dirs(pattern: str, limit: int = 10):
    out = []
    try:
        for p in sorted(glob.glob(pattern)):
            try:
                if os.path.isdir(p):
                    out.append(p)
            except Exception:
                continue
            if len(out) >= limit:
                break
    except Exception:
        return []
    return out


def _count_files_under(path: str, limit: int = 2000) -> int:
    count = 0
    try:
        for _, _, files in os.walk(path):
            count += len(files or [])
            if count >= limit:
                return limit
    except Exception:
        return -1
    return count


def main():
    base = BASE_DIR or os.environ.get('CORE_REMOTE_BASE_DIR') or '/tmp/core-topo-gen'
    base = os.path.abspath(base)
    candidates = {
        'base_dir': base,
        'tmp_vulns': '/tmp/vulns',
        'base_vulns': os.path.join(base, 'vulns'),
        'base_outputs_vulns': os.path.join(base, 'outputs', 'vulns'),
    }

    assignments_candidates = [
        os.path.join(base, 'vulns', 'compose_assignments.json'),
        os.path.join(base, 'outputs', 'vulns', 'compose_assignments.json'),
        os.path.join(base, 'compose_assignments.json'),
        '/tmp/vulns/compose_assignments.json',
    ]
    assignments_path = _first_existing(assignments_candidates)

    run_dirs = []
    for root in (candidates.get('tmp_vulns'), candidates.get('base_vulns'), candidates.get('base_outputs_vulns')):
        if not root:
            continue
        run_dirs.extend(_list_dirs(os.path.join(root, 'flag_generators_runs', '*'), limit=10))
        run_dirs.extend(_list_dirs(os.path.join(root, 'flag_node_generators_runs', '*'), limit=10))

    run_dir_summaries = []
    for p in run_dirs[:10]:
        run_dir_summaries.append({
            'path': p,
            'files': _count_files_under(p, limit=2000),
        })

    payload = {
        'ok': True,
        'base_dir': base,
        'paths': {k: {'path': v, 'exists': _safe_exists(v)} for k, v in candidates.items()},
        'compose_assignments': {
            'candidates': assignments_candidates,
            'found': assignments_path,
            'found_exists': bool(assignments_path and _safe_exists(assignments_path)),
        },
        'run_dirs': run_dir_summaries,
    }
    print(json.dumps(payload))


if __name__ == '__main__':
    main()
"""
    ).replace('__BASE_DIR_LITERAL__', base_dir_literal)


def _log_remote_vulns_inventory(meta: Dict[str, Any] | None, *, stage: str, log_prefix: str = '[remote] ') -> None:
    if not meta or not meta.get('remote'):
        return
    cfg = meta.get('core_cfg')
    if not isinstance(cfg, dict):
        return
    base_dir = meta.get('remote_base_dir') or (meta.get('remote_context') or {}).get('base_dir')

    _append_async_run_log_line(meta, f"{log_prefix}=== remote.vulns_inventory({stage}) ===")
    try:
        payload = _run_remote_python_json(
            cfg,
            _remote_vulns_inventory_script(str(base_dir) if base_dir else None),
            logger=app.logger,
            label=f'remote.vulns_inventory({stage})',
            timeout=60.0,
        )
    except Exception as exc:
        _append_async_run_log_line(meta, f"{log_prefix}remote.vulns_inventory({stage}) failed: {exc}")
        return

    try:
        base = payload.get('base_dir') if isinstance(payload, dict) else None
        if base:
            _append_async_run_log_line(meta, f"{log_prefix}CORE_REMOTE_BASE_DIR={base}")
    except Exception:
        pass
    try:
        paths = payload.get('paths') if isinstance(payload, dict) else None
        if isinstance(paths, dict):
            for key in ('tmp_vulns', 'base_vulns', 'base_outputs_vulns'):
                entry = paths.get(key)
                if isinstance(entry, dict):
                    _append_async_run_log_line(
                        meta,
                        f"{log_prefix}{key} path={entry.get('path')} exists={bool(entry.get('exists'))}",
                    )
    except Exception:
        pass
    try:
        ca = payload.get('compose_assignments') if isinstance(payload, dict) else None
        if isinstance(ca, dict):
            _append_async_run_log_line(
                meta,
                f"{log_prefix}compose_assignments found={ca.get('found')} exists={bool(ca.get('found_exists'))}",
            )
    except Exception:
        pass
    try:
        runs = payload.get('run_dirs') if isinstance(payload, dict) else None
        if isinstance(runs, list) and runs:
            _append_async_run_log_line(meta, f"{log_prefix}run_dirs found={len(runs)} (showing up to 10)")
            for it in runs[:10]:
                if not isinstance(it, dict):
                    continue
                _append_async_run_log_line(meta, f"{log_prefix}run_dir path={it.get('path')} files={it.get('files')}")
        else:
            _append_async_run_log_line(meta, f"{log_prefix}run_dirs found=0")
    except Exception:
        pass


def _log_remote_vulns_inventory_to_handle(
    *,
    core_cfg: Dict[str, Any],
    log_handle: Any,
    stage: str,
    base_dir: str | None = None,
    log_prefix: str = '[remote] ',
) -> None:
    """Write the same inventory info as _log_remote_vulns_inventory, but without requiring RUNS meta.

    This is useful for early-run diagnostics before RUNS[run_id] is populated, or
    in failure paths where we never reach the postrun copy stage.
    """
    try:
        log_handle.write(f"{log_prefix}=== remote.vulns_inventory({stage}) ===\n")
    except Exception:
        return

    try:
        payload = _run_remote_python_json(
            core_cfg,
            _remote_vulns_inventory_script(str(base_dir) if base_dir else None),
            logger=app.logger,
            label=f'remote.vulns_inventory({stage})',
            timeout=60.0,
        )
    except Exception as exc:
        try:
            log_handle.write(f"{log_prefix}remote.vulns_inventory({stage}) failed: {exc}\n")
        except Exception:
            pass
        return

    def _w(line: str) -> None:
        try:
            log_handle.write(line.rstrip('\n') + "\n")
        except Exception:
            pass

    try:
        base = payload.get('base_dir') if isinstance(payload, dict) else None
        if base:
            _w(f"{log_prefix}CORE_REMOTE_BASE_DIR={base}")
    except Exception:
        pass
    try:
        paths = payload.get('paths') if isinstance(payload, dict) else None
        if isinstance(paths, dict):
            for key in ('tmp_vulns', 'base_vulns', 'base_outputs_vulns'):
                entry = paths.get(key)
                if isinstance(entry, dict):
                    _w(f"{log_prefix}{key} path={entry.get('path')} exists={bool(entry.get('exists'))}")
    except Exception:
        pass
    try:
        ca = payload.get('compose_assignments') if isinstance(payload, dict) else None
        if isinstance(ca, dict):
            _w(f"{log_prefix}compose_assignments found={ca.get('found')} exists={bool(ca.get('found_exists'))}")
    except Exception:
        pass
    try:
        runs = payload.get('run_dirs') if isinstance(payload, dict) else None
        if isinstance(runs, list) and runs:
            _w(f"{log_prefix}run_dirs found={len(runs)} (showing up to 10)")
            for it in runs[:10]:
                if not isinstance(it, dict):
                    continue
                _w(f"{log_prefix}run_dir path={it.get('path')} files={it.get('files')}")
        else:
            _w(f"{log_prefix}run_dirs found=0")
    except Exception:
        pass


def _remote_docker_exec_flow_artifacts_listing_script(
    *,
    containers: List[str],
    sudo_password: str | None = None,
    max_find: int = 200,
) -> str:
    containers_literal = json.dumps([str(x) for x in (containers or [])])
    sudo_password_literal = json.dumps(str(sudo_password) if sudo_password else "")
    max_find_literal = json.dumps(int(max_find))
    return (
        r"""
import json, subprocess

CONTAINERS = __CONTAINERS_LITERAL__
SUDO_PASSWORD = __SUDO_PASSWORD_LITERAL__
MAX_FIND = __MAX_FIND_LITERAL__


def _run(cmd, timeout=25):
    try:
        p = subprocess.run(['sudo', '-n'] + list(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=timeout)
        if p.returncode == 0:
            return p
        if SUDO_PASSWORD:
            return subprocess.run(
                ['sudo', '-S', '-k', '-p', ''] + list(cmd),
                input=str(SUDO_PASSWORD) + "\n",
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                timeout=timeout,
            )
        return p
    except Exception as e:
        return subprocess.CompletedProcess(cmd, 127, stdout=str(e))


def main():
    items = []
    for c in CONTAINERS:
        c = str(c or '').strip()
        if not c:
            continue
        shell = (
            'set -e; '
            'echo "--- ls -la /flow_artifacts ---"; '
            'ls -la /flow_artifacts 2>&1 || true; '
            'echo "--- find /flow_artifacts (files) ---"; '
            f"find /flow_artifacts -maxdepth 3 -type f -print 2>/dev/null | head -n {MAX_FIND} || true; "
        )
        p = _run(['docker', 'exec', c, 'sh', '-c', shell], timeout=25)
        items.append({'container': c, 'ok': p.returncode == 0, 'rc': int(p.returncode), 'output': (p.stdout or '')})
    print(json.dumps({'ok': True, 'items': items}))


if __name__ == '__main__':
    main()
"""
    ).replace('__CONTAINERS_LITERAL__', containers_literal) \
     .replace('__SUDO_PASSWORD_LITERAL__', sudo_password_literal) \
     .replace('__MAX_FIND_LITERAL__', max_find_literal)


@app.route('/run_cli_async', methods=['POST'])
def run_cli_async():
    seed = None
    xml_path = None
    preview_plan_path = None
    core_override = None
    scenario_core_override = None
    scenario_name_hint = None
    scenario_index_hint: Optional[int] = None
    update_remote_repo = False
    adv_fix_docker_daemon = False
    adv_run_core_cleanup = False
    adv_check_core_version = False
    adv_restart_core_daemon = False
    adv_start_core_daemon = False
    adv_auto_kill_sessions = False
    docker_remove_conflicts = False
    docker_cleanup_before_run = False
    docker_remove_all_containers = False
    overwrite_existing_images = False
    upload_only_injected_artifacts = False
    scenarios_inline = None
    flow_enabled = True
    # Prefer form fields (existing UI) but fall back to JSON
    if request.form:
        xml_path = request.form.get('xml_path')
        raw_seed = request.form.get('seed')
        if raw_seed:
            try: seed = int(raw_seed)
            except Exception: seed = None
        preview_plan_path = request.form.get('preview_plan') or preview_plan_path
        scenario_name_hint = request.form.get('scenario') or request.form.get('scenario_name') or scenario_name_hint
        try:
            raw_index = request.form.get('scenario_index')
            if raw_index not in (None, ''):
                scenario_index_hint = int(raw_index)
        except Exception:
            scenario_index_hint = scenario_index_hint
        try:
            core_json = request.form.get('core_json')
            if core_json:
                core_override = json.loads(core_json)
        except Exception:
            core_override = None
        try:
            hitl_core_json = request.form.get('hitl_core_json')
            if hitl_core_json:
                scenario_core_override = json.loads(hitl_core_json)
        except Exception:
            scenario_core_override = None
        if 'update_remote_repo' in request.form:
            update_remote_repo = _coerce_bool(request.form.get('update_remote_repo'))
        adv_fix_docker_daemon = _coerce_bool(request.form.get('adv_fix_docker_daemon'))
        adv_run_core_cleanup = _coerce_bool(request.form.get('adv_run_core_cleanup'))
        adv_check_core_version = _coerce_bool(request.form.get('adv_check_core_version'))
        adv_restart_core_daemon = _coerce_bool(request.form.get('adv_restart_core_daemon'))
        adv_start_core_daemon = _coerce_bool(request.form.get('adv_start_core_daemon'))
        adv_auto_kill_sessions = _coerce_bool(request.form.get('adv_auto_kill_sessions'))
        docker_remove_conflicts = _coerce_bool(request.form.get('docker_remove_conflicts'))
        docker_cleanup_before_run = _coerce_bool(request.form.get('docker_cleanup_before_run'))
        docker_remove_all_containers = _coerce_bool(
            request.form.get('docker_remove_all_containers')
        ) or _coerce_bool(request.form.get('docker_nuke_all'))
        overwrite_existing_images = _coerce_bool(request.form.get('overwrite_existing_images'))
        upload_only_injected_artifacts = _coerce_bool(
            request.form.get('upload_only_injected_artifacts')
            or request.form.get('upload_only_injected')
        )
        if 'flow_enabled' in request.form:
            flow_enabled = _coerce_bool(request.form.get('flow_enabled'))
    try:
        j = request.get_json(silent=True) or {}
        if 'flow_enabled' in j:
            flow_enabled = _coerce_bool(j.get('flow_enabled'))
    except Exception:
        j = {}
    if not xml_path:
        try:
            xml_path = j.get('xml_path')
            scenarios_inline = j.get('scenarios')
            if 'seed' in j:
                try: seed = int(j.get('seed'))
                except Exception: seed = None
            if 'preview_plan' in j and not preview_plan_path:
                preview_plan_path = j.get('preview_plan')
            if 'core' in j and j.get('core') is not None:
                core_override = j.get('core')
            if 'hitl_core' in j and isinstance(j.get('hitl_core'), dict):
                scenario_core_override = j.get('hitl_core')
            if 'scenario' in j and j.get('scenario') not in (None, ''):
                scenario_name_hint = j.get('scenario')
            if 'scenario_index' in j:
                try:
                    scenario_index_hint = int(j.get('scenario_index'))
                except Exception:
                    scenario_index_hint = None
            if 'update_remote_repo' in j:
                update_remote_repo = _coerce_bool(j.get('update_remote_repo'))
            adv_fix_docker_daemon = _coerce_bool(j.get('adv_fix_docker_daemon'))
            adv_run_core_cleanup = _coerce_bool(j.get('adv_run_core_cleanup'))
            adv_check_core_version = _coerce_bool(j.get('adv_check_core_version'))
            adv_restart_core_daemon = _coerce_bool(j.get('adv_restart_core_daemon'))
            adv_start_core_daemon = _coerce_bool(j.get('adv_start_core_daemon'))
            adv_auto_kill_sessions = _coerce_bool(j.get('adv_auto_kill_sessions'))
            docker_remove_conflicts = _coerce_bool(j.get('docker_remove_conflicts'))
            docker_cleanup_before_run = _coerce_bool(j.get('docker_cleanup_before_run'))
            docker_remove_all_containers = _coerce_bool(
                j.get('docker_remove_all_containers')
            ) or _coerce_bool(j.get('docker_nuke_all'))
            overwrite_existing_images = _coerce_bool(j.get('overwrite_existing_images'))
            upload_only_injected_artifacts = _coerce_bool(
                j.get('upload_only_injected_artifacts')
                or j.get('upload_only_injected')
            )
        except Exception:
            pass
    if not xml_path:
        # Builder/participant roles may execute without saving by posting scenarios/core.
        if isinstance(scenarios_inline, list):
            try:
                # Build a temporary XML for this run (single scenario for isolation).
                core_meta = core_override if isinstance(core_override, dict) else None
                normalized_core = _normalize_core_config(core_meta, include_password=True) if core_meta else None
                scenario_pick = None
                if scenario_name_hint:
                    for sc in scenarios_inline:
                        if isinstance(sc, dict) and str(sc.get('name') or '').strip() == str(scenario_name_hint).strip():
                            scenario_pick = sc
                            break
                if scenario_pick is None and scenario_index_hint is not None:
                    if 0 <= scenario_index_hint < len(scenarios_inline):
                        candidate = scenarios_inline[scenario_index_hint]
                        if isinstance(candidate, dict):
                            scenario_pick = candidate
                if scenario_pick is None:
                    scenario_pick = next((sc for sc in scenarios_inline if isinstance(sc, dict)), None)
                if scenario_pick is None:
                    return jsonify({"error": "No valid scenario supplied for execution."}), 400
                tree = _build_scenarios_xml({ 'scenarios': [scenario_pick], 'core': normalized_core })
                ts = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')
                run_tag = str(uuid.uuid4())[:8]
                out_dir = os.path.join(_outputs_dir(), f'tmp-exec-{ts}-{run_tag}')
                os.makedirs(out_dir, exist_ok=True)
                # Filename hint: active scenario name if available, else generic.
                stem_raw = None
                try:
                    if scenario_name_hint:
                        stem_raw = str(scenario_name_hint)
                except Exception:
                    stem_raw = None
                if not stem_raw:
                    try:
                        first_name = None
                        if isinstance(scenario_pick, dict) and scenario_pick.get('name'):
                            first_name = scenario_pick.get('name')
                        stem_raw = first_name or 'scenario'
                    except Exception:
                        stem_raw = 'scenario'
                stem = secure_filename(str(stem_raw)).strip('_-.') or 'scenario'
                xml_path = os.path.join(out_dir, f"{stem}.xml")
                # Pretty print when possible
                try:
                    from lxml import etree as LET  # type: ignore
                    raw = ET.tostring(tree.getroot(), encoding='utf-8')
                    lroot = LET.fromstring(raw)
                    pretty = LET.tostring(lroot, pretty_print=True, xml_declaration=True, encoding='utf-8')
                    with open(xml_path, 'wb') as f:
                        f.write(pretty)
                except Exception:
                    tree.write(xml_path, encoding='utf-8', xml_declaration=True)
                if not scenario_name_hint:
                    try:
                        scenario_name_hint = str((scenario_pick or {}).get('name') or '').strip() or scenario_name_hint
                    except Exception:
                        pass
            except Exception as exc:
                return jsonify({"error": f"Failed to render XML for execution: {exc}"}), 400
        else:
            return jsonify({"error": "XML path missing. Save XML first."}), 400
    xml_path = os.path.abspath(xml_path)
    if not os.path.exists(xml_path) and '/outputs/' in xml_path:
        try:
            alt = xml_path.replace('/app/outputs', '/app/webapp/outputs')
            if alt != xml_path and os.path.exists(alt):
                app.logger.info("[async] Remapped XML path %s -> %s", xml_path, alt)
                xml_path = alt
        except Exception:
            pass
    if not os.path.exists(xml_path):
        try:
            recovered = _try_resolve_latest_outputs_xml(xml_path)
            if recovered and os.path.exists(recovered):
                app.logger.warning('[async] XML path missing; recovered to newest match: %s -> %s', xml_path, recovered)
                xml_path = recovered
        except Exception:
            pass
    if not os.path.exists(xml_path):
        return jsonify({"error": f"XML path not found: {xml_path}"}), 400
    preview_plan_path = (preview_plan_path or '').strip() or None
    if preview_plan_path:
        try:
            preview_plan_path = os.path.abspath(preview_plan_path)
            if preview_plan_path.lower().endswith('.xml'):
                if not os.path.exists(preview_plan_path):
                    app.logger.warning('[async] preview plan path missing: %s', preview_plan_path)
                    try:
                        log_f.write(f"[async] preview plan rejected (missing): {preview_plan_path}\n")
                    except Exception:
                        pass
                    preview_plan_path = None
            else:
                app.logger.warning('[async] preview plan rejected (non-xml): %s', preview_plan_path)
                try:
                    log_f.write(f"[async] preview plan rejected (non-xml): {preview_plan_path}\n")
                except Exception:
                    pass
                preview_plan_path = None
        except Exception:
            preview_plan_path = None

    if not flow_enabled:
        preview_plan_path = None

    # If no preview plan path was provided (or it was rejected), but the user has
    # saved a Flag Sequencing (flow) plan for this scenario, use it automatically.
    # Without a plan, the CLI will execute with 0 flows (no flags/generators).
    if not preview_plan_path and flow_enabled:
        try:
            scenario_norm = None
            if scenario_name_hint:
                scenario_norm = _normalize_scenario_label(str(scenario_name_hint))
            if not scenario_norm and isinstance(scenario_payload, dict) and isinstance(scenario_payload.get('name'), str):
                scenario_norm = _normalize_scenario_label(str(scenario_payload.get('name') or ''))
            if scenario_norm:
                flow_plan = _latest_flow_plan_for_scenario_norm(scenario_norm)
                if flow_plan and os.path.exists(flow_plan):
                    preview_plan_path = os.path.abspath(flow_plan)
                    try:
                        log_f.write(f"[async] Auto-selected plan for scenario={scenario_norm}: {preview_plan_path}\n")
                    except Exception:
                        pass
        except Exception:
            pass
    scenario_for_plan: str | None = None
    try:
        scenario_for_plan = str(scenario_name_hint or '').strip() or None
    except Exception:
        scenario_for_plan = None
    if not scenario_for_plan:
        try:
            names_for_cli = _scenario_names_from_xml(xml_path)
            if names_for_cli:
                scenario_for_plan = names_for_cli[0]
        except Exception:
            scenario_for_plan = None
    # If XML flow state explicitly disables flag sequencing, honor it.
    try:
        if flow_enabled and xml_path and scenario_for_plan:
            parsed = _parse_scenarios_xml(xml_path)
            scen_list = parsed.get('scenarios') if isinstance(parsed, dict) else None
            if isinstance(scen_list, list):
                for sc in scen_list:
                    if not isinstance(sc, dict):
                        continue
                    nm = str(sc.get('name') or '').strip()
                    if _normalize_scenario_label(nm) != _normalize_scenario_label(scenario_for_plan):
                        continue
                    fs = sc.get('flow_state') if isinstance(sc.get('flow_state'), dict) else None
                    if isinstance(fs, dict) and fs.get('flow_enabled') is False:
                        flow_enabled = False
                        preview_plan_path = None
                    break
    except Exception:
        pass
    if preview_plan_path and scenario_for_plan and flow_enabled:
        try:
            plan_payload = _load_preview_payload_from_path(preview_plan_path, scenario_for_plan)
            if not (isinstance(plan_payload, dict) and plan_payload):
                try:
                    if str(preview_plan_path).lower().endswith('.xml'):
                        _planner_persist_flow_plan(xml_path=preview_plan_path, scenario=scenario_for_plan, seed=seed, persist_plan_file=False)
                        plan_payload = _load_preview_payload_from_path(preview_plan_path, scenario_for_plan)
                except Exception:
                    pass
            if isinstance(plan_payload, dict) and plan_payload:
                _update_plan_preview_in_xml(xml_path, scenario_for_plan, plan_payload)
                try:
                    meta = plan_payload.get('metadata') if isinstance(plan_payload.get('metadata'), dict) else {}
                    flow_meta = meta.get('flow') if isinstance(meta.get('flow'), dict) else None
                    if isinstance(flow_meta, dict) and flow_meta:
                        def _flow_has_runtime_values(flow_obj: dict[str, Any]) -> bool:
                            try:
                                fas = flow_obj.get('flag_assignments') if isinstance(flow_obj.get('flag_assignments'), list) else None
                                if not isinstance(fas, list) or not fas:
                                    return False
                                for fa in fas:
                                    if not isinstance(fa, dict):
                                        continue
                                    flag_val = str(fa.get('flag_value') or '').strip()
                                    outs = fa.get('resolved_outputs') if isinstance(fa.get('resolved_outputs'), dict) else None
                                    if not flag_val and isinstance(outs, dict):
                                        flag_val = str(outs.get('Flag(flag_id)') or outs.get('flag') or '').strip()
                                    if flag_val:
                                        return True
                                    if str(fa.get('artifacts_dir') or fa.get('run_dir') or '').strip():
                                        return True
                                return False
                            except Exception:
                                return False
                        if _flow_has_runtime_values(flow_meta):
                            _update_flow_state_in_xml(xml_path, scenario_for_plan, flow_meta)
                except Exception:
                    pass
        except Exception:
            pass
    # Enforce that Flow-generated values already exist before Execute runs.
    try:
        if flow_enabled and preview_plan_path and scenario_for_plan:
            plan_payload = _load_preview_payload_from_path(preview_plan_path, scenario_for_plan)
            meta = plan_payload.get('metadata') if isinstance(plan_payload, dict) else None
            flow_meta = meta.get('flow') if isinstance(meta, dict) else None
            flag_assignments = flow_meta.get('flag_assignments') if isinstance(flow_meta, dict) else None
            def _flow_assignments_have_runtime(fas: list[dict[str, Any]] | None) -> bool:
                if not isinstance(fas, list) or not fas:
                    return False
                for fa in fas:
                    if not isinstance(fa, dict):
                        continue
                    flag_val = str(fa.get('flag_value') or '').strip()
                    outs = fa.get('resolved_outputs') if isinstance(fa.get('resolved_outputs'), dict) else None
                    if not flag_val and isinstance(outs, dict):
                        flag_val = str(outs.get('Flag(flag_id)') or outs.get('flag') or '').strip()
                    if flag_val:
                        return True
                    if str(fa.get('artifacts_dir') or fa.get('run_dir') or '').strip():
                        return True
                return False

            if ((not isinstance(flag_assignments, list)) or (not flag_assignments) or (not _flow_assignments_have_runtime(flag_assignments))) and xml_path:
                try:
                    parsed = _parse_scenarios_xml(xml_path)
                    scen_list = parsed.get('scenarios') if isinstance(parsed, dict) else None
                    if isinstance(scen_list, list):
                        for sc in scen_list:
                            if not isinstance(sc, dict):
                                continue
                            nm = str(sc.get('name') or '').strip()
                            if scenario_for_plan and _normalize_scenario_label(nm) != _normalize_scenario_label(scenario_for_plan):
                                continue
                            fs = sc.get('flow_state') if isinstance(sc.get('flow_state'), dict) else None
                            if isinstance(fs, dict):
                                flag_assignments = fs.get('flag_assignments') if isinstance(fs.get('flag_assignments'), list) else flag_assignments
                            break
                except Exception:
                    pass
            if not isinstance(flag_assignments, list) or not flag_assignments:
                return jsonify({
                    'error': 'No Flow flag assignments found. Generate the chain and resolve flags before Execute.',
                }), 422
            flow_remote_expected = False
            try:
                history = _load_run_history()
                scenario_label = scenario_for_plan or scenario_name_hint or ''
                selected_cfg = _select_core_config_for_page(_normalize_scenario_label(str(scenario_label or '')), history, include_password=True)
                if isinstance(selected_cfg, dict) and _coerce_bool(selected_cfg.get('ssh_enabled')):
                    flow_remote_expected = True
            except Exception:
                flow_remote_expected = False
            missing_values: list[dict[str, Any]] = []
            for idx, fa in enumerate(flag_assignments or []):
                if not isinstance(fa, dict):
                    continue
                gen_id = str(fa.get('id') or fa.get('generator_id') or '').strip()
                node_id = str(fa.get('node_id') or '').strip()
                outputs = fa.get('resolved_outputs') if isinstance(fa.get('resolved_outputs'), dict) else None
                flag_val = str(fa.get('flag_value') or '').strip()
                if not flag_val and isinstance(outputs, dict):
                    try:
                        flag_val = str(outputs.get('Flag(flag_id)') or outputs.get('flag') or '').strip()
                    except Exception:
                        flag_val = ''
                art_dir = str(fa.get('artifacts_dir') or fa.get('run_dir') or '').strip()
                has_outputs = False
                if art_dir:
                    try:
                        if not flow_remote_expected:
                            has_outputs = bool(_flow_read_outputs_map_from_artifacts_dir(art_dir))
                    except Exception:
                        has_outputs = False
                if not flag_val and not has_outputs:
                    missing_values.append({
                        'index': idx,
                        'node_id': node_id,
                        'generator_id': gen_id,
                        'reason': 'missing flag outputs',
                    })
                elif art_dir and (not flow_remote_expected) and (not os.path.isdir(art_dir)):
                    missing_values.append({
                        'index': idx,
                        'node_id': node_id,
                        'generator_id': gen_id,
                        'reason': 'artifacts_dir missing',
                        'artifacts_dir': art_dir,
                    })
            if missing_values:
                return jsonify({
                    'error': 'Execute requires pre-generated Flow values. Run Generate (with resolve) first.',
                    'details': missing_values,
                }), 422
    except Exception:
        pass

    # Enforce flow eligibility when enabled.
    try:
        if flow_enabled and preview_plan_path and scenario_for_plan:
            plan_payload = _load_preview_payload_from_path(preview_plan_path, scenario_for_plan)
            preview = plan_payload.get('full_preview') if isinstance(plan_payload, dict) else None
            docker_count = 0
            vuln_count = 0
            if isinstance(preview, dict):
                role_counts = preview.get('role_counts') if isinstance(preview.get('role_counts'), dict) else None
                if isinstance(role_counts, dict):
                    try:
                        docker_count = int(role_counts.get('Docker') or 0)
                    except Exception:
                        docker_count = 0
                hosts = preview.get('hosts') if isinstance(preview.get('hosts'), list) else []
                if isinstance(hosts, list):
                    for h in hosts:
                        if not isinstance(h, dict):
                            continue
                        role = str(h.get('role') or '').strip().lower()
                        if role == 'docker':
                            docker_count += 1
                        vulns = h.get('vulnerabilities') if isinstance(h.get('vulnerabilities'), list) else []
                        if vulns:
                            vuln_count += 1
                vuln_by_node = preview.get('vulnerabilities_by_node') if isinstance(preview.get('vulnerabilities_by_node'), dict) else None
                if isinstance(vuln_by_node, dict):
                    vuln_count = max(vuln_count, len([k for k, v in vuln_by_node.items() if v]))
            if (docker_count <= 0) and (vuln_count <= 0):
                return jsonify({
                    'error': 'Flag sequencing requires Docker or vulnerability nodes in the topology.',
                    'detail': f'Docker nodes: {docker_count}, vulnerability nodes: {vuln_count}',
                }), 422
    except Exception:
        pass
    try:
        if preview_plan_path:
            flow_summary, flow_meta = _summary_from_preview_plan_path(preview_plan_path, scenario_for_plan)
            xml_summary, xml_seed = _summary_from_xml_plan(xml_path, scenario_for_plan, seed)
            diffs = _diff_plan_summaries(flow_summary, xml_summary)
            if diffs:
                diff_lines = []
                for entry in diffs:
                    field = entry.get('field')
                    diff_lines.append(f"{field}: flow={entry.get('flow')} xml={entry.get('xml')}")
                detail_text = "\n".join(diff_lines)
                flow_scen = None
                try:
                    flow_scen = (flow_meta or {}).get('scenario')
                    if not flow_scen and isinstance((flow_meta or {}).get('flow'), dict):
                        flow_scen = (flow_meta or {}).get('flow', {}).get('scenario')
                except Exception:
                    flow_scen = None
                return jsonify({
                    "error": "Flow/preview plan mismatch with XML-derived plan.",
                    "detail": detail_text,
                    "mismatch": {
                        "plan_path": preview_plan_path,
                        "plan_scenario": flow_scen,
                        "xml_path": xml_path,
                        "xml_scenario": scenario_for_plan,
                        "xml_seed": xml_seed,
                        "differences": diffs,
                    },
                }), 409
    except Exception as exc:
        return jsonify({"error": f"Failed to validate flow/preview plan vs XML: {exc}"}), 500
    # Skip schema validation: format differs from CORE XML
    run_id = str(uuid.uuid4())
    out_dir = os.path.dirname(xml_path)
    log_path = os.path.join(out_dir, f'cli-{run_id}.log')
    # Redirect output directly to log file for easy tailing
    # Open log file in line-buffered mode so subprocess logging (stdout+stderr) flushes promptly for UI streaming
    try:
        log_f = open(log_path, 'w', encoding='utf-8', buffering=1)
    except Exception:
        # Fallback to default buffering if line buffering not available
        log_f = open(log_path, 'w', encoding='utf-8')
    try:
        app.logger.debug("[async] Opened CLI log (line-buffered) at %s", log_path)
    except Exception:
        pass
    app.logger.info("[async] Starting CLI; log: %s", log_path)
    payload_for_core: Dict[str, Any] | None = None
    try:
        payload_for_core = _parse_scenarios_xml(xml_path)
    except Exception:
        payload_for_core = None
    scenario_payload: Dict[str, Any] | None = None
    if payload_for_core:
        scen_list = payload_for_core.get('scenarios') or []
        if isinstance(scen_list, list) and scen_list:
            if scenario_name_hint:
                for scen_entry in scen_list:
                    if not isinstance(scen_entry, dict):
                        continue
                    if str(scen_entry.get('name') or '').strip() == str(scenario_name_hint).strip():
                        scenario_payload = scen_entry
                        break
            if scenario_payload is None and scenario_index_hint is not None:
                if 0 <= scenario_index_hint < len(scen_list):
                    candidate = scen_list[scenario_index_hint]
                    if isinstance(candidate, dict):
                        scenario_payload = candidate
            if scenario_payload is None:
                for scen_entry in scen_list:
                    if isinstance(scen_entry, dict):
                        scenario_payload = scen_entry
                        break
    scenario_core_saved = None
    if scenario_payload and isinstance(scenario_payload.get('hitl'), dict):
        scenario_core_saved = scenario_payload['hitl'].get('core')
    global_core_saved = payload_for_core.get('core') if (payload_for_core and isinstance(payload_for_core.get('core'), dict)) else None
    scenario_core_public: Dict[str, Any] | None = None
    candidate_scenario_core = scenario_core_override if isinstance(scenario_core_override, dict) else None
    if not candidate_scenario_core and isinstance(scenario_core_saved, dict):
        candidate_scenario_core = scenario_core_saved
    if candidate_scenario_core:
        scenario_core_public = _scrub_scenario_core_config(candidate_scenario_core)
    core_cfg = _merge_core_configs(
        global_core_saved,
        scenario_core_saved,
        core_override if isinstance(core_override, dict) else None,
        scenario_core_override if isinstance(scenario_core_override, dict) else None,
        include_password=True,
    )
    # Flag Sequencing (flow.html) invokes /run_cli_async via JSON without sending core config.
    # In that case, we must honor the "Select CORE VM" dialog (including ssh_port) rather than
    # defaulting to backend/env values.
    try:
        request_provided_core = bool(
            (isinstance(core_override, dict) and core_override)
            or (isinstance(scenario_core_override, dict) and scenario_core_override)
        )
        history = _load_run_history()
        scenario_for_secret = None
        try:
            scenario_for_secret = _normalize_scenario_label(str(scenario_name_hint or '').strip())
        except Exception:
            scenario_for_secret = None
        if not scenario_for_secret:
            try:
                if isinstance(scenario_payload, dict) and isinstance(scenario_payload.get('name'), str):
                    scenario_for_secret = _normalize_scenario_label(str(scenario_payload.get('name') or '').strip())
            except Exception:
                scenario_for_secret = None

        selected_cfg = None
        if scenario_for_secret:
            selected_cfg = _select_core_config_for_page(scenario_for_secret, history, include_password=True)

        pw_raw = core_cfg.get('ssh_password') if isinstance(core_cfg, dict) else None
        pw_ok = bool(str(pw_raw).strip()) if pw_raw not in (None, '') else False

        if request_provided_core:
            # Only fill missing secrets; do not override request-provided host/ports.
            if selected_cfg and not pw_ok:
                core_cfg = _merge_core_configs(selected_cfg, core_cfg, include_password=True)
        else:
            # Only auto-select a saved CORE config when we have a scenario label.
            if selected_cfg:
                core_cfg = _merge_core_configs(core_cfg, selected_cfg, include_password=True)
    except Exception:
        pass
    try:
        core_cfg = _require_core_ssh_credentials(core_cfg)
    except _SSHTunnelError as exc:
        try:
            log_f.close()
        except Exception:
            pass
        return jsonify({"error": str(exc)}), 400
    preferred_cli_venv = _sanitize_venv_bin_path(core_cfg.get('venv_bin'))
    venv_is_explicit = _venv_is_explicit(core_cfg, preferred_cli_venv)
    if preferred_cli_venv and venv_is_explicit:
        venv_error: Optional[str] = None
        remote_venv_allowed = bool(core_cfg.get('ssh_enabled'))
        if not os.path.isabs(preferred_cli_venv):
            venv_error = f"Preferred CORE venv bin must be an absolute path: {preferred_cli_venv}"
        elif os.path.isdir(preferred_cli_venv):
            if not _find_python_in_venv_bin(preferred_cli_venv):
                expected = ', '.join(PYTHON_EXECUTABLE_NAMES)
                venv_error = (
                    f"Preferred CORE venv bin {preferred_cli_venv} does not contain a python interpreter "
                    f"(expected one of {expected})."
                )
        elif not remote_venv_allowed:
            venv_error = f"Preferred CORE venv bin not found: {preferred_cli_venv}"
        else:
            try:
                app.logger.info(
                    "[async] Preferred CORE venv bin %s not present locally; assuming remote path",
                    preferred_cli_venv,
                )
            except Exception:
                pass
            try:
                log_f.write(
                    f"[remote] Preferred CORE venv bin {preferred_cli_venv} not present locally; assuming remote path\n"
                )
            except Exception:
                pass
        if venv_error:
            try:
                app.logger.warning("[async] %s", venv_error)
            except Exception:
                pass
            try:
                log_f.write(venv_error + "\n")
            except Exception:
                pass
            try:
                log_f.close()
            except Exception:
                pass
            try:
                os.remove(log_path)
            except Exception:
                pass
            return jsonify({"error": venv_error}), 400
    if update_remote_repo:
        try:
            log_f.write("[remote] Repo upload starting (requested by execute dialog)\n")
        except Exception:
            pass
        try:
            repo_sync = _push_repo_to_remote(
                core_cfg,
                logger=app.logger,
                upload_only_injected_artifacts=bool(upload_only_injected_artifacts),
                log_handle=log_f,
            )
        except Exception as exc:
            try:
                log_f.write(f"[remote] Repo upload failed: {exc}\n")
            except Exception:
                pass
            try:
                log_f.close()
            except Exception:
                pass
            try:
                os.remove(log_path)
            except Exception:
                pass
            return jsonify({"error": f"Repo upload failed: {exc}"}), 500
        else:
            repo_path = repo_sync.get('repo_path') if isinstance(repo_sync, dict) else None
            try:
                detail = f" ({repo_path})" if repo_path else ''
                log_f.write(f"[remote] Repo upload complete{detail}\n")
            except Exception:
                pass
            # Verify installed vuln catalogs arrived on the CORE VM when present locally.
            try:
                local_catalog_root = _installed_vuln_catalogs_root()
                local_has_catalogs = False
                if os.path.isdir(local_catalog_root):
                    try:
                        for _entry in os.scandir(local_catalog_root):
                            local_has_catalogs = True
                            break
                    except Exception:
                        local_has_catalogs = True
                if local_has_catalogs and repo_path and core_cfg.get('ssh_enabled'):
                    client = None
                    try:
                        client = _open_ssh_client(core_cfg)
                        remote_catalog_root = _remote_path_join(repo_path, 'outputs', 'installed_vuln_catalogs')
                        check_cmd = (
                            f"if [ -d {shlex.quote(remote_catalog_root)} ] && "
                            f"[ \"$(ls -A {shlex.quote(remote_catalog_root)} 2>/dev/null)\" ]; then "
                            f"echo OK; else echo MISSING; fi"
                        )
                        _rc, out_text, _err_text = _exec_ssh_command(client, check_cmd, timeout=30.0, check=False)
                        if 'OK' not in (out_text or ''):
                            try:
                                log_f.write(
                                    "[remote] ERROR: outputs/installed_vuln_catalogs missing after repo upload. "
                                    "Compose-based vulnerabilities may fail to resolve.\n"
                                )
                            except Exception:
                                pass
                            try:
                                log_f.close()
                            except Exception:
                                pass
                            try:
                                os.remove(log_path)
                            except Exception:
                                pass
                            return jsonify({"error": "Repo upload missing outputs/installed_vuln_catalogs on CORE VM."}), 500
                        else:
                            try:
                                log_f.write("[remote] Verified outputs/installed_vuln_catalogs on CORE VM\n")
                            except Exception:
                                pass
                    except Exception as exc:
                        try:
                            log_f.write(f"[remote] WARN: failed to verify installed catalogs: {exc}\n")
                        except Exception:
                            pass
                    finally:
                        try:
                            if client is not None:
                                client.close()
                        except Exception:
                            pass
            except Exception:
                pass
    else:
        try:
            log_f.write("[remote] Repo upload skipped; remote repository may be stale.\n")
        except Exception:
            pass
    core_host = core_cfg.get('host', '127.0.0.1')
    try:
        core_port = int(core_cfg.get('port', 50051))
    except Exception:
        core_port = 50051
    remote_desc = f"{core_host}:{core_port}"

    def _collect_blocking_sessions() -> list[dict]:
        blocking: list[dict] = []
        try:
            sessions = _list_active_core_sessions(core_host, core_port, core_cfg, errors=[], meta={})
        except Exception as exc:
            try:
                app.logger.warning('[async] Failed to enumerate existing CORE sessions: %s', exc)
            except Exception:
                pass
            sessions = []
        for entry in sessions:
            state_raw = str(entry.get('state') or '').strip().lower()
            if state_raw in {'shutdown'}:
                continue
            blocking.append(entry)
        return blocking

    # Check for active session conflicts early so we can block quickly without requiring SSH
    # tunnel or remote docker access.
    # NOTE: Active-session checks are performed later, after SSH is confirmed and any
    # selected cleanup actions are executed.
    app.logger.info("[async] CORE remote=%s (ssh_enabled=%s)", remote_desc, core_cfg.get('ssh_enabled'))
    pre_saved = None
    # Establish SSH tunnel so CLI subprocess can reach CORE
    conn_host = core_host
    conn_port = core_port
    ssh_tunnel = None
    try:
        tunnel = _SshTunnel(
            ssh_host=str(core_cfg.get('ssh_host') or core_host),
            ssh_port=int(core_cfg.get('ssh_port') or 22),
            username=str(core_cfg.get('ssh_username') or ''),
            password=(core_cfg.get('ssh_password') or None),
            remote_host=str(core_host),
            remote_port=int(core_port),
        )
        conn_host, conn_port = tunnel.start()
        ssh_tunnel = tunnel
        app.logger.info(
            "[async] SSH tunnel active: %s -> %s",
            f"{conn_host}:{conn_port}",
            remote_desc,
        )
        try:
            log_f.write(f"[remote] SSH tunnel active: {conn_host}:{conn_port} -> {remote_desc}\n")
            _write_sse_marker(
                log_f,
                'phase',
                {
                    'stage': 'ssh.tunnel',
                    'kind': 'tunnel',
                    'local': f"{conn_host}:{conn_port}",
                    'remote': remote_desc,
                },
            )
        except Exception:
            pass
    except _SSHTunnelError as exc:
        try:
            ssh_host = str(core_cfg.get('ssh_host') or core_host)
            ssh_port = int(core_cfg.get('ssh_port') or 22)
            ssh_user = str(core_cfg.get('ssh_username') or '')
            
            # Helper for common Docker misconfiguration
            hint = ""
            if str(core_host) in ('localhost', '127.0.0.1') and core_cfg.get('ssh_enabled'):
                hint = " (Hint: 'localhost' usually means THIS container. Did you mean to use the SSH server's internal checking address?)"

            app.logger.warning(
                "[async] SSH tunnel setup failed: %s. Context: ssh=%s:%s user=%s target=%s:%s%s",
                exc, ssh_host, ssh_port, ssh_user, core_host, core_port, hint
            )
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        if ssh_tunnel:
            try:
                ssh_tunnel.close()
            except Exception:
                pass
        return jsonify({"error": f"SSH tunnel failed: {exc}"}), 500
    except Exception as exc:
        try:
            app.logger.exception("[async] Unexpected SSH tunnel failure: %s", exc)
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        if ssh_tunnel:
            try:
                ssh_tunnel.close()
            except Exception:
                pass
        return jsonify({"error": "Failed to establish SSH tunnel"}), 500
    # Capture scenario names from the editor XML now (CORE post XML will not be parsable by our scenarios parser)
    scen_names = _scenario_names_from_xml(xml_path)
    log_prefix = "[remote] "
    remote_client = None
    remote_ctx: Dict[str, Any] | None = None
    remote_python = None

    def _purge_remote_run_dir() -> None:
        if not remote_ctx:
            return
        run_dir = remote_ctx.get('run_dir') if isinstance(remote_ctx, dict) else None
        if not run_dir or not remote_client:
            return
        try:
            _remote_remove_path(remote_client, run_dir)
        except Exception:
            pass

    try:
        remote_client = _open_ssh_client(core_cfg)
    except Exception as exc:
        try:
            log_f.write(f"{log_prefix}Failed to open SSH session for CLI: {exc}\n")
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
        return jsonify({"error": f"Failed to open SSH session for CLI: {exc}"}), 500
    auto_start_allowed = _coerce_bool(core_cfg.get('auto_start_daemon')) or bool(adv_restart_core_daemon)
    try:
        log_f.write(f"{log_prefix}core-daemon auto-start allowed: {auto_start_allowed}\n")
    except Exception:
        pass

    def _exec_sudo(
        cmd: str,
        *,
        timeout: float = 30.0,
        stage: str = 'sudo',
    ) -> tuple[int, str, str]:
        sudo_password = core_cfg.get('ssh_password')
        # Wrap in `timeout` to avoid hanging indefinitely.
        wrapped = f"sh -c 'timeout {int(max(5, timeout))}s {cmd}'"
        if sudo_password:
            sudo_cmd = f"sudo -S -p '' {wrapped}"
        else:
            sudo_cmd = f"sudo -n {wrapped}"
        stdin = stdout = stderr = None
        try:
            stdin, stdout, stderr = remote_client.exec_command(sudo_cmd, timeout=timeout + 5.0, get_pty=True)
            if sudo_password:
                try:
                    stdin.write(str(sudo_password) + '\n')
                    stdin.flush()
                except Exception:
                    pass
            stdout_data = stdout.read()
            stderr_data = stderr.read()
            try:
                exit_code = stdout.channel.recv_exit_status() if hasattr(stdout, 'channel') else 0
            except Exception:
                exit_code = 0
            out_text = stdout_data.decode('utf-8', 'ignore') if isinstance(stdout_data, bytes) else str(stdout_data or '')
            err_text = stderr_data.decode('utf-8', 'ignore') if isinstance(stderr_data, bytes) else str(stderr_data or '')
            try:
                log_f.write(
                    f"{log_prefix}{stage}: {sudo_cmd} -> exit={exit_code} stdout={_summarize_for_log(out_text.strip())} stderr={_summarize_for_log(err_text.strip())}\n"
                )
            except Exception:
                pass
            return exit_code, out_text, err_text
        finally:
            try:
                if stdin:
                    stdin.close()
            except Exception:
                pass

    # (Scenario tag + Docker existing-image precheck runs later, after cleanup actions.)
    def _check_core_version(required: str = '9.2.1') -> None:
        candidates = [
            "sh -c 'timeout 6s core-daemon --version 2>/dev/null || true'",
            "sh -c 'timeout 6s core-daemon -v 2>/dev/null || true'",
            "sh -c 'timeout 6s dpkg-query -W -f=\"${Version}\" core-daemon 2>/dev/null || true'",
            "sh -c 'timeout 6s rpm -q --qf \"%{VERSION}-%{RELEASE}\" core-daemon 2>/dev/null || true'",
            "sh -c 'timeout 6s core --version 2>/dev/null || true'",
        ]
        raw = ''
        for cmd in candidates:
            try:
                code, out, err = _exec_ssh_command(remote_client, cmd, timeout=10.0)
            except Exception:
                continue
            text = (out or '').strip() or (err or '').strip()
            if not text:
                continue
            raw = text
            break
        found = None
        try:
            m = re.search(r"(\d+\.\d+\.\d+)", raw)
            if m:
                found = m.group(1)
        except Exception:
            found = None
        try:
            log_f.write(f"{log_prefix}CORE version probe: {raw or '(no output)'}\n")
        except Exception:
            pass
        if not found:
            raise RuntimeError('Unable to determine CORE version on remote host')
        if found != required:
            raise RuntimeError(f"CORE version mismatch: expected {required}, found {found}")

    def _maybe_fix_docker_daemon() -> None:
        desired = {'bridge': 'none', 'iptables': False}
        # Best-effort merge with existing daemon.json if readable.
        existing: dict[str, Any] = {}
        try:
            code, out, err = _exec_ssh_command(remote_client, "sh -c 'timeout 5s cat /etc/docker/daemon.json 2>/dev/null || true'", timeout=10.0)
            text = (out or '').strip()
            if text:
                try:
                    existing = json.loads(text)
                except Exception:
                    existing = {}
        except Exception:
            existing = {}
        merged = dict(existing) if isinstance(existing, dict) else {}
        merged.update(desired)
        payload = json.dumps(merged, indent=2, sort_keys=True) + "\n"
        tmp_local = None
        remote_tmp = None
        try:
            import tempfile

            with tempfile.NamedTemporaryFile('w', delete=False, encoding='utf-8') as tf:
                tf.write(payload)
                tmp_local = tf.name
            remote_tmp = _upload_file_to_core_host(core_cfg, tmp_local, remote_dir='/tmp/core-topo-gen/uploads')
            try:
                log_f.write(f"{log_prefix}Uploaded docker daemon.json to {remote_tmp}\n")
            except Exception:
                pass
            # Install into /etc/docker/daemon.json with sudo.
            try:
                log_f.write(f"{log_prefix}Installing docker daemon config to /etc/docker/daemon.json (sudo)\n")
            except Exception:
                pass
            # Some images don't have /etc/docker pre-created.
            exit_code, _out, err = _exec_sudo("install -d -m 0755 /etc/docker", timeout=15.0, stage='docker.etcdir')
            if exit_code != 0:
                err_lower = (err or '').lower()
                if (not core_cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                    raise RuntimeError('Fix docker daemon failed: sudo requires a password (none provided).')
                raise RuntimeError('Fix docker daemon failed: could not create /etc/docker')
            exit_code, _out, err = _exec_sudo(f"install -m 0644 {shlex.quote(remote_tmp)} /etc/docker/daemon.json", timeout=20.0, stage='docker.daemon.json')
            if exit_code != 0:
                err_lower = (err or '').lower()
                if (not core_cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                    raise RuntimeError('Fix docker daemon failed: sudo requires a password (none provided).')
                raise RuntimeError('Fix docker daemon failed: could not write /etc/docker/daemon.json')
            try:
                log_f.write(f"{log_prefix}Wrote /etc/docker/daemon.json successfully.\n")
            except Exception:
                pass
            # Verify file exists and is readable.
            _exec_sudo("ls -l /etc/docker/daemon.json", timeout=10.0, stage='docker.daemon.json.verify')
            # Restart docker so changes take effect.
            try:
                log_f.write(f"{log_prefix}Restarting docker daemon (sudo systemctl restart docker)\n")
            except Exception:
                pass
            # Capture systemd service info (best-effort) so we can verify a real restart.
            show_cmd = "systemctl show docker -p MainPID -p ActiveEnterTimestampMonotonic -p SubState --no-pager"
            show_cmd_alt = "systemctl show docker.service -p MainPID -p ActiveEnterTimestampMonotonic -p SubState --no-pager"
            show_before = None
            try:
                rc_show, out_show, _err_show = _exec_sudo(show_cmd, timeout=15.0, stage='docker.show.before')
                if rc_show != 0:
                    rc_show, out_show, _err_show = _exec_sudo(show_cmd_alt, timeout=15.0, stage='docker.show.before')
                show_before = (out_show or '').strip() or None
            except Exception:
                show_before = None
            rc, _o, _e = _exec_sudo("systemctl restart docker", timeout=35.0, stage='docker.restart')
            if rc != 0:
                try:
                    log_f.write(f"{log_prefix}systemctl restart docker failed; trying: sudo service docker restart\n")
                except Exception:
                    pass
                rc2, _o2, _e2 = _exec_sudo("service docker restart", timeout=35.0, stage='docker.restart')
                if rc2 != 0:
                    raise RuntimeError('Fix docker daemon failed: docker restart did not succeed')
            else:
                # Some systems require the explicit unit name.
                show_after = None
                try:
                    rc_show2, out_show2, _err_show2 = _exec_sudo(show_cmd, timeout=15.0, stage='docker.show.after')
                    if rc_show2 != 0:
                        rc_show2, out_show2, _err_show2 = _exec_sudo(show_cmd_alt, timeout=15.0, stage='docker.show.after')
                    show_after = (out_show2 or '').strip() or None
                except Exception:
                    show_after = None
                try:
                    if show_before and show_after and show_before == show_after:
                        log_f.write(f"{log_prefix}NOTE: docker systemd service metadata unchanged after restart; retrying with docker.service\n")
                        _exec_sudo("systemctl restart docker.service", timeout=35.0, stage='docker.restart')
                        _exec_sudo(show_cmd_alt, timeout=15.0, stage='docker.show.after')
                except Exception:
                    pass
            rc3, out3, err3 = _exec_sudo("systemctl is-active docker", timeout=15.0, stage='docker.is-active')
            try:
                status = (out3 or '').strip() or (err3 or '').strip() or f"(exit={rc3})"
                log_f.write(f"{log_prefix}docker service status: {status}\n")
            except Exception:
                pass
        finally:
            try:
                if tmp_local and os.path.exists(tmp_local):
                    os.remove(tmp_local)
            except Exception:
                pass
            try:
                if remote_tmp:
                    _remove_remote_file(core_cfg, remote_tmp)
            except Exception:
                pass

    def _maybe_core_cleanup() -> None:
        # Prefer system-provided core-cleanup if available; otherwise do a safe stale /tmp/pycore.* purge.
        try:
            code, out, err = _exec_ssh_command(remote_client, "sh -c 'command -v core-cleanup >/dev/null 2>&1; echo $?'", timeout=10.0)
            has_core_cleanup = (out or '').strip() == '0'
        except Exception:
            has_core_cleanup = False
        if has_core_cleanup:
            exit_code, _out, err = _exec_sudo('core-cleanup', timeout=60.0, stage='core.cleanup')
            if exit_code != 0:
                err_lower = (err or '').lower()
                if (not core_cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                    raise RuntimeError('core-cleanup failed: sudo requires a password (none provided).')
                raise RuntimeError('core-cleanup failed')
            return
        # Fallback: remove stale pycore directories not in active session ids.
        try:
            sessions = _list_active_core_sessions(core_host, core_port, core_cfg, errors=[], meta={})
        except Exception:
            sessions = []
        active_ids: set[int] = set()
        for entry in sessions:
            try:
                sid = entry.get('id')
                if sid is None:
                    continue
                active_ids.add(int(str(sid).strip()))
            except Exception:
                continue
        active_json = json.dumps(sorted(active_ids))
        cleanup_cmd = (
            "python3 - <<'PY'\n"
            "import os, json, glob, shutil, time\n"
            "active=set(json.loads(os.environ.get('ACTIVE_IDS','[]')))\n"
            "removed=[]\nkept=[]\nnow=time.time()\n"
            "for p in glob.glob('/tmp/pycore.*'):\n"
            "  base=os.path.basename(p)\n"
            "  try: sid=int(base.split('.')[-1])\n"
            "  except Exception: kept.append(p); continue\n"
            "  if sid in active: kept.append(p); continue\n"
            "  try: age=now-os.stat(p).st_mtime\n"
            "  except Exception: age=999\n"
            "  if age < 30: kept.append(p); continue\n"
            "  try: shutil.rmtree(p); removed.append(p)\n"
            "  except Exception: kept.append(p)\n"
            "print(json.dumps({'removed':removed,'kept':kept,'active_session_ids':sorted(active)}))\n"
            "PY"
        )
        shell_cmd = f"ACTIVE_IDS={shlex.quote(active_json)} {cleanup_cmd}"
        code, out, err = _exec_ssh_command(
            remote_client,
            f"sh -c {shlex.quote(shell_cmd)}",
            timeout=25.0,
        )
        try:
            log_f.write(f"{log_prefix}pycore cleanup result: {(out or err or '').strip()}\n")
        except Exception:
            pass

    def _maybe_restart_core_daemon() -> None:
        exit_code, _out, err = _exec_sudo('systemctl restart core-daemon', timeout=30.0, stage='core-daemon.restart')
        if exit_code != 0:
            err_lower = (err or '').lower()
            if (not core_cfg.get('ssh_password')) and ('password' in err_lower or 'sudo' in err_lower):
                raise CoreDaemonMissingError(
                    'Restart core-daemon failed: sudo requires a password (none provided).',
                    can_auto_start=False,
                    start_command='sudo systemctl restart core-daemon',
                )
            raise CoreDaemonMissingError(
                'Restart core-daemon failed.',
                can_auto_start=False,
                start_command='sudo systemctl restart core-daemon',
            )

    def _maybe_kill_active_sessions() -> tuple[list[int], list[str]]:
        deleted: list[int] = []
        errors: list[str] = []
        try:
            sessions = _list_active_core_sessions(core_host, core_port, core_cfg, errors=[], meta={})
        except Exception as exc:
            errors.append(f"Failed listing active CORE sessions: {exc}")
            return deleted, errors
        ids: list[int] = []
        for entry in sessions:
            sid = entry.get('id')
            if sid in (None, ''):
                continue
            try:
                ids.append(int(str(sid).strip()))
            except Exception:
                continue
        seen: set[int] = set()
        ordered: list[int] = []
        for sid in ids:
            if sid in seen:
                continue
            seen.add(sid)
            ordered.append(sid)
        for sid in ordered:
            try:
                _execute_remote_core_session_action(core_cfg, 'delete', sid, logger=app.logger)
                deleted.append(sid)
            except Exception as exc:
                errors.append(f"Failed deleting session {sid}: {exc}")
        return deleted, errors

    # These are populated during preflight; initialize so later code can safely reference them.
    active_scenario_name = None
    scenario_tag = _safe_name('scenario')
    try:
        log_f.write(f"{log_prefix}=== CORE services startup (core-daemon) ===\n")
        _write_sse_marker(
            log_f,
            'phase',
            {
                'stage': 'core-daemon.startup',
                'detail': 'Starting/validating core-daemon before launching CLI',
            },
        )
    except Exception:
        pass
    try:
        # Execute advanced pre-flight actions (remote only)
        try:
            log_f.write(
                f"{log_prefix}Advanced options received: "
                f"fix_docker_daemon={adv_fix_docker_daemon} "
                f"run_core_cleanup={adv_run_core_cleanup} "
                f"check_core_version={adv_check_core_version} "
                f"restart_core_daemon={adv_restart_core_daemon} "
                f"auto_kill_sessions={adv_auto_kill_sessions}\n"
            )
        except Exception:
            pass
        # Cleanup actions MUST run before any checks.
        if adv_fix_docker_daemon:
            try:
                log_f.write(f"{log_prefix}=== Advanced: Fix docker daemon for CORE ===\n")
            except Exception:
                pass
            _maybe_fix_docker_daemon()
            try:
                log_f.write(f"{log_prefix}Docker daemon.json updated and docker restarted.\n")
            except Exception:
                pass

        if adv_run_core_cleanup:
            try:
                log_f.write(f"{log_prefix}=== Advanced: Run core cleanup ===\n")
            except Exception:
                pass
            _maybe_core_cleanup()
            try:
                log_f.write(f"{log_prefix}CORE cleanup complete.\n")
            except Exception:
                pass

        if docker_remove_all_containers:
            try:
                log_f.write(f"{log_prefix}=== Docker: REMOVE ALL containers (DANGEROUS) ===\n")
                _write_sse_marker(
                    log_f,
                    'phase',
                    {
                        'stage': 'docker.remove_all_containers.prerun',
                        'detail': 'Removing ALL docker containers',
                    },
                )
            except Exception:
                pass
            try:
                payload = _run_remote_python_json(
                    core_cfg,
                    _remote_docker_remove_all_containers_script(core_cfg.get('ssh_password')),
                    logger=app.logger,
                    label='docker.remove_all_containers(prerun)',
                    timeout=900.0,
                )
                try:
                    if isinstance(payload, dict):
                        c = payload.get('containers') or {}
                        log_f.write(
                            f"{log_prefix}Remove-all summary: containers_found={c.get('found')} removed_attempted={c.get('removed_attempted')}\n"
                        )
                except Exception:
                    pass
            except Exception as exc:
                try:
                    log_f.write(f"{log_prefix}Docker remove-all-containers skipped/failed: {exc}\n")
                except Exception:
                    pass

        if docker_cleanup_before_run:
            try:
                log_f.write(f"{log_prefix}=== Docker: cleanup before run (containers + wrapper images) ===\n")
                _write_sse_marker(
                    log_f,
                    'phase',
                    {
                        'stage': 'docker.cleanup.prerun',
                        'detail': 'Stopping/removing vuln containers and wrapper images',
                    },
                )
            except Exception:
                pass
            try:
                status_payload = _run_remote_python_json(
                    core_cfg,
                    _remote_docker_status_script(core_cfg.get('ssh_password')),
                    logger=app.logger,
                    label='docker.status(for prerun cleanup)',
                    timeout=60.0,
                )
                names: list[str] = []
                if isinstance(status_payload, dict) and isinstance(status_payload.get('items'), list):
                    for it in status_payload.get('items') or []:
                        if isinstance(it, dict) and it.get('name'):
                            names.append(str(it.get('name')))
                if names:
                    try:
                        log_f.write(f"{log_prefix}Docker containers to cleanup: {', '.join(names[:20])}{' ...' if len(names) > 20 else ''}\n")
                    except Exception:
                        pass
                    _run_remote_python_json(
                        core_cfg,
                        _remote_docker_cleanup_script(names, core_cfg.get('ssh_password')),
                        logger=app.logger,
                        label='docker.cleanup(prerun)',
                        timeout=120.0,
                    )
                else:
                    try:
                        log_f.write(f"{log_prefix}No vuln docker-compose node names found for cleanup.\n")
                    except Exception:
                        pass

                payload = _run_remote_python_json(
                    core_cfg,
                    _remote_docker_remove_wrapper_images_script(core_cfg.get('ssh_password')),
                    logger=app.logger,
                    label='docker.wrapper_images.cleanup(prerun)',
                    timeout=180.0,
                )
                try:
                    removed = payload.get('removed') if isinstance(payload, dict) else None
                    if isinstance(removed, list) and removed:
                        log_f.write(f"{log_prefix}Removed wrapper images: {', '.join(str(x) for x in removed[:12])}{' ...' if len(removed) > 12 else ''}\n")
                    else:
                        log_f.write(f"{log_prefix}No wrapper images removed (or none found).\n")
                except Exception:
                    pass
            except Exception as exc:
                try:
                    log_f.write(f"{log_prefix}Pre-run docker cleanup skipped/failed: {exc}\n")
                except Exception:
                    pass

        # Determine active scenario name for tag scoping and CLI args.
        active_scenario_name = None
        if scenario_name_hint:
            active_scenario_name = scenario_name_hint
        elif scen_names and len(scen_names) > 0:
            active_scenario_name = scen_names[0]

        # Scope wrapper images by upload/scenario to avoid cross-scenario image reuse.
        try:
            out_dir_for_tag = os.path.dirname(xml_path) if xml_path else ''
            upload_base = os.path.basename(out_dir_for_tag) if out_dir_for_tag else ''
            parts = []
            if upload_base:
                parts.append(upload_base)
            if active_scenario_name:
                parts.append(active_scenario_name)
            if run_id:
                parts.append(str(run_id)[:8])
            scenario_tag = _safe_name('-'.join(parts) if parts else (active_scenario_name or 'scenario'))
        except Exception:
            scenario_tag = _safe_name(active_scenario_name or 'scenario')

        # Pre-run safety check: existing scenario-scoped images -> prompt abort/overwrite.
        existing_images: list[str] = []
        try:
            prefix_repo = f"coretg/{scenario_tag}-"
            cmd = "docker images --format '{{.Repository}}:{{.Tag}}'"
            rc, out, _err = _exec_sudo(cmd, timeout=35.0, stage='docker.images.check')
            if rc == 0:
                for ln in (out or '').splitlines():
                    ln = (ln or '').strip()
                    if not ln or ln.startswith('<none>:'):
                        continue
                    if ln.startswith(prefix_repo):
                        existing_images.append(ln)
        except Exception:
            existing_images = []
        if existing_images:
            try:
                log_f.write(
                    f"{log_prefix}Found {len(existing_images)} existing scenario image(s) on CORE VM for tag '{scenario_tag}'.\n"
                )
                log_f.write(
                    f"{log_prefix}Existing images: {', '.join(existing_images[:8])}{' ...' if len(existing_images) > 8 else ''}\n"
                )
            except Exception:
                pass
            if not overwrite_existing_images:
                try:
                    _write_sse_marker(
                        log_f,
                        'phase',
                        {
                            'stage': 'docker.images.precheck',
                            'detail': 'Existing scenario images detected; awaiting overwrite confirmation',
                            'scenario_tag': scenario_tag,
                            'existing_images': existing_images[:25],
                        },
                    )
                except Exception:
                    pass
                try:
                    remote_client.close()
                except Exception:
                    pass
                try:
                    log_f.close()
                except Exception:
                    pass
                _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
                return jsonify({
                    'error': 'Existing scenario-scoped Docker images detected on the CORE VM.',
                    'kind': 'existing_images',
                    'scenario_tag': scenario_tag,
                    'existing_images': existing_images,
                    'can_overwrite': True,
                }), 412

            # Overwrite: remove any containers using these images, then remove the images.
            try:
                log_f.write(f"{log_prefix}Overwrite confirmed; removing existing scenario images\n")
                _write_sse_marker(
                    log_f,
                    'phase',
                    {
                        'stage': 'docker.images.overwrite',
                        'detail': 'Removing existing scenario images before execution',
                        'scenario_tag': scenario_tag,
                        'count': len(existing_images),
                    },
                )
            except Exception:
                pass
            try:
                images_json = json.dumps(existing_images)
                py_rm = (
                    "import os, json, subprocess\n"
                    "imgs=json.loads(os.environ.get('IMAGES_JSON','[]'))\n"
                    "removed=[]\nfailed=[]\n"
                    "for img in imgs:\n"
                    "  if not img or '<none>' in img: continue\n"
                    "  try:\n"
                    "    ids=subprocess.check_output(['docker','ps','-aq','--filter',f'ancestor={img}'], text=True, stderr=subprocess.STDOUT).split()\n"
                    "  except Exception:\n"
                    "    ids=[]\n"
                    "  if ids:\n"
                    "    try:\n"
                    "      subprocess.run(['docker','rm','-f']+ids, check=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n"
                    "    except Exception:\n"
                    "      pass\n"
                    "  p=subprocess.run(['docker','rmi','-f',img], check=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n"
                    "  if int(p.returncode or 0)==0:\n"
                    "    removed.append(img)\n"
                    "  else:\n"
                    "    failed.append({'image':img,'out':(p.stdout or '').strip()})\n"
                    "print('removed=' + str(len(removed)) + ' failed=' + str(len(failed)))\n"
                    "for item in failed[:5]:\n"
                    "  print('FAIL ' + item.get('image','') + ' ' + (item.get('out','')[:200]))\n"
                )
                cmd = f"IMAGES_JSON={shlex.quote(images_json)} python3 -c {shlex.quote(py_rm)}"
                _exec_sudo(cmd, timeout=180.0, stage='docker.images.overwrite')
            except Exception as exc:
                try:
                    log_f.write(f"{log_prefix}Failed removing existing scenario images: {exc}\n")
                except Exception:
                    pass
                try:
                    remote_client.close()
                except Exception:
                    pass
                try:
                    log_f.close()
                except Exception:
                    pass
                _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
                return jsonify({
                    'error': f'Failed removing existing scenario images on CORE VM: {exc}',
                    'kind': 'existing_images_overwrite_failed',
                    'scenario_tag': scenario_tag,
                }), 500

        # Now that SSH is confirmed and cleanup has run, block on active session conflicts.
        blocking_sessions = _collect_blocking_sessions()
        if blocking_sessions and not adv_auto_kill_sessions:
            count = len(blocking_sessions)
            message = (
                f"CORE VM {remote_desc} already has {count} active session(s); finish or stop the running scenario before starting another."
            )
            try:
                log_f.write(f"[remote] {message}\n")
            except Exception:
                pass
            try:
                remote_client.close()
            except Exception:
                pass
            try:
                log_f.close()
            except Exception:
                pass
            _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
            payload = {
                "error": message,
                "session_count": count,
                "core_host": core_host,
                "core_port": core_port,
                "active_sessions": [
                    {
                        "id": entry.get('id'),
                        "state": entry.get('state'),
                        "nodes": entry.get('nodes'),
                        "file": entry.get('file'),
                    }
                    for entry in blocking_sessions[:5]
                ],
            }
            return jsonify(payload), 423
        if blocking_sessions and adv_auto_kill_sessions:
            try:
                log_f.write(
                    f"[remote] NOTE: CORE VM {remote_desc} has {len(blocking_sessions)} active session(s); Advanced option will attempt to delete them before running.\n"
                )
            except Exception:
                pass

        # Checks run after cleanup actions.
        if adv_check_core_version:
            try:
                log_f.write(f"{log_prefix}=== Advanced: Check CORE version (expected 9.2.1) ===\n")
            except Exception:
                pass
            _check_core_version('9.2.1')
            try:
                log_f.write(f"{log_prefix}CORE version check passed.\n")
            except Exception:
                pass

        if adv_restart_core_daemon:
            try:
                log_f.write(f"{log_prefix}=== Advanced: Restart core-daemon ===\n")
            except Exception:
                pass
            _maybe_restart_core_daemon()
            try:
                log_f.write(f"{log_prefix}core-daemon restart requested.\n")
            except Exception:
                pass

        try:
            _write_sse_marker(
                log_f,
                'phase',
                {
                    'stage': 'core-daemon.precheck.begin',
                    'detail': 'Ensuring core-daemon is ready (pre-workspace)',
                },
            )
        except Exception:
            pass
        _check_remote_daemon_before_setup(
            client=remote_client,
            core_cfg=core_cfg,
            auto_start_allowed=auto_start_allowed,
            log_handle=log_f,
            log_prefix=log_prefix,
        )
        try:
            _write_sse_marker(
                log_f,
                'phase',
                {
                    'stage': 'core-daemon.precheck.done',
                    'detail': 'core-daemon verified (pre-workspace)',
                },
            )
        except Exception:
            pass
        try:
            pre_dir = os.path.join(out_dir or _outputs_dir(), 'core-pre')
            pre_saved = _grpc_save_current_session_xml_with_config(core_cfg, pre_dir)
        except Exception:
            pre_saved = None
        if pre_saved:
            app.logger.debug("[async] Pre-run session XML saved to %s", pre_saved)
    except CoreDaemonError as exc:
        try:
            log_f.write(f"{log_prefix}{exc}\n")
        except Exception:
            pass
        try:
            remote_client.close()
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
        status = 428 if isinstance(exc, CoreDaemonMissingError) else 409
        payload = {
            "error": str(exc),
            "daemon_missing": isinstance(exc, CoreDaemonMissingError),
            "daemon_conflict": isinstance(exc, CoreDaemonConflictError),
            "can_auto_start": bool(getattr(exc, 'can_auto_start', False)),
            "daemon_pids": getattr(exc, 'pids', []),
            "start_command": getattr(exc, 'start_command', CORE_DAEMON_START_COMMAND),
        }
        return jsonify(payload), status

    # Advanced: kill active sessions (if requested). This is done after core-daemon is confirmed,
    # to maximize chances that gRPC session deletion works.
    if adv_auto_kill_sessions:
        try:
            log_f.write(f"{log_prefix}=== Advanced: Auto-kill any running sessions ===\n")
        except Exception:
            pass
        deleted_ids, kill_errors = _maybe_kill_active_sessions()
        try:
            if deleted_ids:
                log_f.write(f"{log_prefix}Deleted sessions: {', '.join(str(x) for x in deleted_ids)}\n")
            else:
                log_f.write(f"{log_prefix}No sessions deleted.\n")
            for err in kill_errors:
                log_f.write(f"{log_prefix}{err}\n")
        except Exception:
            pass
        # Re-check session conflicts.
        blocking_sessions = _collect_blocking_sessions()
        if blocking_sessions:
            count = len(blocking_sessions)
            message = (
                f"CORE VM {remote_desc} still has {count} active session(s); cannot start another session."
            )
            try:
                log_f.write(f"{log_prefix}{message}\n")
            except Exception:
                pass
            try:
                remote_client.close()
            except Exception:
                pass
            try:
                log_f.close()
            except Exception:
                pass
            _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
            payload = {
                "error": message,
                "session_count": count,
                "core_host": core_host,
                "core_port": core_port,
                "deleted": deleted_ids,
                "errors": kill_errors,
                "active_sessions": [
                    {
                        "id": entry.get('id'),
                        "state": entry.get('state'),
                        "nodes": entry.get('nodes'),
                        "file": entry.get('file'),
                    }
                    for entry in blocking_sessions[:5]
                ],
            }
            return jsonify(payload), 423

    # Always emit a remote filesystem inventory early so Execute logs show whether
    # /tmp/vulns artifacts exist on the CORE VM even if the run fails before postrun.
    try:
        _log_remote_vulns_inventory_to_handle(core_cfg=core_cfg, log_handle=log_f, stage='pre_run')
    except Exception:
        pass

    try:
        remote_ctx = _prepare_remote_cli_context(
            client=remote_client,
            run_id=run_id,
            xml_path=xml_path,
            preview_plan_path=preview_plan_path,
            log_handle=log_f,
            upload_only_injected_artifacts=bool(upload_only_injected_artifacts),
        )
        try:
            log_f.write(f"{log_prefix}Workspace: repo={remote_ctx.get('repo_dir')} run_dir={remote_ctx.get('run_dir')}\n")
            _write_sse_marker(
                log_f,
                'phase',
                {
                    'stage': 'remote.workspace',
                    'repo_dir': remote_ctx.get('repo_dir'),
                    'run_dir': remote_ctx.get('run_dir'),
                    'xml_path': remote_ctx.get('xml_path'),
                    'preview_plan_path': remote_ctx.get('preview_plan_path'),
                },
            )
        except Exception:
            pass
        daemon_pid = _ensure_remote_core_daemon_ready(
            client=remote_client,
            core_cfg=core_cfg,
            auto_start_allowed=auto_start_allowed,
            sudo_password=core_cfg.get('ssh_password'),
            logger=getattr(app, 'logger', logging.getLogger(__name__)),
            log_handle=log_f,
            log_prefix=log_prefix,
        )
        try:
            log_f.write(f"{log_prefix}core-daemon PID {daemon_pid} is active\n")
        except Exception:
            pass
        remote_python = _select_remote_python_interpreter(remote_client, core_cfg)
        try:
            log_f.write(f"{log_prefix}Using python interpreter: {remote_python}\n")
        except Exception:
            pass
    except RemoteRepoMissingError as exc:
        try:
            log_f.write(f"{log_prefix}{exc}\n")
        except Exception:
            pass
        _purge_remote_run_dir()
        try:
            remote_client.close()
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
        return jsonify({"error": str(exc), "missing_repo": exc.repo_path, "can_push_repo": True}), 409
    except CoreDaemonMissingError as exc:
        try:
            log_f.write(f"{log_prefix}{exc}\n")
        except Exception:
            pass
        _purge_remote_run_dir()
        try:
            remote_client.close()
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
        return jsonify({
            "error": str(exc),
            "daemon_missing": True,
            "can_auto_start": bool(getattr(exc, 'can_auto_start', False)),
            "start_command": getattr(exc, 'start_command', CORE_DAEMON_START_COMMAND),
        }), 428
    except CoreDaemonConflictError as exc:
        detail = str(exc)
        try:
            log_f.write(f"{log_prefix}{detail}\n")
        except Exception:
            pass
        _purge_remote_run_dir()
        try:
            remote_client.close()
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
        return jsonify({
            "error": detail,
            "daemon_conflict": True,
            "daemon_pids": getattr(exc, 'pids', []),
        }), 409
    except Exception as exc:
        try:
            log_f.write(f"{log_prefix}{exc}\n")
        except Exception:
            pass
        _purge_remote_run_dir()
        try:
            remote_client.close()
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        _close_async_run_tunnel({'ssh_tunnel': ssh_tunnel})
        return jsonify({"error": str(exc)}), 500
    # NOTE: docker cleanup steps are executed earlier in the run (before checks).
    cli_args = [
        remote_python,
        '-u',
        '-m',
        'core_topo_gen.cli',
        '--xml',
        remote_ctx['xml_path'] if remote_ctx else xml_path,
        '--host',
        core_host,
        '--port',
        str(core_port),
        '--verbose',
    ]
    if docker_remove_conflicts:
        cli_args.append('--docker-remove-conflicts')
    if seed is not None:
        cli_args.extend(['--seed', str(seed)])
    if active_scenario_name:
        cli_args.extend(['--scenario', active_scenario_name])
    if remote_ctx and remote_ctx.get('preview_plan_path'):
        cli_args.extend(['--preview-plan', remote_ctx['preview_plan_path']])
    elif preview_plan_path:
        # Fallback (should not happen if remote_ctx is populated)
        cli_args.extend(['--preview-plan', preview_plan_path])
    cli_cmd = ' '.join(shlex.quote(arg) for arg in cli_args)
    work_dir = remote_ctx['repo_dir'] if remote_ctx else '.'
    activate_prefix = ''
    venv_bin_remote = str(core_cfg.get('venv_bin') or '').strip()
    if venv_bin_remote:
        activate_path = posixpath.join(venv_bin_remote.rstrip('/'), 'activate')
        activate_prefix = f". {shlex.quote(activate_path)} >/dev/null 2>&1 || true; "
    # In remote CORE VM mode, docker operations happen on the remote host.
    # Many CORE VMs require sudo for docker access (no docker group membership).
    # We enable sudo mode for docker commands and, when a password is available,
    # provide it via stdin to avoid leaking secrets in the command line.
    docker_env_parts: list[str] = []
    try:
        if _coerce_bool(core_cfg.get('ssh_enabled')):
            docker_env_parts.append('CORETG_DOCKER_USE_SUDO=1')
            # Fail fast if docker compose pull fails (we surface the error and cancel the run).
            docker_env_parts.append('CORETG_DOCKER_STRICT_PULL=1')
            if core_cfg.get('ssh_password'):
                docker_env_parts.append('CORETG_DOCKER_SUDO_PASSWORD_STDIN=1')
    except Exception:
        docker_env_parts = []
    docker_env_prefix = (' '.join(docker_env_parts) + ' ') if docker_env_parts else ''
    flow_env_parts: list[str] = []
    # Deliver Flow generator artifacts into vuln containers by copying files in,
    # rather than bind-mounting directories from the host.
    flow_env_parts.append('CORETG_FLOW_ARTIFACTS_MODE=copy')
    try:
        if remote_ctx and remote_ctx.get('base_dir'):
            flow_env_parts.append(f"CORE_REMOTE_BASE_DIR={shlex.quote(str(remote_ctx.get('base_dir')))}")
    except Exception:
        pass
    flow_env_prefix = (' '.join(flow_env_parts) + ' ') if flow_env_parts else ''
    remote_command = (
        f"{activate_prefix}cd {shlex.quote(work_dir)} && "
        f"CORETG_SCENARIO_TAG={shlex.quote(scenario_tag)} {flow_env_prefix}{docker_env_prefix}PYTHONUNBUFFERED=1 {cli_cmd}"
    )
    try:
        log_f.write(f"{log_prefix}Launching CLI in {work_dir}\n")
        try:
            log_f.write(f"{log_prefix}Flow artifacts mode: copy (docker cp into containers)\n")
        except Exception:
            pass
        _write_sse_marker(
            log_f,
            'phase',
            {
                'stage': 'remote.cli.launch',
                'work_dir': work_dir,
                'command': cli_cmd,
                'scenario_tag': scenario_tag,
                'venv_bin': venv_bin_remote or None,
            },
        )
    except Exception:
        pass
    try:
        transport = remote_client.get_transport()
        if transport is None or not transport.is_active():
            raise RuntimeError('SSH transport unavailable while launching remote CLI')
        channel = transport.open_session()
        channel.set_combine_stderr(True)
        channel.exec_command(f"bash -lc {shlex.quote(remote_command)}")

        # If configured, send sudo password for docker helpers via stdin (single line).
        try:
            pw = core_cfg.get('ssh_password')
            if pw and _coerce_bool(core_cfg.get('ssh_enabled')):
                channel.send(str(pw) + "\n")
        except Exception:
            pass
        output_thread = threading.Thread(
            target=_relay_remote_channel_to_log,
            args=(channel, log_f),
            name=f'remote-cli-{run_id[:8]}',
            daemon=True,
        )
        output_thread.start()
        proc = _RemoteProcessHandle(channel=channel, client=remote_client)
        proc.attach_output_thread(output_thread)
    except Exception as exc:
        try:
            log_f.write(f"{log_prefix}Failed to start remote CLI: {exc}\n")
        except Exception:
            pass
        try:
            remote_client.close()
        except Exception:
            pass
        try:
            log_f.close()
        except Exception:
            pass
        if ssh_tunnel:
            try:
                ssh_tunnel.close()
            except Exception:
                pass
        return jsonify({"error": f"Failed to launch remote CLI: {exc}"}), 500
    core_public = dict(core_cfg)
    core_public.pop('ssh_password', None)
    RUNS[run_id] = {
        'proc': proc,
        'log_path': log_path,
        'xml_path': xml_path,
        'done': False,
        'returncode': None,
        'pre_xml_path': pre_saved,
        'core_host': core_host,
        'core_port': core_port,
        'core_cfg': core_cfg,
        'core_cfg_public': core_public,
        'forward_host': conn_host,
        'forward_port': conn_port,
        'ssh_tunnel': ssh_tunnel,
        'scenario_names': scen_names,
        'scenario_name': active_scenario_name,
        'scenario_core': scenario_core_public,
        'post_xml_path': None,
        'history_added': False,
        'preview_plan_path': preview_plan_path,
        'summary_path': None,
    }
    # Start a background finalizer so history is appended even if the UI does not poll /run_status
    def _wait_and_finalize_async(run_id_local: str):
        meta: Dict[str, Any] | None = None
        try:
            meta = RUNS.get(run_id_local)
            if not meta:
                return
            p = meta.get('proc')
            if not p:
                return
            rc = p.wait()
            meta['done'] = True
            meta['returncode'] = rc

            try:
                _maybe_copy_flow_artifacts_into_containers(meta, stage='postrun')
            except Exception:
                pass
            try:
                _sync_remote_artifacts(meta)
            except Exception:
                pass
            # mirror the logic in run_status to extract artifacts and append history
            try:
                xml_path_local = meta.get('xml_path')
                report_md = None
                txt = ''
                try:
                    lp = meta.get('log_path')
                    if lp and os.path.exists(lp):
                        with open(lp, 'r', encoding='utf-8', errors='ignore') as f:
                            txt = f.read()
                        report_md = _extract_report_path_from_text(txt)
                except Exception:
                    report_md = None
                if not report_md:
                    report_md = _find_latest_report_path()
                if report_md:
                    app.logger.info("[async-finalizer] Detected report path: %s", report_md)
                summary_json = _extract_summary_path_from_text(txt)
                if not summary_json:
                    summary_json = _derive_summary_from_report(report_md)
                if not summary_json and not report_md:
                    summary_json = _find_latest_summary_path()
                if summary_json and not os.path.exists(summary_json):
                    summary_json = None
                if summary_json:
                    meta['summary_path'] = summary_json
                    app.logger.info("[async-finalizer] Detected summary path: %s", summary_json)
                # Best-effort: capture post-run CORE session XML
                post_saved = None
                try:
                    out_dir = os.path.dirname(xml_path_local or '')
                    post_dir = os.path.join(out_dir, 'core-post') if out_dir else os.path.join(_outputs_dir(), 'core-post')
                    sid = _extract_session_id_from_text(txt)
                    scenario_label = meta.get('scenario_name') or active_scenario_name
                    if not scenario_label:
                        try:
                            sns_meta = meta.get('scenario_names') or []
                            if isinstance(sns_meta, list) and sns_meta:
                                scenario_label = sns_meta[0]
                        except Exception:
                            scenario_label = None
                    if sid:
                        _record_session_mapping(xml_path_local, sid, scenario_label)
                        try:
                            sid_int = int(str(sid).strip())
                            cfg_for_meta = meta.get('core_cfg') if isinstance(meta.get('core_cfg'), dict) else None
                            if cfg_for_meta:
                                _write_remote_session_scenario_meta(
                                    cfg_for_meta,
                                    session_id=sid_int,
                                    scenario_name=scenario_label,
                                    scenario_xml_basename=os.path.basename(xml_path_local or '') or None,
                                    logger=app.logger,
                                )
                        except Exception:
                            pass
                    cfg_for_post = meta.get('core_cfg') or {
                        'host': meta.get('core_host') or CORE_HOST,
                        'port': meta.get('core_port') or CORE_PORT,
                    }
                    post_saved = _grpc_save_current_session_xml_with_config(cfg_for_post, post_dir, session_id=sid)
                except Exception:
                    post_saved = None
                if post_saved:
                    meta['post_xml_path'] = post_saved
                    app.logger.debug("[async-finalizer] Post-run session XML saved to %s", post_saved)
                else:
                    try:
                        _append_async_run_log_line(meta, "[validate] WARNING: post-run session XML missing; skipping validation")
                    except Exception:
                        pass
                try:
                    _append_session_scenario_discrepancies(
                        report_md,
                        xml_path_local,
                        post_saved,
                        scenario_label=scenario_label,
                    )
                except Exception:
                    pass
                try:
                    _append_async_run_log_line(meta, "[validate] Starting session validation")
                    validation = _validate_session_nodes_and_injects(
                        scenario_xml_path=xml_path_local,
                        session_xml_path=post_saved,
                        core_cfg=meta.get('core_cfg') if isinstance(meta.get('core_cfg'), dict) else None,
                        preview_plan_path=meta.get('preview_plan_path'),
                        scenario_label=scenario_label,
                    )
                    meta['validation_summary'] = validation
                    _append_async_run_log_line(meta, "[validate] VALIDATION_SUMMARY_JSON: " + json.dumps(validation))
                    if validation.get('ok'):
                        _append_async_run_log_line(meta, "[validate] Nodes created; containers running; injects present.")
                    else:
                        _append_async_run_log_line(
                            meta,
                            "[validate] WARNING: issues detected: "
                            f"missing_nodes={len(validation.get('missing_nodes') or [])}, "
                            f"docker_missing={len(validation.get('docker_missing') or [])}, "
                            f"docker_not_running={len(validation.get('docker_not_running') or [])}, "
                            f"injects_missing={len(validation.get('injects_missing') or [])}"
                        )
                except Exception:
                    pass
                # Build single-scenario XML, then a Full Scenario bundle including scripts
                single_xml = None
                try:
                    single_xml = _write_single_scenario_xml(xml_path_local, active_scenario_name, out_dir=os.path.dirname(xml_path_local or ''))
                except Exception:
                    single_xml = None
                bundle_xml = single_xml or xml_path_local
                full_bundle = _build_full_scenario_archive(
                    os.path.dirname(bundle_xml or ''),
                    bundle_xml,
                    (report_md if (report_md and os.path.exists(report_md)) else None),
                    meta.get('pre_xml_path'),
                    post_saved,
                    summary_path=summary_json,
                    run_id=run_id_local,
                )
                if full_bundle:
                    meta['full_scenario_path'] = full_bundle
                session_xml_path = post_saved if (post_saved and os.path.exists(post_saved)) else None
                history_ok = _append_run_history({
                    'timestamp': datetime.datetime.utcnow().isoformat() + 'Z',
                    'mode': 'async',
                    'xml_path': xml_path_local,
                    'post_xml_path': session_xml_path,
                    'session_xml_path': session_xml_path,
                    'scenario_xml_path': xml_path_local,
                    'report_path': report_md if (report_md and os.path.exists(report_md)) else None,
                    'summary_path': summary_json if (summary_json and os.path.exists(summary_json)) else None,
                    'pre_xml_path': meta.get('pre_xml_path'),
                    'full_scenario_path': full_bundle,
                    'single_scenario_xml_path': single_xml,
                    'returncode': rc,
                    'run_id': run_id_local,
                    'scenario_names': meta.get('scenario_names') or [],
                    'scenario_name': meta.get('scenario_name'),
                    'preview_plan_path': meta.get('preview_plan_path'),
                    'core': meta.get('core_cfg_public') or _normalize_core_config(meta.get('core_cfg') or {}, include_password=False),
                    'scenario_core': meta.get('scenario_core'),
                })
                if history_ok:
                    meta['history_added'] = True
            except Exception as e_final:
                try:
                    app.logger.exception("[async-finalizer] failed finalizing run %s: %s", run_id_local, e_final)
                except Exception:
                    pass
            finally:
                try:
                    _cleanup_remote_workspace(meta)
                except Exception:
                    pass
        except Exception:
            # swallow all exceptions to avoid crashing the web server
            try:
                app.logger.exception("[async-finalizer] unexpected error for run %s", run_id_local)
            except Exception:
                pass
        finally:
            if meta:
                _close_async_run_tunnel(meta)

    try:
        t = threading.Thread(target=_wait_and_finalize_async, args=(run_id,), daemon=True)
        t.start()
        app.logger.debug("[async] Finalizer thread started for run_id=%s", run_id)
    except Exception:
        pass
    return jsonify({"run_id": run_id})


@app.route('/core/check_remote_repo', methods=['POST'])
def check_remote_repo():
    payload = request.get_json(silent=True) or {}
    core_cfg = payload.get('core')
    if not isinstance(core_cfg, dict):
        try:
            core_json = payload.get('core_json')
            if core_json:
                core_cfg = json.loads(core_json)
        except Exception:
            core_cfg = None
    if not isinstance(core_cfg, dict):
        return jsonify({'error': 'core config missing'}), 400
    try:
        core_cfg = _merge_core_configs(core_cfg, include_password=True)
        core_cfg = _require_core_ssh_credentials(core_cfg)
    except Exception as exc:
        return jsonify({'error': str(exc)}), 400
    client = None
    sftp = None
    try:
        client = _open_ssh_client(core_cfg)
        sftp = client.open_sftp()
        repo_dir = _remote_static_repo_dir(sftp)
        try:
            sftp.stat(repo_dir)
        except Exception as exc:
            raise RemoteRepoMissingError(repo_dir) from exc
        package_dir = _remote_path_join(repo_dir, 'core_topo_gen')
        package_init = _remote_path_join(package_dir, '__init__.py')
        try:
            sftp.stat(package_dir)
            sftp.stat(package_init)
        except Exception as exc:
            raise RemoteRepoMissingError(repo_dir) from exc
        return jsonify({'ok': True, 'repo_path': repo_dir})
    except RemoteRepoMissingError as exc:
        return jsonify({'error': str(exc), 'missing_repo': exc.repo_path, 'can_push_repo': True}), 404
    except Exception as exc:
        return jsonify({'error': str(exc)}), 500
    finally:
        try:
            if sftp:
                sftp.close()
        except Exception:
            pass
        try:
            if client:
                client.close()
        except Exception:
            pass


@app.route('/run_status/<run_id>', methods=['GET'])
def run_status(run_id: str):
    meta = RUNS.get(run_id)
    if not meta:
        return jsonify({"error": "not found"}), 404
    proc = meta.get('proc')
    if proc and meta.get('returncode') is None:
        rc = proc.poll()
        if rc is not None:
            meta['done'] = True
            meta['returncode'] = rc
            try:
                _maybe_copy_flow_artifacts_into_containers(meta, stage='postrun')
            except Exception:
                pass
            try:
                _sync_remote_artifacts(meta)
            except Exception:
                pass
            # Append history once (success or failure)
            if not meta.get('history_added'):
                try:
                    active_scenario_name = None
                    try:
                        sns = meta.get('scenario_names') or []
                        if isinstance(sns, list) and sns:
                            active_scenario_name = sns[0]
                    except Exception:
                        active_scenario_name = None
                    xml_path_local = meta.get('xml_path')
                    # Parse report path from log contents; fallback to latest under reports/
                    report_md = None
                    txt = ''
                    try:
                        lp = meta.get('log_path')
                        if lp and os.path.exists(lp):
                            with open(lp, 'r', encoding='utf-8', errors='ignore') as f:
                                txt = f.read()
                            report_md = _extract_report_path_from_text(txt)
                    except Exception:
                        report_md = None
                    if not report_md:
                        report_md = _find_latest_report_path()
                    if report_md:
                        app.logger.info("[async] Detected report path: %s", report_md)
                    summary_json = _extract_summary_path_from_text(txt)
                    if not summary_json:
                        summary_json = _derive_summary_from_report(report_md)
                    if not summary_json and not report_md:
                        summary_json = _find_latest_summary_path()
                    if summary_json and not os.path.exists(summary_json):
                        summary_json = None
                    if summary_json:
                        meta['summary_path'] = summary_json
                        app.logger.info("[async] Detected summary path: %s", summary_json)
                    # Best-effort: capture post-run CORE session XML
                    post_saved = None
                    try:
                        out_dir = os.path.dirname(xml_path_local or '')
                        post_dir = os.path.join(out_dir, 'core-post') if out_dir else os.path.join(_outputs_dir(), 'core-post')
                        # Parse session id from logs if available for precise save
                        sid = _extract_session_id_from_text(txt)
                        scenario_label = meta.get('scenario_name') or active_scenario_name
                        if not scenario_label:
                            try:
                                sns_meta = meta.get('scenario_names') or []
                                if isinstance(sns_meta, list) and sns_meta:
                                    scenario_label = sns_meta[0]
                            except Exception:
                                scenario_label = None
                        if sid:
                            _record_session_mapping(xml_path_local, sid, scenario_label)
                            try:
                                sid_int = int(str(sid).strip())
                                cfg_for_meta = meta.get('core_cfg') if isinstance(meta.get('core_cfg'), dict) else None
                                if cfg_for_meta:
                                    _write_remote_session_scenario_meta(
                                        cfg_for_meta,
                                        session_id=sid_int,
                                        scenario_name=scenario_label,
                                        scenario_xml_basename=os.path.basename(xml_path_local or '') or None,
                                        logger=app.logger,
                                    )
                            except Exception:
                                pass
                        cfg_for_post = meta.get('core_cfg') or {
                            'host': meta.get('core_host') or CORE_HOST,
                            'port': meta.get('core_port') or CORE_PORT,
                        }
                        post_saved = _grpc_save_current_session_xml_with_config(cfg_for_post, post_dir, session_id=sid)
                    except Exception:
                        post_saved = None
                    if post_saved:
                        meta['post_xml_path'] = post_saved
                        app.logger.debug("[async] Post-run session XML saved to %s", post_saved)
                    else:
                        try:
                            _append_async_run_log_line(meta, "[validate] WARNING: post-run session XML missing; skipping validation")
                        except Exception:
                            pass
                    try:
                        _append_session_scenario_discrepancies(
                            report_md,
                            xml_path_local,
                            post_saved,
                            scenario_label=scenario_label,
                        )
                    except Exception:
                        pass
                    try:
                        _append_async_run_log_line(meta, "[validate] Starting session validation")
                        validation = _validate_session_nodes_and_injects(
                            scenario_xml_path=xml_path_local,
                            session_xml_path=post_saved,
                            core_cfg=meta.get('core_cfg') if isinstance(meta.get('core_cfg'), dict) else None,
                            preview_plan_path=meta.get('preview_plan_path'),
                            scenario_label=scenario_label,
                        )
                        meta['validation_summary'] = validation
                        _append_async_run_log_line(meta, "[validate] VALIDATION_SUMMARY_JSON: " + json.dumps(validation))
                        if validation.get('ok'):
                            _append_async_run_log_line(meta, "[validate] Nodes created; containers running; injects present.")
                        else:
                            _append_async_run_log_line(
                                meta,
                                "[validate] WARNING: issues detected: "
                                f"missing_nodes={len(validation.get('missing_nodes') or [])}, "
                                f"docker_missing={len(validation.get('docker_missing') or [])}, "
                                f"docker_not_running={len(validation.get('docker_not_running') or [])}, "
                                f"injects_missing={len(validation.get('injects_missing') or [])}"
                            )
                    except Exception:
                        pass
                    # Build single-scenario XML, then a Full Scenario bundle including scripts
                    single_xml = None
                    try:
                        single_xml = _write_single_scenario_xml(xml_path_local, active_scenario_name, out_dir=os.path.dirname(xml_path_local or ''))
                    except Exception:
                        single_xml = None
                    bundle_xml = single_xml or xml_path_local
                    full_bundle = _build_full_scenario_archive(
                        os.path.dirname(bundle_xml or ''),
                        bundle_xml,
                        (report_md if (report_md and os.path.exists(report_md)) else None),
                        meta.get('pre_xml_path'),
                        post_saved,
                        summary_path=summary_json,
                        run_id=run_id,
                    )
                    if full_bundle:
                        meta['full_scenario_path'] = full_bundle
                    session_xml_path = post_saved if (post_saved and os.path.exists(post_saved)) else None
                    history_ok = _append_run_history({
                        'timestamp': datetime.datetime.utcnow().isoformat() + 'Z',
                        'mode': 'async',
                        'xml_path': xml_path_local,
                        'post_xml_path': session_xml_path,
                        'session_xml_path': session_xml_path,
                        'scenario_xml_path': xml_path_local,
                        'report_path': report_md if (report_md and os.path.exists(report_md)) else None,
                        'summary_path': summary_json if (summary_json and os.path.exists(summary_json)) else None,
                        'pre_xml_path': meta.get('pre_xml_path'),
                        'full_scenario_path': full_bundle,
                        'single_scenario_xml_path': single_xml,
                        'returncode': rc,
                        'run_id': run_id,
                        'scenario_names': meta.get('scenario_names') or [],
                        'scenario_name': meta.get('scenario_name') or active_scenario_name,
                        'preview_plan_path': meta.get('preview_plan_path'),
                        'core': meta.get('core_cfg_public') or _normalize_core_config(meta.get('core_cfg') or {}, include_password=False),
                        'scenario_core': meta.get('scenario_core'),
                    })
                except Exception as e_hist:
                    try:
                        app.logger.exception("[async] failed appending run history: %s", e_hist)
                    except Exception:
                        pass
                finally:
                    if 'history_ok' in locals() and history_ok:
                        meta['history_added'] = True
                    _close_async_run_tunnel(meta)
                    try:
                        _cleanup_remote_workspace(meta)
                    except Exception:
                        pass
    if meta.get('done'):
        try:
            _sync_remote_artifacts(meta)
        except Exception:
            pass
    # Determine report path
    xml_path = meta.get('xml_path', '')
    out_dir = os.path.dirname(xml_path)
    # Determine report path (attempt to parse log each time so UI can link it without refresh)
    report_md = None
    txt = ''
    try:
        lp = meta.get('log_path')
        if lp and os.path.exists(lp):
            with open(lp, 'r', encoding='utf-8', errors='ignore') as f:
                txt = f.read()
            report_md = _extract_report_path_from_text(txt)
    except Exception:
        report_md = None

    docker_conflicts = _extract_docker_conflicts_from_text(txt)
    summary_json = _extract_summary_path_from_text(txt)
    if summary_json and not os.path.exists(summary_json):
        summary_json = None
    if summary_json:
        meta['summary_path'] = summary_json
    if meta.get('done'):
        try:
            scenario_label = meta.get('scenario_name')
            if not scenario_label:
                try:
                    sns = meta.get('scenario_names') or []
                    if isinstance(sns, list) and sns:
                        scenario_label = sns[0]
                except Exception:
                    scenario_label = None
            post_xml = meta.get('post_xml_path')
            summary = meta.get('validation_summary')
            summary_has_error = isinstance(summary, dict) and bool(summary.get('error'))
            summary_missing_docker = isinstance(summary, dict) and 'docker_nodes' not in summary
            summary_docker_empty = False
            try:
                if isinstance(summary, dict):
                    expected_docker = summary.get('expected_docker_nodes') or []
                    docker_nodes = summary.get('docker_nodes') or []
                    if expected_docker and isinstance(docker_nodes, list) and len(docker_nodes) == 0:
                        summary_docker_empty = True
            except Exception:
                summary_docker_empty = False
            summary_missing_nodes = False
            try:
                if isinstance(summary, dict):
                    if 'expected_nodes' in summary:
                        expected_nodes = summary.get('expected_nodes')
                        if isinstance(expected_nodes, list) and not expected_nodes:
                            summary_missing_nodes = True
                    if 'actual_nodes' in summary:
                        actual_nodes = summary.get('actual_nodes')
                        if isinstance(actual_nodes, list) and not actual_nodes:
                            summary_missing_nodes = True
            except Exception:
                summary_missing_nodes = False
            if summary is None or summary_has_error or summary_missing_docker or summary_docker_empty or summary_missing_nodes:
                meta['validation_summary'] = {
                    'ok': False,
                    'error': 'validation summary unavailable or incomplete; refusing fallback',
                }
        except Exception:
            pass
    return jsonify({
        'done': bool(meta.get('done')),
        'returncode': meta.get('returncode'),
        'docker_conflicts': docker_conflicts,
        'validation_summary': meta.get('validation_summary'),
        'report_path': report_md if (report_md and os.path.exists(report_md)) else None,
        'summary_path': summary_json if (summary_json and os.path.exists(summary_json)) else None,
        'xml_path': (meta.get('post_xml_path') if meta.get('post_xml_path') and os.path.exists(meta.get('post_xml_path')) else None),
        'log_path': meta.get('log_path'),
        'scenario_xml_path': xml_path,
        'pre_xml_path': meta.get('pre_xml_path'),
        'full_scenario_path': (lambda p: p if (p and os.path.exists(p)) else None)(meta.get('full_scenario_path')),
        'core': meta.get('core_cfg_public') or _normalize_core_config(meta.get('core_cfg') or {}, include_password=False),
        'forward_host': meta.get('forward_host'),
        'forward_port': meta.get('forward_port'),
    })


@app.route('/upload_base', methods=['POST'])
def upload_base():
    user = _current_user()
    f = request.files.get('base_xml')
    if not f or f.filename == '':
        flash('No base scenario file selected.')
        return redirect(url_for('index'))
    filename = secure_filename(f.filename)
    base_dir = os.path.join(app.config['UPLOAD_FOLDER'], 'base')
    os.makedirs(base_dir, exist_ok=True)
    unique = datetime.datetime.now().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:8]
    saved_path = os.path.join(base_dir, f"{unique}-{filename}")
    f.save(saved_path)
    ok, errs = _validate_core_xml(saved_path)
    payload = _default_scenarios_payload()
    payload['base_upload'] = {
        'path': saved_path,
        'valid': bool(ok),
        'display_name': filename,
        'exists': True,
    }
    if not ok:
        flash('Base scenario XML is INVALID. See details link for errors.')
    else:
        flash('Base scenario uploaded and validated.')
        try:
            # set the base scenario file path on the first scenario for convenience
            payload['scenarios'][0]['base']['filepath'] = saved_path
            payload['scenarios'][0]['base']['display_name'] = filename
        except Exception:
            pass
    _attach_base_upload(payload)
    if payload.get('base_upload'):
        _save_base_upload_state(payload['base_upload'])
    payload = _prepare_payload_for_index(payload, user=user)
    return render_template('index.html', payload=payload, logs=(errs if not ok else ''), xml_preview='')

@app.route('/remove_base', methods=['POST'])
def remove_base():
    """Clear the base scenario file reference from the first scenario."""
    user = _current_user()
    try:
        payload = _default_scenarios_payload()
        # If scenarios_json posted, honor that to keep user edits
        data_str = request.form.get('scenarios_json')
        if data_str:
            try:
                data = json.loads(data_str)
                if isinstance(data, dict) and 'scenarios' in data:
                    payload['scenarios'] = data['scenarios']
            except Exception:
                pass
        # Clear the base filepath of first scenario
        try:
            if payload['scenarios'] and isinstance(payload['scenarios'][0], dict):
                payload['scenarios'][0].setdefault('base', {}).update({'filepath': '', 'display_name': ''})
        except Exception:
            pass
        flash('Base scenario removed.')
        _clear_base_upload_state()
        payload.pop('base_upload', None)
        # Do not attach base upload (cleared)
        payload = _prepare_payload_for_index(payload, user=user)
        return render_template('index.html', payload=payload, logs='', xml_preview='')
    except Exception as e:
        flash(f'Failed to remove base: {e}')
        return redirect(url_for('index'))


@app.route('/base_details')
def base_details():
    xml_path = request.args.get('path')
    if not xml_path or not os.path.exists(xml_path):
        return "File not found", 404
    ok, errs = _validate_core_xml(xml_path)
    summary = _analyze_core_xml(xml_path) if ok else {'error': errs}
    return render_template('base_details.html', xml_path=xml_path, valid=ok, errors=errs, summary=summary)

# ---------------- CORE Management (sessions and XMLs) ----------------

def _core_sessions_store_path() -> str:
    return os.path.join(_outputs_dir(), 'core_sessions.json')


def _load_core_sessions_store() -> dict:
    p = _core_sessions_store_path()
    try:
        if os.path.exists(p):
            with open(p, 'r', encoding='utf-8') as f:
                d = json.load(f)
                return d if isinstance(d, dict) else {}
    except Exception:
        pass
    return {}

def _session_store_entry_session_id(value: Any) -> Optional[int]:
    candidate: Any
    if isinstance(value, dict):
        candidate = value.get('session_id') or value.get('id')
    else:
        candidate = value
    if candidate in (None, ''):
        return None
    try:
        return int(candidate)
    except Exception:
        return None


def _session_store_entry_scenario_norm(value: Any) -> str:
    if not isinstance(value, dict):
        return ''
    raw = value.get('scenario_norm') or value.get('scenario_name') or value.get('scenario')
    if not raw:
        return ''
    return _normalize_scenario_label(raw)


def _session_store_entry_updated_at_epoch(value: Any) -> Optional[float]:
    """Best-effort parse of mapping update time.

    Stored as ISO8601 (usually with trailing 'Z'). Returns epoch seconds.
    """
    if not isinstance(value, dict):
        return None
    raw = value.get('updated_at') or value.get('recorded_at') or value.get('ts')
    if not raw:
        return None
    try:
        text = str(raw).strip()
    except Exception:
        return None
    if not text:
        return None
    try:
        # Accept both explicit Z and +00:00 offsets.
        if text.endswith('Z'):
            text = text[:-1] + '+00:00'
        dt = datetime.datetime.fromisoformat(text)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=datetime.timezone.utc)
        return dt.timestamp()
    except Exception:
        return None


def _safe_path_mtime_epoch(path_value: Any) -> Optional[float]:
    if not path_value:
        return None
    try:
        ap = os.path.abspath(str(path_value))
    except Exception:
        ap = str(path_value)
    try:
        return os.path.getmtime(ap)
    except Exception:
        return None


def _save_core_sessions_store(d: dict) -> None:
    try:
        os.makedirs(os.path.dirname(_core_sessions_store_path()), exist_ok=True)
        tmp = _core_sessions_store_path() + '.tmp'
        with open(tmp, 'w', encoding='utf-8') as f:
            json.dump(d, f, indent=2)
        os.replace(tmp, _core_sessions_store_path())
    except Exception:
        pass


def _session_store_entry_core_host(value: Any) -> str:
    if not isinstance(value, dict):
        return ''
    raw = value.get('core_host') or value.get('host')
    return str(raw).strip() if raw not in (None, '') else ''


def _session_store_entry_core_port(value: Any) -> Optional[int]:
    if not isinstance(value, dict):
        return None
    raw = value.get('core_port') or value.get('port')
    if raw in (None, ''):
        return None
    try:
        return int(raw)
    except Exception:
        return None


def _session_store_entry_matches_core(value: Any, host: str, port: int) -> bool:
    """True when the mapping entry belongs to the given CORE daemon."""
    entry_host = _session_store_entry_core_host(value)
    entry_port = _session_store_entry_core_port(value)
    if entry_host and entry_port is not None:
        return (entry_host == str(host).strip()) and (int(entry_port) == int(port))
    # Legacy entries without core_host/core_port are treated as non-matching to avoid
    # cross-VM session-id collisions.
    return False


def _migrate_core_sessions_store_with_core_targets(store: dict, history: list[dict]) -> dict:
    """Best-effort: tag legacy core_sessions.json entries with core_host/core_port.

    We use the stored scenario_norm to look up that scenario's CORE host/port from run history.
    """
    if not isinstance(store, dict) or not store:
        return store
    if not isinstance(history, list) or not history:
        return store

    # Build scenario_norm -> (core_host, core_port) map.
    scenario_to_core: dict[str, tuple[str, int]] = {}
    for entry in history:
        if not isinstance(entry, dict):
            continue
        scen = (entry.get('scenario_name') or '').strip()
        norm = _normalize_scenario_label(scen)
        if not norm or norm in scenario_to_core:
            continue
        try:
            cfg = _select_core_config_for_page(norm, history, include_password=False)
            chost = str(cfg.get('host') or CORE_HOST).strip()
            cport = int(cfg.get('port') or CORE_PORT)
            scenario_to_core[norm] = (chost, cport)
        except Exception:
            continue

    dirty = False
    migrated = dict(store)
    for path, value in migrated.items():
        if not isinstance(value, dict):
            continue
        # Add a best-effort timestamp for older entries so "latest mapping" selection
        # behaves sensibly even when keys are reused across runs.
        if not value.get('updated_at'):
            mt = _safe_path_mtime_epoch(path)
            if mt is not None:
                try:
                    value['updated_at'] = datetime.datetime.fromtimestamp(
                        mt,
                        tz=datetime.timezone.utc,
                    ).isoformat().replace('+00:00', 'Z')
                    dirty = True
                except Exception:
                    pass
        if _session_store_entry_core_host(value) and (_session_store_entry_core_port(value) is not None):
            continue
        norm = _session_store_entry_scenario_norm(value)
        if not norm:
            continue
        target = scenario_to_core.get(norm)
        if not target:
            continue
        chost, cport = target
        value['core_host'] = chost
        value['core_port'] = int(cport)
        dirty = True

    if dirty:
        try:
            _save_core_sessions_store(migrated)
        except Exception:
            pass
    return migrated


def _filter_core_sessions_store_for_core(store: dict, host: str, port: int) -> dict:
    if not isinstance(store, dict) or not store:
        return {}
    filtered: dict = {}
    for path, value in store.items():
        if _session_store_entry_matches_core(value, host, port):
            filtered[path] = value
    return filtered


def _update_xml_session_mapping(
    xml_path: str,
    session_id: int | None,
    *,
    scenario_name: Optional[str] = None,
    core_host: Optional[str] = None,
    core_port: Optional[int] = None,
) -> None:
    try:
        store = _load_core_sessions_store()
        key = os.path.abspath(xml_path)
        if session_id is None:
            if key in store:
                store.pop(key, None)
        else:
            scenario_label = (scenario_name or '').strip()
            entry: dict[str, Any] = {'session_id': int(session_id)}
            try:
                entry['updated_at'] = datetime.datetime.now(datetime.timezone.utc).isoformat().replace('+00:00', 'Z')
            except Exception:
                pass
            scenario_norm = _normalize_scenario_label(scenario_label)
            if scenario_norm:
                entry['scenario_norm'] = scenario_norm
            if scenario_label:
                entry['scenario_name'] = scenario_label
            if core_host not in (None, ''):
                entry['core_host'] = str(core_host).strip()
            if core_port is not None:
                try:
                    entry['core_port'] = int(core_port)
                except Exception:
                    pass
            store[key] = entry
        _save_core_sessions_store(store)
    except Exception:
        pass


def _record_session_mapping(
    xml_path: str | None,
    session_id: Any,
    scenario_name: Optional[str] = None,
    core_host: Optional[str] = None,
    core_port: Optional[int] = None,
) -> None:
    if not xml_path or session_id in (None, ''):
        return
    try:
        sid_int = int(str(session_id).strip())
    except Exception:
        return
    _update_xml_session_mapping(
        xml_path,
        sid_int,
        scenario_name=scenario_name,
        core_host=core_host,
        core_port=core_port,
    )


def _list_active_core_sessions(
    host: str,
    port: int,
    core_cfg: Optional[Dict[str, Any]] = None,
    *,
    errors: Optional[List[str]] = None,
    meta: Optional[Dict[str, Any]] = None,
) -> list[dict]:
    """Return list of active CORE sessions via gRPC. Best-effort if gRPC missing."""
    logger = getattr(app, 'logger', logging.getLogger(__name__))
    cfg = _normalize_core_config(core_cfg or {'host': host, 'port': port}, include_password=True)
    sessions_raw = _list_active_core_sessions_via_remote_python(cfg, errors=errors, meta=meta, logger=logger)
    items: list[dict] = []
    for entry in sessions_raw:
        try:
            sid = entry.get('id')
            state = entry.get('state')
            file_path = entry.get('file')
            sess_dir = entry.get('dir')
            # If remote session doesn't provide a usable file path *and* we don't even
            # have a session directory hint, fall back to our local store mapping.
            # Prefer paths under outputs/core-sessions (gRPC save_xml exports) when present.
            if (not file_path) and (not sess_dir) and sid is not None:
                try:
                    store_map = _load_core_sessions_store()
                    outputs_root = _outputs_dir()
                    preferred_prefix = os.path.join(outputs_root, 'core-sessions')
                    best_path: Optional[str] = None
                    best_ts: Optional[float] = None
                    best_is_preferred = False
                    legacy_candidates: list[str] = []
                    for pth, stored_entry in store_map.items():
                        try:
                            stored_sid = _session_store_entry_session_id(stored_entry)
                            if stored_sid is None or int(stored_sid) != int(sid):
                                continue
                            # Prefer exact CORE host/port match.
                            if not _session_store_entry_matches_core(stored_entry, host, port):
                                # Legacy entries without core_host/core_port are ambiguous across CORE VMs.
                                if (not _session_store_entry_core_host(stored_entry)) and (
                                    _session_store_entry_core_port(stored_entry) is None
                                ):
                                    legacy_candidates.append(pth)
                                continue

                            ts = _session_store_entry_updated_at_epoch(stored_entry)
                            if ts is None:
                                ts = _safe_path_mtime_epoch(pth)
                            if ts is None:
                                ts = 0.0

                            is_preferred = False
                            try:
                                ap = os.path.abspath(str(pth))
                            except Exception:
                                ap = str(pth)
                            try:
                                is_preferred = bool(ap and preferred_prefix and ap.startswith(preferred_prefix + os.sep))
                            except Exception:
                                is_preferred = False

                            if best_path is None:
                                best_path, best_ts, best_is_preferred = pth, ts, is_preferred
                                continue

                            # Prefer core-sessions exports over any other mapping path.
                            if is_preferred and (not best_is_preferred):
                                best_path, best_ts, best_is_preferred = pth, ts, True
                                continue
                            if is_preferred == best_is_preferred:
                                if best_ts is None or ts >= best_ts:
                                    best_path, best_ts = pth, ts
                        except Exception:
                            continue
                    if best_path:
                        file_path = best_path
                    elif len(legacy_candidates) == 1:
                        file_path = legacy_candidates[0]
                except Exception:
                    pass
            if (not file_path) and sess_dir and os.path.isdir(sess_dir):
                try:
                    for fn in os.listdir(sess_dir):
                        if fn.lower().endswith('.xml'):
                            file_path = os.path.join(sess_dir, fn)
                            break
                except Exception:
                    pass
            nodes_count = entry.get('nodes')
            items.append({
                'id': sid,
                'state': state,
                'nodes': nodes_count if nodes_count is not None else None,
                'file': file_path,
                'dir': sess_dir,
                'filename': None,
            })
        except Exception:
            logger.exception('[core.grpc] Failed processing remote session entry: %s', entry)
            continue
    return items


def _collect_node_ips(node: dict[str, Any]) -> list[str]:
    ips: list[str] = []
    try:
        interfaces = node.get('interfaces') or []
        for iface in interfaces:
            if not isinstance(iface, dict):
                continue
            ip4 = (iface.get('ipv4') or '').strip()
            ip4_mask = (iface.get('ipv4_mask') or '').strip()
            ip6 = (iface.get('ipv6') or '').strip()
            ip6_mask = (iface.get('ipv6_mask') or '').strip()
            if ip4:
                val = f"{ip4}/{ip4_mask}" if ip4_mask else ip4
                if val not in ips:
                    ips.append(val)
            if ip6:
                val6 = f"{ip6}/{ip6_mask}" if ip6_mask else ip6
                if val6 not in ips:
                    ips.append(val6)
    except Exception:
        pass
    return ips


def _collect_named_iface_ips(node: dict[str, Any], *, name_prefix: str) -> list[str]:
    ips: list[str] = []
    prefix = (name_prefix or '').strip().lower()
    if not prefix:
        return ips
    try:
        interfaces = node.get('interfaces') or []
        for iface in interfaces:
            if not isinstance(iface, dict):
                continue
            iface_name = (iface.get('name') or '').strip().lower()
            if not iface_name.startswith(prefix):
                continue
            ip4 = (iface.get('ipv4') or '').strip()
            ip4_mask = (iface.get('ipv4_mask') or '').strip()
            ip6 = (iface.get('ipv6') or '').strip()
            ip6_mask = (iface.get('ipv6_mask') or '').strip()
            if ip4:
                val = f"{ip4}/{ip4_mask}" if ip4_mask else ip4
                if val not in ips:
                    ips.append(val)
            if ip6:
                val6 = f"{ip6}/{ip6_mask}" if ip6_mask else ip6
                if val6 not in ips:
                    ips.append(val6)
    except Exception:
        pass
    return ips


_IPV4_ADDR_RE = re.compile(r"^(?P<ip>(?:\d{1,3}\.){3}\d{1,3})(?:/(?P<suffix>[^\s]+))?$")


def _force_ipv4_prefixlen(ips: list[str], prefixlen: str = "24") -> list[str]:
    forced: list[str] = []
    seen: set[str] = set()
    for raw in ips or []:
        if raw is None:
            continue
        text = str(raw).strip()
        if not text:
            continue
        m = _IPV4_ADDR_RE.match(text)
        if m:
            text = f"{m.group('ip')}/{prefixlen}"
        if text in seen:
            continue
        seen.add(text)
        forced.append(text)
    return forced


def _hitl_details_from_path(xml_path: str) -> list[dict[str, Any]]:
    try:
        abs_path = os.path.abspath(xml_path)
    except Exception:
        abs_path = xml_path
    if not abs_path:
        return []
    try:
        st = os.stat(abs_path)
        mtime = st.st_mtime
    except Exception:
        mtime = None
    cached = _SESSION_HITL_CACHE.get(abs_path)
    if cached and cached.get('mtime') == mtime:
        return cached.get('data', [])
    hitl_details: list[dict[str, Any]] = []
    try:
        summary = _analyze_core_xml(abs_path)
        nodes_summary = summary.get('nodes') or []
        has_rj45 = any(
            isinstance(node, dict) and str(node.get('type') or '').strip().lower() == 'rj45'
            for node in nodes_summary
        )
        has_hitl_nodes = bool(summary.get('hitl_nodes') or [])
        has_explicit_hitl_indicator = has_rj45 or has_hitl_nodes
        # Prefer the gateway/router-side interface IP for the RJ45 attachment.
        # Our generated scenarios name that interface "hitl<idx>", so we can reliably
        # pick only the gateway IP (not router uplinks or the RJ45 node IP).
        gateway_preferred: list[dict[str, Any]] = []
        for node in nodes_summary:
            if not isinstance(node, dict):
                continue
            node_type = (node.get('type') or '').strip()
            if node_type.lower() != 'router':
                continue
            ips = _collect_named_iface_ips(node, name_prefix='hitl')
            ips = _force_ipv4_prefixlen(ips, prefixlen='24')
            if not ips:
                continue
            name = (node.get('name') or node.get('id') or '').strip()
            gateway_preferred.append({'name': name or 'HITL gateway', 'type': node_type, 'ips': ips[:1]})

        # Only accept hitl* router interfaces as HITL if there is an explicit HITL indicator
        # in the XML analysis (e.g., an RJ45/external node or analyzer-provided hitl_nodes).
        if gateway_preferred and has_explicit_hitl_indicator:
            hitl_details = gateway_preferred
        else:
            # Legacy fallback: rely on explicit HITL nodes from the analyzer.
            # Avoid inferring HITL from RJ45 nodes, as some sessions include RJ45-like
            # devices even when HITL is not configured.
            nodes = summary.get('hitl_nodes') or []
            for node in nodes:
                if not isinstance(node, dict):
                    continue
                name = (node.get('name') or node.get('id') or '').strip()
                node_type = (node.get('type') or '').strip()
                ips = _force_ipv4_prefixlen(_collect_node_ips(node), prefixlen="24")
                hitl_details.append({
                    'name': name or node_type or 'HITL',
                    'type': node_type,
                    'ips': ips,
                })
    except Exception:
        hitl_details = []
    _SESSION_HITL_CACHE[abs_path] = {'mtime': mtime, 'data': hitl_details}
    return hitl_details


def _session_hitl_metadata(
    session: dict,
    *,
    core_cfg: Optional[Dict[str, Any]] = None,
    session_store: Optional[dict] = None,
) -> list[dict[str, Any]]:
    candidate_paths: list[str] = []
    file_path = session.get('file')
    # If the session already points at an existing XML, treat it as authoritative.
    # Avoid falling back to session-id-based store lookups which can be stale when
    # CORE session IDs are reused.
    if file_path:
        try:
            fp = str(file_path)
            if os.path.exists(fp):
                details = _hitl_details_from_path(fp)
                session['_hitl_source'] = 'session.file'
                return details
        except Exception:
            pass
        candidate_paths.append(str(file_path))
    store = session_store if isinstance(session_store, dict) else _load_core_sessions_store()
    sid = session.get('id')
    sid_int: Optional[int] = None
    if sid not in (None, ''):
        try:
            sid_int = int(sid)
        except Exception:
            sid_int = None

    # If the session has a file path but it's not accessible locally, prefer fetching
    # the current session XML via gRPC (when allowed) before consulting any stored
    # mappings by session id.
    if core_cfg and sid_int is not None and file_path:
        try:
            fp = str(file_path)
            if fp and (not os.path.exists(fp)):
                try:
                    out_dir = os.path.join(_outputs_dir(), 'core-sessions')
                except Exception:
                    out_dir = _outputs_dir()
                saved = _grpc_save_current_session_xml_with_config(core_cfg, out_dir, session_id=str(sid_int))
                if saved and os.path.exists(saved):
                    try:
                        _update_xml_session_mapping(
                            saved,
                            sid_int,
                            scenario_name=session.get('scenario_name') or None,
                            core_host=core_cfg.get('host', CORE_HOST) if isinstance(core_cfg, dict) else None,
                            core_port=core_cfg.get('port', CORE_PORT) if isinstance(core_cfg, dict) else None,
                        )
                    except Exception:
                        pass
                    session['file'] = saved
                    session['_hitl_source'] = 'grpc.save_xml'
                    return _hitl_details_from_path(saved)
        except Exception:
            pass

    if sid_int is not None and isinstance(store, dict):
        matches: list[str] = []
        for path, stored in store.items():
            if _session_store_entry_session_id(stored) == sid_int:
                matches.append(path)
        if matches:
            # Prefer the most recently modified existing file when multiple paths map to the same session id.
            best = None
            best_mtime = -1.0
            for p in matches:
                if not p:
                    continue
                try:
                    ap = os.path.abspath(str(p))
                except Exception:
                    ap = str(p)
                try:
                    if not os.path.exists(ap):
                        continue
                    mt = os.path.getmtime(ap)
                except Exception:
                    mt = -1.0
                if mt > best_mtime:
                    best_mtime = mt
                    best = ap
            if best:
                candidate_paths.append(best)
    seen: set[str] = set()
    for path in candidate_paths:
        if not path:
            continue
        try:
            abs_path = os.path.abspath(path)
        except Exception:
            abs_path = path
        if abs_path in seen:
            continue
        seen.add(abs_path)
        if not os.path.exists(abs_path):
            continue
        details = _hitl_details_from_path(abs_path)
        if details:
            try:
                # Only set a file path if the session didn't already have one.
                # Avoid overwriting a remote CORE VM path with a stale local store path.
                existing_file = session.get('file')
                if not existing_file:
                    session['file'] = abs_path
            except Exception:
                pass
            return details
    if core_cfg and sid_int is not None:
        try:
            out_dir = os.path.join(_outputs_dir(), 'core-sessions')
        except Exception:
            out_dir = _outputs_dir()
        try:
            saved = _grpc_save_current_session_xml_with_config(core_cfg, out_dir, session_id=str(sid_int))
        except Exception:
            saved = None
        if saved and os.path.exists(saved):
            try:
                _update_xml_session_mapping(
                    saved,
                    sid_int,
                    scenario_name=session.get('scenario_name') or None,
                    core_host=core_cfg.get('host', CORE_HOST) if isinstance(core_cfg, dict) else None,
                    core_port=core_cfg.get('port', CORE_PORT) if isinstance(core_cfg, dict) else None,
                )
            except Exception:
                pass
            try:
                # Prefer showing the fetched local XML path for details/debugging.
                session['file'] = saved
            except Exception:
                pass
            return _hitl_details_from_path(saved)
    return []


def _attach_participant_urls_to_sessions(
    sessions: list[dict],
    session_store: dict,
    scenario_paths: dict[str, set[str]],
    participant_urls: dict[str, str],
) -> None:
    if not sessions or not participant_urls:
        return
    path_to_norm: dict[str, str] = {}
    for norm_key, candidates in (scenario_paths or {}).items():
        for candidate in candidates or []:
            try:
                ap = os.path.abspath(str(candidate))
            except Exception:
                ap = str(candidate)
            if ap:
                path_to_norm[ap] = norm_key
    sessions_by_id: dict[int, tuple[dict, str]] = {}
    for path, entry in (session_store or {}).items():
        sid = _session_store_entry_session_id(entry)
        if sid is None:
            continue
        try:
            ap = os.path.abspath(path)
        except Exception:
            ap = str(path)
        sessions_by_id[sid] = (entry, ap)
    path_lookup_cache: dict[str, dict[str, str]] = {}
    for sess in sessions:
        scenario_norm = ''
        candidate_paths: list[str] = []
        sid_raw = sess.get('id')
        sid_int: Optional[int] = None
        if sid_raw not in (None, ''):
            try:
                sid_int = int(sid_raw)
            except Exception:
                sid_int = None
        if sid_int is not None and sid_int in sessions_by_id:
            entry, mapped_path = sessions_by_id[sid_int]
            if mapped_path:
                candidate_paths.append(mapped_path)
            scenario_norm = _session_store_entry_scenario_norm(entry)
            if (not scenario_norm) and mapped_path:
                scenario_norm = path_to_norm.get(mapped_path, '')
            if (not scenario_norm) and mapped_path:
                base = os.path.splitext(os.path.basename(mapped_path))[0]
                scenario_norm = _normalize_scenario_label(base)
        if not scenario_norm:
            scenario_name = sess.get('scenario_name')
            if scenario_name:
                scenario_norm = _normalize_scenario_label(scenario_name)
        if not scenario_norm:
            candidate_path = sess.get('file') or sess.get('dir')
            if candidate_path:
                try:
                    ap = os.path.abspath(candidate_path)
                except Exception:
                    ap = str(candidate_path)
                scenario_norm = path_to_norm.get(ap, '')
                if not scenario_norm:
                    base = os.path.splitext(os.path.basename(ap))[0]
                    scenario_norm = _normalize_scenario_label(base)
                if ap:
                    candidate_paths.append(ap)
        if scenario_norm:
            sess['scenario_norm'] = scenario_norm
            participant_url = participant_urls.get(scenario_norm)
            if not participant_url:
                for candidate_path in candidate_paths:
                    if not candidate_path:
                        continue
                    if candidate_path not in path_lookup_cache:
                        path_lookup_cache[candidate_path] = _participant_urls_from_xml(candidate_path)
                    candidate_url = path_lookup_cache[candidate_path].get(scenario_norm)
                    if candidate_url:
                        participant_url = candidate_url
                        participant_urls.setdefault(scenario_norm, candidate_url)
                        break
            if participant_url:
                sess['participant_proxmox_url'] = participant_url


def _attach_hitl_metadata_to_sessions(
    sessions: list[dict],
    core_cfg: Optional[Dict[str, Any]] = None,
    *,
    allow_remote_fetch: bool = False,
    session_store: Optional[Dict[str, Any]] = None,
) -> None:
    store = session_store if session_store is not None else _load_core_sessions_store()
    cfg = core_cfg if allow_remote_fetch else None
    for sess in sessions:
        sess['hitl'] = _session_hitl_metadata(sess, core_cfg=cfg, session_store=store)


def _scan_core_xmls(max_count: int = 200) -> list[dict]:
    """Scan for runnable CORE XMLs and exclude scenario editor saves.

    Include only:
      - uploads/core/*.xml (user-uploaded CORE XMLs)
      - outputs/core-sessions/**/*.xml (saved via gRPC from running sessions)

    Exclude:
      - outputs/scenarios-*/** (scenario editor saves)

    Returns list of dicts: { path, name, size, mtime, valid } sorted by mtime desc.
    """
    uploads_core = os.path.join(_uploads_dir(), 'core')
    outputs_sessions = os.path.join(_outputs_dir(), 'core-sessions')
    allowed_roots = [uploads_core, outputs_sessions]
    paths: list[str] = []
    for root in allowed_roots:
        try:
            if not root or not os.path.exists(root):
                continue
            for dp, _dn, files in os.walk(root):
                for fn in files:
                    if fn.lower().endswith('.xml'):
                        paths.append(os.path.join(dp, fn))
        except Exception:
            continue
    # Dedup and sort by mtime desc
    seen = set()
    recs: list[tuple[float, dict]] = []
    for p in paths:
        ap = os.path.abspath(p)
        if ap in seen:
            continue
        seen.add(ap)
        try:
            st = os.stat(ap)
            mt = st.st_mtime
            size = st.st_size
        except Exception:
            mt = 0.0
            size = -1
        valid = False
        ok, _errs = _validate_core_xml(ap)
        valid = bool(ok)
        recs.append((mt, {'path': ap, 'name': os.path.basename(ap), 'size': size, 'mtime': mt, 'valid': valid}))
    recs.sort(key=lambda x: x[0], reverse=True)
    return [r for _mt, r in recs[:max_count]]


@app.route('/core')
def core_page():
    history = _load_run_history()
    current_user = _current_user()
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
    scenario_participant_urls = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
    participant_url_flags = {
        norm: bool(url)
        for norm, url in scenario_participant_urls.items()
        if isinstance(norm, str) and norm
    }
    scenario_query = request.args.get('scenario', '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    # Enforce per-scenario view: default to the first available scenario when unset/invalid.
    if scenario_names:
        if not scenario_norm or not any(_normalize_scenario_label(n) == scenario_norm for n in scenario_names):
            scenario_norm = _normalize_scenario_label(scenario_names[0])
    active_scenario = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query)
    core_cfg = _select_core_config_for_page(scenario_norm, history, include_password=True)
    core_cfg = _ensure_core_vm_metadata(core_cfg)
    host = core_cfg.get('host', CORE_HOST)
    port = int(core_cfg.get('port', CORE_PORT))
    core_vm_configured, core_vm_summary = _build_core_vm_summary(core_cfg)
    core_errors: list[str] = []
    app.logger.info(
        '[core.page] scenario=%s host=%s:%s ssh=%s@%s:%s',
        active_scenario or '<none>',
        host,
        port,
        (core_cfg.get('ssh_username') or '').strip() or '<none>',
        core_cfg.get('ssh_host'),
        core_cfg.get('ssh_port'),
    )
    # Active sessions via gRPC
    session_meta: Dict[str, Any] = {}
    sessions = _list_active_core_sessions(host, port, core_cfg, errors=core_errors, meta=session_meta)
    grpc_command = session_meta.get('grpc_command')
    app.logger.info('[core.page] session_count=%d', len(sessions))
    # Known XMLs
    xmls = _scan_core_xmls()
    # Map running by file path, with fallback to local store. Scope by CORE VM.
    mapping = _load_core_sessions_store()
    mapping = _migrate_core_sessions_store_with_core_targets(mapping, history)
    mapping = _filter_core_sessions_store_for_core(mapping, host, port)
    session_label_map = _build_session_scenario_labels(mapping, scenario_names, scenario_paths)
    scenario_session_ids = _session_ids_for_scenario(mapping, scenario_norm, scenario_paths) if scenario_norm else set()
    _annotate_sessions_with_scenarios(
        sessions,
        session_label_map,
        scenario_norm,
        scenario_names,
        scenario_paths,
        scenario_query,
        scenario_session_ids,
    )
    if scenario_norm:
        filtered_sessions, matched = _filter_sessions_by_scenario(sessions, scenario_norm, scenario_paths, scenario_session_ids)
        sessions = filtered_sessions
        if not matched:
            app.logger.info('[core.page] scenario=%s produced no session matches', scenario_norm)
        filtered_xmls, xml_matched = _filter_xmls_by_scenario(xmls, scenario_norm, scenario_paths, mapping)
        if xml_matched:
            xmls = filtered_xmls
        else:
            app.logger.info('[core.page] scenario=%s produced no XML matches; showing all XMLs', scenario_norm)
    # Provide desired display filenames for the UI: <scenario-name><timestamp>.xml.
    # For pycore sessions, the CORE-VM session-meta is the authoritative scenario source.
    try:
        all_sids: list[int] = []
        for s in sessions:
            try:
                sid = s.get('id')
                if sid in (None, ''):
                    continue
                all_sids.append(int(sid))
            except Exception:
                continue
        meta_by_sid = _read_remote_session_scenario_meta_bulk(core_cfg, session_ids=all_sids, logger=app.logger) if all_sids else {}
    except Exception:
        meta_by_sid = {}

    for s in sessions:
        try:
            sid = s.get('id')
            sid_int = int(sid) if sid not in (None, '') else None
        except Exception:
            sid_int = None

        # Prefer meta scenario label when present.
        if sid_int is not None:
            try:
                meta = meta_by_sid.get(sid_int) or {}
                meta_label = (meta.get('scenario_name') or '').strip() if isinstance(meta, dict) else ''
                if meta_label:
                    s['scenario_name'] = meta_label
            except Exception:
                pass

        try:
            scen = (s.get('scenario_name') or '').strip() or active_scenario or ''
            ts_epoch = None
            if sid_int is not None:
                try:
                    meta = meta_by_sid.get(sid_int) or {}
                    if isinstance(meta, dict) and meta.get('written_at_epoch') not in (None, ''):
                        ts_epoch = float(meta.get('written_at_epoch'))
                except Exception:
                    ts_epoch = None
            if ts_epoch is None and sid_int is not None:
                ts_epoch = _session_store_updated_at_for_session_id(mapping, sid_int, host=host, port=port)
            s['filename'] = _scenario_timestamped_filename(scen or None, ts_epoch)
        except Exception:
            pass

    _attach_hitl_metadata_to_sessions(sessions, core_cfg, allow_remote_fetch=False, session_store=mapping)
    _attach_participant_urls_to_sessions(sessions, mapping, scenario_paths, scenario_participant_urls)
    file_to_sid: dict[str, int] = {}
    # From gRPC session summaries (file path may be absolute)
    for s in sessions:
        f = s.get('file')
        sid = s.get('id')
        if f and sid is not None:
            file_to_sid[os.path.abspath(f)] = int(sid)
    # Merge with prior mappings
    for k, v in mapping.items():
        sid = _session_store_entry_session_id(v)
        if sid is None:
            continue
        file_to_sid.setdefault(os.path.abspath(k), sid)
    # Annotate xmls
    for x in xmls:
        sid = file_to_sid.get(x['path'])
        x['session_id'] = sid
        x['running'] = sid is not None
    return render_template(
        'core.html',
        sessions=sessions,
        xmls=xmls,
        host=host,
        port=port,
        scenarios=scenario_names,
        active_scenario=active_scenario,
        core_errors=core_errors,
        core_grpc_command=grpc_command,
        core_vm_configured=core_vm_configured,
        core_vm_summary=core_vm_summary,
        core_log_entries=_current_core_ui_logs(),
        participant_url_flags=participant_url_flags,
    )


@app.route('/core/data')
def core_data():
    def _query_bool_param(name: str, default: bool) -> bool:
        raw = request.args.get(name)
        if raw is None:
            return default
        try:
            v = str(raw).strip().lower()
        except Exception:
            return default
        if v in ('1', 'true', 'yes', 'on'):
            return True
        if v in ('0', 'false', 'no', 'off'):
            return False
        return default

    history = _load_run_history()
    current_user = _current_user()
    scenario_names, scenario_paths, scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
    scenario_participant_urls = _collect_scenario_participant_urls(scenario_paths, scenario_url_hints)
    participant_url_flags = {
        norm: bool(url)
        for norm, url in scenario_participant_urls.items()
        if isinstance(norm, str) and norm
    }
    scenario_query = request.args.get('scenario', '').strip()
    scenario_norm = _normalize_scenario_label(scenario_query)
    # Enforce per-scenario view: default to the first available scenario when unset/invalid.
    if scenario_names:
        if not scenario_norm or not any(_normalize_scenario_label(n) == scenario_norm for n in scenario_names):
            scenario_norm = _normalize_scenario_label(scenario_names[0])
    scenario_display = _resolve_scenario_display(scenario_norm, scenario_names, scenario_query)
    core_cfg = _select_core_config_for_page(scenario_norm, history, include_password=True)
    core_cfg = _ensure_core_vm_metadata(core_cfg)
    host = core_cfg.get('host', CORE_HOST)
    port = int(core_cfg.get('port', CORE_PORT))
    core_vm_configured, core_vm_summary = _build_core_vm_summary(core_cfg)
    core_errors: list[str] = []
    app.logger.info(
        '[core.data] scenario=%s host=%s:%s ssh=%s@%s:%s',
        scenario_display or '<none>',
        host,
        port,
        (core_cfg.get('ssh_username') or '').strip() or '<none>',
        core_cfg.get('ssh_host'),
        core_cfg.get('ssh_port'),
    )
    session_meta: Dict[str, Any] = {}
    include_xmls = _query_bool_param('include_xmls', True)

    sessions = _list_active_core_sessions(host, port, core_cfg, errors=core_errors, meta=session_meta)
    grpc_command = session_meta.get('grpc_command')
    app.logger.info('[core.data] session_count=%d', len(sessions))
    xmls: Optional[list[dict]] = None
    if include_xmls:
        xmls = _scan_core_xmls()
    # annotate xmls with running/session_id best-effort mapping, as in core_page. Scope by CORE VM.
    mapping = _load_core_sessions_store()
    mapping = _migrate_core_sessions_store_with_core_targets(mapping, history)
    mapping = _filter_core_sessions_store_for_core(mapping, host, port)
    session_label_map = _build_session_scenario_labels(mapping, scenario_names, scenario_paths)
    scenario_session_ids = _session_ids_for_scenario(mapping, scenario_norm, scenario_paths) if scenario_norm else set()
    _annotate_sessions_with_scenarios(
        sessions,
        session_label_map,
        scenario_norm,
        scenario_names,
        scenario_paths,
        scenario_query,
        scenario_session_ids,
    )

    # If session file/dir paths are on the CORE VM (common for webapp not running on CORE),
    # local path-based inference is disabled. Use CORE-VM session-meta as a fallback.
    missing_sids: list[int] = []
    for s in sessions:
        try:
            scen = (s.get('scenario_name') or '').strip()
            sid = s.get('id')
            if sid in (None, ''):
                continue
            candidate_path = ''
            try:
                candidate_path = str(s.get('file') or s.get('dir') or '').strip()
            except Exception:
                candidate_path = ''
            is_pycore = False
            try:
                if candidate_path.replace('\\', '/').startswith('/tmp/pycore.') or '/tmp/pycore.' in candidate_path.replace('\\', '/'):
                    is_pycore = True
            except Exception:
                is_pycore = False
            # Always try remote meta for pycore sessions because their filenames are generic.
            if is_pycore or (not scen):
                missing_sids.append(int(sid))
        except Exception:
            continue
    remote_meta_by_sid: dict[int, dict[str, Any]] = {}
    if missing_sids:
        try:
            remote_meta_by_sid = _read_remote_session_scenario_meta_bulk(core_cfg, session_ids=missing_sids, logger=app.logger)
        except Exception:
            remote_meta_by_sid = {}
    if remote_meta_by_sid:
        for s in sessions:
            try:
                sid = s.get('id')
                sid_int = int(sid) if sid not in (None, '') else None
            except Exception:
                sid_int = None
            if sid_int is None:
                continue
            try:
                meta = remote_meta_by_sid.get(sid_int) or {}
                label = (meta.get('scenario_name') or '').strip() if isinstance(meta, dict) else ''
                if not label:
                    continue

                # If the session file path is remote/unavailable locally, treat the
                # CORE-VM meta label as authoritative (it was written at session start).
                candidate_path = ''
                try:
                    candidate_path = str(s.get('file') or s.get('dir') or '').strip()
                except Exception:
                    candidate_path = ''
                is_remote = False
                try:
                    if candidate_path and (candidate_path.startswith('/tmp/pycore.') or candidate_path.startswith('\\tmp\\pycore.')):
                        is_remote = True
                except Exception:
                    is_remote = False
                if not is_remote:
                    try:
                        if candidate_path and (not os.path.exists(candidate_path)):
                            is_remote = True
                    except Exception:
                        is_remote = True

                if is_remote or (not (s.get('scenario_name') or '').strip()):
                    s['scenario_name'] = label
            except Exception:
                continue

    # Provide counts for *all* scenarios so the UI can indicate which scenario(s)
    # currently have active sessions, even though this endpoint is scenario-filtered.
    active_session_counts: dict[str, int] = {}
    try:
        norm_to_display: dict[str, str] = {}
        for name in scenario_names or []:
            try:
                norm_to_display[_normalize_scenario_label(name)] = str(name)
            except Exception:
                continue
        for s in sessions:
            try:
                lbl_norm = _normalize_scenario_label((s.get('scenario_name') or '').strip())
            except Exception:
                lbl_norm = ''
            if not lbl_norm:
                continue
            disp = norm_to_display.get(lbl_norm)
            if not disp:
                continue
            active_session_counts[disp] = active_session_counts.get(disp, 0) + 1
    except Exception:
        active_session_counts = {}

    # Scenario "items" = saved artifacts known for that scenario (e.g., saved XMLs).
    # This is intentionally derived from the scenario catalog (paths) to avoid extra
    # filesystem scanning cost during polling.
    scenario_item_counts: dict[str, int] = {}
    try:
        for name in scenario_names or []:
            try:
                disp = str(name)
                norm = _normalize_scenario_label(name)
                paths = scenario_paths.get(norm) if isinstance(scenario_paths, dict) else None
                scenario_item_counts[disp] = len(paths or [])
            except Exception:
                continue
    except Exception:
        scenario_item_counts = {}

    if scenario_norm:
        # Prefer scenario_name equality when we have it (including from remote meta).
        filtered: list[dict] = []
        matched = False
        for s in sessions:
            try:
                lbl = _normalize_scenario_label((s.get('scenario_name') or '').strip())
            except Exception:
                lbl = ''
            if lbl and lbl == scenario_norm:
                filtered.append(s)
                matched = True
        if not matched:
            # Fall back to the legacy path/id-based filter only when we couldn't match by label.
            filtered, matched = _filter_sessions_by_scenario(sessions, scenario_norm, scenario_paths, scenario_session_ids)
        sessions = filtered
        if not matched:
            app.logger.info('[core.data] scenario=%s produced no session matches', scenario_norm)

        # Finalize display label: if the session is in this scenario view by owner-id or
        # by path, ensure the badge reflects the selected scenario. This avoids stale
        # labels from remote CORE paths or session-meta when session IDs are reused.
        for s in sessions:
            try:
                sid = s.get('id')
                sid_int = int(sid) if sid not in (None, '') else None
            except Exception:
                sid_int = None
            try:
                is_owned = sid_int is not None and sid_int in (scenario_session_ids or set())
            except Exception:
                is_owned = False
            try:
                is_path_match = _path_matches_scenario(s.get('file'), scenario_norm, scenario_paths) or _path_matches_scenario(s.get('dir'), scenario_norm, scenario_paths)
            except Exception:
                is_path_match = False
            if is_owned or is_path_match:
                # If CORE-VM meta already supplied a scenario label, keep it.
                try:
                    sid = s.get('id')
                    sid_int = int(sid) if sid not in (None, '') else None
                except Exception:
                    sid_int = None
                if sid_int is not None:
                    try:
                        meta = remote_meta_by_sid.get(sid_int) or {}
                        meta_label = (meta.get('scenario_name') or '').strip() if isinstance(meta, dict) else ''
                    except Exception:
                        meta_label = ''
                    if meta_label:
                        continue
                try:
                    s['scenario_name'] = scenario_display or s.get('scenario_name') or ''
                except Exception:
                    pass
        if include_xmls and xmls is not None:
            filtered_xmls, xml_matched = _filter_xmls_by_scenario(xmls, scenario_norm, scenario_paths, mapping)
            if xml_matched:
                xmls = filtered_xmls
            else:
                app.logger.info('[core.data] scenario=%s produced no XML matches; showing all XMLs', scenario_norm)
    # Provide the requested generated filename: <scenario-name><timestamp>.xml.
    for s in sessions:
        try:
            sid = s.get('id')
            sid_int = int(sid) if sid not in (None, '') else None
        except Exception:
            sid_int = None
        try:
            scen = (s.get('scenario_name') or '').strip() or scenario_display or ''
            ts_epoch = None
            if sid_int is not None and sid_int in remote_meta_by_sid:
                meta = remote_meta_by_sid.get(sid_int) or {}
                if isinstance(meta, dict) and meta.get('written_at_epoch') not in (None, ''):
                    try:
                        ts_epoch = float(meta.get('written_at_epoch'))
                    except Exception:
                        ts_epoch = None
            if ts_epoch is None and sid_int is not None:
                ts_epoch = _session_store_updated_at_for_session_id(mapping, sid_int, host=host, port=port)
            s['filename'] = _scenario_timestamped_filename(scen or None, ts_epoch)
        except Exception:
            pass

    _attach_hitl_metadata_to_sessions(sessions, core_cfg, allow_remote_fetch=True, session_store=mapping)
    _attach_participant_urls_to_sessions(sessions, mapping, scenario_paths, scenario_participant_urls)
    file_to_sid: dict[str, int] = {}
    for s in sessions:
        f = s.get('file')
        sid = s.get('id')
        if f and sid is not None:
            file_to_sid[os.path.abspath(f)] = int(sid)
    for k, v in mapping.items():
        sid = _session_store_entry_session_id(v)
        if sid is None:
            continue
        file_to_sid.setdefault(os.path.abspath(k), sid)
    if include_xmls and xmls is not None:
        for x in xmls:
            sid = file_to_sid.get(x['path'])
            x['session_id'] = sid
            x['running'] = sid is not None

    daemon_status = 'ok'
    if core_errors:
        # Simple heuristic: if we have "Connection refused" or similar in errors, assume daemon is down.
        # errors come from _list_active_core_sessions -> _list_active_core_sessions_via_remote_python
        for err in core_errors:
            if 'Connection refused' in err or '111' in err or 'Remote CORE session fetch failed' in err:
                daemon_status = 'down'
                break

    core_modal_href = url_for('index', core_modal=1, scenario=scenario_display) if scenario_display else url_for('index', core_modal=1)
    payload: dict[str, Any] = {
        'sessions': sessions,
        'scenarios': scenario_names,
        'active_scenario': scenario_display,
        'participant_url_flags': participant_url_flags,
        'active_session_counts': active_session_counts,
        'scenario_item_counts': scenario_item_counts,
        'host': host,
        'port': port,
        'core_vm_configured': bool(core_vm_configured),
        'core_vm_summary': core_vm_summary or {},
        'core_modal_href': core_modal_href,
        'errors': core_errors,
        'grpc_command': grpc_command,
        'logs': _current_core_ui_logs(),
        'daemon_status': daemon_status,
    }
    if include_xmls:
        payload['xmls'] = xmls or []
    return jsonify(payload)


@app.route('/core/restart_core_daemon', methods=['POST'])
@login_required
def restart_core_daemon():
    """Attempt to restart the CORE daemon on the configured CORE VM."""
    scenario_norm = _normalize_scenario_label(request.args.get('scenario', ''))
    core_cfg = _select_core_config_for_page(scenario_norm, include_password=True)

    if not core_cfg.get('ssh_host'):
        return jsonify({'error': 'No CORE VM configured via SSH.'}), 400

    try:
        app.logger.info('[core.daemon] Attempting restart via SSH')
        client = _open_ssh_client(core_cfg)
        exit_code, _out, err = _exec_sudo('systemctl restart core-daemon', timeout=40.0, stage='core-daemon.restart', client=client)
        if exit_code != 0:
            err = (err or '').strip()
            return jsonify({'error': f'Restart failed (exit {exit_code}): {err}'}), 500

        # Check status briefly
        chk_code, _, _ = _exec_sudo('systemctl is-active core-daemon', timeout=10.0, stage='core-daemon.check', client=client)
        if chk_code != 0:
             return jsonify({'error': 'Restart command succeeded but service is not active.'}), 500

        app.logger.info('[core.daemon] Restart successful')
        # Allow time for gRPC to come up
        time.sleep(2.0)
        return jsonify({'status': 'ok', 'message': 'CORE daemon restarted successfully.'})
    except Exception as exc:
        app.logger.error('Failed to restart CORE daemon: %s', exc, exc_info=True)
        msg = str(exc)
        if 'Authentication failed' in msg:
             msg = 'SSH authentication failed. Check your credentials in Scenarios > Config.'
        return jsonify({'error': msg}), 500


@app.route('/core/upload', methods=['POST'])
def core_upload():
    f = request.files.get('xml_file')
    if not f or f.filename == '':
        flash('No file selected.')
        return _redirect_core_page_with_scenario()
    filename = secure_filename(f.filename)
    if not filename.lower().endswith('.xml'):
        flash('Only .xml allowed.')
        return _redirect_core_page_with_scenario()
    dest_dir = os.path.join(app.config['UPLOAD_FOLDER'], 'core')
    os.makedirs(dest_dir, exist_ok=True)
    unique = datetime.datetime.now().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:6]
    path = os.path.join(dest_dir, f"{unique}-{filename}")
    f.save(path)
    ok, errs = _validate_core_xml(path)
    if not ok:
        try: os.remove(path)
        except Exception: pass
        flash(f'Invalid CORE XML: {errs}')
        return _redirect_core_page_with_scenario()
    flash('XML uploaded and validated.')
    return _redirect_core_page_with_scenario()


def _redirect_core_page_with_scenario(*, scenario_hint: Optional[str] = None):
    """Redirect back to /core while preserving the current scenario filter.

    Many /core actions are POSTs from core.html. If we drop the scenario on redirect,
    the page will default back to the first scenario (often "Scenario 1").
    """
    try:
        scenario = (scenario_hint or request.values.get('scenario') or request.values.get('scenario_name') or '').strip()
    except Exception:
        scenario = (scenario_hint or '').strip() if isinstance(scenario_hint, str) else ''
    if scenario:
        return redirect(url_for('core_page', scenario=scenario))
    return redirect(url_for('core_page'))


@app.route('/core/push_repo', methods=['POST'])
def core_push_repo_route():
    """Upload the local repository snapshot to the remote CORE host."""

    xml_path: Optional[str] = None
    scenario_name_hint: Optional[str] = None
    scenario_index_hint: Optional[int] = None
    core_override: Optional[Dict[str, Any]] = None
    scenario_core_override: Optional[Dict[str, Any]] = None

    if request.form:
        xml_path = request.form.get('xml_path') or None
        scenario_name_hint = request.form.get('scenario') or request.form.get('scenario_name') or None
        raw_index = request.form.get('scenario_index')
        if raw_index not in (None, ''):
            try:
                scenario_index_hint = int(raw_index)
            except Exception:
                scenario_index_hint = None
        core_json = request.form.get('core_json')
        if core_json:
            try:
                core_override = json.loads(core_json)
            except Exception:
                core_override = None
        hitl_core_json = request.form.get('hitl_core_json')
        if hitl_core_json:
            try:
                scenario_core_override = json.loads(hitl_core_json)
            except Exception:
                scenario_core_override = None
    else:
        payload = request.get_json(silent=True) or {}
        if isinstance(payload, dict):
            xml_path = payload.get('xml_path') or payload.get('scenario_xml_path') or xml_path
            scenario_name_hint = (
                payload.get('scenario')
                or payload.get('scenario_name')
                or payload.get('active_scenario')
                or scenario_name_hint
            )
            if 'scenario_index' in payload:
                try:
                    scenario_index_hint = int(payload.get('scenario_index'))
                except Exception:
                    scenario_index_hint = None
            if isinstance(payload.get('core'), dict):
                core_override = payload.get('core')
            elif isinstance(payload.get('core_json'), str):
                try:
                    core_override = json.loads(payload.get('core_json') or '{}')
                except Exception:
                    core_override = None
            if isinstance(payload.get('hitl_core'), dict):
                scenario_core_override = payload.get('hitl_core')
            elif isinstance(payload.get('hitl_core_json'), str):
                try:
                    scenario_core_override = json.loads(payload.get('hitl_core_json') or '{}')
                except Exception:
                    scenario_core_override = None

    payload_for_core: Optional[Dict[str, Any]] = None
    scenario_payload: Optional[Dict[str, Any]] = None
    if xml_path:
        xml_path = os.path.abspath(xml_path)
        if os.path.exists(xml_path):
            try:
                payload_for_core = _parse_scenarios_xml(xml_path)
            except Exception:
                payload_for_core = None
        else:
            app.logger.warning('[core.push_repo] XML path not found: %s', xml_path)
    if payload_for_core:
        scen_list = payload_for_core.get('scenarios') or []
        if isinstance(scen_list, list) and scen_list:
            if scenario_name_hint:
                for scen_entry in scen_list:
                    if isinstance(scen_entry, dict) and str(scen_entry.get('name') or '').strip() == str(scenario_name_hint).strip():
                        scenario_payload = scen_entry
                        break
            if scenario_payload is None and scenario_index_hint is not None:
                if 0 <= scenario_index_hint < len(scen_list):
                    candidate = scen_list[scenario_index_hint]
                    if isinstance(candidate, dict):
                        scenario_payload = candidate
            if scenario_payload is None:
                for scen_entry in scen_list:
                    if isinstance(scen_entry, dict):
                        scenario_payload = scen_entry
                        break
    scenario_core_saved = None
    if scenario_payload and isinstance(scenario_payload.get('hitl'), dict):
        scenario_core_saved = scenario_payload['hitl'].get('core')
    global_core_saved = (
        payload_for_core.get('core') if (payload_for_core and isinstance(payload_for_core.get('core'), dict)) else None
    )
    core_cfg = _merge_core_configs(
        global_core_saved,
        scenario_core_saved,
        core_override if isinstance(core_override, dict) else None,
        scenario_core_override if isinstance(scenario_core_override, dict) else None,
        include_password=True,
    )
    progress_id = uuid.uuid4().hex
    _init_repo_push_progress(progress_id, stage='queued', detail='Queued repository sync', status='queued', percent=0.0)
    try:
        _schedule_repo_push_to_remote(progress_id, core_cfg, logger=app.logger)
    except _SSHTunnelError as exc:
        _update_repo_push_progress(progress_id, status='error', stage='error', detail=str(exc))
        return jsonify({'error': str(exc)}), 400
    except Exception as exc:
        app.logger.exception('[core.push_repo] Failed syncing repo: %s', exc)
        _update_repo_push_progress(progress_id, status='error', stage='error', detail=str(exc))
        return jsonify({'error': f'Failed pushing repo: {exc}', 'progress_id': progress_id}), 500
    return jsonify({'ok': True, 'progress_id': progress_id})


@app.route('/core/push_repo/status/<progress_id>', methods=['GET'])
def core_push_repo_status(progress_id: str):
    payload = _get_repo_push_progress(progress_id)
    if not payload:
        return jsonify({'progress_id': progress_id, 'status': 'unknown'}), 404
    response = {
        'progress_id': progress_id,
        'status': payload.get('status') or 'pending',
        'stage': payload.get('stage'),
        'detail': payload.get('detail'),
        'percent': payload.get('percent'),
        'updated_at': payload.get('updated_at'),
        'created_at': payload.get('created_at'),
    }
    return jsonify(response)


@app.route('/core/push_repo/cancel/<progress_id>', methods=['POST'])
def core_push_repo_cancel(progress_id: str):
    payload = _get_repo_push_progress(progress_id)
    if not payload:
        return jsonify({'progress_id': progress_id, 'status': 'unknown'}), 404
    status = (payload.get('status') or '').strip().lower()
    if status in ('complete', 'error', 'cancelled'):
        return jsonify({'ok': True, 'progress_id': progress_id, 'status': status, 'noop': True})
    _update_repo_push_progress(
        progress_id,
        cancel_requested=True,
        status='cancelled',
        stage='cancelled',
        detail='Cancelled by user.',
    )

    # Best-effort: attempt to stop any in-flight remote finalize by killing its PID.
    kill_info: Dict[str, Any] = {
        'attempted': False,
        'pidfile': None,
        'pidfile_found': False,
        'pid': None,
        'term_sent': False,
        'kill_sent': False,
        'pidfile_removed': False,
        'archive': None,
        'archive_existed_before': None,
        'archive_exists_after': None,
    }
    try:
        ctx = _get_repo_push_cancel_ctx(progress_id)
        if isinstance(ctx, dict):
            core_cfg = ctx.get('core_cfg')
            pidfile = ctx.get('remote_pidfile')
            remote_archive = ctx.get('remote_archive')
            if isinstance(core_cfg, dict):
                kill_info['attempted'] = True
                if isinstance(pidfile, str) and pidfile.strip():
                    kill_info['pidfile'] = pidfile
                if isinstance(remote_archive, str) and remote_archive.strip():
                    kill_info['archive'] = remote_archive

                client = _open_ssh_client(core_cfg)
                try:
                    kill_script = (
                        "set -e; "
                        f"pidfile={shlex.quote(pidfile or '')}; "
                        f"archive={shlex.quote(remote_archive or '')}; "
                        "pidfile_found=0; pid=''; "
                        "if [ -n \"$pidfile\" ] && [ -f \"$pidfile\" ]; then pidfile_found=1; pid=$(cat \"$pidfile\" 2>/dev/null || true); fi; "
                        "term_sent=0; kill_sent=0; "
                        "if [ -n \"$pid\" ]; then "
                        "kill -TERM \"$pid\" 2>/dev/null && term_sent=1 || term_sent=0; "
                        "sleep 0.5; "
                        "kill -KILL \"$pid\" 2>/dev/null && kill_sent=1 || kill_sent=0; "
                        "fi; "
                        "pidfile_removed=0; "
                        "if [ -n \"$pidfile\" ]; then rm -f -- \"$pidfile\" 2>/dev/null && pidfile_removed=1 || pidfile_removed=0; fi; "
                        "archive_existed_before=''; archive_exists_after=''; "
                        "if [ -n \"$archive\" ]; then "
                        "if [ -f \"$archive\" ]; then archive_existed_before=1; else archive_existed_before=0; fi; "
                        "rm -f -- \"$archive\" 2>/dev/null || true; "
                        "if [ -f \"$archive\" ]; then archive_exists_after=1; else archive_exists_after=0; fi; "
                        "fi; "
                        "echo PIDFILE_FOUND=\"$pidfile_found\"; "
                        "echo PID=\"$pid\"; "
                        "echo TERM_SENT=\"$term_sent\"; "
                        "echo KILL_SENT=\"$kill_sent\"; "
                        "echo PIDFILE_REMOVED=\"$pidfile_removed\"; "
                        "echo ARCHIVE_EXISTED_BEFORE=\"$archive_existed_before\"; "
                        "echo ARCHIVE_EXISTS_AFTER=\"$archive_exists_after\""
                    )
                    _code, out, _err = _exec_ssh_command(client, f"sh -lc {shlex.quote(kill_script)}", timeout=25.0)
                    for line in (out or '').splitlines():
                        if '=' not in line:
                            continue
                        k, v = line.split('=', 1)
                        k = k.strip().upper()
                        v = v.strip().strip('"')
                        if k == 'PIDFILE_FOUND':
                            kill_info['pidfile_found'] = v == '1'
                        elif k == 'PID':
                            kill_info['pid'] = v or None
                        elif k == 'TERM_SENT':
                            kill_info['term_sent'] = v == '1'
                        elif k == 'KILL_SENT':
                            kill_info['kill_sent'] = v == '1'
                        elif k == 'PIDFILE_REMOVED':
                            kill_info['pidfile_removed'] = v == '1'
                        elif k == 'ARCHIVE_EXISTED_BEFORE':
                            kill_info['archive_existed_before'] = None if v == '' else (v == '1')
                        elif k == 'ARCHIVE_EXISTS_AFTER':
                            kill_info['archive_exists_after'] = None if v == '' else (v == '1')
                finally:
                    try:
                        client.close()
                    except Exception:
                        pass
    except Exception:
        pass

    return jsonify({'ok': True, 'progress_id': progress_id, 'status': 'cancelled', 'remote': kill_info})


@app.route('/core/start', methods=['POST'])
def core_start():
    xml_path = request.form.get('path')
    if not xml_path:
        flash('Missing XML path')
        return _redirect_core_page_with_scenario()
    scenario_label = (request.form.get('scenario') or '').strip()
    ap = os.path.abspath(xml_path)
    if not os.path.exists(ap):
        flash('File not found')
        return _redirect_core_page_with_scenario(scenario_hint=scenario_label)
    ok, errs = _validate_core_xml(ap)
    if not ok:
        flash(f'Invalid CORE XML: {errs}')
        return _redirect_core_page_with_scenario(scenario_hint=scenario_label)
    core_cfg = _core_config_for_request(include_password=True)
    cfg = _normalize_core_config(core_cfg, include_password=True)
    ssh_user = (cfg.get('ssh_username') or '').strip() or '<unknown>'
    ssh_host = cfg.get('ssh_host') or cfg.get('host') or 'localhost'
    address = f"{cfg.get('host') or CORE_HOST}:{cfg.get('port') or CORE_PORT}"
    remote_xml_path: Optional[str] = None
    try:
        remote_xml_path = _upload_file_to_core_host(cfg, ap)
    except Exception as exc:
        flash(f'Failed to upload XML to CORE host: {exc}')
        return _redirect_core_page_with_scenario(scenario_hint=scenario_label)
    try:
        script = _remote_core_open_xml_script(address, remote_xml_path, auto_start=True)
        command_desc = (
            f"remote ssh {ssh_user}@{ssh_host} -> CoreGrpcClient.open_xml {address} ({os.path.basename(ap)})"
        )
        payload = _run_remote_python_json(
            cfg,
            script,
            logger=app.logger,
            label='core.open_xml',
            command_desc=command_desc,
        )
    except Exception as exc:
        flash(f'Failed to start CORE session: {exc}')
        return _redirect_core_page_with_scenario(scenario_hint=scenario_label)
    finally:
        if remote_xml_path:
            _remove_remote_file(cfg, remote_xml_path)
    if payload.get('error'):
        msg = payload.get('error')
        app.logger.warning('[core.start] remote error: %s', msg)
        flash(f'CORE rejected the XML: {msg}')
        tb = payload.get('traceback')
        if tb:
            app.logger.debug('[core.start] traceback: %s', tb)
        return _redirect_core_page_with_scenario(scenario_hint=scenario_label)
    sid = payload.get('session_id')
    if sid is None:
        flash('Remote CORE did not return a session id.')
        return _redirect_core_page_with_scenario(scenario_hint=scenario_label)
    try:
        sid_int = int(sid)
    except Exception:
        sid_int = sid
    app.logger.info('[core.start] Started session %s via %s (scenario=%r)', sid_int, address, scenario_label)
    _update_xml_session_mapping(
        ap,
        sid_int,
        scenario_name=scenario_label or None,
        core_host=cfg.get('host', CORE_HOST) if isinstance(cfg, dict) else None,
        core_port=cfg.get('port', CORE_PORT) if isinstance(cfg, dict) else None,
    )
    # Also persist a session->scenario tag on the CORE VM itself so a remote
    # session XML path (e.g., /tmp/pycore.<sid>/...) can be mapped back to a scenario.
    try:
        _write_remote_session_scenario_meta(
            cfg,
            session_id=int(sid_int) if isinstance(sid_int, int) else int(str(sid_int)),
            scenario_name=scenario_label or None,
            scenario_xml_basename=os.path.basename(ap),
            logger=app.logger,
        )
    except Exception:
        pass
    flash(f'Started session {sid_int}.')
    return _redirect_core_page_with_scenario(scenario_hint=scenario_label)


@app.route('/core/stop', methods=['POST'])
def core_stop():
    sid = request.form.get('session_id')
    if not sid:
        flash('Missing session id')
        return _redirect_core_page_with_scenario()
    try:
        sid_int = int(sid)
    except Exception:
        flash('Invalid session id')
        return _redirect_core_page_with_scenario()
    core_cfg = _core_config_for_request(include_password=True)
    try:
        _execute_remote_core_session_action(core_cfg, 'stop', sid_int, logger=app.logger)
        # Best-effort: cleanup Docker artifacts generated by this tool after stopping a session.
        # - Containers are derived from compose_assignments.json on the CORE VM (vuln docker-compose nodes).
        # - Images are limited to tool wrapper images under coretg/*:iproute2*.
        cleanup_containers = 0
        cleanup_images = 0
        cleanup_notes: list[str] = []
        try:
            status_payload = _run_remote_python_json(
                core_cfg,
                _remote_docker_status_script(core_cfg.get('ssh_password')),
                logger=app.logger,
                label='docker.status(for stop cleanup)',
                timeout=60.0,
            )
            names: list[str] = []
            if isinstance(status_payload, dict) and isinstance(status_payload.get('items'), list):
                for it in status_payload.get('items') or []:
                    if isinstance(it, dict) and it.get('name'):
                        names.append(str(it.get('name')))
            if names:
                payload = _run_remote_python_json(
                    core_cfg,
                    _remote_docker_cleanup_script(names, core_cfg.get('ssh_password')),
                    logger=app.logger,
                    label='docker.cleanup(on stop)',
                    timeout=120.0,
                )
                if isinstance(payload, dict) and isinstance(payload.get('results'), list):
                    cleanup_containers = len(payload.get('results') or [])
            else:
                cleanup_notes.append('no docker-compose node containers to cleanup')
        except Exception as exc:
            cleanup_notes.append(f'container cleanup skipped/failed: {exc}')

        try:
            payload = _run_remote_python_json(
                core_cfg,
                _remote_docker_remove_wrapper_images_script(core_cfg.get('ssh_password')),
                logger=app.logger,
                label='docker.wrapper_images.cleanup(on stop)',
                timeout=180.0,
            )
            if isinstance(payload, dict) and isinstance(payload.get('removed'), list):
                cleanup_images = len(payload.get('removed') or [])
        except Exception as exc:
            cleanup_notes.append(f'wrapper image cleanup skipped/failed: {exc}')

        msg = f'Stopped session {sid_int}.'
        extra = []
        if cleanup_containers:
            extra.append(f'docker containers cleaned={cleanup_containers}')
        if cleanup_images:
            extra.append(f'wrapper images removed={cleanup_images}')
        if cleanup_notes:
            extra.append('; '.join(cleanup_notes)[:240])
        if extra:
            msg = msg + ' ' + '  '.join(extra)
        flash(msg)
    except Exception as exc:
        flash(f'Failed to stop session: {exc}')
    return _redirect_core_page_with_scenario()


@app.route('/core/kill_active_sessions_api', methods=['POST'])
def core_kill_active_sessions_api():
    """Delete active CORE sessions.

    Intended for the Execute flow: when a run is blocked due to active sessions,
    the UI can prompt the user to kill the previous session(s) and retry.
    """
    payload = request.get_json(silent=True) or {}
    kill_all = bool(payload.get('kill_all'))
    session_ids_raw = payload.get('session_ids')

    core_cfg = _core_config_for_request(include_password=True)
    core_host = core_cfg.get('host', CORE_HOST)
    try:
        core_port = int(core_cfg.get('port', CORE_PORT))
    except Exception:
        core_port = CORE_PORT

    session_ids: list[int] = []
    if not kill_all:
        if isinstance(session_ids_raw, list):
            for item in session_ids_raw:
                try:
                    session_ids.append(int(str(item).strip()))
                except Exception:
                    continue

    if kill_all or not session_ids:
        try:
            sessions = _list_active_core_sessions(core_host, int(core_port), core_cfg, errors=[], meta={})
        except Exception:
            sessions = []
        for entry in sessions:
            sid = entry.get('id')
            if sid in (None, ''):
                continue
            try:
                session_ids.append(int(str(sid).strip()))
            except Exception:
                continue

    # De-dupe while preserving order
    seen: set[int] = set()
    ordered_ids: list[int] = []
    for sid in session_ids:
        if sid in seen:
            continue
        seen.add(sid)
        ordered_ids.append(sid)

    deleted: list[int] = []
    errors: list[str] = []
    for sid in ordered_ids:
        try:
            _execute_remote_core_session_action(core_cfg, 'delete', sid, logger=app.logger)
            deleted.append(sid)
        except Exception as exc:
            errors.append(f"Failed deleting session {sid}: {exc}")

    return jsonify({
        'ok': not errors,
        'deleted': deleted,
        'errors': errors,
        'core_host': core_host,
        'core_port': core_port,
    }), 200


@app.route('/core/stop_duplicate_daemons_api', methods=['POST'])
def core_stop_duplicate_daemons_api():
    """Stop duplicate core-daemon processes and restart core-daemon.

    Intended for the Execute flow: when a run is blocked due to multiple core-daemon
    processes, the UI can prompt the user to stop them and retry.

    Notes:
    - Requires SSH access and sudo privileges on the CORE VM.
    - If specific PIDs are provided, they are targeted first, but we still
      perform a best-effort cleanup (systemctl stop + pkill) to ensure a single daemon.
    """
    payload = request.get_json(silent=True) or {}
    pids_raw = payload.get('pids')
    requested_pids: list[int] = []
    if isinstance(pids_raw, list):
        for item in pids_raw:
            try:
                requested_pids.append(int(str(item).strip()))
            except Exception:
                continue

    core_cfg = _core_config_for_request(include_password=True)
    ssh_password = core_cfg.get('ssh_password')
    if not ssh_password:
        return jsonify({
            'ok': False,
            'error': 'Stopping core-daemon requires sudo; provide an SSH password.',
            'can_stop_daemons': False,
        }), 400
    if paramiko is None:
        return jsonify({'ok': False, 'error': 'Paramiko unavailable; cannot SSH to CORE VM.'}), 500
    _ensure_paramiko_available()

    ssh_client = paramiko.SSHClient()  # type: ignore[assignment]
    ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
    try:
        ssh_client.connect(
            hostname=str(core_cfg.get('ssh_host') or core_cfg.get('host') or 'localhost'),
            port=int(core_cfg.get('ssh_port') or 22),
            username=str(core_cfg.get('ssh_username') or ''),
            password=ssh_password,
            look_for_keys=False,
            allow_agent=False,
            timeout=10.0,
            banner_timeout=10.0,
            auth_timeout=10.0,
        )
        before = _collect_remote_core_daemon_pids(ssh_client)
        # Prefer requested pids (e.g. from conflict error), else operate on what we see.
        target = requested_pids or before
        _stop_remote_core_daemon_conflict(
            ssh_client,
            sudo_password=ssh_password,
            pids=target,
            logger=app.logger,
        )
        try:
            time.sleep(1.0)
        except Exception:
            pass
        after = _collect_remote_core_daemon_pids(ssh_client)
        return jsonify({
            'ok': True,
            'daemon_pids_before': before,
            'daemon_pids_after': after,
        }), 200
    except Exception as exc:
        return jsonify({'ok': False, 'error': str(exc)}), 500
    finally:
        try:
            ssh_client.close()
        except Exception:
            pass


@app.route('/core/start_session', methods=['POST'])
def core_start_session():
    sid = request.form.get('session_id')
    if not sid:
        flash('Missing session id')
        return _redirect_core_page_with_scenario()
    try:
        sid_int = int(sid)
    except Exception:
        flash('Invalid session id')
        return _redirect_core_page_with_scenario()
    core_cfg = _core_config_for_request(include_password=True)
    try:
        _execute_remote_core_session_action(core_cfg, 'start', sid_int, logger=app.logger)
        flash(f'Started session {sid_int}.')
    except Exception as exc:
        flash(f'Failed to start session: {exc}')
    return _redirect_core_page_with_scenario()


@app.route('/core/delete', methods=['POST'])
def core_delete():
    # Delete session (if provided) and/or delete XML file under uploads/ or outputs/
    sid = request.form.get('session_id')
    xml_path = request.form.get('path')
    if sid:
        try:
            sid_int = int(sid)
            core_cfg = _core_config_for_request(include_password=True)
            _execute_remote_core_session_action(core_cfg, 'delete', sid_int, logger=app.logger)
            flash(f'Deleted session {sid_int}.')
        except Exception as e:
            flash(f'Failed to delete session: {e}')
    if xml_path:
        ap = os.path.abspath(xml_path)
        # Safety: only delete inside uploads/ or outputs/
        try:
            allowed = [os.path.abspath(_uploads_dir()), os.path.abspath(_outputs_dir())]
            if any(ap.startswith(a + os.sep) or ap == a for a in allowed):
                try:
                    os.remove(ap)
                    flash('Deleted XML file.')
                except FileNotFoundError:
                    pass
                except Exception as e:
                    flash(f'Failed deleting XML: {e}')
                # clear mapping
                _update_xml_session_mapping(ap, None)
            else:
                flash('Refusing to delete file outside outputs/ or uploads/.')
        except Exception:
            pass
    return _redirect_core_page_with_scenario()



@app.route('/core/details')
def core_details():
    scenario_param = (request.args.get('scenario_name') or request.args.get('scenario') or '').strip()
    scenario_norm = _normalize_scenario_label(scenario_param)
    history = _load_run_history()
    current_user = _current_user()
    scenario_names, scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
    scenario_label = _resolve_scenario_display(scenario_norm, scenario_names, scenario_param)
    core_cfg = _core_config_for_request(include_password=True)
    core_host = core_cfg.get('host', CORE_HOST)
    core_port = int(core_cfg.get('port', CORE_PORT))
    xml_param = request.args.get('path')
    xml_path = os.path.abspath(xml_param) if xml_param else None
    # Safety: only allow inspecting local files we manage.
    if xml_path:
        try:
            allowed_roots = [os.path.abspath(_uploads_dir()), os.path.abspath(_outputs_dir())]
            if not any(xml_path == root or xml_path.startswith(root + os.sep) for root in allowed_roots):
                xml_path = None
        except Exception:
            xml_path = None
    if xml_path and not os.path.exists(xml_path):
        xml_path = None
    sid = request.args.get('session_id')
    xml_summary = None
    xml_valid = False
    errors = ''
    classification = None  # 'scenario' | 'session' | 'unknown' | 'planner'
    container_flag = False
    planner_bundle = False
    graph_nodes: list[dict[str, Any]] | None = None
    graph_links: list[dict[str, Any]] | None = None
    flow_meta: dict[str, Any] | None = None
    # If no XML path given but we have a session id, attempt to export the session XML so we can show details
    if not xml_path and sid:
        try:
            out_dir = os.path.join(_outputs_dir(), 'core-sessions')
            os.makedirs(out_dir, exist_ok=True)
            saved = _grpc_save_current_session_xml_with_config(core_cfg, out_dir, session_id=str(sid))
            if saved and os.path.exists(saved):
                xml_path = saved
        except Exception:
            pass
    if not xml_path and scenario_norm:
        fallback = _select_existing_path(scenario_paths.get(scenario_norm))
        if fallback:
            xml_path = fallback
            try:
                app.logger.info('[core.details] Using scenario fallback %s for %s', xml_path, scenario_label or scenario_norm)
            except Exception:
                pass
    if xml_path and os.path.exists(xml_path):
        try:
            # Lightweight classification: scenario XML should have <Scenarios>, session XML will have <session> and possibly <container>
            import xml.etree.ElementTree as _ET
            with open(xml_path, 'rb') as f:
                data_head = f.read(4096)
            try:
                root = _ET.fromstring(data_head + b"</dummy>")
            except Exception:
                try:
                    tree = _ET.parse(xml_path)
                    root = tree.getroot()
                except Exception:
                    root = None
            if root is not None:
                tag_lower = root.tag.lower()
                if 'scenarios' in tag_lower:
                    if root.find('.//ScenarioEditor') is not None:
                        planner_bundle = True
                        classification = 'planner'
                    else:
                        classification = 'scenario'
                elif 'session' in tag_lower:
                    classification = 'session'
                else:
                    classification = 'unknown'
                if root.find('.//container') is not None:
                    container_flag = True
                    if classification != 'scenario':
                        classification = 'session'
            if planner_bundle:
                xml_valid = True
                errors = ''
                xml_summary = _summarize_planner_scenarios(xml_path)
            else:
                ok, errs = _validate_core_xml(xml_path)
                if ok:
                    xml_valid = True
                else:
                    if classification == 'session':
                        xml_valid = True
                        # Suppress schema errors for session exports; treat as advisory only.
                    else:
                        xml_valid = False
                        if errs and not errors:
                            errors = errs
                # Always attempt analysis so graph can render even for invalid/session XML; mark summary with invalid flag
                try:
                    xml_summary = _analyze_core_xml(xml_path)
                    if xml_summary is None:
                        xml_summary = {}
                    if classification == 'session':
                        xml_summary['__session_export'] = True
                    if not xml_valid:
                        xml_summary['__invalid'] = True
                except Exception:
                    # On total failure keep prior xml_summary (None)
                    xml_summary = xml_summary or None
        except Exception as _e:
            errors = errors or f'XML inspection failed: {_e}'

    # For session XML exports, prefer the full topology payload (includes networks + vuln tagging).
    if xml_path and os.path.exists(xml_path) and classification == 'session':
        try:
            graph_nodes, graph_links, _adj = _build_topology_graph_from_session_xml(xml_path)
        except Exception:
            graph_nodes, graph_links = None, None

    # Best-effort attach saved Flow chain metadata so we can annotate sequence nodes.
    try:
        scenario_norm_for_flow = _normalize_scenario_label(scenario_norm)
        if scenario_norm_for_flow:
            flow_meta = _flow_state_from_latest_xml(scenario_norm_for_flow)
    except Exception:
        flow_meta = None
    session_info = None
    if sid:
        try:
            sid_int = int(sid)
            # lookup session info via gRPC
            sessions = _list_active_core_sessions(core_host, core_port, core_cfg)
            for s in sessions:
                if int(s.get('id')) == sid_int:
                    session_info = s
                    break
        except Exception:
            session_info = None
    try:
        if xml_summary is not None:
            app.logger.debug(
                "[core_details] xml_path=%s classification=%s valid=%s nodes=%s switch_nodes=%s links_detail=%s",
                xml_path, classification, xml_valid,
                len(xml_summary.get('nodes') or []),
                len(xml_summary.get('switch_nodes') or []),
                len(xml_summary.get('links_detail') or [])
            )
        else:
            app.logger.debug(
                "[core_details] xml_path=%s classification=%s valid=%s (no summary)",
                xml_path, classification, xml_valid
            )
    except Exception:
        pass
    # Render without legacy approved-plan context
    return render_template(
        'core_details.html',
        xml_path=xml_path,
        valid=xml_valid,
        errors=errors,
        summary=xml_summary,
        session=session_info,
        classification=classification,
        container_flag=container_flag,
        scenario_label=scenario_label,
        scenario_norm=scenario_norm,
        graph_nodes=graph_nodes,
        graph_links=graph_links,
        flow_meta=flow_meta,
    )


@app.route('/api/core-details/topology')
def api_core_details_topology():
    """Best-effort topology payload for Scenario Details.

    Priority:
    1) Latest session XML for scenario (most enriched: interfaces/ipv4 + vuln tagging)
    2) Latest preview plan for scenario (pre-execute: docker/vuln role semantics)
    """
    scenario_norm = _normalize_scenario_label(request.args.get('scenario') or '')
    if not scenario_norm:
        return jsonify({'ok': False, 'error': 'Missing scenario.'}), 400

    # Enforce assignment-based access for restricted roles.
    try:
        allowed_norms = _builder_allowed_norms(_current_user())
        if allowed_norms is not None and scenario_norm not in allowed_norms:
            return jsonify({'ok': False, 'error': 'Scenario not assigned.'}), 403
    except Exception:
        pass

    nodes: list[dict[str, Any]] = []
    links: list[dict[str, Any]] = []
    status = ''
    source = ''

    # 1) Prefer latest session XML for the scenario.
    session_xml_path = _latest_session_xml_for_scenario_norm(scenario_norm)
    if session_xml_path and os.path.exists(session_xml_path):
        try:
            nodes, links, _adj = _build_topology_graph_from_session_xml(session_xml_path)
            status = 'ok'
            source = 'session_xml'
        except Exception as e:
            status = f'Failed building session topology: {e}'
            nodes, links = [], []

    # 2) Fallback: build from latest preview plan.
    if not nodes:
        try:
            plan_path = _latest_preview_plan_for_scenario_norm(scenario_norm, prefer_flow=True)
        except Exception:
            plan_path = None
        if plan_path and os.path.exists(plan_path):
            try:
                payload = _load_preview_payload_from_path(plan_path, scenario_norm)
                full_prev = None
                if isinstance(payload, dict):
                    full_prev = payload.get('full_preview') if isinstance(payload.get('full_preview'), dict) else None
                    if full_prev is None and isinstance(payload.get('preview'), dict):
                        full_prev = payload.get('preview')
                if isinstance(full_prev, dict):
                    nodes2, links2, _adj2 = _build_topology_graph_from_preview_plan(full_prev)
                    # Normalize vuln hint field to what core_graph.js understands.
                    for n in nodes2 or []:
                        if not isinstance(n, dict):
                            continue
                        if n.get('is_vulnerability') is None:
                            n['is_vulnerability'] = bool(n.get('is_vuln'))
                    nodes = nodes2 or []
                    links = links2 or []
                    status = 'ok'
                    source = 'preview_plan'
                else:
                    status = 'Preview plan not embedded in XML.'
            except Exception as e:
                status = f'Failed building preview topology: {e}'

    # Best-effort attach flow metadata for sequence indices.
    flow_meta: dict[str, Any] | None = None
    try:
        flow_meta = _flow_state_from_latest_xml(scenario_norm)
    except Exception:
        flow_meta = None

    return jsonify({
        'ok': True,
        'scenario_norm': scenario_norm,
        'status': status,
        'source': source,
        'nodes': nodes,
        'links': links,
        'flow': flow_meta or None,
    })


@app.route('/admin/cleanup_pycore', methods=['POST'])
def admin_cleanup_pycore():
    """Remove stale /tmp/pycore.* directories not associated with active sessions.

    Returns JSON summary: {removed: [...], kept: [...]}"""
    try:
        core_cfg = _core_config_for_request(include_password=True)
        core_host = core_cfg.get('host', CORE_HOST)
        core_port = int(core_cfg.get('port', CORE_PORT))
        active_ids = set()
        try:
            sessions = _list_active_core_sessions(core_host, core_port, core_cfg)
            for s in sessions:
                try:
                    active_ids.add(int(s.get('id')))
                except Exception:
                    continue
        except Exception:
            pass
        removed = []
        kept = []
        for p in Path('/tmp').glob('pycore.*'):
            try:
                sid = int(p.name.split('.')[-1])
            except Exception:
                kept.append(str(p))
                continue
            if sid in active_ids:
                kept.append(str(p))
                continue
            # Only remove if directory exists and not recently modified (older than 30s) to avoid race with creation
            try:
                age = time.time() - p.stat().st_mtime
            except Exception:
                age = 999
            if age < 30:
                kept.append(str(p))
                continue
            try:
                shutil.rmtree(p)
                removed.append(str(p))
            except Exception:
                kept.append(str(p))
        return jsonify({'ok': True, 'removed': removed, 'kept': kept, 'active_session_ids': sorted(active_ids)})
    except Exception as e:
        return jsonify({'ok': False, 'error': str(e)})


@app.route('/core/save_xml', methods=['POST'])
def core_save_xml():
    sid = request.form.get('session_id')
    try:
        sid_int = int(sid) if sid is not None else None
    except Exception:
        sid_int = None
    out_dir = os.path.join(_outputs_dir(), 'core-sessions')
    os.makedirs(out_dir, exist_ok=True)
    core_cfg = _core_config_for_request(include_password=True)
    try:
        saved = _grpc_save_current_session_xml_with_config(
            core_cfg,
            out_dir,
            session_id=str(sid_int) if sid_int is not None else None,
        )
        if not saved or not os.path.exists(saved):
            return Response('Failed to save session XML', status=500)
        # Stream back as a download so frontend can save via blob
        return send_file(saved, as_attachment=True, download_name=os.path.basename(saved), mimetype='application/xml')
    except Exception as e:
        return Response(f'Error saving session XML: {e}', status=500)


@app.route('/core/session_scenario', methods=['GET'])
def core_session_scenario():
    """Resolve a CORE session (by sid or by remote path) to a scenario label.

    Query params:
      - sid: CORE session id
      - path: remote CORE VM path (e.g. /tmp/pycore.17/Scenario_1.xml)
    """
    sid_raw = (request.args.get('sid') or '').strip()
    path_raw = (request.args.get('path') or '').strip()
    sid: int | None = None
    if sid_raw:
        try:
            sid = int(sid_raw)
        except Exception:
            sid = None
    if sid is None and path_raw:
        sid = _extract_session_id_from_core_path(path_raw)
    if sid is None:
        return jsonify({'ok': False, 'error': 'Provide sid or path.'}), 400

    history = _load_run_history()
    current_user = _current_user()
    scenario_names, scenario_paths, _scenario_url_hints = _scenario_catalog_for_user(history, user=current_user)
    core_cfg = _core_config_for_request(include_password=True)
    host = core_cfg.get('host', CORE_HOST)
    try:
        port = int(core_cfg.get('port', CORE_PORT))
    except Exception:
        port = CORE_PORT

    # 1) Prefer local mapping store (already scoped by CORE host/port and timestamps).
    scenario_label: str | None = None
    try:
        store = _load_core_sessions_store()
        store = _migrate_core_sessions_store_with_core_targets(store, history)
        store = _filter_core_sessions_store_for_core(store, host, port)
        scenario_label = _session_store_scenario_for_session_id(store, int(sid), host=host, port=port)
    except Exception:
        scenario_label = None

    # 2) Fallback to CORE-VM-resident meta file.
    remote_meta: dict[str, Any] | None = None
    if not scenario_label:
        try:
            remote_meta = _read_remote_session_scenario_meta(core_cfg, session_id=int(sid), logger=app.logger)
            if isinstance(remote_meta, dict):
                scenario_label = (remote_meta.get('scenario_name') or '').strip() or None
        except Exception:
            remote_meta = None

    scenario_norm = _normalize_scenario_label(scenario_label or '') if scenario_label else ''
    # Enforce assignment-based access for restricted roles.
    allowed_norms = _builder_allowed_norms(current_user)
    if allowed_norms is not None and scenario_norm and scenario_norm not in allowed_norms:
        return jsonify({'ok': False, 'error': 'Scenario not assigned.'}), 403
    scenario_display = _resolve_scenario_display(scenario_norm, scenario_names, scenario_label or '') if scenario_norm else ''
    return jsonify({
        'ok': True,
        'session_id': int(sid),
        'scenario_name': scenario_display or (scenario_label or ''),
        'scenario_norm': scenario_norm,
        'core_host': host,
        'core_port': port,
        'source': 'local_store' if scenario_label and not remote_meta else ('remote_meta' if remote_meta else 'unknown'),
    })


@app.route('/core/session/<int:sid>')
def core_session(sid: int):
    """Convenience route to view a specific session's details.
    Attempts to look up the session and its file path, then reuses the core_details template.
    """
    session_info = None
    xml_path = None
    core_cfg = _core_config_for_request(include_password=True)
    try:
        sessions = _list_active_core_sessions(
            core_cfg.get('host', CORE_HOST),
            int(core_cfg.get('port', CORE_PORT)),
            core_cfg,
        )
        for s in sessions:
            if int(s.get('id')) == int(sid):
                session_info = s
                xml_path = s.get('file')
                break
    except Exception:
        session_info = None
    xml_valid = False
    errors = ''
    xml_summary = None
    if xml_path and os.path.exists(xml_path):
        ok, errs = _validate_core_xml(xml_path)
        xml_valid = bool(ok)
        errors = errs if not ok else ''
        xml_summary = _analyze_core_xml(xml_path) if ok else None
    return render_template('core_details.html', xml_path=xml_path, valid=xml_valid, errors=errors, summary=xml_summary, session=session_info)


@app.route('/test_core', methods=['POST'])
def test_core():
    current = _current_user()
    if not current or _normalize_role_value(current.get('role')) != 'admin':
        return jsonify({'ok': False, 'error': 'Admin privileges required'}), 403
    try:
        data: Dict[str, Any] = {}
        if request.is_json:
            data = request.get_json(silent=True) or {}
        else:
            data = {
                "host": request.form.get('host'),
                "port": request.form.get('port'),
                "ssh_enabled": request.form.get('ssh_enabled'),
                "ssh_host": request.form.get('ssh_host'),
                "ssh_port": request.form.get('ssh_port'),
                "ssh_username": request.form.get('ssh_username'),
                "ssh_password": request.form.get('ssh_password'),
                "core": request.form.get('core_json'),
            }
            try:
                if isinstance(data.get('core'), str) and data['core']:
                    data['core'] = json.loads(data['core'])
            except Exception:
                data['core'] = None
        direct_override = {
            key: data.get(key)
            for key in ('host', 'port', 'ssh_enabled', 'ssh_host', 'ssh_port', 'ssh_username', 'ssh_password')
            if data.get(key) not in (None, '')
        }
        scenario_core_raw = data.get('hitl_core')
        if not scenario_core_raw and request.form.get('hitl_core_json'):
            try:
                scenario_core_raw = json.loads(request.form.get('hitl_core_json') or '')
            except Exception:
                scenario_core_raw = None
        scenario_core_dict = scenario_core_raw if isinstance(scenario_core_raw, dict) else {}
        scenario_vm_key = str(scenario_core_dict.get('vm_key') or '').strip()
        scenario_vm_node = str(scenario_core_dict.get('vm_node') or '').strip()
        scenario_vm_name = str(scenario_core_dict.get('vm_name') or '').strip()
        scenario_vm_id_raw = scenario_core_dict.get('vmid') or scenario_core_dict.get('vm_id')
        scenario_vm_id = str(scenario_vm_id_raw).strip() if isinstance(scenario_vm_id_raw, (str, int)) else ''
        if not scenario_vm_node and scenario_vm_key and '::' in scenario_vm_key:
            scenario_vm_node = scenario_vm_key.split('::', 1)[0].strip()
        cfg = _merge_core_configs(
            data.get('core') if isinstance(data.get('core'), dict) else None,
            scenario_core_dict,
            direct_override if direct_override else None,
            include_password=True,
        )
        if not scenario_vm_key:
            return jsonify({'ok': False, 'error': 'Select a CORE VM before validating the connection.'}), 400
        cfg['vm_key'] = scenario_vm_key
        if scenario_vm_node:
            cfg['vm_node'] = scenario_vm_node
        if scenario_vm_name:
            cfg.setdefault('vm_name', scenario_vm_name)
        if scenario_vm_id:
            cfg.setdefault('vmid', scenario_vm_id)
        core_secret_id = str(cfg.get('core_secret_id') or '').strip()
        stored_secret = None
        if core_secret_id:
            stored_secret = _load_core_credentials(core_secret_id)
            if not stored_secret:
                return jsonify({'ok': False, 'error': 'Stored CORE credentials are unavailable. Re-enter the SSH password for the selected CORE VM.'}), 400
            stored_vm_key = str(stored_secret.get('vm_key') or '').strip()
            if stored_vm_key and stored_vm_key != scenario_vm_key:
                stored_vm_name = str(stored_secret.get('vm_name') or stored_secret.get('vm_key') or 'previous VM')
                mismatch_message = (
                    f'Stored CORE credentials target {stored_vm_name}, ' \
                    f'but Step 2 is configured for {scenario_vm_name or scenario_vm_key}. '
                    'Clear the Step 2 credentials and validate with the selected CORE VM.'
                )
                return jsonify({'ok': False, 'error': mismatch_message, 'vm_mismatch': True}), 409
            stored_vm_node = str(stored_secret.get('vm_node') or '').strip()
            if stored_vm_node and scenario_vm_node and stored_vm_node != scenario_vm_node:
                mismatch_message = (
                    f'Stored CORE credentials reference node {stored_vm_node}, '
                    f'but the selected CORE VM resides on node {scenario_vm_node}. '
                    'Clear the Step 2 credentials and validate with the selected CORE VM.'
                )
                return jsonify({'ok': False, 'error': mismatch_message, 'vm_mismatch': True}), 409
        auto_start_daemon = bool(cfg.get('auto_start_daemon'))
        install_custom_services = bool(cfg.get('install_custom_services'))
        stop_duplicate_daemons = bool(cfg.get('stop_duplicate_daemons'))
        adv_fix_docker_daemon = bool(cfg.get('adv_fix_docker_daemon'))
        adv_run_core_cleanup = bool(cfg.get('adv_run_core_cleanup'))
        adv_check_core_version = bool(cfg.get('adv_check_core_version'))
        adv_restart_core_daemon = bool(cfg.get('adv_restart_core_daemon'))
        adv_start_core_daemon = bool(cfg.get('adv_start_core_daemon'))
        adv_auto_kill_sessions = bool(cfg.get('adv_auto_kill_sessions'))
        is_pytest = bool(os.environ.get('PYTEST_CURRENT_TEST') or ('pytest' in sys.modules))
        daemon_pids: List[int] = []
        install_meta: Optional[Dict[str, Any]] = None

        advanced_checks: Dict[str, Dict[str, Any]] = {}
        advanced_warnings: List[str] = []
        if (adv_fix_docker_daemon or adv_run_core_cleanup or adv_check_core_version or adv_restart_core_daemon or adv_start_core_daemon or adv_auto_kill_sessions):
            # These checks require remote access.
            if is_pytest:
                app.logger.info('[core] advanced checks enabled but skipping remote execution (pytest)')
                advanced_checks = _run_core_connection_advanced_checks(
                    cfg,
                    adv_fix_docker_daemon=False,
                    adv_run_core_cleanup=False,
                    adv_check_core_version=False,
                    adv_restart_core_daemon=False,
                    adv_start_core_daemon=False,
                    adv_auto_kill_sessions=False,
                )
            else:
                advanced_checks = _run_core_connection_advanced_checks(
                    cfg,
                    adv_fix_docker_daemon=adv_fix_docker_daemon,
                    adv_run_core_cleanup=adv_run_core_cleanup,
                    adv_check_core_version=adv_check_core_version,
                    adv_restart_core_daemon=adv_restart_core_daemon,
                    adv_start_core_daemon=adv_start_core_daemon,
                    adv_auto_kill_sessions=adv_auto_kill_sessions,
                )
                failures = [
                    (key, res)
                    for key, res in (advanced_checks or {}).items()
                    if isinstance(res, dict) and res.get('enabled') and (res.get('ok') is False)
                ]
                if failures:
                    parts = []
                    for key, res in failures:
                        msg = str(res.get('message') or '').strip()
                        parts.append(f"{key}: {msg or 'failed'}")
                    advanced_warnings.append('Advanced checks failed: ' + '; '.join(parts))
        if is_pytest and not (auto_start_daemon or install_custom_services or stop_duplicate_daemons):
            # Unit tests mock the tunnel/socket checks and should not depend on real DNS/SSH.
            app.logger.info('[core] skipping core-daemon SSH inspection (pytest)')
        elif paramiko is None:
            if auto_start_daemon:
                app.logger.warning('[core] Paramiko unavailable; cannot auto-start or inspect core-daemon remotely.')
            if install_custom_services:
                app.logger.warning('[core] Paramiko unavailable; cannot install custom services remotely.')
        else:
            _ensure_paramiko_available()
            ssh_client = paramiko.SSHClient()  # type: ignore[assignment]
            ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
            try:
                ssh_client.connect(
                    hostname=str(cfg.get('ssh_host') or cfg.get('host') or 'localhost'),
                    port=int(cfg.get('ssh_port') or 22),
                    username=str(cfg.get('ssh_username') or ''),
                    password=cfg.get('ssh_password'),
                    look_for_keys=False,
                    allow_agent=False,
                    timeout=10.0,
                    banner_timeout=10.0,
                    auth_timeout=10.0,
                )
                daemon_pids = _collect_remote_core_daemon_pids(ssh_client)
                if len(daemon_pids) > 1:
                    if stop_duplicate_daemons:
                        try:
                            _stop_remote_core_daemon_conflict(
                                ssh_client,
                                sudo_password=cfg.get('ssh_password'),
                                pids=daemon_pids,
                                logger=app.logger,
                            )
                            try:
                                time.sleep(1.0)
                            except Exception:
                                pass
                            daemon_pids = _collect_remote_core_daemon_pids(ssh_client)
                        except Exception as exc:
                            msg = (
                                'Multiple core-daemon processes are running on the CORE VM, and the automatic stop failed. '
                                f'PIDs: {", ".join(str(pid) for pid in daemon_pids)}. Error: {exc}'
                            )
                            return jsonify({
                                'ok': False,
                                'error': msg,
                                'daemon_conflict': True,
                                'daemon_pids': daemon_pids,
                                'can_stop_daemons': bool(cfg.get('ssh_password')),
                                'code': 'core_daemon_conflict',
                            }), 409
                    if len(daemon_pids) > 1:
                        msg = (
                            'Multiple core-daemon processes are running on the CORE VM. '
                            f'PIDs: {", ".join(str(pid) for pid in daemon_pids)}. '
                            'Stop duplicate daemons before continuing.'
                        )
                        return jsonify({
                            'ok': False,
                            'error': msg,
                            'daemon_conflict': True,
                            'daemon_pids': daemon_pids,
                            'can_stop_daemons': bool(cfg.get('ssh_password')),
                            'code': 'core_daemon_conflict',
                        }), 409

                if install_custom_services:
                    app.logger.info('[core] Installing custom services on CORE VM...')
                    install_meta = _install_custom_services_to_core_vm(
                        ssh_client,
                        sudo_password=cfg.get('ssh_password'),
                        logger=app.logger,
                    )
                    app.logger.info(
                        '[core] Custom services installed: modules=%s target=%s',
                        ','.join(install_meta.get('modules') or []),
                        install_meta.get('services_dir'),
                    )
                    try:
                        time.sleep(1.0)
                    except Exception:
                        pass
                    daemon_pids = _collect_remote_core_daemon_pids(ssh_client)
                    if len(daemon_pids) > 1:
                        if stop_duplicate_daemons:
                            try:
                                _stop_remote_core_daemon_conflict(
                                    ssh_client,
                                    sudo_password=cfg.get('ssh_password'),
                                    pids=daemon_pids,
                                    logger=app.logger,
                                )
                                try:
                                    time.sleep(1.0)
                                except Exception:
                                    pass
                                daemon_pids = _collect_remote_core_daemon_pids(ssh_client)
                            except Exception as exc:
                                msg = (
                                    'Multiple core-daemon processes are running on the CORE VM after installing services, '
                                    'and the automatic stop failed. '
                                    f'PIDs: {", ".join(str(pid) for pid in daemon_pids)}. Error: {exc}'
                                )
                                return jsonify({
                                    'ok': False,
                                    'error': msg,
                                    'daemon_conflict': True,
                                    'daemon_pids': daemon_pids,
                                    'can_stop_daemons': bool(cfg.get('ssh_password')),
                                    'code': 'core_daemon_conflict',
                                }), 409
                        if len(daemon_pids) > 1:
                            msg = (
                                'Multiple core-daemon processes are running on the CORE VM after installing services. '
                                f'PIDs: {", ".join(str(pid) for pid in daemon_pids)}. '
                                'Stop duplicate daemons before continuing.'
                            )
                            return jsonify({
                                'ok': False,
                                'error': msg,
                                'daemon_conflict': True,
                                'daemon_pids': daemon_pids,
                                'can_stop_daemons': bool(cfg.get('ssh_password')),
                                'code': 'core_daemon_conflict',
                            }), 409

                if auto_start_daemon:
                    if daemon_pids:
                        app.logger.info('[core] Skipping auto-start; core-daemon already running (PID %s).', daemon_pids[0])
                    else:
                        _start_remote_core_daemon(ssh_client, cfg.get('ssh_password'), app.logger)
                        try:
                            time.sleep(1.0)
                        except Exception:
                            pass
                        daemon_pids = _collect_remote_core_daemon_pids(ssh_client)
                        if len(daemon_pids) != 1:
                            msg = 'core-daemon auto-start attempted but a single running process was not detected.'
                            app.logger.warning('[core] %s (pids=%s)', msg, daemon_pids)
                            return jsonify({'ok': False, 'error': msg, 'daemon_conflict': True}), 502
                else:
                    if not daemon_pids:
                        app.logger.warning('[core] No core-daemon process detected during validation.')
            except Exception as conn_exc:
                app.logger.warning('[core] core-daemon SSH inspection failed: %s', conn_exc)
            finally:
                try:
                    ssh_client.close()
                except Exception:
                    pass
        if is_pytest:
            app.logger.info('[core] skipping daemon listening check (pytest)')
        elif paramiko is None:
            app.logger.warning('[core] skipping daemon listening check (paramiko unavailable)')
        else:
            try:
                _ensure_core_daemon_listening(cfg, timeout=5.0)
            except Exception as exc:
                app.logger.warning('[core] daemon listening check failed: %s', exc)
                return jsonify({'ok': False, 'error': f'core-daemon is not reachable on {cfg.get("host")}:{cfg.get("port")}: {exc}'}), 502
        remote_desc = f"{cfg.get('host')}:{cfg.get('port')}"
        import socket
        forwarded_host = ''
        forwarded_port = 0
        with _core_connection(cfg) as (conn_host, conn_port):
            forwarded_host = conn_host
            forwarded_port = int(conn_port)
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2.0)
            try:
                sock.connect((forwarded_host, forwarded_port))
            finally:
                try:
                    sock.close()
                except Exception:
                    pass
        scenario_index_raw = data.get('scenario_index')
        try:
            scenario_index_val = int(scenario_index_raw) if scenario_index_raw not in (None, '') else None
        except Exception:
            scenario_index_val = None
        scenario_name_val = str(data.get('scenario_name') or data.get('scenario') or '').strip()
        secret_payload = {
            'scenario_name': scenario_name_val,
            'scenario_index': scenario_index_val,
            'grpc_host': cfg.get('host'),
            'grpc_port': cfg.get('port'),
            'ssh_host': cfg.get('ssh_host'),
            'ssh_port': cfg.get('ssh_port'),
            'ssh_username': cfg.get('ssh_username'),
            'ssh_password': cfg.get('ssh_password'),
            'ssh_enabled': cfg.get('ssh_enabled'),
            'venv_bin': cfg.get('venv_bin'),
        }
        if isinstance(scenario_core_raw, dict):
            secret_payload.update({
                'vm_key': scenario_core_raw.get('vm_key'),
                'vm_name': scenario_core_raw.get('vm_name'),
                'vm_node': scenario_core_raw.get('vm_node'),
                'vmid': scenario_core_raw.get('vmid'),
                'proxmox_secret_id': scenario_core_raw.get('proxmox_secret_id') or scenario_core_raw.get('secret_id'),
                'proxmox_target': scenario_core_raw.get('proxmox_target') if isinstance(scenario_core_raw.get('proxmox_target'), dict) else None,
            })
        try:
            stored_meta = _save_core_credentials(secret_payload)
        except RuntimeError as exc:
            return jsonify({"ok": False, "error": str(exc)}), 500
        except Exception as exc:
            app.logger.exception('[core] failed to persist credentials: %s', exc)
            return jsonify({"ok": False, "error": 'CORE connection succeeded but credentials could not be stored'}), 500
        summary_vm_label = stored_meta.get('vm_name') or stored_meta.get('vm_key')
        if summary_vm_label:
            summary_message = (
                f"Validated CORE access for {stored_meta['ssh_username']} @ "
                f"{stored_meta['ssh_host']}:{stored_meta['ssh_port']} (VM {summary_vm_label})"
            )
        else:
            summary_message = f"Validated CORE access for {stored_meta['ssh_username']} @ {stored_meta['ssh_host']}:{stored_meta['ssh_port']}"

        # Persist a safe shared hint so builders/participants can see the validated state.
        try:
            if scenario_name_val:
                _merge_hitl_validation_into_scenario_catalog(
                    scenario_name_val,
                    core={
                        'core_secret_id': stored_meta.get('identifier'),
                        'validated': bool(stored_meta.get('identifier')),
                        'last_validated_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
                        'grpc_host': stored_meta.get('grpc_host') or stored_meta.get('host'),
                        'grpc_port': stored_meta.get('grpc_port') or stored_meta.get('port'),
                        'ssh_host': stored_meta.get('ssh_host'),
                        'ssh_port': stored_meta.get('ssh_port'),
                        'vm_key': stored_meta.get('vm_key'),
                        'vm_name': stored_meta.get('vm_name'),
                        'vm_node': stored_meta.get('vm_node'),
                        'stored_at': stored_meta.get('stored_at'),
                    },
                )
        except Exception:
            pass
        return jsonify({
            "ok": True,
            "forward_host": forwarded_host,
            "forward_port": forwarded_port,
            "remote": remote_desc,
            "ssh_enabled": bool(cfg.get('ssh_enabled')),
            "host": cfg.get('host'),
            "port": int(cfg.get('port', 0)) if cfg.get('port') is not None else None,
            "daemon_pids": daemon_pids,
            "install_custom_services": install_meta,
            "advanced_checks": advanced_checks,
            "warnings": advanced_warnings,
            "core": _normalize_core_config(cfg, include_password=False),
            "core_secret_id": stored_meta['identifier'],
            "core_summary": stored_meta,
            "scenario_index": scenario_index_val,
            "scenario_name": scenario_name_val,
            "message": summary_message,
        })
    except _SSHTunnelError as e:
        return jsonify({"ok": False, "error": str(e), "ssh_error": True}), 200
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 200


@app.route('/test_core_venv', methods=['POST'])
def test_core_venv():
    payload = request.get_json(silent=True) or {}
    raw_path = payload.get('venv_bin') or payload.get('path') or ''
    sanitized = _sanitize_venv_bin_path(raw_path)
    if not sanitized:
        return jsonify({"ok": False, "error": 'Provide the CORE virtualenv bin path to test.'}), 400
    if not os.path.isabs(sanitized):
        return jsonify({"ok": False, "error": f'CORE venv bin must be an absolute path: {sanitized}'}), 400
    ssh_host = str(payload.get('ssh_host') or payload.get('host') or '').strip()
    if not ssh_host:
        return jsonify({"ok": False, "error": 'Provide the SSH host for the CORE VM to test the virtualenv.'}), 400
    try:
        ssh_port = int(payload.get('ssh_port') or 22)
    except Exception:
        return jsonify({"ok": False, "error": 'SSH port must be an integer.'}), 400
    ssh_username = str(payload.get('ssh_username') or '').strip()
    if not ssh_username:
        return jsonify({"ok": False, "error": 'Enter the SSH username before testing the CORE virtualenv.'}), 400
    ssh_password_raw = payload.get('ssh_password')
    ssh_password = '' if ssh_password_raw in (None, '') else str(ssh_password_raw)
    if not ssh_password:
        return jsonify({"ok": False, "error": 'Enter the SSH password before testing the CORE virtualenv.'}), 400
    try:
        _ensure_paramiko_available()
    except RuntimeError as exc:
        return jsonify({"ok": False, "error": str(exc)}), 500
    client = paramiko.SSHClient()  # type: ignore[assignment]
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # type: ignore[attr-defined]
    try:
        client.connect(
            hostname=ssh_host,
            port=ssh_port,
            username=ssh_username,
            password=ssh_password,
            look_for_keys=False,
            allow_agent=False,
            timeout=15.0,
            banner_timeout=15.0,
            auth_timeout=15.0,
        )
    except Exception as exc:
        return jsonify({"ok": False, "error": f'Failed to open SSH session to {ssh_host}:{ssh_port}: {exc}'}), 502
    python_candidates = [
        os.path.join(sanitized, exe_name)
        for exe_name in PYTHON_EXECUTABLE_NAMES
    ]
    candidate_literal = ' '.join(shlex.quote(path) for path in python_candidates)
    python_probe = textwrap.dedent(
        """
        import json
        import sys

        result = {
            "python": sys.executable,
            "version": sys.version.split()[0],
        }
        try:
            import core  # type: ignore  # noqa: F401
            import core.api.grpc.client  # type: ignore  # noqa: F401
        except Exception as exc:  # pragma: no cover - remote execution
            result["status"] = "error"
            result["error"] = repr(exc)
        else:
            result["status"] = "ok"
        print("::VENVCHECK::" + json.dumps(result))
        if result.get("status") != "ok":
            sys.exit(3)
        """
    ).strip()
    missing_payload = json.dumps({
        "status": "error",
        "error": f"No python executable found in {sanitized}",
    })
    remote_cmd = textwrap.dedent(
        f"""
        CANDIDATES=({candidate_literal})
        FOUND=0
        for candidate in "${{CANDIDATES[@]}}"; do
            if [ -x "$candidate" ]; then
                FOUND=1
                "$candidate" - <<'PY'
{python_probe}
PY
                exit $?
            fi
        done
        if [ $FOUND -eq 0 ]; then
            echo "::VENVCHECK::{missing_payload}"
            exit 10
        fi
        """
    ).strip()
    try:
        stdin, stdout, stderr = client.exec_command(remote_cmd, timeout=30.0)
        try:
            stdout_data = stdout.read()
            stderr_data = stderr.read()
        finally:
            try:
                stdin.close()
            except Exception:
                pass
        exit_code = stdout.channel.recv_exit_status() if hasattr(stdout, 'channel') else 0
    except Exception as exc:
        try:
            client.close()
        except Exception:
            pass
        return jsonify({"ok": False, "error": f'Failed to probe CORE virtualenv via SSH: {exc}'}), 500
    finally:
        try:
            client.close()
        except Exception:
            pass
    def _decode(data: Any) -> str:
        if isinstance(data, bytes):
            return data.decode('utf-8', 'ignore')
        return str(data or '')
    stdout_text = _decode(stdout_data)
    stderr_text = _decode(stderr_data)
    summary: Dict[str, Any] | None = None
    for blob in (stdout_text, stderr_text):
        if not blob:
            continue
        for line in blob.splitlines():
            line = line.strip()
            if not line:
                continue
            if not line.startswith('::VENVCHECK::'):
                continue
            payload_text = line.split('::VENVCHECK::', 1)[-1]
            try:
                summary = json.loads(payload_text)
                break
            except Exception:
                continue
        if summary:
            break
    python_version = summary.get('version') if isinstance(summary, dict) else None
    python_path = summary.get('python') if isinstance(summary, dict) else None
    status = summary.get('status') if isinstance(summary, dict) else None
    error_detail = summary.get('error') if isinstance(summary, dict) else None
    if exit_code == 0 and status == 'ok':
        message = summary.get('message') or f"Python {python_version or ''} imported core.api.grpc successfully.".strip()
        return jsonify({
            "ok": True,
            "message": message,
            "venv_bin": sanitized,
            "python_executable": python_path,
            "python_version": python_version,
            "ssh_host": ssh_host,
            "ssh_port": ssh_port,
            "stdout": stdout_text.strip(),
            "stderr": stderr_text.strip(),
        })
    error_message = error_detail or stderr_text.strip() or stdout_text.strip() or 'core.api.grpc import failed in this environment.'
    return jsonify({
        "ok": False,
        "error": error_message,
        "venv_bin": sanitized,
        "python_executable": python_path,
        "python_version": python_version,
        "ssh_host": ssh_host,
        "ssh_port": ssh_port,
        "stdout": stdout_text.strip(),
        "stderr": stderr_text.strip(),
        "returncode": exit_code,
    }), 400


@app.route('/stream/<run_id>')
def stream_logs(run_id: str):
    meta = RUNS.get(run_id)
    if not meta:
        return Response('event: error\ndata: not found\n\n', mimetype='text/event-stream')
    log_path = meta.get('log_path')

    marker_re = re.compile(
        r'^' + re.escape(_SSE_MARKER_PREFIX) + r'\s+(?P<event>[a-zA-Z0-9_\-]+)\s+(?P<data>\{.*\})\s*$'
    )

    def _emit_line(line: str):
        """Translate marker lines into typed SSE events; otherwise emit as message."""
        try:
            m = marker_re.match(line or '')
            if m:
                ev_name = m.group('event')
                payload_text = m.group('data')
                yield f"event: {ev_name}\n"
                yield f"data: {payload_text}\n\n"
                return
        except Exception:
            pass
        yield f"data: {line}\n\n"

    def generate():
        # 1) Send existing backlog first for immediate context
        last_pos = 0
        last_emit_ts = time.time()
        try:
            with open(log_path, 'r', encoding='utf-8', errors='ignore') as f_init:
                backlog = f_init.read()
                last_pos = f_init.tell()
            if backlog:
                for line in backlog.splitlines():
                    for payload in _emit_line(line):
                        yield payload
                    last_emit_ts = time.time()
        except FileNotFoundError:
            pass
        # 2) Tail incremental additions
        while True:
            try:
                with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
                    f.seek(last_pos)
                    chunk = f.read()
                    if chunk:
                        last_pos = f.tell()
                        # Split into lines to keep events reasonable
                        for line in chunk.splitlines():
                            for payload in _emit_line(line):
                                yield payload
                            last_emit_ts = time.time()
            except FileNotFoundError:
                pass
            # Check process status
            proc = meta.get('proc')
            rc = None
            if proc:
                rc = proc.poll()
                if rc is not None and meta.get('returncode') is None:
                    meta['returncode'] = rc
                    meta['done'] = True
            if meta.get('done'):
                # Drain any remaining buffered log lines before ending so late
                # cleanup markers are visible to the client.
                try:
                    with open(log_path, 'r', encoding='utf-8', errors='ignore') as f_final:
                        f_final.seek(last_pos)
                        tail = f_final.read()
                        if tail:
                            last_pos = f_final.tell()
                            for line in tail.splitlines():
                                for payload in _emit_line(line):
                                    yield payload
                                last_emit_ts = time.time()
                except FileNotFoundError:
                    pass
                # Signal end regardless; client will stop listening
                yield "event: end\ndata: done\n\n"
                break
            # Keepalive ping to avoid idle timeouts
            if time.time() - last_emit_ts >= 5:
                yield "event: ping\ndata: {}\n\n"
                last_emit_ts = time.time()
            time.sleep(0.5)

    headers = {
        'Cache-Control': 'no-cache',
        'X-Accel-Buffering': 'no',  # for some proxies
        'Content-Type': 'text/event-stream',
        'Connection': 'keep-alive',
    }
    return Response(generate(), headers=headers)


@app.route('/flag_generators_test/cleanup/<run_id>', methods=['POST'])
def flag_generators_test_cleanup(run_id: str):
    """Delete all artifacts for a flag-generator test run.

    This is intentionally scoped to outputs/ to avoid deleting arbitrary paths.
    """
    t0 = time.time()
    app.logger.info("[flaggen_test] POST /flag_generators_test/cleanup run_id=%s", run_id)
    meta = RUNS.get(run_id)
    if meta and meta.get('kind') != 'flag_generator_test':
        app.logger.info("[flaggen_test] cleanup refusing: kind=%r", meta.get('kind'))
        return jsonify({'ok': False, 'error': 'not found'}), 404

    run_dir = meta.get('run_dir') if isinstance(meta, dict) else None
    if not isinstance(run_dir, str) or not run_dir:
        run_dir = _flaggen_run_dir_for_id(run_id)
    if not isinstance(run_dir, str) or not run_dir:
        app.logger.warning("[flaggen_test] cleanup missing run dir run_id=%s", run_id)
        return jsonify({'ok': False, 'error': 'missing run dir'}), 500

    abs_run_dir = os.path.abspath(run_dir)
    outputs_root = os.path.abspath(_outputs_dir())
    if not (abs_run_dir == outputs_root or abs_run_dir.startswith(outputs_root + os.sep)):
        app.logger.warning(
            "[flaggen_test] cleanup refusing run_id=%s abs_run_dir=%s outputs_root=%s",
            run_id,
            abs_run_dir,
            outputs_root,
        )
        return jsonify({'ok': False, 'error': 'refusing'}), 400

    app.logger.info(
        "[flaggen_test] cleanup resolved run_id=%s run_dir=%s exists=%s",
        run_id,
        abs_run_dir,
        os.path.isdir(abs_run_dir),
    )

    # Best-effort: stop any still-running runner process.
    try:
        if isinstance(meta, dict):
            meta['cleanup_requested'] = True
        proc = meta.get('proc') if isinstance(meta, dict) else None
        if proc and hasattr(proc, 'poll') and proc.poll() is None:
            try:
                app.logger.info("[flaggen_test] cleanup terminating runner run_id=%s", run_id)
                proc.terminate()
                proc.wait(timeout=5)
            except Exception:
                try:
                    app.logger.info("[flaggen_test] cleanup killing runner run_id=%s", run_id)
                    proc.kill()
                except Exception:
                    pass
    except Exception:
        pass

    # Emit cleanup markers to the log if possible.
    try:
        lp = meta.get('log_path') if isinstance(meta, dict) else os.path.join(abs_run_dir, 'run.log')
        if isinstance(lp, str) and lp:
            with open(lp, 'a', encoding='utf-8') as log_f:
                _write_sse_marker(log_f, 'phase', {'phase': 'cleanup_start', 'run_id': run_id})
    except Exception:
        pass

    removed = False
    try:
        if os.path.isdir(abs_run_dir):
            shutil.rmtree(abs_run_dir, ignore_errors=True)
        removed = True
    except Exception:
        removed = False

    app.logger.info(
        "[flaggen_test] cleanup removed=%s run_id=%s elapsed_ms=%d",
        removed,
        run_id,
        int((time.time() - t0) * 1000),
    )

    try:
        if isinstance(meta, dict):
            meta['done'] = True
    except Exception:
        pass

    try:
        if isinstance(meta, dict):
            lp = meta.get('log_path')
        else:
            lp = os.path.join(abs_run_dir, 'run.log')
        if isinstance(lp, str) and lp and os.path.exists(lp):
            with open(lp, 'a', encoding='utf-8') as log_f2:
                _write_sse_marker(log_f2, 'phase', {'phase': 'cleanup_done', 'run_id': run_id, 'removed': removed})
    except Exception:
        pass

    try:
        RUNS.pop(run_id, None)
    except Exception:
        pass

    app.logger.info(
        "[flaggen_test] cleanup complete run_id=%s elapsed_ms=%d",
        run_id,
        int((time.time() - t0) * 1000),
    )

    return jsonify({'ok': True, 'removed': removed}), 200


@app.route('/cancel_run/<run_id>', methods=['POST'])
def cancel_run(run_id: str):
    meta = RUNS.get(run_id)
    if not meta:
        return jsonify({"error": "not found"}), 404
    proc = meta.get('proc')
    try:
        if proc and proc.poll() is None:
            # Append a cancel marker to log, then terminate
            lp = meta.get('log_path')
            try:
                with open(lp, 'a', encoding='utf-8') as f:
                    f.write("\n== Run cancelled by user ==\n")
            except Exception:
                pass
            proc.terminate()
            try:
                proc.wait(timeout=5)
            except Exception:
                proc.kill()
        meta['done'] = True
        if meta.get('returncode') is None:
            meta['returncode'] = -1
        try:
            _cleanup_remote_workspace(meta)
        except Exception:
            pass
        _close_async_run_tunnel(meta)
        return jsonify({"ok": True})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ---------------- Flag Catalog / Generator Packs -----------------


def _require_builder_or_admin() -> None:
    user = _current_user()
    if not user or not _is_admin_view_role(user.get('role')):
        abort(403)


def _split_lines(value: Any) -> list[str]:
    if value is None:
        return []
    if isinstance(value, list):
        out: list[str] = []
        for v in value:
            if v is None:
                continue
            s = str(v).strip()
            if s:
                out.append(s)
        return out
    text = str(value)
    return [s.strip() for s in text.splitlines() if s.strip()]


def _split_artifact_list(value: Any) -> list[str]:
    """Coerce an artifact list.

    Accepts:
      - list[str]
      - list[dict] with {'artifact': '...'}
      - newline-delimited str
    """
    if value is None:
        return []
    if isinstance(value, list):
        out: list[str] = []
        for item in value:
            if item is None:
                continue
            if isinstance(item, dict):
                s = str(item.get('artifact') or '').strip()
            else:
                s = str(item).strip()
            if s:
                out.append(s)
        return out
    return _split_lines(value)


def _normalize_requires_with_optional(payload: dict[str, Any]) -> tuple[list[str], list[str]]:
    """Return (required_requires, optional_requires)."""
    # Generator Builder UI shape: requires = [{artifact, optional}, ...]
    req_items = payload.get('requires')
    if not (isinstance(req_items, list) and all(isinstance(x, dict) for x in req_items)):
        raise ValueError('requires must be a list of {artifact, optional} objects')

    if isinstance(req_items, list):
        required: list[str] = []
        optional: list[str] = []
        for item in req_items:
            art = str(item.get('artifact') or '').strip()
            if not art:
                continue
            is_opt = _coerce_bool(item.get('optional'), False)
            if is_opt:
                optional.append(art)
            else:
                required.append(art)
        # De-dupe, prefer required if both appear.
        required_set = {x for x in required if x}
        optional_set = {x for x in optional if x and x not in required_set}
        return sorted(required_set), sorted(optional_set)


def _normalize_plugin_type(raw: Any) -> str:
    value = (str(raw or '')).strip()
    if value not in {'flag-generator', 'flag-node-generator'}:
        return 'flag-generator'
    return value


def _sanitize_id(raw: Any) -> str:
    value = (str(raw or '')).strip()
    value = re.sub(r'[^a-zA-Z0-9_.\-]+', '_', value)
    value = re.sub(r'_+', '_', value).strip('_')
    return value


def _sanitize_folder(raw: Any) -> str:
    value = (str(raw or '')).strip()
    value = value.replace('\\', '/').strip('/')
    value = re.sub(r'[^a-zA-Z0-9_\-/\.]+', '_', value)
    value = value.split('/', 1)[0]
    value = value.strip().strip('.')
    value = re.sub(r'_+', '_', value)
    return value


def _inputs_schema_from_flags(flags: dict[str, Any], *, plugin_type: str) -> dict[str, Any]:
    inputs: dict[str, Any] = {}

    def _add(name: str, *, sensitive: bool = False, required: bool = True) -> None:
        inputs[name] = {'type': 'string', 'required': bool(required)}
        if sensitive:
            inputs[name]['sensitive'] = True

    if flags.get('seed'):
        _add('seed', required=True)
    if flags.get('secret') and plugin_type == 'flag-generator':
        _add('secret', sensitive=True, required=True)
    if flags.get('node_name') and plugin_type == 'flag-node-generator':
        _add('node_name', required=True)
    if flags.get('flag_prefix'):
        _add('flag_prefix', required=False)
    return inputs


def _build_generator_scaffold(payload: dict[str, Any]) -> tuple[dict[str, str], str, str]:
    plugin_type = _normalize_plugin_type(payload.get('plugin_type'))
    plugin_id = _sanitize_id(payload.get('plugin_id'))
    if not plugin_id:
        raise ValueError('plugin_id is required')

    folder_name = _sanitize_folder(payload.get('folder_name'))
    if not folder_name:
        folder_name = f"py_{plugin_id}"
    if folder_name.startswith('py__'):
        folder_name = folder_name.replace('py__', 'py_', 1)
    if folder_name == 'py_':
        folder_name = f"py_{plugin_id}"

    display_name = str(payload.get('name') or '').strip() or plugin_id
    description = str(payload.get('description') or '').strip() or f"Generator {plugin_id}"

    requires, optional_requires = _normalize_requires_with_optional(payload)
    produces = _split_artifact_list(payload.get('produces'))
    hint_templates = _split_lines(payload.get('hint_templates'))
    inject_files_raw = payload.get('inject_files')
    inject_files = []
    if isinstance(inject_files_raw, list):
        for x in inject_files_raw:
            s = str(x or '').strip()
            if s:
                inject_files.append(s)

    if 'Flag(flag_id)' not in produces and 'flag' not in produces:
        produces = ['Flag(flag_id)'] + produces

    inputs_flags = payload.get('inputs') if isinstance(payload.get('inputs'), dict) else {}
    inputs_schema = _inputs_schema_from_flags(inputs_flags, plugin_type=plugin_type)

    # Manifest v1: preferred workflow for installed Generator Packs.
    inputs_manifest: list[dict[str, Any]] = []
    try:
        for name, meta in (inputs_schema or {}).items():
            nm = str(name or '').strip()
            if not nm:
                continue
            rec: dict[str, Any] = {'name': nm, 'type': str((meta or {}).get('type') or 'string')}
            if isinstance(meta, dict):
                if 'required' in meta:
                    rec['required'] = bool(meta.get('required'))
                if meta.get('sensitive') is True:
                    rec['sensitive'] = True
            inputs_manifest.append(rec)
    except Exception:
        inputs_manifest = []

    manifest_yaml = (
        "manifest_version: 1\n"
        f"id: {plugin_id}\n"
        f"kind: {plugin_type}\n"
        f"name: {display_name}\n"
        f"description: {description}\n"
        "version: 1.0\n"
        "\n"
        "runtime:\n"
        "  type: docker-compose\n"
        "  compose_file: docker-compose.yml\n"
        "  service: generator\n"
    )
    if inputs_manifest:
        manifest_yaml += "\ninputs:\n"
        for inp in inputs_manifest:
            try:
                nm = str(inp.get('name') or '').strip()
                if not nm:
                    continue
                manifest_yaml += f"  - name: {nm}\n"
                tp = str(inp.get('type') or 'string').strip() or 'string'
                manifest_yaml += f"    type: {tp}\n"
                if 'required' in inp:
                    manifest_yaml += f"    required: {'true' if bool(inp.get('required')) else 'false'}\n"
                if inp.get('sensitive') is True:
                    manifest_yaml += "    sensitive: true\n"
            except Exception:
                continue

    # Artifacts for Flow chaining.
    manifest_yaml += "\nartifacts:\n"
    manifest_yaml += "  requires:\n"
    for a in (requires or []):
        s = str(a or '').strip()
        if s:
            manifest_yaml += f"    - {s}\n"

    if optional_requires:
        manifest_yaml += "  optional_requires:\n"
        for a in (optional_requires or []):
            s = str(a or '').strip()
            if s:
                manifest_yaml += f"    - {s}\n"

    manifest_yaml += "  produces:\n"
    for a in (produces or []):
        s = str(a or '').strip()
        if s:
            manifest_yaml += f"    - {s}\n"

    # Hints
    if hint_templates:
        manifest_yaml += "\nhint_templates:\n"
        for t in hint_templates:
            s = str(t or '').strip()
            if not s:
                continue
            s2 = s.replace('"', '\\"')
            manifest_yaml += f"  - \"{s2}\"\n"

    # Injected artifacts allowlist for safe mounting.
    if inject_files:
        manifest_yaml += "\ninjects:\n"
        for x in inject_files:
            s = str(x or '').strip()
            if s:
                manifest_yaml += f"  - {s}\n"

    # Fixed env vars.
    gen_env = payload.get('env')
    if isinstance(gen_env, dict) and gen_env:
        manifest_yaml += "\nenv:\n"
        for k, v in gen_env.items():
            kk = str(k or '').strip()
            if not kk:
                continue
            vv = str(v if v is not None else '')
            vv2 = vv.replace('"', '\\"')
            manifest_yaml += f"  {kk}: \"{vv2}\"\n"

    base_dir = 'flag_generators' if plugin_type == 'flag-generator' else 'flag_node_generators'
    folder_path = f"{base_dir}/{folder_name}"

    compose_yaml = (
        "services:\n"
        "  generator:\n"
        "    image: python:3.12-slim\n"
        "    working_dir: /app\n"
        "    command: [\"python\", \"/app/generator.py\"]\n"
        "    environment:\n"
        "      INPUTS_DIR: ${INPUTS_DIR}\n"
        "      OUTPUTS_DIR: ${OUTPUTS_DIR}\n"
        "    volumes:\n"
        "      - ./generator.py:/app/generator.py:ro\n"
        "      - ${INPUTS_DIR}:/inputs:ro\n"
        "      - ${OUTPUTS_DIR}:/outputs\n"
    )

    compose_override = str(payload.get('compose_text') or '').strip('\n')
    if compose_override.strip():
        compose_yaml = compose_override + ("\n" if not compose_override.endswith("\n") else "")

    if plugin_type == 'flag-generator':
        generator_py = f"""import hashlib
import json
from pathlib import Path


def _read_config() -> dict:
    try:
        return json.loads(Path('/inputs/config.json').read_text('utf-8'))
    except Exception:
        return {{}}


def _flag(seed: str, secret: str, flag_prefix: str = 'FLAG') -> str:
    digest = hashlib.sha256(f"{{seed}}|{{secret}}|flag".encode('utf-8', 'replace')).hexdigest()[:24]
    prefix = (flag_prefix or 'FLAG').strip() or 'FLAG'
    return f"{{prefix}}{{{{{{digest}}}}}}"


def main() -> None:
    cfg = _read_config()
    seed = str(cfg.get('seed') or '').strip()
    secret = str(cfg.get('secret') or '').strip()
    flag_prefix = str(cfg.get('flag_prefix') or 'FLAG').strip() or 'FLAG'

    if not seed:
        raise SystemExit('Missing seed in /inputs/config.json')
    if not secret:
        raise SystemExit('Missing secret in /inputs/config.json')

    # TODO: implement your generator logic and add additional outputs.
    flag_value = _flag(seed, secret, flag_prefix)

    out_dir = Path('/outputs')
    out_dir.mkdir(parents=True, exist_ok=True)

    outputs = {{
        'generator_id': {json.dumps(plugin_id)},
        'outputs': {{
            'Flag(flag_id)': flag_value,
        }},
    }}

    (out_dir / 'outputs.json').write_text(json.dumps(outputs, indent=2) + '\n', encoding='utf-8')
    # Optional: write /outputs/hint.txt if you need a standalone hint file.
    # Prefer hint_templates in the catalog for Flow.


if __name__ == '__main__':
    main()
"""
    else:
        generator_py = f"""import hashlib
import json
from pathlib import Path


def _read_config() -> dict:
    try:
        return json.loads(Path('/inputs/config.json').read_text('utf-8'))
    except Exception:
        return {{}}


def _flag(seed: str, node_name: str, flag_prefix: str = 'FLAG') -> str:
    digest = hashlib.sha256(f"{{seed}}|{{node_name}}|flag".encode('utf-8', 'replace')).hexdigest()[:16]
    prefix = (flag_prefix or 'FLAG').strip() or 'FLAG'
    return f"{{prefix}}{{{{{{digest}}}}}}"


def main() -> None:
    cfg = _read_config()
    seed = str(cfg.get('seed') or '').strip()
    node_name = str(cfg.get('node_name') or '').strip()
    flag_prefix = str(cfg.get('flag_prefix') or 'FLAG').strip() or 'FLAG'

    if not seed:
        raise SystemExit('Missing seed in /inputs/config.json')
    if not node_name:
        raise SystemExit('Missing node_name in /inputs/config.json')

    # TODO: implement your per-node compose generation.
    flag_value = _flag(seed, node_name, flag_prefix)
    compose_text = (
        "services:\n"
        "  node:\n"
        "    image: alpine:3.19\n"
        "    command: [\\"sh\\", \\"-lc\\", \\"echo \\\\\"$FLAG\\\\\" > /flag.txt && tail -f /dev/null\\"]\n"
        "    environment:\n"
        f"      FLAG: {{json.dumps(flag_value)}}\n"
    )

    out_dir = Path('/outputs')
    out_dir.mkdir(parents=True, exist_ok=True)
    (out_dir / 'docker-compose.yml').write_text(compose_text, encoding='utf-8')

    outputs = {{
        'generator_id': {json.dumps(plugin_id)},
        'outputs': {{
            'File(path)': 'docker-compose.yml',
            'Flag(flag_id)': flag_value,
        }},
    }}
    (out_dir / 'outputs.json').write_text(json.dumps(outputs, indent=2) + '\n', encoding='utf-8')


if __name__ == '__main__':
    main()
"""

    readme_override = str(payload.get('readme_text') or '')
    if readme_override and not readme_override.endswith('\n'):
        readme_override = readme_override + '\n'

    scaffold_files: dict[str, str] = {
        f"{folder_path}/docker-compose.yml": compose_yaml,
        f"{folder_path}/generator.py": generator_py,
        f"{folder_path}/README.md": readme_override or f"# {display_name}\n\nTODO: describe this generator.\n\n- plugin_id: `{plugin_id}`\n- type: `{plugin_type}`\n",
        f"{folder_path}/manifest.yaml": manifest_yaml,
    }

    return scaffold_files, manifest_yaml, folder_path


@app.route('/generator_builder')
def generator_builder_page():
    _require_builder_or_admin()
    return render_template('generator_builder.html', active_page='generator_builder')


def _custom_artifacts_path() -> str:
    return os.path.abspath(os.path.join(_outputs_dir(), 'artifacts_custom.json'))


def _load_custom_artifacts() -> dict[str, dict[str, Any]]:
    """Load persisted custom artifact definitions.

    Stored as a mapping artifact -> {type?: str}.
    """
    path = _custom_artifacts_path()
    try:
        if not os.path.exists(path):
            return {}
        with open(path, 'r', encoding='utf-8') as fh:
            doc = json.load(fh)
        if not isinstance(doc, dict):
            return {}
        out: dict[str, dict[str, Any]] = {}
        for k, v in doc.items():
            art = str(k or '').strip()
            if not art:
                continue
            meta: dict[str, Any] = {}
            if isinstance(v, dict):
                tp = str(v.get('type') or '').strip()
                if tp:
                    meta['type'] = tp
            out[art] = meta
        return out
    except Exception:
        return {}


def _save_custom_artifacts(doc: dict[str, dict[str, Any]]) -> None:
    path = _custom_artifacts_path()
    os.makedirs(os.path.dirname(path), exist_ok=True)
    tmp = path + '.tmp'
    with open(tmp, 'w', encoding='utf-8') as fh:
        json.dump(doc, fh, indent=2)
        fh.write('\n')
    os.replace(tmp, path)


def _upsert_custom_artifact(artifact: str, *, type_value: str | None = None) -> dict[str, Any]:
    art = str(artifact or '').strip()
    if not art:
        raise ValueError('artifact is required')
    if len(art) > 200:
        raise ValueError('artifact too long')

    tp = str(type_value or '').strip()
    custom = _load_custom_artifacts()
    meta = dict(custom.get(art) or {})
    if tp:
        meta['type'] = tp
    custom[art] = meta
    _save_custom_artifacts(custom)
    return {'artifact': art, **meta}


@app.route('/api/generators/artifacts_index')
def api_generators_artifacts_index():
    """Return known artifact keys (from enabled generator sources) + producer metadata."""
    _require_builder_or_admin()
    try:
        flag_gens, _errs1 = _flag_generators_from_enabled_sources()
        node_gens, _errs2 = _flag_node_generators_from_enabled_sources()

        idx: dict[str, dict[str, Any]] = {}

        def _add_from(gens: list[dict], plugin_type: str) -> None:
            for g in gens:
                if not isinstance(g, dict):
                    continue
                gid = str(g.get('id') or '').strip()
                gname = str(g.get('name') or '').strip() or gid
                outs = g.get('outputs') if isinstance(g.get('outputs'), list) else []
                for o in outs:
                    if not isinstance(o, dict):
                        continue
                    art = str(o.get('name') or '').strip()
                    if not art:
                        continue
                    tp = str(o.get('type') or '').strip()
                    desc = str(o.get('description') or '').strip()
                    sensitive = o.get('sensitive') is True
                    entry = idx.get(art)
                    if not entry:
                        entry = {'artifact': art, 'type': tp, 'description': desc, 'sensitive': sensitive, 'producers': []}
                        idx[art] = entry
                    if not entry.get('type') and tp:
                        entry['type'] = tp
                    if not str(entry.get('description') or '').strip() and desc:
                        entry['description'] = desc
                    if entry.get('sensitive') is not True and sensitive is True:
                        entry['sensitive'] = True
                    producers = entry.get('producers') if isinstance(entry.get('producers'), list) else []
                    if not any((p.get('plugin_id') == gid and p.get('plugin_type') == plugin_type) for p in producers if isinstance(p, dict)):
                        producers.append({'plugin_id': gid, 'plugin_type': plugin_type, 'name': gname})
                    entry['producers'] = producers

        _add_from(flag_gens, 'flag-generator')
        _add_from(node_gens, 'flag-node-generator')

        # Always include reserved artifact keys as a starting point.
        # These can be used in Requires/Produces even if no generator currently
        # advertises them in enabled sources.
        try:
            for art, meta in _RESERVED_ARTIFACTS.items():
                if art not in idx:
                    idx[art] = {
                        'artifact': art,
                        'type': str(meta.get('type') or '').strip(),
                        'description': str(meta.get('description') or '').strip(),
                        'sensitive': meta.get('sensitive') is True,
                        'producers': [{'plugin_id': '(reserved)', 'plugin_type': 'reserved', 'name': 'Reserved'}],
                    }
                else:
                    if not str(idx[art].get('type') or '').strip() and str(meta.get('type') or '').strip():
                        idx[art]['type'] = str(meta.get('type') or '').strip()
                    if not str(idx[art].get('description') or '').strip() and str(meta.get('description') or '').strip():
                        idx[art]['description'] = str(meta.get('description') or '').strip()
                    if idx[art].get('sensitive') is not True and meta.get('sensitive') is True:
                        idx[art]['sensitive'] = True
        except Exception:
            pass

        # Merge in persisted custom artifacts (no producers).
        try:
            custom = _load_custom_artifacts()
            for art, meta in custom.items():
                if art not in idx:
                    idx[art] = {'artifact': art, 'type': str(meta.get('type') or '').strip(), 'producers': []}
                else:
                    # Prefer existing type; fill missing type from custom.
                    if not str(idx[art].get('type') or '').strip() and str(meta.get('type') or '').strip():
                        idx[art]['type'] = str(meta.get('type') or '').strip()
        except Exception:
            pass

        artifacts = sorted(idx.values(), key=lambda x: str(x.get('artifact') or ''))
        return jsonify({'ok': True, 'artifacts': artifacts})
    except Exception as exc:
        return jsonify({'ok': False, 'error': str(exc)}), 500


@app.route('/api/generators/artifacts_index/custom', methods=['POST'])
def api_generators_artifacts_index_custom_add():
    """Persist a custom artifact key so it appears in the selector going forward."""
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    try:
        artifact = str(payload.get('artifact') or '').strip()
        type_value = str(payload.get('type') or '').strip() or None
        item = _upsert_custom_artifact(artifact, type_value=type_value)
        return jsonify({'ok': True, 'artifact': item})
    except Exception as exc:
        return jsonify({'ok': False, 'error': str(exc)}), 400


@app.route('/api/generators/scaffold_meta', methods=['POST'])
def api_generators_scaffold_meta():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    try:
        scaffold_files, manifest_yaml, _folder_path = _build_generator_scaffold(payload)
    except Exception as exc:
        return jsonify({'ok': False, 'error': str(exc)}), 400
    return jsonify({
        'ok': True,
        'manifest_yaml': manifest_yaml,
        'scaffold_paths': sorted(scaffold_files.keys()),
    })


@app.route('/api/generators/scaffold_zip', methods=['POST'])
def api_generators_scaffold_zip():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    try:
        scaffold_files, _manifest_yaml, _folder_path = _build_generator_scaffold(payload)
        plugin_id = _sanitize_id(payload.get('plugin_id')) or 'generator'
    except Exception as exc:
        return jsonify({'ok': False, 'error': str(exc)}), 400

    mem = io.BytesIO()
    with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as zf:
        for path, content in scaffold_files.items():
            zf.writestr(path, content)
    mem.seek(0)
    return send_file(
        mem,
        mimetype='application/zip',
        as_attachment=True,
        download_name=f'generator_scaffold_{plugin_id}.zip',
    )


@app.route('/flag_catalog')
def flag_catalog_page():
    packs_state = _load_installed_generator_packs_state()
    # Render-time quality-of-life: group installed entries by kind so the UI
    # doesn't repeat labels like "flag-generator" multiple times.
    try:
        packs = packs_state.get('packs', []) if isinstance(packs_state, dict) else []
        for p in packs:
            if not isinstance(p, dict):
                continue
            installed = p.get('installed')
            if not isinstance(installed, list):
                continue
            grouped: dict[str, list[str]] = {}
            for it in installed:
                if not isinstance(it, dict):
                    continue
                kind = str(it.get('kind') or '').strip()
                gid = str(it.get('id') or '').strip()
                if not kind or not gid:
                    continue
                grouped.setdefault(kind, []).append(gid)
            installed_grouped = []
            for kind, ids in grouped.items():
                # Preserve original order from the installed list.
                uniq_ids = []
                seen = set()
                for x in ids:
                    if x in seen:
                        continue
                    seen.add(x)
                    uniq_ids.append(x)
                installed_grouped.append({'kind': kind, 'ids': uniq_ids, 'count': len(uniq_ids)})
            if installed_grouped:
                p['installed_grouped'] = installed_grouped
    except Exception:
        # Never fail page render due to cosmetic grouping.
        pass
    return render_template(
        'flag_catalog.html',
        packs=packs_state.get('packs', []),
        active_page='flag_catalog',
    )


@app.route('/data_sources')
def data_sources_page():
    """Legacy page removed.

    Keep a tombstone route so old bookmarks don't throw 500s.
    """
    # Not linked from the UI anymore; intentionally excluded from OpenAPI.
    return render_template('data_sources.html', active_page='')


def _validate_and_normalize_data_source_csv(csv_path: str, *, skip_invalid: bool = False) -> tuple[bool, str, list[list[str]], list[int]]:
    """Validate and normalize a legacy data-sources CSV.

    This is a small helper primarily used by tests. The legacy Data Sources UI
    has been removed, but we keep the parser behavior stable.

    Returns:
      (ok, note, normalized_rows, skipped_row_numbers)

    Normalization:
      - Strips UTF-8 BOM from the first header cell.
      - Canonicalizes standard headers to: Name, Path, Type, Startup, Vector.
      - Lowercases/strips known enum-ish fields.

    Validation (best-effort):
      - Requires non-empty Name and Path.
      - Requires Type in a small allowed set (currently includes 'artifact').
      - Startup accepts yes/no/true/false/1/0 and normalizes to yes|no.
      - Vector accepts local|remote (and a few aliases).
    """
    import csv

    path = str(csv_path or '').strip()
    if not path:
        return False, 'Missing csv path', [], []
    if not os.path.exists(path) or not os.path.isfile(path):
        return False, f'CSV not found: {path}', [], []

    allowed_types = {
        # Minimal set (legacy + tests).
        'artifact',
        # A few pragmatic extras for backward compatibility.
        'file', 'path', 'url', 'text', 'json',
    }
    startup_true = {'1', 'true', 'yes', 'y', 'on'}
    startup_false = {'0', 'false', 'no', 'n', 'off'}
    vector_alias = {
        'local': 'local',
        'localhost': 'local',
        'host': 'local',
        'remote': 'remote',
        'network': 'remote',
    }

    normalized_rows: list[list[str]] = []
    skipped: list[int] = []
    try:
        with open(path, 'r', encoding='utf-8-sig', newline='') as f:
            reader = csv.reader(f)
            rows = list(reader)
    except Exception as exc:
        return False, f'Failed reading CSV: {exc}', [], []

    if not rows:
        return False, 'Empty CSV', [], []

    raw_header = [str(x or '').strip() for x in (rows[0] or [])]
    if raw_header:
        # utf-8-sig should remove BOM, but be defensive.
        raw_header[0] = raw_header[0].lstrip('\ufeff').strip()

    # Canonicalize standard headers.
    canon_map = {
        'name': 'Name',
        'path': 'Path',
        'type': 'Type',
        'startup': 'Startup',
        'vector': 'Vector',
    }
    header = []
    for h in raw_header:
        k = str(h or '').strip()
        header.append(canon_map.get(k.lower(), k))

    # Determine column indices (case-insensitive).
    idx: dict[str, int] = {}
    for i, h in enumerate(header):
        hl = str(h or '').strip().lower()
        if hl and hl not in idx:
            idx[hl] = i

    required_cols = ['name', 'path', 'type', 'startup', 'vector']
    missing = [c for c in required_cols if c not in idx]
    if missing:
        msg = f"Missing required columns: {', '.join(missing)}"
        return (False, msg, [header], []) if not skip_invalid else (True, msg + ' (skipped all rows)', [header], list(range(2, 2 + max(0, len(rows) - 1))))

    normalized_rows.append(header)

    invalid_count = 0
    for row_i, row in enumerate(rows[1:], start=2):
        # row_i is 1-based line number in the CSV file.
        values = [str(x or '').strip() for x in (row or [])]
        # Pad to header length to avoid IndexError.
        while len(values) < len(header):
            values.append('')

        name = values[idx['name']].strip()
        path_value = values[idx['path']].strip()
        type_value = values[idx['type']].strip().lower()
        startup_value = values[idx['startup']].strip().lower()
        vector_value = values[idx['vector']].strip().lower()

        ok_row = True
        if not name or not path_value:
            ok_row = False
        if type_value not in allowed_types:
            ok_row = False

        if startup_value in startup_true:
            startup_norm = 'yes'
        elif startup_value in startup_false:
            startup_norm = 'no'
        else:
            ok_row = False
            startup_norm = startup_value or ''

        vector_norm = vector_alias.get(vector_value, '')
        if not vector_norm:
            ok_row = False

        if not ok_row:
            invalid_count += 1
            if skip_invalid:
                skipped.append(row_i)
                continue
            return False, f'Invalid row at line {row_i}', normalized_rows, [row_i]

        # Apply normalized enums back into the row.
        values[idx['type']] = type_value
        values[idx['startup']] = startup_norm
        values[idx['vector']] = vector_norm
        normalized_rows.append(values)

    note = f'OK ({len(normalized_rows) - 1} row(s))'
    if invalid_count:
        note += f'; skipped {invalid_count} invalid row(s)'
    return True, note, normalized_rows, skipped




# ---------------- Flag Generators (manifest) -----------------


def _flag_generators_from_manifests(*, kind: str) -> tuple[list[dict], dict[str, dict[str, Any]], list[dict]]:
    """Load generators + plugin contracts from strict YAML manifests.

    Returns: (generator_views, plugins_by_id, errors)
    """
    try:
        from core_topo_gen.generator_manifests import discover_generator_manifests
    except Exception as exc:
        return [], {}, [{'error': f'failed to import manifest loader: {exc}'}]

    try:
        repo_root = _get_repo_root()
    except Exception:
        repo_root = os.getcwd()

    gens, plugins_by_id, errs = discover_generator_manifests(repo_root=repo_root, kind=kind)
    errors: list[dict] = []
    for e in (errs or []):
        try:
            errors.append({'path': getattr(e, 'path', ''), 'error': getattr(e, 'error', str(e))})
        except Exception:
            continue
    return gens, plugins_by_id, errors


def _flag_generators_from_enabled_sources() -> tuple[list[dict], list[dict]]:
    """Return enabled flag-generators.

    Enabled means:
      - installed under outputs/installed_generators (installed-only policy)
      - not disabled (pack-level or generator-level)
    """
    gens, _plugins_by_id, errors = _flag_generators_from_manifests(kind='flag-generator')
    out: list[dict] = []
    for g in (gens or []):
        if not isinstance(g, dict):
            continue
        if not _is_installed_generator_view(g):
            continue
        gid = str(g.get('id') or '').strip()
        if not gid:
            continue
        if _is_installed_generator_disabled(kind='flag-generator', generator_id=gid):
            continue
        out.append(g)
    return out, errors


def _flag_node_generators_from_enabled_sources() -> tuple[list[dict], list[dict]]:
    """Return enabled flag-node-generators.

    Enabled means:
      - installed under outputs/installed_generators (installed-only policy)
      - not disabled (pack-level or generator-level)
    """
    gens, _plugins_by_id, errors = _flag_generators_from_manifests(kind='flag-node-generator')
    out: list[dict] = []
    for g in (gens or []):
        if not isinstance(g, dict):
            continue
        if not _is_installed_generator_view(g):
            continue
        gid = str(g.get('id') or '').strip()
        if not gid:
            continue
        if _is_installed_generator_disabled(kind='flag-node-generator', generator_id=gid):
            continue
        out.append(g)
    return out, errors


@app.route('/flag_generators_data')
def flag_generators_data():
    try:
        generators, errors = _flag_generators_from_enabled_sources()
        generators = [g for g in (generators or []) if isinstance(g, dict) and _is_installed_generator_view(g)]
        generators = _annotate_disabled_state(generators, kind='flag-generator')
        return jsonify({'generators': generators, 'errors': errors})
    except Exception as e:
        return jsonify({'generators': [], 'errors': [{'error': str(e)}]}), 500


@app.route('/flag_node_generators_data')
def flag_node_generators_data():
    try:
        generators, errors = _flag_node_generators_from_enabled_sources()
        generators = [g for g in (generators or []) if isinstance(g, dict) and _is_installed_generator_view(g)]
        generators = _annotate_disabled_state(generators, kind='flag-node-generator')
        return jsonify({'generators': generators, 'errors': errors})
    except Exception as e:
        return jsonify({'generators': [], 'errors': [{'error': str(e)}]}), 500


def _installed_generators_root() -> str:
    """Root directory for installed generator packs.

    Defaults to ./outputs/installed_generators under repo root.
    Overridable via CORETG_INSTALLED_GENERATORS_DIR for tests/dev.
    """
    env = str(os.environ.get('CORETG_INSTALLED_GENERATORS_DIR') or '').strip()
    if env:
        root = os.path.abspath(os.path.expanduser(env))
    else:
        try:
            repo_root = _get_repo_root()
        except Exception:
            repo_root = os.getcwd()
        root = os.path.join(repo_root, 'outputs', 'installed_generators')
    os.makedirs(root, exist_ok=True)
    return root


def _installed_generator_packs_state_path() -> str:
    return os.path.join(_installed_generators_root(), '_packs_state.json')


def _load_installed_generator_packs_state() -> dict:
    path = _installed_generator_packs_state_path()
    try:
        if not os.path.exists(path):
            state = {'packs': []}
            tmp = path + '.tmp'
            with open(tmp, 'w', encoding='utf-8') as fh:
                json.dump(state, fh, indent=2)
            os.replace(tmp, path)
            return state
        with open(path, 'r', encoding='utf-8') as fh:
            state = json.load(fh)
        if not isinstance(state, dict):
            return {'packs': []}
        if not isinstance(state.get('packs'), list):
            state['packs'] = []
        return state
    except Exception:
        return {'packs': []}


def _save_installed_generator_packs_state(state: dict) -> None:
    path = _installed_generator_packs_state_path()
    try:
        tmp = path + '.tmp'
        with open(tmp, 'w', encoding='utf-8') as fh:
            json.dump(state if isinstance(state, dict) else {'packs': []}, fh, indent=2)
        os.replace(tmp, path)
    except Exception:
        pass


def _normalize_installed_source_path(p: str) -> str:
    return str(p or '').replace('\\', '/').strip().lower()


def _is_installed_generator_view(gen: dict) -> bool:
    """Best-effort check that a generator came from outputs/installed_generators."""
    try:
        src = ''
        if isinstance(gen.get('source'), dict):
            src = str(gen.get('source', {}).get('path') or '')
        norm = _normalize_installed_source_path(src)
        if 'outputs/installed_generators' in norm:
            return True

        mp = str(gen.get('_source_path') or '').strip()
        if mp:
            installed_root = os.path.abspath(_installed_generators_root())
            abs_mp = os.path.abspath(mp)
            return os.path.commonpath([installed_root, abs_mp]) == installed_root
    except Exception:
        return False
    return False


def _build_installed_disable_maps() -> tuple[dict[str, dict[str, Any]], dict[tuple[str, str], dict[str, Any]]]:
    """Return (pack_by_id, gen_by_kind_id) maps from the installed packs state."""
    state = _load_installed_generator_packs_state()
    packs = state.get('packs') if isinstance(state, dict) else None
    if not isinstance(packs, list):
        packs = []

    pack_by_id: dict[str, dict[str, Any]] = {}
    gen_by_kind_id: dict[tuple[str, str], dict[str, Any]] = {}

    for p in packs:
        if not isinstance(p, dict):
            continue
        pid = str(p.get('id') or '').strip()
        if not pid:
            continue
        pack_disabled = bool(p.get('disabled') is True)
        pack_label = str(p.get('label') or '').strip() or pid
        pack_by_id[pid] = {
            'id': pid,
            'label': pack_label,
            'disabled': pack_disabled,
            'origin': str(p.get('origin') or '').strip(),
        }
        for it in (p.get('installed') or []):
            if not isinstance(it, dict):
                continue
            gid = str(it.get('id') or '').strip()
            kind = str(it.get('kind') or '').strip()
            if not gid or not kind:
                continue
            item_disabled = bool(it.get('disabled') is True)
            info_obj = {
                'pack_id': pid,
                'pack_label': pack_label,
                'pack_disabled': pack_disabled,
                'disabled': pack_disabled or item_disabled,
                'item_disabled': item_disabled,
            }

            # Back-compat: generator packs may use numeric ids in the packs state,
            # but Flow/UI operate on stable `source_generator_id`. If we can
            # resolve a source id from the installed pack marker, map both.
            gen_by_kind_id[(kind, gid)] = info_obj
            try:
                pack_path = str(it.get('path') or '').strip()
                if pack_path:
                    marker_path = os.path.join(pack_path, '.coretg_pack.json')
                    if os.path.exists(marker_path) and os.path.isfile(marker_path):
                        with open(marker_path, 'r', encoding='utf-8') as fh:
                            marker = json.load(fh)
                        if isinstance(marker, dict):
                            src_id = str(marker.get('source_generator_id') or '').strip()
                            if src_id:
                                gen_by_kind_id.setdefault((kind, src_id), info_obj)
            except Exception:
                pass

    return pack_by_id, gen_by_kind_id


def _annotate_disabled_state(generators: list[dict], *, kind: str) -> list[dict]:
    """Annotate generator views with _disabled/_pack_* fields using pack state."""
    _pack_by_id, gen_by_kind_id = _build_installed_disable_maps()
    out: list[dict] = []
    for g in (generators or []):
        if not isinstance(g, dict):
            continue
        gid = str(g.get('id') or '').strip()
        info = gen_by_kind_id.get((kind, gid)) if gid else None
        g['_installed'] = True
        if info:
            g['_pack_id'] = info.get('pack_id')
            g['_pack_label'] = info.get('pack_label')
            g['_pack_disabled'] = bool(info.get('pack_disabled'))
            g['_disabled'] = bool(info.get('disabled'))
            g['_item_disabled'] = bool(info.get('item_disabled'))
        else:
            g['_disabled'] = False
        out.append(g)
    return out


def _is_installed_generator_disabled(*, kind: str, generator_id: str) -> bool:
    gid = str(generator_id or '').strip()
    if not gid:
        return False
    _pack_by_id, gen_by_kind_id = _build_installed_disable_maps()
    info = gen_by_kind_id.get((str(kind or '').strip(), gid))
    return bool(info and info.get('disabled') is True)


def _is_safe_remote_zip_url(url: str) -> tuple[bool, str]:
    """Basic SSRF guard: only allow http(s) and block private/reserved IPs."""
    try:
        from urllib.parse import urlparse
        import ipaddress
        import socket

        u = str(url or '').strip()
        if not u:
            return False, 'Missing URL'
        parsed = urlparse(u)
        if parsed.scheme not in ('http', 'https'):
            return False, 'Only http(s) URLs are allowed'
        host = parsed.hostname
        if not host:
            return False, 'Invalid URL host'
        if host.lower() in ('localhost',):
            return False, 'Blocked host'

        infos = socket.getaddrinfo(host, None)
        ips: set[str] = set()
        for info in infos:
            try:
                ips.add(str(info[4][0]))
            except Exception:
                continue
        if not ips:
            return False, 'Could not resolve host'
        for ip_s in ips:
            try:
                ip = ipaddress.ip_address(ip_s)
            except Exception:
                return False, f'Invalid resolved IP: {ip_s}'
            if (
                ip.is_loopback
                or ip.is_private
                or ip.is_link_local
                or ip.is_multicast
                or ip.is_reserved
                or ip.is_unspecified
            ):
                return False, f'Blocked IP: {ip}'
        return True, ''
    except Exception:
        # Fail closed.
        return False, 'URL validation failed'


def _download_zip_from_url(url: str, *, max_bytes: int = 50_000_000) -> bytes:
    ok, reason = _is_safe_remote_zip_url(url)
    if not ok:
        raise ValueError(reason)

    u = str(url or '').strip()
    try:
        import requests  # type: ignore

        resp = requests.get(u, stream=True, timeout=20)
        resp.raise_for_status()
        buf = bytearray()
        for chunk in resp.iter_content(chunk_size=1024 * 64):
            if not chunk:
                continue
            buf.extend(chunk)
            if len(buf) > max_bytes:
                raise ValueError('Download too large')
        return bytes(buf)
    except Exception:
        # Fallback to urllib
        from urllib.request import urlopen

        with urlopen(u, timeout=20) as r:  # nosec - guarded by _is_safe_remote_zip_url
            buf = r.read(max_bytes + 1)
        if len(buf) > max_bytes:
            raise ValueError('Download too large')
        return buf


# ---------------- Vulnerability Catalog Packs -----------------


def _installed_vuln_catalogs_root() -> str:
    return os.path.join(_outputs_dir(), 'installed_vuln_catalogs')


def _vuln_catalogs_state_path() -> str:
    return os.path.join(_installed_vuln_catalogs_root(), '_catalogs_state.json')


def _load_vuln_catalogs_state() -> dict:
    try:
        p = _vuln_catalogs_state_path()
        if not os.path.exists(p):
            return {'catalogs': [], 'active_id': ''}
        with open(p, 'r', encoding='utf-8') as f:
            obj = json.load(f)
        if not isinstance(obj, dict):
            return {'catalogs': [], 'active_id': ''}
        obj.setdefault('catalogs', [])
        obj.setdefault('active_id', '')
        return obj
    except Exception:
        return {'catalogs': [], 'active_id': ''}


def _write_vuln_catalogs_state(state: dict) -> None:
    os.makedirs(_installed_vuln_catalogs_root(), exist_ok=True)
    p = _vuln_catalogs_state_path()
    tmp = p + '.tmp'
    with open(tmp, 'w', encoding='utf-8') as f:
        json.dump(state, f, indent=2, sort_keys=True)
    os.replace(tmp, p)


def _vuln_catalog_pack_dir(catalog_id: str) -> str:
    return os.path.join(_installed_vuln_catalogs_root(), str(catalog_id))


def _vuln_catalog_pack_zip_path(catalog_id: str) -> str:
    return os.path.join(_vuln_catalog_pack_dir(catalog_id), 'catalog.zip')


def _vuln_catalog_pack_content_dir(catalog_id: str) -> str:
    return os.path.join(_vuln_catalog_pack_dir(catalog_id), 'content')


def _active_vuln_catalog_label() -> str:
    """Return the active vulnerability catalog pack label (best-effort)."""
    try:
        state = _load_vuln_catalogs_state()
        active_id = str(state.get('active_id') or '').strip() if isinstance(state, dict) else ''
        catalogs = state.get('catalogs') if isinstance(state, dict) else None
        if not active_id or not isinstance(catalogs, list):
            return ''
        for c in catalogs:
            if not isinstance(c, dict):
                continue
            cid = str(c.get('id') or '').strip()
            if cid != active_id:
                continue
            label = str(c.get('label') or c.get('name') or '').strip()
            return label or cid
    except Exception:
        return ''
    return ''


def _safe_path_under(base_dir: str, subpath: str) -> str:
    """Resolve a user-provided subpath under base_dir safely."""
    base_abs = os.path.abspath(base_dir)
    rel = str(subpath or '').replace('\\', '/').strip()
    rel = rel.lstrip('/')
    parts = [p for p in rel.split('/') if p not in ('', '.')]
    if any(p == '..' for p in parts):
        raise ValueError('Invalid path')
    out = os.path.abspath(os.path.join(base_abs, *parts))
    if out == base_abs:
        return out
    if not out.startswith(base_abs + os.sep):
        raise ValueError('Path escaped base directory')
    return out


def _pick_vuln_csv_from_zip(zf: zipfile.ZipFile) -> str | None:
    names = [n for n in (zf.namelist() or []) if isinstance(n, str)]
    preferred = [n for n in names if n.lower().endswith('/vuln_list_w_url.csv') or n.lower() == 'vuln_list_w_url.csv']
    if preferred:
        preferred.sort(key=lambda s: (s.count('/'), len(s)))
        return preferred[0]
    alt = [n for n in names if n.lower().endswith('/vuln_list.csv') or n.lower() == 'vuln_list.csv']
    if alt:
        alt.sort(key=lambda s: (s.count('/'), len(s)))
        return alt[0]
    return None


def _install_vuln_catalog_zip_bytes(*, zip_bytes: bytes, label: str, origin: str) -> dict:
    os.makedirs(_installed_vuln_catalogs_root(), exist_ok=True)
    catalog_id = time.strftime('%Y%m%d-%H%M%S') + '-' + secrets.token_hex(3)
    pack_dir = _vuln_catalog_pack_dir(catalog_id)
    os.makedirs(pack_dir, exist_ok=True)

    zip_path = _vuln_catalog_pack_zip_path(catalog_id)
    with open(zip_path, 'wb') as f:
        f.write(zip_bytes or b'')

    csv_rel_paths: list[str] = []
    compose_items: list[dict[str, Any]] = []
    try:
        # Extract the entire ZIP directory tree so compose directories (and their
        # support files) are preserved.
        content_dir = _vuln_catalog_pack_content_dir(catalog_id)
        _safe_extract_zip_to_dir(zip_path, content_dir)

        # Discover all directories containing docker-compose.yml (required).
        compose_paths: list[tuple[str, str]] = []
        for dirpath, _dirnames, filenames in os.walk(content_dir):
            if '__MACOSX' in dirpath:
                continue
            if not filenames:
                continue
            if 'docker-compose.yml' not in filenames:
                continue
            compose_path = os.path.join(dirpath, 'docker-compose.yml')
            if not os.path.isfile(compose_path):
                continue
            rel_dir = os.path.relpath(dirpath, content_dir).replace('\\', '/')
            name = rel_dir if rel_dir not in ('.', '') else 'root'
            compose_paths.append((name, compose_path))

        if not compose_paths:
            raise ValueError('ZIP must contain at least one directory with docker-compose.yml')

        # Build item metadata (stable numeric id; UI name uses the parent folder name).
        norm_label = str(label or '').strip()
        for idx, (rel_name, compose_path) in enumerate(sorted(compose_paths, key=lambda t: t[0]), start=1):
            rel_dir = rel_name
            # Display name should include the parent folder when nested: <parent>/<leaf>.
            if rel_dir in ('', '.', 'root'):
                display_name = 'root'
            else:
                parts = [p for p in str(rel_dir).replace('\\', '/').split('/') if p]
                if len(parts) >= 2:
                    display_name = f"{parts[-2]}/{parts[-1]}"
                else:
                    display_name = parts[-1] if parts else 'root'
            compose_rel = os.path.relpath(compose_path, content_dir).replace('\\', '/')
            dir_rel = os.path.relpath(os.path.dirname(compose_path), content_dir).replace('\\', '/')
            compose_items.append({
                'id': idx,
                'name': display_name,
                'rel_dir': rel_dir,
                'dir_rel': dir_rel,
                'compose_rel': compose_rel,
                # Keep repo-relative paths for convenience / compatibility.
                'compose_path': os.path.relpath(compose_path, _get_repo_root()).replace('\\', '/'),
                'dir': os.path.relpath(os.path.dirname(compose_path), _get_repo_root()).replace('\\', '/'),
                'from_source': norm_label,
                'disabled': False,
            })

        # Generate a catalog CSV from discovered compose directories.
        out_path = os.path.join(pack_dir, 'vuln_list_w_url.csv')
        with open(out_path, 'w', newline='', encoding='utf-8') as f:
            w = csv.writer(f)
            w.writerow(['Name', 'Path', 'Type', 'Vector', 'Startup', 'CVE', 'Description', 'References'])
            for it in compose_items:
                abs_compose = os.path.abspath(os.path.join(content_dir, str(it.get('compose_rel') or 'docker-compose.yml')))
                w.writerow([str(it.get('name') or ''), abs_compose, 'docker-compose', '', '', '', '', ''])

        # Validate generated CSV is parseable and non-empty.
        try:
            from core_topo_gen.utils.vuln_process import _read_csv as _read_vuln_csv
            if not _read_vuln_csv(out_path):
                raise ValueError('Generated catalog CSV parsed as empty')
        except Exception as exc:
            raise ValueError(f'Invalid vulnerability catalog CSV: {exc}')

        csv_rel_paths = [os.path.relpath(out_path, _get_repo_root())]
    except Exception:
        shutil.rmtree(pack_dir, ignore_errors=True)
        raise

    entry = {
        'id': catalog_id,
        'label': str(label or '').strip() or catalog_id,
        'from_source': str(label or '').strip() or catalog_id,
        'origin': str(origin or '').strip(),
        'installed_at': datetime.datetime.utcnow().isoformat(timespec='seconds') + 'Z',
        'csv_paths': csv_rel_paths,
        'content_dir': os.path.relpath(_vuln_catalog_pack_content_dir(catalog_id), _get_repo_root()).replace('\\', '/'),
        'compose_items': compose_items,
        'compose_count': len(compose_items),
    }

    state = _load_vuln_catalogs_state()
    catalogs = state.get('catalogs')
    if not isinstance(catalogs, list):
        catalogs = []
    catalogs.append(entry)
    state['catalogs'] = catalogs
    if not str(state.get('active_id') or '').strip():
        state['active_id'] = catalog_id
    _write_vuln_catalogs_state(state)
    return entry


@app.route('/vuln_catalog_page')
def vuln_catalog_page():
    _require_builder_or_admin()
    state = _load_vuln_catalogs_state()
    catalogs = [c for c in (state.get('catalogs') or []) if isinstance(c, dict)]
    active_id = str(state.get('active_id') or '').strip()
    active_label = None
    for c in catalogs:
        if str(c.get('id') or '').strip() == active_id:
            active_label = str(c.get('label') or '').strip() or active_id
            break
    items_count = None
    try:
        from core_topo_gen.utils.vuln_process import load_vuln_catalog
        items_count = len(load_vuln_catalog(_get_repo_root()))
    except Exception:
        items_count = None
    return render_template(
        'vuln_catalog.html',
        catalogs=catalogs,
        active_id=active_id,
        active_label=active_label,
        items_count=items_count,
        active_page='vuln_catalog',
    )


@app.route('/vuln_catalog_packs/upload', methods=['POST'])
def vuln_catalog_packs_upload():
    _require_builder_or_admin()
    f = request.files.get('zip_file')
    is_ajax = str(request.headers.get('X-Requested-With') or '').lower() == 'xmlhttprequest' or 'application/json' in str(request.headers.get('Accept') or '')
    if not f:
        if is_ajax:
            return jsonify({'ok': False, 'error': 'Missing zip_file'}), 400
        flash('Missing zip_file')
        return redirect(url_for('vuln_catalog_page'))
    try:
        data = f.read() or b''
        label = secure_filename(getattr(f, 'filename', '') or '') or 'vuln-catalog'
        entry = _install_vuln_catalog_zip_bytes(zip_bytes=data, label=label, origin='upload')
        if is_ajax:
            return jsonify({'ok': True, 'message': 'Vulnerability catalog pack installed.', 'catalog_id': str(entry.get('id') or '')})
        flash('Vulnerability catalog pack installed.')
    except Exception as exc:
        if is_ajax:
            return jsonify({'ok': False, 'error': f'Failed to install vulnerability catalog pack: {exc}'}), 400
        flash(f'Failed to install vulnerability catalog pack: {exc}')
    return redirect(url_for('vuln_catalog_page'))


@app.route('/vuln_catalog_packs/import_url', methods=['POST'])
def vuln_catalog_packs_import_url():
    _require_builder_or_admin()
    url = str(request.form.get('zip_url') or '').strip()
    ok, msg = _is_safe_remote_zip_url(url)
    if not ok:
        flash(f'Blocked URL: {msg}')
        return redirect(url_for('vuln_catalog_page'))
    try:
        data = _download_zip_from_url(url)
        label = os.path.basename(urlparse(url).path) or 'vuln-catalog'
        _install_vuln_catalog_zip_bytes(zip_bytes=data, label=label, origin=url)
        flash('Vulnerability catalog pack installed from URL.')
    except Exception as exc:
        flash(f'Failed to import vulnerability catalog pack: {exc}')
    return redirect(url_for('vuln_catalog_page'))


@app.route('/vuln_catalog_packs/download/<catalog_id>')
def vuln_catalog_packs_download(catalog_id: str):
    _require_builder_or_admin()
    cid = str(catalog_id or '').strip()
    if not cid:
        abort(404)
    zip_path = _vuln_catalog_pack_zip_path(cid)
    if not os.path.exists(zip_path):
        abort(404)
    return send_file(zip_path, as_attachment=True, download_name=f'vuln_catalog_{cid}.zip')


@app.route('/vuln_catalog_packs/browse/<catalog_id>')
@app.route('/vuln_catalog_packs/browse/<catalog_id>/<path:subpath>')
def vuln_catalog_packs_browse(catalog_id: str, subpath: str = ''):
    """Browse extracted files for an installed vulnerability catalog pack."""
    _require_builder_or_admin()
    cid = str(catalog_id or '').strip()
    if not cid:
        abort(404)
    base_dir = _vuln_catalog_pack_content_dir(cid)
    if not os.path.isdir(base_dir):
        abort(404)

    try:
        cur_abs = _safe_path_under(base_dir, subpath or '')
    except Exception:
        abort(400)

    if os.path.isfile(cur_abs):
        rel = os.path.relpath(cur_abs, base_dir).replace('\\', '/')
        return redirect(url_for('vuln_catalog_packs_file', catalog_id=cid, subpath=rel))
    if not os.path.isdir(cur_abs):
        abort(404)

    rel_dir = os.path.relpath(cur_abs, base_dir).replace('\\', '/')
    if rel_dir in ('.', ''):
        rel_dir = ''

    entries: list[dict[str, Any]] = []
    try:
        for nm in sorted(os.listdir(cur_abs)):
            if nm in ('.', '..'):
                continue
            p = os.path.join(cur_abs, nm)
            kind = 'dir' if os.path.isdir(p) else 'file'
            entries.append({'name': nm, 'kind': kind})
    except Exception:
        entries = []

    crumbs: list[dict[str, str]] = [{'name': 'root', 'href': url_for('vuln_catalog_packs_browse', catalog_id=cid)}]
    if rel_dir:
        acc: list[str] = []
        for part in [p for p in rel_dir.split('/') if p]:
            acc.append(part)
            crumbs.append({
                'name': part,
                'href': url_for('vuln_catalog_packs_browse', catalog_id=cid, subpath='/'.join(acc)),
            })

    return render_template(
        'vuln_catalog_browse.html',
        catalog_id=cid,
        rel_dir=rel_dir,
        entries=entries,
        crumbs=crumbs,
        active_page='vuln_catalog',
    )


@app.route('/vuln_catalog_packs/file/<catalog_id>/<path:subpath>')
def vuln_catalog_packs_file(catalog_id: str, subpath: str):
    """Download a single extracted file from an installed vulnerability catalog pack."""
    _require_builder_or_admin()
    cid = str(catalog_id or '').strip()
    if not cid:
        abort(404)
    base_dir = _vuln_catalog_pack_content_dir(cid)
    if not os.path.isdir(base_dir):
        abort(404)
    try:
        abs_p = _safe_path_under(base_dir, subpath or '')
    except Exception:
        abort(400)
    if not os.path.isfile(abs_p):
        abort(404)
    return send_file(abs_p, as_attachment=True, download_name=os.path.basename(abs_p))


@app.route('/vuln_catalog_packs/view/<catalog_id>/<path:subpath>')
def vuln_catalog_packs_view(catalog_id: str, subpath: str):
    """View a single extracted file inline (no forced download).

    Used for README links in the Vulnerability Catalog UI.
    """
    _require_builder_or_admin()
    cid = str(catalog_id or '').strip()
    if not cid:
        abort(404)
    base_dir = _vuln_catalog_pack_content_dir(cid)
    if not os.path.isdir(base_dir):
        abort(404)
    try:
        abs_p = _safe_path_under(base_dir, subpath or '')
    except Exception:
        abort(400)
    if not os.path.isfile(abs_p):
        abort(404)
    # Let Flask infer mimetype; keep it inline.
    return send_file(abs_p, as_attachment=False, download_name=os.path.basename(abs_p))


@app.route('/vuln_catalog_packs/readme/<catalog_id>/<path:subpath>')
def vuln_catalog_packs_readme(catalog_id: str, subpath: str):
    """Render a README file nicely (Markdown -> sanitized HTML; txt -> preformatted).

    This is used by the Vuln-Catalog "Description" column.
    """
    _require_builder_or_admin()
    cid = str(catalog_id or '').strip()
    if not cid:
        abort(404)
    base_dir = _vuln_catalog_pack_content_dir(cid)
    if not os.path.isdir(base_dir):
        abort(404)
    try:
        abs_p = _safe_path_under(base_dir, subpath or '')
    except Exception:
        abort(400)
    if not os.path.isfile(abs_p):
        abort(404)

    ext = os.path.splitext(abs_p)[1].lower().lstrip('.')
    if ext not in ('md', 'markdown', 'txt'):
        abort(404)

    try:
        with open(abs_p, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()
    except Exception:
        abort(404)

    readme_title = os.path.basename(abs_p)
    readme_rel = os.path.relpath(abs_p, base_dir).replace('\\', '/')

    def _rewrite_pack_relative_urls(*, html_in: str, readme_rel_path: str) -> str:
        """Rewrite relative URLs in rendered HTML to point at pack routes.

        This makes relative image/link references inside README.md work, e.g. "1.png".
        """
        import html as _html
        import posixpath as _posixpath
        from html.parser import HTMLParser as _HTMLParser
        from urllib.parse import urlparse as _urlparse

        base_rel_dir = _posixpath.dirname(readme_rel_path or '')

        def _is_relative_url(u: str) -> bool:
            u = (u or '').strip()
            if not u:
                return False
            if u.startswith('#'):
                return False
            # Absolute-path URLs ("/foo") are already rooted.
            if u.startswith('/'):
                return False
            parsed = _urlparse(u)
            return not bool(parsed.scheme)

        def _resolve_rel(u: str) -> str | None:
            # Prevent escaping out of pack content.
            u = (u or '').strip().replace('\\', '/')
            if not u or u.startswith('/'):
                return None
            resolved = _posixpath.normpath(_posixpath.join(base_rel_dir, u))
            # normpath can return '.'
            if resolved in ('', '.'):
                return None
            if resolved.startswith('..'):
                return None
            return resolved

        def _rewrite_href(u: str) -> str:
            if not _is_relative_url(u):
                return u
            resolved = _resolve_rel(u)
            if not resolved:
                return u
            ext = os.path.splitext(resolved)[1].lower().lstrip('.')
            if ext in ('md', 'markdown', 'txt'):
                return url_for('vuln_catalog_packs_readme', catalog_id=cid, subpath=resolved)
            return url_for('vuln_catalog_packs_view', catalog_id=cid, subpath=resolved)

        def _rewrite_src(u: str) -> str:
            if not _is_relative_url(u):
                return u
            resolved = _resolve_rel(u)
            if not resolved:
                return u
            return url_for('vuln_catalog_packs_view', catalog_id=cid, subpath=resolved)

        class _Rewriter(_HTMLParser):
            def __init__(self) -> None:
                super().__init__(convert_charrefs=False)
                self._out: list[str] = []

            def handle_starttag(self, tag: str, attrs: list[tuple[str, str | None]]) -> None:
                self._emit_tag(tag, attrs, closed=False)

            def handle_startendtag(self, tag: str, attrs: list[tuple[str, str | None]]) -> None:
                self._emit_tag(tag, attrs, closed=True)

            def _emit_tag(self, tag: str, attrs: list[tuple[str, str | None]], *, closed: bool) -> None:
                tag_l = (tag or '').lower()
                new_attrs: list[tuple[str, str | None]] = []
                for k, v in (attrs or []):
                    if not k:
                        continue
                    k_l = k.lower()
                    if v is not None and tag_l == 'a' and k_l == 'href':
                        v = _rewrite_href(v)
                    elif v is not None and tag_l == 'img' and k_l == 'src':
                        v = _rewrite_src(v)
                    new_attrs.append((k, v))

                self._out.append('<')
                self._out.append(tag)
                for k, v in new_attrs:
                    if v is None:
                        self._out.append(f' {k}')
                    else:
                        self._out.append(f' {k}="{_html.escape(str(v), quote=True)}"')
                self._out.append(' />' if closed else '>')

            def handle_endtag(self, tag: str) -> None:
                self._out.append(f'</{tag}>')

            def handle_data(self, data: str) -> None:
                self._out.append(data)

            def handle_entityref(self, name: str) -> None:
                self._out.append(f'&{name};')

            def handle_charref(self, name: str) -> None:
                self._out.append(f'&#{name};')

            def handle_comment(self, data: str) -> None:
                self._out.append(f'<!--{data}-->')

            def handle_decl(self, decl: str) -> None:
                self._out.append(f'<!{decl}>')

        p = _Rewriter()
        try:
            p.feed(html_in or '')
            p.close()
        except Exception:
            return html_in
        return ''.join(p._out)

    rendered_html = ''
    plain_text = ''
    render_warning = ''
    if ext == 'txt':
        plain_text = content
    else:
        # Render markdown and sanitize it.
        try:
            import markdown as _markdown  # type: ignore
        except Exception:
            _markdown = None
        try:
            import bleach as _bleach  # type: ignore
        except Exception:
            _bleach = None

        if _markdown is None or _bleach is None:
            plain_text = content
            missing = []
            if _markdown is None:
                missing.append('Markdown')
            if _bleach is None:
                missing.append('bleach')
            try:
                import sys as _sys
                py_exe = str(getattr(_sys, 'executable', '') or '').strip()
            except Exception:
                py_exe = ''
            if missing:
                render_warning = f"Markdown rendering unavailable (missing: {', '.join(missing)})."
            else:
                render_warning = 'Markdown rendering unavailable.'
            if py_exe:
                render_warning += f" Python: {py_exe}."
            render_warning += ' Showing plain text.'
        else:
            html = _markdown.markdown(
                content,
                extensions=['fenced_code', 'tables', 'sane_lists'],
                output_format='html5',
            )
            html = _rewrite_pack_relative_urls(html_in=html, readme_rel_path=readme_rel)
            allowed_tags = [
                'a', 'p', 'br', 'hr',
                'strong', 'em', 'code', 'pre', 'blockquote',
                'ul', 'ol', 'li',
                'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
                'table', 'thead', 'tbody', 'tr', 'th', 'td',
                'img',
            ]
            allowed_attrs = {
                'a': ['href', 'title', 'target', 'rel'],
                'code': ['class'],
                'pre': ['class'],
                'th': ['align'],
                'td': ['align'],
                'img': ['src', 'alt', 'title'],
            }
            cleaned = _bleach.clean(
                html,
                tags=allowed_tags,
                attributes=allowed_attrs,
                protocols=['http', 'https', 'mailto'],
                strip=True,
            )
            rendered_html = cleaned

    return render_template(
        'vuln_catalog_readme.html',
        active_page='vuln_catalog',
        catalog_id=cid,
        readme_title=readme_title,
        readme_rel=readme_rel,
        rendered_html=rendered_html,
        plain_text=plain_text,
        render_warning=render_warning,
    )


@app.route('/vuln_catalog_packs/item_files/<catalog_id>/<int:item_id>')
def vuln_catalog_pack_item_files(catalog_id: str, item_id: int):
    """Return a list of files for a single vulnerability item directory.

    Used by the Topology "Select Vulnerability" modal to populate a Files dropdown.
    """
    _require_builder_or_admin()
    cid = str(catalog_id or '').strip()
    if not cid:
        return jsonify({'ok': False, 'error': 'Missing catalog id'}), 404

    state = _load_vuln_catalogs_state()
    entry = None
    for c in (state.get('catalogs') or []):
        if isinstance(c, dict) and str(c.get('id') or '').strip() == cid:
            entry = c
            break
    if not entry:
        return jsonify({'ok': False, 'error': 'Unknown catalog id'}), 404

    items = _normalize_vuln_catalog_items(entry)
    target = None
    for it in items:
        try:
            if int(it.get('id') or 0) == int(item_id):
                target = it
                break
        except Exception:
            continue
    if not target:
        return jsonify({'ok': False, 'error': 'Unknown item id'}), 404

    base_dir = _vuln_catalog_pack_content_dir(cid)
    if not os.path.isdir(base_dir):
        return jsonify({'ok': False, 'error': 'Pack content missing'}), 404

    rel_dir = str(target.get('rel_dir') or target.get('dir_rel') or '').strip().replace('\\', '/')
    try:
        abs_dir = _safe_path_under(base_dir, rel_dir)
    except Exception:
        return jsonify({'ok': False, 'error': 'Invalid item path'}), 400
    if not os.path.isdir(abs_dir):
        return jsonify({'ok': False, 'error': 'Item directory missing'}), 404

    files: list[dict[str, str]] = []
    try:
        for nm in sorted(os.listdir(abs_dir)):
            if nm in ('.', '..'):
                continue
            ap = os.path.join(abs_dir, nm)
            if not os.path.isfile(ap):
                continue
            rel = os.path.relpath(ap, base_dir).replace('\\', '/')
            files.append({
                'name': nm,
                'url': url_for('vuln_catalog_packs_file', catalog_id=cid, subpath=rel),
            })
    except Exception:
        files = []

    return jsonify({'ok': True, 'catalog_id': cid, 'item_id': int(item_id), 'files': files})


@app.route('/vuln_catalog_packs/set_active/<catalog_id>', methods=['POST'])
def vuln_catalog_packs_set_active(catalog_id: str):
    _require_builder_or_admin()
    cid = str(catalog_id or '').strip()
    state = _load_vuln_catalogs_state()
    catalogs = [c for c in (state.get('catalogs') or []) if isinstance(c, dict)]
    if not any(str(c.get('id') or '').strip() == cid for c in catalogs):
        flash('Unknown catalog id')
        return redirect(url_for('vuln_catalog_page'))
    state['active_id'] = cid
    _write_vuln_catalogs_state(state)
    flash('Active vulnerability catalog updated.')
    return redirect(url_for('vuln_catalog_page'))


@app.route('/vuln_catalog_packs/delete/<catalog_id>', methods=['POST'])
def vuln_catalog_packs_delete(catalog_id: str):
    _require_builder_or_admin()
    cid = str(catalog_id or '').strip()
    state = _load_vuln_catalogs_state()
    catalogs = [c for c in (state.get('catalogs') or []) if isinstance(c, dict)]
    kept = [c for c in catalogs if str(c.get('id') or '').strip() != cid]
    state['catalogs'] = kept
    if str(state.get('active_id') or '').strip() == cid:
        state['active_id'] = str((kept[0].get('id') if kept else '') or '').strip()
    _write_vuln_catalogs_state(state)
    shutil.rmtree(_vuln_catalog_pack_dir(cid), ignore_errors=True)
    flash('Vulnerability catalog pack deleted.')
    return redirect(url_for('vuln_catalog_page'))


def _get_active_vuln_catalog_entry(state: dict) -> dict | None:
    active_id = str((state or {}).get('active_id') or '').strip()
    catalogs = [c for c in ((state or {}).get('catalogs') or []) if isinstance(c, dict)]
    for c in catalogs:
        if str(c.get('id') or '').strip() == active_id:
            return c
    return None


def _normalize_vuln_catalog_items(entry: dict) -> list[dict[str, Any]]:
    items = entry.get('compose_items')
    if not isinstance(items, list):
        items = []
    out: list[dict[str, Any]] = []
    next_id = 1
    for raw in items:
        if not isinstance(raw, dict):
            continue
        it = dict(raw)
        iid = it.get('id')
        try:
            iid_int = int(iid)
        except Exception:
            iid_int = next_id
        if iid_int < 1:
            iid_int = next_id
        next_id = max(next_id, iid_int + 1)
        it['id'] = iid_int
        it['disabled'] = bool(it.get('disabled', False))
        it['name'] = str(it.get('name') or '').strip() or 'root'
        it['rel_dir'] = str(it.get('rel_dir') or '').strip()
        it['dir_rel'] = str(it.get('dir_rel') or '').strip()
        it['compose_rel'] = str(it.get('compose_rel') or '').strip()
        out.append(it)
    out.sort(key=lambda d: int(d.get('id') or 0))
    return out


def _vuln_catalog_item_abs_compose_path(*, catalog_id: str, item: dict[str, Any]) -> str:
    content_dir = _vuln_catalog_pack_content_dir(catalog_id)
    rel = str(item.get('compose_rel') or '').strip()
    if rel:
        return _safe_path_under(content_dir, rel)
    # Back-compat: older items may only have repo-relative compose_path
    repo_rel = str(item.get('compose_path') or '').strip().replace('\\', '/')
    if not repo_rel:
        raise ValueError('Missing compose path')
    abs_p = os.path.abspath(os.path.join(_get_repo_root(), repo_rel))
    repo_root = os.path.abspath(_get_repo_root())
    if os.path.commonpath([repo_root, abs_p]) != repo_root:
        raise ValueError('Compose path escaped repo root')
    return abs_p


def _write_vuln_catalog_csv_from_items(*, catalog_id: str, items: list[dict[str, Any]]) -> list[str]:
    """Rewrite vuln_list_w_url.csv for a pack based on enabled items."""
    pack_dir = _vuln_catalog_pack_dir(catalog_id)
    content_dir = _vuln_catalog_pack_content_dir(catalog_id)
    os.makedirs(pack_dir, exist_ok=True)
    os.makedirs(content_dir, exist_ok=True)

    enabled = [it for it in (items or []) if not bool(it.get('disabled', False))]
    out_path = os.path.join(pack_dir, 'vuln_list_w_url.csv')
    with open(out_path, 'w', newline='', encoding='utf-8') as f:
        w = csv.writer(f)
        w.writerow(['Name', 'Path', 'Type', 'Vector', 'Startup', 'CVE', 'Description', 'References'])
        for it in enabled:
            abs_compose = _vuln_catalog_item_abs_compose_path(catalog_id=catalog_id, item=it)
            w.writerow([str(it.get('name') or ''), os.path.abspath(abs_compose), 'docker-compose', '', '', '', '', ''])

    return [os.path.relpath(out_path, _get_repo_root()).replace('\\', '/')]


@app.route('/vuln_catalog_items_data')
def vuln_catalog_items_data():
    _require_builder_or_admin()
    state = _load_vuln_catalogs_state()
    entry = _get_active_vuln_catalog_entry(state)
    if not entry:
        return jsonify({'ok': True, 'active': None, 'items': []})
    cid = str(entry.get('id') or '').strip()
    items = _normalize_vuln_catalog_items(entry)
    from_source = str(entry.get('from_source') or entry.get('label') or '').strip()
    def _display_name(it: dict[str, Any]) -> str:
        # Prefer stored name, but for deeper folders compute <parent>/<leaf> from rel_dir/dir_rel.
        base = str(it.get('name') or '').strip() or 'root'
        rel_dir = str(it.get('rel_dir') or it.get('dir_rel') or '').strip()
        if not rel_dir or rel_dir in ('', '.', 'root'):
            return base
        parts = [p for p in rel_dir.replace('\\', '/').split('/') if p]
        if len(parts) >= 2:
            return f"{parts[-2]}/{parts[-1]}"
        return parts[-1] if parts else base

    base_dir = _vuln_catalog_pack_content_dir(cid)
    out_items: list[dict[str, Any]] = []
    for it in items:
        readme_url = ''
        try:
            rel_dir = str(it.get('dir_rel') or it.get('rel_dir') or '').strip().replace('\\', '/')
            abs_dir = _safe_path_under(base_dir, rel_dir)
            if os.path.isdir(abs_dir):
                # Look for a README (case-insensitive) in the item directory.
                # Prefer the canonical names first, then fall back to README.*.md/markdown/txt.
                best_nm = None
                best_rank = 10**9
                for nm in os.listdir(abs_dir):
                    if not isinstance(nm, str):
                        continue
                    low = nm.lower().strip()
                    if not low.startswith('readme'):
                        continue
                    ext = os.path.splitext(low)[1].lstrip('.')
                    if ext not in ('md', 'markdown', 'txt'):
                        continue
                    # Rank exact matches highest.
                    if low in ('readme.md', 'readme.markdown', 'readme.txt'):
                        rank = 0
                    # Prefer English-ish before localized variants if present.
                    elif low.startswith('readme.en'):
                        rank = 1
                    else:
                        rank = 2
                    if rank < best_rank:
                        best_rank = rank
                        best_nm = nm

                if best_nm:
                    rel = os.path.relpath(os.path.join(abs_dir, best_nm), base_dir).replace('\\', '/')
                    readme_url = url_for('vuln_catalog_packs_readme', catalog_id=cid, subpath=rel)
        except Exception:
            readme_url = ''
        out_items.append({
            'id': int(it.get('id') or 0),
            'name': _display_name(it),
            'type': 'docker-compose',
            'from_source': from_source,
            'disabled': bool(it.get('disabled', False)),
            'readme_url': readme_url,
            'validated_ok': bool(it.get('validated_ok')) if it.get('validated_ok') is not None else None,
            'validated_at': str(it.get('validated_at') or '').strip() or None,
        })
    return jsonify({
        'ok': True,
        'active': {
            'id': cid,
            'label': str(entry.get('label') or '').strip() or cid,
            'from_source': from_source,
        },
        'items': out_items,
    })


@app.route('/vuln_catalog_items/set_disabled', methods=['POST'])
def vuln_catalog_items_set_disabled():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    item_id_raw = payload.get('item_id')
    disabled = bool(payload.get('disabled', False))
    try:
        item_id = int(item_id_raw)
    except Exception:
        return jsonify({'ok': False, 'error': 'Invalid item_id'}), 400

    state = _load_vuln_catalogs_state()
    entry = _get_active_vuln_catalog_entry(state)
    if not entry:
        return jsonify({'ok': False, 'error': 'No active catalog pack'}), 404
    cid = str(entry.get('id') or '').strip()
    catalogs = [c for c in (state.get('catalogs') or []) if isinstance(c, dict)]

    updated = False
    for c in catalogs:
        if str(c.get('id') or '').strip() != cid:
            continue
        items = _normalize_vuln_catalog_items(c)
        for it in items:
            if int(it.get('id') or 0) == item_id:
                it['disabled'] = disabled
                updated = True
                break
        c['compose_items'] = items
        c['csv_paths'] = _write_vuln_catalog_csv_from_items(catalog_id=cid, items=items)
        break
    if not updated:
        return jsonify({'ok': False, 'error': 'Unknown item id'}), 404

    state['catalogs'] = catalogs
    _write_vuln_catalogs_state(state)
    return jsonify({'ok': True})


@app.route('/api/flag-sequencing/flow_progress', methods=['GET'])
def api_flow_progress():
    """Return recent Flow-related log lines for the Generator Output modal."""
    try:
        port = int(os.environ.get('CORETG_PORT') or 9090)
    except Exception:
        port = 9090
    log_path = os.path.join(_outputs_dir(), 'logs', f'webui-{port}.log')
    lines: list[str] = []
    try:
        if os.path.exists(log_path):
            with open(log_path, 'r', encoding='utf-8', errors='ignore') as fh:
                raw = fh.read().splitlines()[-400:]
            for ln in raw:
                if ('[flow.progress]' in ln) or ('[flow.' in ln) or ('[remote-sync]' in ln) or ('Repo upload' in ln):
                    lines.append(ln.strip())
    except Exception:
        lines = []
    return jsonify({'ok': True, 'lines': lines})


@app.route('/vuln_catalog_items/delete', methods=['POST'])
def vuln_catalog_items_delete():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    item_id_raw = payload.get('item_id')
    try:
        item_id = int(item_id_raw)
    except Exception:
        return jsonify({'ok': False, 'error': 'Invalid item_id'}), 400

    state = _load_vuln_catalogs_state()
    entry = _get_active_vuln_catalog_entry(state)
    if not entry:
        return jsonify({'ok': False, 'error': 'No active catalog pack'}), 404
    cid = str(entry.get('id') or '').strip()
    catalogs = [c for c in (state.get('catalogs') or []) if isinstance(c, dict)]

    removed = False
    for c in catalogs:
        if str(c.get('id') or '').strip() != cid:
            continue
        items = _normalize_vuln_catalog_items(c)
        kept = [it for it in items if int(it.get('id') or 0) != item_id]
        removed = len(kept) != len(items)
        c['compose_items'] = kept
        c['compose_count'] = len(kept)
        c['csv_paths'] = _write_vuln_catalog_csv_from_items(catalog_id=cid, items=kept)
        break
    if not removed:
        return jsonify({'ok': False, 'error': 'Unknown item id'}), 404

    state['catalogs'] = catalogs
    _write_vuln_catalogs_state(state)
    return jsonify({'ok': True})


@app.route('/vuln_catalog_items/test/start', methods=['POST'])
def vuln_catalog_items_test_start():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    force_replace = bool(payload.get('force_replace') or payload.get('replace'))
    item_id_raw = payload.get('item_id')
    try:
        item_id = int(item_id_raw)
    except Exception:
        return jsonify({'ok': False, 'error': 'Invalid item_id'}), 400

    state = _load_vuln_catalogs_state()
    entry = _get_active_vuln_catalog_entry(state)
    if not entry:
        return jsonify({'ok': False, 'error': 'No active catalog pack'}), 404
    cid = str(entry.get('id') or '').strip()
    items = _normalize_vuln_catalog_items(entry)
    target = None
    for it in items:
        if int(it.get('id') or 0) == item_id:
            target = it
            break
    if not target:
        return jsonify({'ok': False, 'error': 'Unknown item id'}), 404

    try:
        compose_path = _vuln_catalog_item_abs_compose_path(catalog_id=cid, item=target)
    except Exception as exc:
        return jsonify({'ok': False, 'error': f'Invalid compose path: {exc}'}), 400
    if not os.path.isfile(compose_path):
        return jsonify({'ok': False, 'error': 'docker-compose.yml not found'}), 404

    try:
        for meta in RUNS.values():
            if isinstance(meta, dict) and meta.get('kind') == 'vuln_test' and not meta.get('done'):
                return jsonify({'ok': False, 'error': 'Another vulnerability test is already running'}), 409
    except Exception:
        pass

    run_id = str(uuid.uuid4())[:12]
    project_name = f"coretg-vuln-test-{item_id}-{int(time.time())}"
    run_dir = os.path.join(_outputs_dir(), 'vuln-tests', f'test-{run_id}')
    os.makedirs(run_dir, exist_ok=True)
    log_path = os.path.join(run_dir, 'run.log')

    # Resolve CORE VM credentials from payload.
    try:
        core_cfg = _merge_core_configs(payload.get('core'), include_password=True)
        # Default gRPC host/port to SSH host when missing.
        if not core_cfg.get('host'):
            core_cfg['host'] = core_cfg.get('ssh_host') or '127.0.0.1'
        if not core_cfg.get('port'):
            core_cfg['port'] = CORE_PORT
        core_cfg = _require_core_ssh_credentials(core_cfg)
    except Exception as exc:
        return jsonify({'ok': False, 'error': f'CORE VM SSH config required: {exc}'}), 400

    # Ensure no active CORE sessions before running tests.
    try:
        errors: list[str] = []
        host = str(core_cfg.get('host') or '127.0.0.1')
        port = int(core_cfg.get('port') or CORE_PORT)
        sessions = _list_active_core_sessions(host, port, core_cfg, errors=errors, meta={})
        if errors:
            return jsonify({'ok': False, 'error': f'Unable to verify CORE sessions: {errors[0]}' }), 409
        if sessions:
            return jsonify({'ok': False, 'error': 'CORE VM has active session(s). Stop running scenario before testing.'}), 409
    except Exception as exc:
        return jsonify({'ok': False, 'error': f'Unable to verify CORE sessions: {exc}' }), 409

    remote_run_dir = f"/tmp/tests/test-{run_id}"
    prepared_compose_path = compose_path
    try:
        from core_topo_gen.utils.vuln_process import prepare_compose_for_assignments
        node_name = f"vuln-test-{item_id}"
        rec = {
            'Name': str(target.get('name') or target.get('Name') or target.get('Title') or f'vuln-{item_id}'),
            'Path': compose_path,
            'Type': 'docker-compose',
            'ScenarioTag': f"vuln-test-{item_id}",
        }
        created = prepare_compose_for_assignments({node_name: rec}, out_base=run_dir)
        if created:
            prepared_compose_path = created[0]
    except Exception:
        prepared_compose_path = compose_path

    # Rewrite local paths in prepared compose to remote paths for CORE VM execution.
    def _rewrite_compose_paths_for_remote(compose_path: str, local_base: str, remote_base: str) -> None:
        """Rewrite absolute local paths in docker-compose to remote paths."""
        try:
            import yaml
            with open(compose_path, 'r', encoding='utf-8') as f:
                compose_obj = yaml.safe_load(f)
            if not isinstance(compose_obj, dict):
                return
            services = compose_obj.get('services')
            if not isinstance(services, dict):
                return
            # Rewrite build context paths
            for svc_name, svc in services.items():
                if not isinstance(svc, dict):
                    continue
                build = svc.get('build')
                if isinstance(build, dict):
                    ctx = build.get('context')
                    if isinstance(ctx, str) and ctx.startswith(local_base):
                        rel = os.path.relpath(ctx, local_base)
                        build['context'] = _remote_path_join(remote_base, rel)
                # Rewrite volume bind mount paths
                volumes = svc.get('volumes')
                if isinstance(volumes, list):
                    new_volumes = []
                    for vol in volumes:
                        if isinstance(vol, str) and ':' in vol:
                            parts = vol.split(':', 2)
                            src = parts[0]
                            if src.startswith(local_base):
                                rel = os.path.relpath(src, local_base)
                                parts[0] = _remote_path_join(remote_base, rel)
                                new_volumes.append(':'.join(parts))
                            else:
                                new_volumes.append(vol)
                        else:
                            new_volumes.append(vol)
                    svc['volumes'] = new_volumes
            # Write updated compose
            with open(compose_path, 'w', encoding='utf-8') as f:
                yaml.safe_dump(compose_obj, f, sort_keys=False)
        except Exception:
            pass  # Best-effort path rewriting

    def _summarize_compose_for_log(path: str) -> list[str]:
        lines: list[str] = []
        try:
            import yaml
            with open(path, 'r', encoding='utf-8') as f:
                obj = yaml.safe_load(f) or {}
            if not isinstance(obj, dict):
                return lines
            services = obj.get('services') if isinstance(obj, dict) else None
            if not isinstance(services, dict) or not services:
                return lines
            lines.append(f"[local] compose services={len(services)}")
            inject_helpers = [k for k in services.keys() if str(k).startswith('inject_copy')]
            if inject_helpers:
                lines.append(f"[local] inject helper(s): {', '.join(inject_helpers)}")
            for svc_name, svc in services.items():
                if not isinstance(svc, dict):
                    continue
                img = str(svc.get('image') or '').strip()
                build = svc.get('build')
                ctx = ''
                if isinstance(build, dict):
                    ctx = str(build.get('context') or '').strip()
                vols = svc.get('volumes') if isinstance(svc, dict) else None
                vcount = len(vols) if isinstance(vols, list) else 0
                labels = svc.get('labels') if isinstance(svc, dict) else None
                has_flow_label = False
                if isinstance(labels, dict):
                    for k in labels.keys():
                        if str(k).startswith('coretg.flow_artifacts.'):
                            has_flow_label = True
                            break
                elif isinstance(labels, list):
                    for it in labels:
                        if isinstance(it, str) and it.startswith('coretg.flow_artifacts.'):
                            has_flow_label = True
                            break
                parts: list[str] = []
                if img:
                    parts.append(f"image={img}")
                if ctx:
                    parts.append(f"build={ctx}")
                if vcount:
                    parts.append(f"volumes={vcount}")
                if has_flow_label:
                    parts.append("flow_artifacts_label=1")
                if parts:
                    lines.append(f"[local] service {svc_name}: " + ", ".join(parts))
        except Exception:
            return lines
        return lines

    if prepared_compose_path != compose_path:
        _rewrite_compose_paths_for_remote(prepared_compose_path, run_dir, remote_run_dir)

    def _compose_images_and_containers(path: str) -> tuple[list[str], list[str]]:
        images: list[str] = []
        containers: list[str] = []
        try:
            import yaml
            with open(path, 'r', encoding='utf-8') as f:
                obj = yaml.safe_load(f) or {}
            services = obj.get('services') if isinstance(obj, dict) else None
            if not isinstance(services, dict):
                return images, containers
            for _svc_name, svc in services.items():
                if not isinstance(svc, dict):
                    continue
                img = svc.get('image')
                if isinstance(img, str) and img.strip():
                    images.append(img.strip())
                cname = svc.get('container_name')
                if isinstance(cname, str) and cname.strip():
                    containers.append(cname.strip())
        except Exception:
            pass
        # Deduplicate while preserving order
        images = list(dict.fromkeys(images))
        containers = list(dict.fromkeys(containers))
        return images, containers

    def _exec_ssh_sudo_capture(client: Any, command: str, password: str | None) -> tuple[int, str, str]:
        channel = client.get_transport().open_session() if client.get_transport() else None
        if channel is None:
            return 1, '', 'SSH channel unavailable'
        try:
            channel.get_pty()
        except Exception:
            pass
        channel.exec_command(command)
        try:
            if password:
                channel.send(str(password) + "\n")
        except Exception:
            pass
        stdout_chunks: list[bytes] = []
        stderr_chunks: list[bytes] = []
        while True:
            try:
                if channel.recv_ready():
                    stdout_chunks.append(channel.recv(REMOTE_LOG_CHUNK_SIZE))
                if channel.recv_stderr_ready():
                    stderr_chunks.append(channel.recv_stderr(REMOTE_LOG_CHUNK_SIZE))
                if channel.exit_status_ready():
                    if not channel.recv_ready() and not channel.recv_stderr_ready():
                        break
            except Exception:
                break
            time.sleep(0.15)
        rc = 0
        try:
            rc = int(channel.recv_exit_status())
        except Exception:
            rc = 0
        try:
            out = b''.join(stdout_chunks).decode('utf-8', 'replace')
        except Exception:
            out = b''.join(stdout_chunks).decode('latin-1', 'replace')
        try:
            err = b''.join(stderr_chunks).decode('utf-8', 'replace')
        except Exception:
            err = b''.join(stderr_chunks).decode('latin-1', 'replace')
        return rc, out.strip(), err.strip()

    # Upload prepared compose assets to CORE VM and execute there.
    remote_run_dir = f"/tmp/tests/test-{run_id}"
    remote_compose_path = None
    ssh_client = None
    sftp = None

    try:
        log_f = open(log_path, 'a', encoding='utf-8', buffering=1)
    except Exception:
        return jsonify({'ok': False, 'error': 'Failed to open log file'}), 500

    try:
        log_f.write(f"compose: {os.path.relpath(compose_path, _get_repo_root())}\n")
        if prepared_compose_path != compose_path:
            log_f.write(f"compose_prepared: {os.path.relpath(prepared_compose_path, _get_repo_root())}\n")
        try:
            inject_mode = str(os.getenv('CORETG_INJECT_FILES_MODE') or '').strip() or 'copy'
            flow_mode = str(os.getenv('CORETG_FLOW_ARTIFACTS_MODE') or '').strip() or 'copy'
            log_f.write(f"[local] inject_files_mode={inject_mode} flow_artifacts_mode={flow_mode}\n")
        except Exception:
            pass
        try:
            for line in _summarize_compose_for_log(prepared_compose_path):
                log_f.write(line + "\n")
        except Exception:
            pass
        log_f.write(f"[remote] uploading to CORE VM\n")
        try:
            env_path = os.path.join(run_dir, '.env')
            if not os.path.exists(env_path):
                with open(env_path, 'w', encoding='utf-8') as _envf:
                    _envf.write('')
        except Exception:
            pass
        ssh_client = _open_ssh_client(core_cfg)
        sftp = ssh_client.open_sftp()
        _remote_mkdirs(ssh_client, remote_run_dir)

        # Check for existing images/containers on remote and optionally replace.
        try:
            images, containers = _compose_images_and_containers(prepared_compose_path)
            existing_images: list[str] = []
            existing_containers: list[str] = []
            pw = ''
            try:
                pw = str(core_cfg.get('ssh_password') or '')
            except Exception:
                pw = ''
            for img in images:
                inner = f"sudo -S -p '' -k docker images -q {shlex.quote(img)}"
                cmd = f"bash -lc {shlex.quote(inner)}"
                rc, out, _err = _exec_ssh_sudo_capture(ssh_client, cmd, pw)
                if rc == 0 and out:
                    existing_images.append(img)
            for cname in containers:
                filter_arg = f"name=^{cname}$"
                inner = f"sudo -S -p '' -k docker ps -a --filter {shlex.quote(filter_arg)} --format {{.Names}}"
                cmd = f"bash -lc {shlex.quote(inner)}"
                rc, out, _err = _exec_ssh_sudo_capture(ssh_client, cmd, pw)
                if rc == 0 and out:
                    existing_containers.append(cname)
            if (existing_images or existing_containers) and not force_replace:
                try:
                    log_f.write('[remote] existing images/containers detected; awaiting user confirmation\n')
                    log_f.flush()
                except Exception:
                    pass
                try:
                    if sftp:
                        sftp.close()
                except Exception:
                    pass
                try:
                    if ssh_client:
                        ssh_client.close()
                except Exception:
                    pass
                try:
                    log_f.close()
                except Exception:
                    pass
                return jsonify({
                    'ok': True,
                    'replace_required': True,
                    'existing_images': existing_images,
                    'existing_containers': existing_containers,
                })
            if force_replace and (existing_images or existing_containers):
                try:
                    log_f.write('[remote] replacing existing images/containers\n')
                except Exception:
                    pass
                for cname in existing_containers:
                    inner = f"sudo -S -p '' -k docker rm -f {shlex.quote(cname)}"
                    cmd = f"bash -lc {shlex.quote(inner)}"
                    _exec_ssh_sudo_capture(ssh_client, cmd, pw)
                for img in existing_images:
                    inner = f"sudo -S -p '' -k docker rmi -f {shlex.quote(img)}"
                    cmd = f"bash -lc {shlex.quote(inner)}"
                    _exec_ssh_sudo_capture(ssh_client, cmd, pw)
        except Exception:
            pass

        # Upload prepared dir recursively.
        total_files = 0
        try:
            for _root, _dirs, _files in os.walk(run_dir):
                total_files += sum(1 for _fn in _files if _fn and os.path.isfile(os.path.join(_root, _fn)))
        except Exception:
            total_files = 0
        try:
            if total_files:
                log_f.write(f"[upload] 0/{total_files} (0%)\n")
        except Exception:
            pass
        uploaded = 0
        for root, dirs, files in os.walk(run_dir):
            rel = os.path.relpath(root, run_dir)
            rel = '' if rel == '.' else rel
            remote_root = remote_run_dir if not rel else _remote_path_join(remote_run_dir, rel)
            _remote_mkdirs(ssh_client, remote_root)
            for dn in dirs:
                _remote_mkdirs(ssh_client, _remote_path_join(remote_root, dn))
            for fn in files:
                lp = os.path.join(root, fn)
                rp = _remote_path_join(remote_root, fn)
                if os.path.isfile(lp):
                    sftp.put(lp, rp)
                    try:
                        uploaded += 1
                        if total_files:
                            if uploaded == total_files or uploaded % 10 == 0:
                                pct = int((uploaded / max(1, total_files)) * 100)
                                log_f.write(f"[upload] {uploaded}/{total_files} ({pct}%)\n")
                    except Exception:
                        pass
        try:
            if total_files:
                log_f.write(f"[upload] done ({uploaded}/{total_files})\n")
        except Exception:
            pass

        rel_compose = os.path.relpath(prepared_compose_path, run_dir)
        remote_compose_path = _remote_path_join(remote_run_dir, rel_compose)
        log_f.write(f"[remote] compose: {remote_compose_path}\n")
        log_f.write(f"[remote] docker compose up (project={project_name})\n")

        pw = ''
        try:
            pw = str(core_cfg.get('ssh_password') or '')
        except Exception:
            pw = ''
        inner_cmd = (
            f"cd {shlex.quote(remote_run_dir)} && "
            f"COMPOSE_PROJECT_NAME={shlex.quote(project_name)} sudo -S -p '' -k "
            f"docker compose -f {shlex.quote(remote_compose_path)} up --remove-orphans"
        )
        cmd = f"bash -lc {shlex.quote(inner_cmd)}"
        channel = ssh_client.get_transport().open_session() if ssh_client.get_transport() else None
        if channel is None:
            raise RuntimeError('SSH channel unavailable')
        try:
            channel.get_pty()
        except Exception:
            pass
        channel.exec_command(cmd)
        try:
            if pw:
                channel.send(pw + "\n")
        except Exception:
            pass

        relay_thread = threading.Thread(
            target=_relay_remote_channel_to_log,
            args=(channel, log_f),
            daemon=True,
        )
        relay_thread.start()
    except Exception as exc:
        try:
            log_f.write(str(exc) + "\n")
            log_f.flush()
            log_f.close()
        except Exception:
            pass
        try:
            if sftp:
                sftp.close()
        except Exception:
            pass
        try:
            if ssh_client:
                ssh_client.close()
        except Exception:
            pass
        return jsonify({'ok': False, 'error': str(exc)}), 500

    RUNS[run_id] = {
        'kind': 'vuln_test',
        'proc': None,
        'log_path': log_path,
        'run_dir': run_dir,
        'done': False,
        'returncode': None,
        'cleanup_started': False,
        'cleanup_done': False,
        'compose_path': prepared_compose_path,
        'compose_path_raw': compose_path,
        'compose_dir': os.path.dirname(prepared_compose_path),
        'project_name': project_name,
        'catalog_id': cid,
        'item_id': item_id,
        'remote_run_dir': remote_run_dir,
        'remote_compose_path': remote_compose_path,
        'core_cfg': core_cfg,
        'ssh_client': ssh_client,
        'ssh_channel': channel,
        'ssh_log_handle': log_f,
        'ssh_log_thread': relay_thread,
    }

    def _monitor_remote_vuln_test(_run_id: str) -> None:
        try:
            meta = RUNS.get(_run_id)
            if not isinstance(meta, dict):
                return
            ch = meta.get('ssh_channel')
            if ch is None:
                return
            while True:
                try:
                    if ch.exit_status_ready():
                        rc = None
                        try:
                            rc = int(ch.recv_exit_status())
                        except Exception:
                            rc = 0
                        meta['returncode'] = rc
                        meta['done'] = True
                        break
                except Exception:
                    break
                time.sleep(0.5)
        finally:
            try:
                log_handle = RUNS.get(_run_id, {}).get('ssh_log_handle')
                if log_handle:
                    log_handle.flush()
                    log_handle.close()
            except Exception:
                pass
            try:
                client = RUNS.get(_run_id, {}).get('ssh_client')
                if client:
                    client.close()
            except Exception:
                pass

    try:
        threading.Thread(target=_monitor_remote_vuln_test, args=(run_id,), daemon=True).start()
    except Exception:
        pass
    return jsonify({'ok': True, 'run_id': run_id})


@app.route('/vuln_catalog_items/test/stop', methods=['POST'])
def vuln_catalog_items_test_stop():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    run_id = str(payload.get('run_id') or '').strip()
    ok_value = payload.get('ok')
    user_ok = True if ok_value is True else (False if ok_value is False else None)
    if not run_id:
        return jsonify({'ok': False, 'error': 'run_id required'}), 400
    meta = RUNS.get(run_id)
    if not meta or meta.get('kind') != 'vuln_test':
        return jsonify({'ok': False, 'error': 'not found'}), 404

    return _stop_vuln_test_meta(meta, user_ok)


def _stop_vuln_test_meta(meta: dict, user_ok: bool | None):
    compose_path = meta.get('remote_compose_path')
    remote_run_dir = meta.get('remote_run_dir')
    project_name = meta.get('project_name')
    log_path = meta.get('log_path')
    core_cfg = meta.get('core_cfg') if isinstance(meta.get('core_cfg'), dict) else None
    if not core_cfg:
        return jsonify({'ok': False, 'error': 'Missing CORE VM credentials'}), 400

    try:
        if log_path:
            with open(log_path, 'a', encoding='utf-8') as log_f:
                log_f.write("[stop] User requested stop\n")
    except Exception:
        pass

    try:
        channel = meta.get('ssh_channel')
        if channel is not None and hasattr(channel, 'close'):
            try:
                channel.close()
            except Exception:
                pass
    except Exception:
        pass

    try:
        if isinstance(meta, dict):
            meta['cleanup_started'] = True
            meta['cleanup_done'] = False
    except Exception:
        pass

    def _cleanup_remote() -> None:
        _client = None
        try:
            _client = _open_ssh_client(core_cfg)
            if log_path:
                with open(log_path, 'a', encoding='utf-8') as log_f:
                    log_f.write('[cleanup] docker compose down -v --remove-orphans --rmi all\n')
            pw = ''
            try:
                pw = str(core_cfg.get('ssh_password') or '')
            except Exception:
                pw = ''
            down_cmd = (
                f"cd {shlex.quote(remote_run_dir or '/tmp')} && "
                f"COMPOSE_PROJECT_NAME={shlex.quote(project_name or '')} sudo -S -p '' -k "
                f"docker compose -f {shlex.quote(compose_path or '')} down -v --remove-orphans --rmi all"
            )
            cmd = f"bash -lc {shlex.quote(down_cmd)}"
            channel = _client.get_transport().open_session() if _client.get_transport() else None
            if channel is not None:
                try:
                    channel.get_pty()
                except Exception:
                    pass
                channel.exec_command(cmd)
                try:
                    if pw:
                        channel.send(pw + "\n")
                except Exception:
                    pass
                try:
                    with open(log_path, 'a', encoding='utf-8') as log_f:
                        _relay_remote_channel_to_log(channel, log_f)
                except Exception:
                    pass
        except Exception:
            pass
        try:
            if _client and remote_run_dir:
                _remote_remove_path(_client, remote_run_dir)
        except Exception:
            pass
        try:
            if _client:
                _client.close()
        except Exception:
            pass
        try:
            if isinstance(meta, dict):
                meta['cleanup_done'] = True
        except Exception:
            pass

    try:
        threading.Thread(target=_cleanup_remote, daemon=True).start()
    except Exception:
        _cleanup_remote()

    # Persist validation status on the item.
    try:
        state = _load_vuln_catalogs_state()
        cid = str(meta.get('catalog_id') or '').strip()
        item_id = int(meta.get('item_id') or 0)
        catalogs = [c for c in (state.get('catalogs') or []) if isinstance(c, dict)]
        for c in catalogs:
            if str(c.get('id') or '').strip() != cid:
                continue
            items = _normalize_vuln_catalog_items(c)
            updated = False
            for it in items:
                if int(it.get('id') or 0) == item_id:
                    if user_ok is None:
                        it['validated_incomplete'] = True
                        it['validated_ok'] = None
                    else:
                        it['validated_incomplete'] = False
                        it['validated_ok'] = bool(user_ok)
                    it['validated_at'] = datetime.datetime.utcnow().isoformat(timespec='seconds') + 'Z'
                    updated = True
                    break
            if updated:
                c['compose_items'] = items
                c['csv_paths'] = _write_vuln_catalog_csv_from_items(catalog_id=cid, items=items)
                state['catalogs'] = catalogs
                _write_vuln_catalogs_state(state)
            break
    except Exception:
        pass

    try:
        if isinstance(meta, dict):
            meta['done'] = True
            meta['returncode'] = meta.get('returncode') or 0
    except Exception:
        pass
    return jsonify({'ok': True})


@app.route('/vuln_catalog_items/test/stop_active', methods=['POST'])
def vuln_catalog_items_test_stop_active():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    ok_value = payload.get('ok')
    user_ok = True if ok_value is True else (False if ok_value is False else None)
    active_meta = None
    try:
        for m in RUNS.values():
            if isinstance(m, dict) and m.get('kind') == 'vuln_test' and not m.get('done'):
                active_meta = m
                break
    except Exception:
        active_meta = None
    if not active_meta:
        return jsonify({'ok': False, 'error': 'no active test'}), 404
    return _stop_vuln_test_meta(active_meta, user_ok)


@app.route('/vuln_catalog_items/test/status', methods=['POST'])
def vuln_catalog_items_test_status():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    run_id = str(payload.get('run_id') or '').strip()
    if not run_id:
        return jsonify({'ok': False, 'error': 'run_id required'}), 400
    meta = RUNS.get(run_id)
    if not isinstance(meta, dict) or meta.get('kind') != 'vuln_test':
        return jsonify({'ok': False, 'error': 'not found'}), 404
    return jsonify({
        'ok': True,
        'done': bool(meta.get('done')),
        'cleanup_started': bool(meta.get('cleanup_started')),
        'cleanup_done': bool(meta.get('cleanup_done')),
    })


def _zip_entry_is_symlink(info) -> bool:
    try:
        # ZipInfo.external_attr: upper 16 bits are unix mode if created on unix.
        import stat

        mode = (int(getattr(info, 'external_attr', 0)) >> 16) & 0o170000
        return mode == stat.S_IFLNK
    except Exception:
        return False


def _safe_extract_zip_to_dir(zip_path: str, dest_dir: str) -> None:
    import zipfile

    os.makedirs(dest_dir, exist_ok=True)
    with zipfile.ZipFile(zip_path, 'r') as z:
        for info in z.infolist():
            name = str(info.filename or '')
            if not name:
                continue
            if name.startswith('/') or name.startswith('\\'):
                raise ValueError('Zip contains absolute paths')
            parts = [p for p in name.replace('\\', '/').split('/') if p not in ('', '.')]
            if any(p == '..' for p in parts):
                raise ValueError('Zip contains parent directory traversal')
            if _zip_entry_is_symlink(info):
                raise ValueError('Zip contains symlinks (not allowed)')
            out_path = os.path.abspath(os.path.join(dest_dir, *parts))
            if not out_path.startswith(os.path.abspath(dest_dir) + os.sep):
                raise ValueError('Zip extraction escaped destination')
            if name.endswith('/'):
                os.makedirs(out_path, exist_ok=True)
                continue
            os.makedirs(os.path.dirname(out_path), exist_ok=True)
            with z.open(info, 'r') as src, open(out_path, 'wb') as dst:
                dst.write(src.read())


def _validate_generator_pack_tree(extracted_dir: str) -> tuple[bool, str, list[dict[str, Any]], list[dict[str, Any]]]:
    """Validate generator pack contents.

    Returns (ok, note, items) where items are generator dirs to install.
    Validation includes:
      - manifest.yaml syntax and required fields
      - docker-compose.yml yaml syntax + referenced service exists
      - python file syntax checks via ast.parse
    """
    try:
        from pathlib import Path
        import ast
        import yaml  # type: ignore
    except Exception as exc:
        return False, f'Missing validator dependency: {exc}', []

    try:
        from core_topo_gen.sequencer.facts import parse_fact_ref
    except Exception:
        parse_fact_ref = None  # type: ignore

    root = Path(extracted_dir)
    if not root.exists() or not root.is_dir():
        return False, 'Empty pack', [], []

    # Find all manifest files.
    manifests: list[Path] = []
    for p in root.rglob('manifest.yaml'):
        if '__MACOSX' in str(p):
            continue
        manifests.append(p)
    for p in root.rglob('manifest.yml'):
        if '__MACOSX' in str(p):
            continue
        manifests.append(p)
    # De-dupe
    manifests = sorted({m.resolve() for m in manifests})
    if not manifests:
        return False, 'No manifest.yaml found in zip', [], []

    items: list[dict[str, Any]] = []
    errors: list[str] = []
    warnings: list[dict[str, Any]] = []

    canonical_types = {
        'string', 'int', 'float', 'number', 'boolean', 'json', 'file', 'string_list', 'file_list'
    }

    fact_pattern = re.compile(r'^[A-Za-z_][A-Za-z0-9_]*\s*\([^\n]*\)$')

    def _normalize_artifact_entry(item: Any) -> str:
        if isinstance(item, dict):
            return str(item.get('artifact') or item.get('name') or '').strip()
        return str(item or '').strip()

    def _validate_artifact_list(section: str, items: Any, *, manifest_path: Path) -> None:
        if items is None:
            return
        if not isinstance(items, list):
            errors.append(f'{manifest_path}: artifacts.{section} must be a list')
            return
        for idx, item in enumerate(items):
            art = _normalize_artifact_entry(item)
            if not art:
                errors.append(f'{manifest_path}: artifacts.{section}[{idx}] is empty')
                continue
            if parse_fact_ref is not None:
                try:
                    parse_fact_ref(art)
                except Exception:
                    errors.append(f'{manifest_path}: artifacts.{section}[{idx}] must be fact-style (e.g., File(path))')
            else:
                if not fact_pattern.match(art):
                    errors.append(f'{manifest_path}: artifacts.{section}[{idx}] must be fact-style (e.g., File(path))')

    def _normalize_input_type_for_warning(type_value: Any) -> tuple[str, bool]:
        """Return (canonical_type, used_fallback).

        used_fallback=True means we could not map the provided type (or it was missing)
        and defaulted to "string".
        """
        try:
            t0 = str(type_value or '').strip().lower()
        except Exception:
            t0 = ''
        if not t0:
            return 'string', True

        t = t0
        is_list = False
        try:
            is_list = ('list' in t) or t.endswith('[]')
        except Exception:
            is_list = False

        if t in canonical_types:
            return t, False

        # Common aliases.
        if t in {'text', 'str'}:
            return 'string', False
        if t in {'integer'}:
            return 'int', False
        if t in {'double'}:
            return 'float', False
        if t in {'bool'}:
            return 'boolean', False
        if t in {'object', 'dict', 'map'}:
            return 'json', False
        if t in {'filepath', 'file_path', 'path', 'pathname'}:
            return 'file', False
        if t in {'strings'}:
            return 'string_list', False
        if t in {'files'}:
            return 'file_list', False
        if is_list and ('file' in t or 'path' in t):
            return 'file_list', False
        if is_list and ('string' in t or 'text' in t or 'str' in t):
            return 'string_list', False
        if t.endswith('[]'):
            return 'string_list', False

        return 'string', True
    for mp in manifests:
        try:
            doc = yaml.safe_load(mp.read_text('utf-8', errors='ignore'))
        except Exception as exc:
            errors.append(f'{mp}: invalid yaml: {exc}')
            continue
        if not isinstance(doc, dict):
            errors.append(f'{mp}: manifest must be a mapping')
            continue
        mv = int(doc.get('manifest_version') or 0)
        if mv != 1:
            errors.append(f'{mp}: manifest_version must be 1')
            continue
        manifest_id = str(doc.get('id') or '').strip()
        source_id = manifest_id

        # If this generator was exported from an installed pack, it may carry a
        # marker with the original stable source id. Prefer that over the
        # (numeric) installed manifest id so export/import roundtrips preserve ids.
        try:
            marker_path = mp.parent / '.coretg_pack.json'
            if marker_path.exists() and marker_path.is_file():
                marker = json.loads(marker_path.read_text('utf-8', errors='ignore') or '{}')
                if isinstance(marker, dict):
                    marker_source_id = str(marker.get('source_generator_id') or '').strip()
                    if marker_source_id and len(marker_source_id) <= 256:
                        source_id = marker_source_id
        except Exception:
            pass

        # Allow packs to omit ids; we will assign numeric ids on install.
        gen_id = source_id
        if not gen_id:
            try:
                import uuid

                gen_id = f"gen-{uuid.uuid4().hex[:10]}"
            except Exception:
                gen_id = 'gen-unknown'
        kind = str(doc.get('kind') or '').strip()
        if kind not in ('flag-generator', 'flag-node-generator'):
            errors.append(f'{mp}: kind must be flag-generator or flag-node-generator')
            continue

        # Enforce fact-style artifact keys in manifests.
        try:
            artifacts = doc.get('artifacts') if isinstance(doc.get('artifacts'), dict) else None
            if artifacts:
                _validate_artifact_list('requires', artifacts.get('requires'), manifest_path=mp)
                _validate_artifact_list('optional_requires', artifacts.get('optional_requires'), manifest_path=mp)
                _validate_artifact_list('produces', artifacts.get('produces'), manifest_path=mp)
        except Exception:
            pass

        # Warn on missing/unknown input types (they will fall back to string at runtime).
        try:
            raw_inputs = doc.get('inputs')
            if isinstance(raw_inputs, list):
                for idx, inp in enumerate(raw_inputs):
                    if not isinstance(inp, dict):
                        continue
                    nm = str(inp.get('name') or '').strip()
                    if not nm:
                        continue
                    raw_t = inp.get('type')
                    normalized, used_fallback = _normalize_input_type_for_warning(raw_t)
                    if raw_t is None:
                        warnings.append({
                            'manifest_path': str(mp),
                            'input_name': nm,
                            'raw_type': None,
                            'normalized_type': normalized,
                            'warning': 'inputs[].type is missing; defaulting to string',
                        })
                    elif used_fallback:
                        warnings.append({
                            'manifest_path': str(mp),
                            'input_name': nm,
                            'raw_type': str(raw_t),
                            'normalized_type': normalized,
                            'warning': f'Unknown input type "{raw_t}"; defaulting to string',
                        })
        except Exception:
            pass

        gen_dir = mp.parent
        runtime = doc.get('runtime') if isinstance(doc.get('runtime'), dict) else {}
        runtime_type = str(runtime.get('type') or 'docker-compose').strip().lower()
        if runtime_type in ('docker-compose', 'compose'):
            compose_file = str(runtime.get('compose_file') or runtime.get('file') or 'docker-compose.yml')
            compose_service = str(runtime.get('service') or 'generator')
            compose_path = (gen_dir / compose_file).resolve()
            if not compose_path.exists() or not compose_path.is_file():
                errors.append(f'{mp}: missing compose file: {compose_file}')
                continue
            try:
                compose_doc = yaml.safe_load(compose_path.read_text('utf-8', errors='ignore'))
            except Exception as exc:
                errors.append(f'{compose_path}: invalid yaml: {exc}')
                continue
            if not isinstance(compose_doc, dict):
                errors.append(f'{compose_path}: compose must be a mapping')
                continue
            services = compose_doc.get('services')
            if not isinstance(services, dict) or not services:
                errors.append(f'{compose_path}: compose missing services')
                continue
            if compose_service not in services:
                errors.append(f'{compose_path}: missing service "{compose_service}"')
                continue

        # Basic Python syntax check for any .py file in the generator dir.
        try:
            for py in gen_dir.rglob('*.py'):
                if py.is_file():
                    ast.parse(py.read_text('utf-8', errors='ignore'))
        except Exception as exc:
            errors.append(f'{mp}: python syntax error: {exc}')
            continue

        items.append({
            'id': gen_id,
            'kind': kind,
            'path': str(gen_dir),
            'source_id': source_id,
            'manifest_id': manifest_id,
            'manifest_path': str(mp),
        })

    if errors:
        # Show first few errors.
        return False, '; '.join(errors[:4]) + (f' (+{len(errors)-4} more)' if len(errors) > 4 else ''), [], warnings
    return True, f'Validated {len(items)} generator(s)', items, warnings


def _compute_next_numeric_generator_id(*, repo_root: str) -> int:
    """Return the next numeric generator id across installed packs.

    Generator packs use numeric ids for uniqueness, but manifest discovery may
    remap ids to stable source ids for UI/Flow. To avoid collisions, derive the
    numeric max from installed pack markers.
    """
    max_numeric = 0
    try:
        root = _installed_generators_root()
        for dirpath, _dirnames, filenames in os.walk(root):
            if '.coretg_pack.json' not in filenames:
                continue
            marker_path = os.path.join(dirpath, '.coretg_pack.json')
            try:
                with open(marker_path, 'r', encoding='utf-8') as fh:
                    marker = json.load(fh)
                if not isinstance(marker, dict):
                    continue
                gid = str(marker.get('generator_id') or '').strip()
                if gid.isdigit():
                    max_numeric = max(max_numeric, int(gid))
            except Exception:
                continue
    except Exception:
        max_numeric = 0

    return max_numeric + 1


def _install_generator_pack_payload(
    *,
    zip_path: str,
    pack_id: str,
    safe_label: str,
    pack_origin: str,
    next_numeric: int,
) -> tuple[bool, str, list[dict[str, Any]], int, list[dict[str, Any]]]:
    """Install a pack zip payload and return installed items.

    This does not write to the packs state file; callers decide how to record grouping.
    """
    import shutil
    import tempfile
    from pathlib import Path

    try:
        import yaml  # type: ignore
    except Exception as exc:
        return False, f'Pack install requires PyYAML: {exc}', [], next_numeric, []

    root = _installed_generators_root()
    tmp_dir = tempfile.mkdtemp(prefix='coretg_pack_')
    try:
        _safe_extract_zip_to_dir(zip_path, tmp_dir)
        ok, note, items, warnings = _validate_generator_pack_tree(tmp_dir)
        if not ok:
            return False, note, [], next_numeric, warnings

        installed: list[dict[str, Any]] = []
        for it in items:
            kind = str(it.get('kind') or '')
            src_dir = str(it.get('path') or '')
            source_gid = str(it.get('source_id') or it.get('id') or '')
            assigned_gid = str(next_numeric)
            next_numeric += 1

            if kind == 'flag-node-generator':
                dest_base = os.path.join(root, 'flag_node_generators')
            else:
                dest_base = os.path.join(root, 'flag_generators')
            os.makedirs(dest_base, exist_ok=True)

            dir_name = secure_filename(f"p_{pack_id}__{assigned_gid}") or f"p_{pack_id}__generator"
            dest_dir = os.path.join(dest_base, dir_name)
            suffix = 2
            while os.path.exists(dest_dir):
                dest_dir = os.path.join(dest_base, f"{dir_name}_{suffix}")
                suffix += 1

            shutil.copytree(src_dir, dest_dir)

            # Rewrite installed manifest id and avoid overriding installed source path.
            try:
                manifest_path = None
                for nm in ('manifest.yaml', 'manifest.yml'):
                    p = os.path.join(dest_dir, nm)
                    if os.path.exists(p) and os.path.isfile(p):
                        manifest_path = p
                        break
                if manifest_path is None:
                    for rp in Path(dest_dir).rglob('manifest.yaml'):
                        if '__MACOSX' in str(rp):
                            continue
                        manifest_path = str(rp)
                        break
                if manifest_path is None:
                    for rp in Path(dest_dir).rglob('manifest.yml'):
                        if '__MACOSX' in str(rp):
                            continue
                        manifest_path = str(rp)
                        break

                if manifest_path:
                    doc = yaml.safe_load(Path(manifest_path).read_text('utf-8', errors='ignore'))
                    if isinstance(doc, dict):
                        doc['id'] = assigned_gid
                        doc.pop('source_path', None)
                        if isinstance(doc.get('source'), dict):
                            doc.pop('source', None)
                        Path(manifest_path).write_text(yaml.safe_dump(doc, sort_keys=False), encoding='utf-8')
            except Exception:
                pass

            # Marker for debugging/traceability.
            try:
                marker = {
                    'pack_id': pack_id,
                    'pack_label': safe_label,
                    'origin': pack_origin,
                    'installed_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
                    'generator_id': assigned_gid,
                    'kind': kind,
                    'source_generator_id': source_gid,
                }
                with open(os.path.join(dest_dir, '.coretg_pack.json'), 'w', encoding='utf-8') as fh:
                    json.dump(marker, fh, indent=2)
            except Exception:
                pass

            installed.append({'id': assigned_gid, 'kind': kind, 'path': dest_dir})

        return True, note, installed, next_numeric, warnings
    finally:
        try:
            shutil.rmtree(tmp_dir, ignore_errors=True)
        except Exception:
            pass


def _install_generator_pack(*, zip_path: str, pack_label: str, pack_origin: str) -> tuple[bool, str]:
    try:
        try:
            repo_root = _get_repo_root()
        except Exception:
            repo_root = os.getcwd()

        safe_label = secure_filename(pack_label or 'pack') or 'pack'
        pack_id = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:6]
        next_numeric = _compute_next_numeric_generator_id(repo_root=repo_root)

        ok, note, installed, _next, warnings = _install_generator_pack_payload(
            zip_path=zip_path,
            pack_id=pack_id,
            safe_label=safe_label,
            pack_origin=pack_origin,
            next_numeric=next_numeric,
        )
        if not ok:
            return False, note

        state = _load_installed_generator_packs_state()
        state.setdefault('packs', [])
        state['packs'].append({
            'id': pack_id,
            'label': safe_label,
            'origin': pack_origin,
            'note': note,
            'warnings': warnings,
            'installed_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
            'installed': installed,
        })
        _save_installed_generator_packs_state(state)
        warn_note = ''
        try:
            if isinstance(warnings, list) and warnings:
                warn_note = f" (warnings: {len(warnings)})"
        except Exception:
            warn_note = ''
        return True, f"Installed {len(installed)} generator(s) from {safe_label}" + warn_note
    except ValueError as ve:
        return False, str(ve)
    except Exception as exc:
        return False, str(exc)


def _install_generator_pack_or_bundle(*, zip_path: str, pack_label: str, pack_origin: str) -> tuple[bool, str]:
    """Install either a single generator-pack ZIP, or a bundle ZIP produced by export_all.

    A bundle is a ZIP that contains one or more nested pack ZIPs. By convention,
    export_all writes them under packs/*.zip, but we also accept nested *.zip files
    anywhere in the archive.
    """
    import tempfile
    import zipfile

    try:
        with zipfile.ZipFile(zip_path, 'r') as z:
            names = [str(n or '') for n in z.namelist()]

            has_manifest = any(n.endswith('/manifest.yaml') or n.endswith('/manifest.yml') for n in names)
            nested_all = [
                n for n in names
                if n and (not n.endswith('/')) and n.lower().endswith('.zip') and (not n.startswith('__MACOSX/'))
            ]
            # Prefer the export_all convention, but accept bundles created by users too.
            nested = [n for n in nested_all if n.startswith('packs/')]
            if not nested:
                nested = nested_all

            if (not has_manifest) and nested:
                # Bundle import: install all nested pack ZIPs under a single pack label/state row.
                safe_label = secure_filename(pack_label or 'bundle') or 'bundle'
                pack_id = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:6]
                try:
                    repo_root = _get_repo_root()
                except Exception:
                    repo_root = os.getcwd()
                next_numeric = _compute_next_numeric_generator_id(repo_root=repo_root)

                successes = 0
                failures: list[str] = []
                installed_all: list[dict[str, Any]] = []
                warnings_all: list[dict[str, Any]] = []

                for inner_name in sorted(set(nested)):
                    try:
                        inner_bytes = z.read(inner_name)
                    except Exception as exc:
                        failures.append(f'{inner_name}: read failed ({exc})')
                        continue

                    fd, inner_tmp = tempfile.mkstemp(prefix='coretg_pack_bundle_', suffix='-' + os.path.basename(inner_name))
                    os.close(fd)
                    try:
                        with open(inner_tmp, 'wb') as fh:
                            fh.write(inner_bytes)

                        ok_inner, note_inner, installed_inner, next_numeric, warnings_inner = _install_generator_pack_payload(
                            zip_path=inner_tmp,
                            pack_id=pack_id,
                            safe_label=safe_label,
                            pack_origin=pack_origin,
                            next_numeric=next_numeric,
                        )
                        if ok_inner:
                            installed_all.extend(installed_inner)
                            if isinstance(warnings_inner, list) and warnings_inner:
                                warnings_all.extend(warnings_inner)
                            successes += 1
                        else:
                            failures.append(f'{inner_name}: {note_inner}')
                    finally:
                        try:
                            os.remove(inner_tmp)
                        except Exception:
                            pass

                # Record one combined bundle pack state row.
                try:
                    state = _load_installed_generator_packs_state()
                    state.setdefault('packs', [])
                    state['packs'].append({
                        'id': pack_id,
                        'label': safe_label,
                        'origin': pack_origin,
                        'note': f'Imported {successes} pack(s) from bundle' + (f'; {failures[0]}' if failures else ''),
                        'warnings': warnings_all,
                        'installed_at': datetime.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z',
                        'installed': installed_all,
                    })
                    _save_installed_generator_packs_state(state)
                except Exception:
                    pass

                warn_note = ''
                try:
                    if warnings_all:
                        warn_note = f" (warnings: {len(warnings_all)})"
                except Exception:
                    warn_note = ''
                if failures:
                    return successes > 0, f'Imported {successes} pack(s) from bundle as {safe_label}{warn_note}; {failures[0]}'
                return True, f'Imported {successes} pack(s) from bundle as {safe_label}{warn_note}'

    except Exception as exc:
        return False, f'Invalid zip: {exc}'

    return _install_generator_pack(zip_path=zip_path, pack_label=pack_label, pack_origin=pack_origin)


@app.route('/generator_packs/upload', methods=['POST'])
def generator_packs_upload():
    is_xhr = (request.headers.get('X-Requested-With') == 'XMLHttpRequest')
    f = request.files.get('zip_file')
    if not f or f.filename == '':
        if is_xhr:
            return jsonify({'ok': False, 'error': 'No zip selected.'}), 400
        flash('No zip selected.')
        return redirect(url_for('flag_catalog_page'))
    filename = secure_filename(f.filename)
    if not filename.lower().endswith('.zip'):
        if is_xhr:
            return jsonify({'ok': False, 'error': 'Only .zip allowed.'}), 400
        flash('Only .zip allowed.')
        return redirect(url_for('flag_catalog_page'))

    import tempfile

    fd, tmp_path = tempfile.mkstemp(prefix='coretg_pack_', suffix='-' + filename)
    os.close(fd)
    try:
        f.save(tmp_path)
        label = filename
        if label.lower().endswith('.zip'):
            label = label[:-4]
        ok, note = _install_generator_pack_or_bundle(zip_path=tmp_path, pack_label=label, pack_origin='upload')
        if is_xhr:
            if ok:
                # Include warnings (best-effort) by reading the latest pack state entry.
                warnings: list[dict[str, Any]] = []
                try:
                    state = _load_installed_generator_packs_state()
                    packs = state.get('packs') if isinstance(state, dict) else None
                    if isinstance(packs, list) and packs:
                        last = packs[-1] if isinstance(packs[-1], dict) else {}
                        ww = last.get('warnings') if isinstance(last, dict) else None
                        if isinstance(ww, list):
                            warnings = ww
                except Exception:
                    warnings = []
                return jsonify({'ok': True, 'message': note, 'warnings': warnings}), 200
            return jsonify({'ok': False, 'error': f'Pack install failed: {note}'}), 400

        flash(note if ok else f'Pack install failed: {note}')
    finally:
        try:
            os.remove(tmp_path)
        except Exception:
            pass
    return redirect(url_for('flag_catalog_page'))


@app.route('/generator_packs/import_url', methods=['POST'])
def generator_packs_import_url():
    url = str(request.form.get('zip_url') or '').strip()
    if not url:
        flash('Missing URL.')
        return redirect(url_for('flag_catalog_page'))

    import tempfile

    try:
        data = _download_zip_from_url(url)
        fd, tmp_path = tempfile.mkstemp(prefix='coretg_pack_url_', suffix='.zip')
        os.close(fd)
        try:
            with open(tmp_path, 'wb') as fh:
                fh.write(data)
            ok, note = _install_generator_pack_or_bundle(zip_path=tmp_path, pack_label=url, pack_origin='url')
            flash(note if ok else f'Pack install failed: {note}')
        finally:
            try:
                os.remove(tmp_path)
            except Exception:
                pass
    except Exception as exc:
        flash(f'URL import failed: {exc}')

    return redirect(url_for('flag_catalog_page'))


@app.route('/generator_packs/delete/<pack_id>', methods=['POST'])
def generator_packs_delete(pack_id: str):
    """Uninstall a previously installed generator pack.

    Deletes the installed generator directories recorded in the pack state,
    but only if they reside under the installed-generators root.
    """
    import shutil

    pid = str(pack_id or '').strip()
    if not pid:
        flash('Missing pack id')
        return redirect(url_for('flag_catalog_page'))

    installed_root = os.path.abspath(_installed_generators_root())
    state = _load_installed_generator_packs_state()
    packs = state.get('packs') or []
    if not isinstance(packs, list):
        packs = []

    target = None
    kept = []
    for p in packs:
        if isinstance(p, dict) and str(p.get('id') or '') == pid:
            target = p
            continue
        kept.append(p)

    if not target:
        flash('Pack not found')
        return redirect(url_for('flag_catalog_page'))

    removed = 0
    failures: list[str] = []
    for it in (target.get('installed') or []):
        if not isinstance(it, dict):
            continue
        path = str(it.get('path') or '').strip()
        if not path:
            continue
        abs_path = os.path.abspath(path)
        try:
            # Ensure deletion stays within the installed root.
            if os.path.commonpath([installed_root, abs_path]) != installed_root:
                failures.append(f"refused to delete outside installed root: {abs_path}")
                continue
        except Exception:
            failures.append(f"refused to delete path: {abs_path}")
            continue

        try:
            if os.path.isdir(abs_path):
                shutil.rmtree(abs_path, ignore_errors=False)
                removed += 1
            elif os.path.exists(abs_path):
                os.remove(abs_path)
                removed += 1
        except Exception as exc:
            failures.append(f"failed deleting {abs_path}: {exc}")

    state['packs'] = kept
    _save_installed_generator_packs_state(state)

    if failures:
        flash(f"Uninstalled pack {pid} with warnings: removed={removed}; {failures[0]}")
    else:
        flash(f"Uninstalled pack {pid} (removed {removed} item(s))")
    return redirect(url_for('flag_catalog_page'))


def _set_pack_disabled_state(*, pack_id: str, disabled: bool) -> tuple[bool, str]:
    pid = str(pack_id or '').strip()
    if not pid:
        return False, 'Missing pack id'
    state = _load_installed_generator_packs_state()
    packs = state.get('packs') if isinstance(state, dict) else None
    if not isinstance(packs, list):
        packs = []
    for p in packs:
        if not isinstance(p, dict):
            continue
        if str(p.get('id') or '').strip() != pid:
            continue
        p['disabled'] = bool(disabled)
        state['packs'] = packs
        _save_installed_generator_packs_state(state)
        return True, ('Disabled' if disabled else 'Enabled') + f' pack {pid}'
    return False, 'Pack not found'


def _set_generator_disabled_state(*, kind: str, generator_id: str, disabled: bool) -> tuple[bool, str]:
    gid = str(generator_id or '').strip()
    if not gid:
        return False, 'Missing generator id'
    k = str(kind or '').strip().lower().replace('_', '-')
    if k not in ('flag-generator', 'flag-node-generator'):
        return False, f'Invalid kind: {kind}'

    state = _load_installed_generator_packs_state()
    packs = state.get('packs') if isinstance(state, dict) else None
    if not isinstance(packs, list):
        packs = []

    for p in packs:
        if not isinstance(p, dict):
            continue
        items = p.get('installed')
        if not isinstance(items, list):
            continue
        for it in items:
            if not isinstance(it, dict):
                continue
            if str(it.get('kind') or '').strip().lower().replace('_', '-') != k:
                continue
            if str(it.get('id') or '').strip() != gid:
                continue
            it['disabled'] = bool(disabled)
            state['packs'] = packs
            _save_installed_generator_packs_state(state)
            return True, ('Disabled' if disabled else 'Enabled') + f' {k} {gid}'

    return False, 'Installed generator not found'


@app.route('/generator_packs/set_disabled/<pack_id>', methods=['POST'])
def generator_packs_set_disabled(pack_id: str):
    _require_builder_or_admin()
    disabled = str(request.form.get('disabled') or '').strip().lower() in {'1', 'true', 'yes', 'on'}
    ok, msg = _set_pack_disabled_state(pack_id=pack_id, disabled=disabled)
    flash(msg if ok else f'Failed: {msg}')
    return redirect(url_for('flag_catalog_page'))


def _pack_to_zip_bytes(pack: dict) -> bytes:
    """Build a ZIP archive for a single installed pack."""
    import io
    import zipfile

    mem = io.BytesIO()
    installed = pack.get('installed') if isinstance(pack, dict) else []
    with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
        # Pack metadata
        try:
            meta = json.dumps(pack, indent=2)
        except Exception:
            meta = '{}'
        z.writestr('pack.json', meta + '\n')

        for it in (installed or []):
            if not isinstance(it, dict):
                continue
            kind = str(it.get('kind') or '').strip()
            src = str(it.get('path') or '').strip()
            if not src or not os.path.exists(src):
                continue

            base = 'flag_generators'
            if kind == 'flag-node-generator':
                base = 'flag_node_generators'

            root_name = os.path.basename(src.rstrip('/')) or 'generator'
            if os.path.isdir(src):
                for dirpath, _dirnames, filenames in os.walk(src):
                    for fn in filenames:
                        abs_p = os.path.join(dirpath, fn)
                        rel_p = os.path.relpath(abs_p, src)
                        arc = '/'.join([base, root_name, rel_p.replace('\\', '/')])
                        try:
                            z.write(abs_p, arcname=arc)
                        except Exception:
                            continue
            else:
                arc = '/'.join([base, root_name])
                try:
                    z.write(src, arcname=arc)
                except Exception:
                    pass

    mem.seek(0)
    return mem.read()


def _delete_installed_generator(*, kind: str, generator_id: str) -> tuple[bool, str]:
    """Delete a single installed generator directory by manifest id.

    Safety: only deletes directories under outputs/installed_generators.
    """
    import shutil
    from pathlib import Path

    try:
        import yaml  # type: ignore
    except Exception as exc:
        return False, f'PyYAML required: {exc}'

    gid = str(generator_id or '').strip()
    if not gid:
        return False, 'Missing generator_id'

    k = str(kind or '').strip().lower().replace('_', '-')
    if k not in ('flag-generator', 'flag-node-generator'):
        return False, f'Invalid kind: {kind}'

    installed_root = os.path.abspath(_installed_generators_root())
    base = os.path.join(installed_root, 'flag_node_generators' if k == 'flag-node-generator' else 'flag_generators')
    if not os.path.isdir(base):
        return False, 'No installed generators directory'

    target_dir: str | None = None
    for child in sorted(Path(base).iterdir()):
        if not child.is_dir():
            continue
        manifest_path = None
        for nm in ('manifest.yaml', 'manifest.yml'):
            p = child / nm
            if p.exists() and p.is_file():
                manifest_path = p
                break
        if manifest_path is None:
            continue
        try:
            doc = yaml.safe_load(manifest_path.read_text('utf-8', errors='ignore'))
        except Exception:
            continue
        if not isinstance(doc, dict):
            continue
        mid = str(doc.get('id') or '').strip()
        if mid == gid:
            target_dir = str(child)
            break

    if not target_dir:
        return False, f'Installed generator not found: {gid}'

    abs_target = os.path.abspath(target_dir)
    try:
        if os.path.commonpath([installed_root, abs_target]) != installed_root:
            return False, f'Refused to delete outside installed root: {abs_target}'
    except Exception:
        return False, f'Refused to delete path: {abs_target}'

    try:
        shutil.rmtree(abs_target, ignore_errors=False)
    except Exception as exc:
        return False, f'Failed deleting {abs_target}: {exc}'

    # Update pack state: remove references to this generator.
    try:
        state = _load_installed_generator_packs_state()
        packs = state.get('packs') or []
        if not isinstance(packs, list):
            packs = []
        kept_packs = []
        removed_refs = 0
        for p in packs:
            if not isinstance(p, dict):
                continue
            installed = p.get('installed') or []
            if not isinstance(installed, list):
                installed = []
            new_installed = []
            for it in installed:
                if not isinstance(it, dict):
                    continue
                it_id = str(it.get('id') or '').strip()
                it_kind = str(it.get('kind') or '').strip().lower().replace('_', '-')
                it_path = str(it.get('path') or '').strip()
                if it_id == gid and it_kind == k:
                    removed_refs += 1
                    continue
                if it_path and os.path.abspath(it_path) == abs_target:
                    removed_refs += 1
                    continue
                new_installed.append(it)

            if new_installed:
                p2 = dict(p)
                p2['installed'] = new_installed
                kept_packs.append(p2)
            else:
                # Drop pack row if no installed generators remain.
                continue
        state['packs'] = kept_packs
        _save_installed_generator_packs_state(state)
    except Exception:
        removed_refs = 0

    note = f'Deleted installed {k} {gid}'
    if removed_refs:
        note += f' (updated {removed_refs} pack reference(s))'
    return True, note


@app.route('/api/flag_generators/delete', methods=['POST'])
def api_flag_generators_delete():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    gid = str(payload.get('generator_id') or payload.get('id') or '').strip()
    ok, note = _delete_installed_generator(kind='flag-generator', generator_id=gid)
    return jsonify({'ok': ok, 'message': note} if ok else {'ok': False, 'error': note}), (200 if ok else 400)


@app.route('/api/flag_node_generators/delete', methods=['POST'])
def api_flag_node_generators_delete():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    gid = str(payload.get('generator_id') or payload.get('id') or '').strip()
    ok, note = _delete_installed_generator(kind='flag-node-generator', generator_id=gid)
    return jsonify({'ok': ok, 'message': note} if ok else {'ok': False, 'error': note}), (200 if ok else 400)


@app.route('/api/generator_packs/set_disabled', methods=['POST'])
def api_generator_packs_set_disabled():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    pid = str(payload.get('pack_id') or '').strip()
    disabled = bool(payload.get('disabled') is True)
    ok, note = _set_pack_disabled_state(pack_id=pid, disabled=disabled)
    return jsonify({'ok': ok, 'message': note} if ok else {'ok': False, 'error': note}), (200 if ok else 400)


@app.route('/api/flag_generators/set_disabled', methods=['POST'])
def api_flag_generators_set_disabled():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    gid = str(payload.get('generator_id') or payload.get('id') or '').strip()
    disabled = bool(payload.get('disabled') is True)
    ok, note = _set_generator_disabled_state(kind='flag-generator', generator_id=gid, disabled=disabled)
    return jsonify({'ok': ok, 'message': note} if ok else {'ok': False, 'error': note}), (200 if ok else 400)


@app.route('/api/flag_node_generators/set_disabled', methods=['POST'])
def api_flag_node_generators_set_disabled():
    _require_builder_or_admin()
    payload = request.get_json(silent=True) or {}
    gid = str(payload.get('generator_id') or payload.get('id') or '').strip()
    disabled = bool(payload.get('disabled') is True)
    ok, note = _set_generator_disabled_state(kind='flag-node-generator', generator_id=gid, disabled=disabled)
    return jsonify({'ok': ok, 'message': note} if ok else {'ok': False, 'error': note}), (200 if ok else 400)


@app.route('/generator_packs/download/<pack_id>')
def generator_packs_download(pack_id: str):
    pid = str(pack_id or '').strip()
    state = _load_installed_generator_packs_state()
    packs = state.get('packs') or []
    if not isinstance(packs, list):
        packs = []
    target = None
    for p in packs:
        if isinstance(p, dict) and str(p.get('id') or '') == pid:
            target = p
            break
    if not target:
        flash('Pack not found')
        return redirect(url_for('flag_catalog_page'))

    data = _pack_to_zip_bytes(target)
    label = secure_filename(str(target.get('label') or '')).strip() or 'pack'
    download_name = f"generator-pack-{pid}-{label}.zip"
    import io

    return send_file(io.BytesIO(data), as_attachment=True, download_name=download_name)


@app.route('/generator_packs/export_all')
def generator_packs_export_all():
    """Export all installed packs.

    Returns a ZIP containing one ZIP per pack under packs/<pack_id>.zip.
    """
    import io
    import zipfile

    state = _load_installed_generator_packs_state()
    packs = state.get('packs') or []
    if not isinstance(packs, list):
        packs = []

    mem = io.BytesIO()
    with zipfile.ZipFile(mem, 'w', zipfile.ZIP_DEFLATED) as z:
        for p in packs:
            if not isinstance(p, dict):
                continue
            pid = str(p.get('id') or '').strip()
            if not pid:
                continue
            label = secure_filename(str(p.get('label') or '')).strip() or 'pack'
            arcname = f"packs/{pid}-{label}.zip"
            try:
                z.writestr(arcname, _pack_to_zip_bytes(p))
            except Exception:
                continue
    mem.seek(0)
    return send_file(mem, as_attachment=True, download_name='generator_packs.zip')


def _flag_generators_runs_dir() -> str:
    d = os.path.join(_outputs_dir(), 'flag_generators_runs')
    os.makedirs(d, exist_ok=True)
    return d


def _flaggen_run_dir_for_id(run_id: str) -> str:
    """Resolve a local flag generator run directory for a run_id.

    This allows artifact browsing/downloading even if the webapp restarted and
    the in-memory RUNS registry was lost.
    """
    rid = (run_id or '').strip()
    # Basic sanitization: avoid path traversal.
    rid = rid.replace('..', '').replace('\\', '/').strip('/').strip()
    return os.path.join(_flag_generators_runs_dir(), rid)


def _flag_node_generators_runs_dir() -> str:
    d = os.path.join(_outputs_dir(), 'flag_node_generators_runs')
    os.makedirs(d, exist_ok=True)
    return d


def _flagnodegen_run_dir_for_id(run_id: str) -> str:
    rid = (run_id or '').strip()
    rid = rid.replace('..', '').replace('\\', '/').strip('/').strip()
    return os.path.join(_flag_node_generators_runs_dir(), rid)


def _find_enabled_node_generator_by_id(generator_id: str) -> Optional[dict]:
    gid = (generator_id or '').strip()
    if not gid:
        return None
    try:
        generators, _errors = _flag_node_generators_from_enabled_sources()
    except Exception:
        return None
    for g in generators:
        try:
            if str(g.get('id') or '').strip() == gid:
                return g
        except Exception:
            continue
    return None


def _find_enabled_generator_by_id(generator_id: str) -> Optional[dict]:
    gid = (generator_id or '').strip()
    if not gid:
        return None
    try:
        generators, _errors = _flag_generators_from_enabled_sources()
    except Exception:
        return None
    for g in generators:
        try:
            if str(g.get('id') or '').strip() == gid:
                return g
        except Exception:
            continue
    return None


@app.route('/flag_generators_test/run', methods=['POST'])
def flag_generators_test_run():
    """Start a generator test run.

    Accepts text inputs and file uploads based on the generator's declared inputs.
    Writes a run log and streams it via /stream/<run_id>.
    """
    t0 = time.time()
    generator_id = (request.form.get('generator_id') or '').strip()
    try:
        app.logger.info("[flaggen_test] POST /flag_generators_test/run generator_id=%s", generator_id)
    except Exception:
        pass
    gen = _find_enabled_generator_by_id(generator_id)
    if not gen:
        try:
            app.logger.warning("[flaggen_test] generator not found: %s", generator_id)
        except Exception:
            pass
        return jsonify({'ok': False, 'error': 'Generator not found (must be in an enabled source).'}), 404

    # Respect disabled state for installed generators.
    try:
        if isinstance(gen, dict) and _is_installed_generator_view(gen) and _is_installed_generator_disabled(kind='flag-generator', generator_id=generator_id):
            return jsonify({'ok': False, 'error': f'Generator {generator_id} is disabled.'}), 400
    except Exception:
        pass

    run_id = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:10]
    run_dir = os.path.join(_flag_generators_runs_dir(), run_id)
    inputs_dir = os.path.join(run_dir, 'inputs')
    os.makedirs(inputs_dir, exist_ok=True)
    log_path = os.path.join(run_dir, 'run.log')

    # Build config object from declared inputs.
    # Two-pass: load all text fields first, then save uploads (so uploads can
    # consult optional filename inputs).
    cfg: dict[str, Any] = {}
    saved_uploads: dict[str, dict[str, Any]] = {}
    inputs = gen.get('inputs') if isinstance(gen, dict) else None
    inputs_list = inputs if isinstance(inputs, list) else []

    for inp in inputs_list:
        if not isinstance(inp, dict):
            continue
        name = str(inp.get('name') or '').strip()
        if not name:
            continue
        # Only record text values in pass 1.
        raw_val = request.form.get(name)
        if raw_val is not None:
            cfg[name] = raw_val

    def _unique_dest_filename(dir_path: str, filename: str) -> str:
        base = secure_filename(filename) or 'upload'
        # Ensure a stable file name but avoid overwriting.
        cand = base
        root, ext = os.path.splitext(base)
        i = 1
        while os.path.exists(os.path.join(dir_path, cand)):
            cand = f"{root}_{i}{ext}"
            i += 1
            if i > 5000:
                break
        return cand

    for inp in inputs_list:
        if not isinstance(inp, dict):
            continue
        name = str(inp.get('name') or '').strip()
        if not name:
            continue
        f = request.files.get(name)
        if not (f and getattr(f, 'filename', '')):
            continue

        original_filename = str(getattr(f, 'filename', '') or '')

        # Default behavior: store under <name>__<original>
        safe_uploaded = secure_filename(original_filename) or 'upload'
        stored = f"{name}__{safe_uploaded}"

        # Special-case convenience: if the user provides a desired filename
        # for an uploaded file, respect it. This is primarily used by the
        # sample generator (binary_embed_text) where the input file's name is
        # meaningful.
        desired = None
        try:
            if name == 'input_file':
                desired = cfg.get('File(path)')
        except Exception:
            desired = None
        requested_filename = None
        used_requested = False
        if isinstance(desired, str) and desired.strip():
            requested_filename = desired.strip()
            stored = _unique_dest_filename(inputs_dir, requested_filename)
            used_requested = True
        else:
            stored = _unique_dest_filename(inputs_dir, stored)

        dest = os.path.join(inputs_dir, stored)
        try:
            f.save(dest)
            # Expose the in-container path.
            cfg[name] = f"/inputs/{stored}"
            saved_uploads[name] = {
                'original_filename': original_filename,
                'requested_filename': requested_filename,
                'stored_filename': stored,
                'stored_path': f"inputs/{stored}",
                'container_path': f"/inputs/{stored}",
                'used_requested_filename': used_requested,
            }
        except Exception:
            return jsonify({'ok': False, 'error': f"Failed saving file input: {name}"}), 400

    try:
        app.logger.info("[flaggen_test] inputs keys=%s", sorted(list(cfg.keys())))
    except Exception:
        pass

    # Validate required inputs minimally (only presence).
    missing: list[str] = []
    for inp in inputs_list:
        if not isinstance(inp, dict):
            continue
        try:
            if not inp.get('required'):
                continue
            name = str(inp.get('name') or '').strip()
            if not name:
                continue
            val = cfg.get(name)
            if val is None or (isinstance(val, str) and not val.strip()):
                missing.append(name)
        except Exception:
            continue
    if missing:
        try:
            app.logger.warning("[flaggen_test] missing required inputs: %s", missing)
        except Exception:
            pass
        return jsonify({'ok': False, 'error': f"Missing required input(s): {', '.join(missing)}"}), 400

    # Launch runner.
    repo_root = _get_repo_root()
    runner_path = os.path.join(repo_root, 'scripts', 'run_flag_generator.py')
    cmd = [
        sys.executable or 'python',
        runner_path,
        '--kind',
        'flag-generator',
        '--generator-id',
        generator_id,
        '--out-dir',
        run_dir,
        '--config',
        json.dumps(cfg, ensure_ascii=False),
        '--repo-root',
        repo_root,
    ]

    try:
        with open(log_path, 'a', encoding='utf-8') as log_f:
            log_f.write(f"[flaggen] starting {generator_id}\n")
            _write_sse_marker(log_f, 'phase', {'phase': 'starting', 'generator_id': generator_id, 'run_id': run_id})
    except Exception:
        pass

    try:
        log_handle = open(log_path, 'a', encoding='utf-8')
        proc = subprocess.Popen(
            cmd,
            cwd=repo_root,
            stdout=log_handle,
            stderr=log_handle,
            text=True,
        )
    except Exception as exc:
        try:
            with open(log_path, 'a', encoding='utf-8') as log_f:
                log_f.write(f"[flaggen] failed to start: {exc}\n")
                _write_sse_marker(log_f, 'phase', {'phase': 'error', 'error': str(exc)})
        except Exception:
            pass
        return jsonify({'ok': False, 'error': f"Failed launching generator: {exc}"}), 500

    try:
        app.logger.info(
            "[flaggen_test] spawned pid=%s run_id=%s run_dir=%s elapsed_ms=%s",
            getattr(proc, 'pid', None),
            run_id,
            run_dir,
            int((time.time() - t0) * 1000),
        )
    except Exception:
        pass

    RUNS[run_id] = {
        'proc': proc,
        'log_path': log_path,
        'done': False,
        'returncode': None,
        'run_dir': run_dir,
        'kind': 'flag_generator_test',
        'generator_id': generator_id,
    }

    def _finalize_flaggen(run_id_local: str, log_handle_local: Any):
        try:
            meta = RUNS.get(run_id_local)
            if not meta:
                return
            p = meta.get('proc')
            if not p:
                return
            rc = p.wait()
            meta['done'] = True
            meta['returncode'] = rc
            try:
                with open(meta.get('log_path'), 'a', encoding='utf-8') as log_f:
                    _write_sse_marker(log_f, 'phase', {'phase': 'done', 'returncode': rc})
            except Exception:
                pass
        finally:
            try:
                log_handle_local.close()
            except Exception:
                pass

    threading.Thread(
        target=_finalize_flaggen,
        args=(run_id, log_handle),
        name=f'flaggen-{run_id[:8]}',
        daemon=True,
    ).start()

    try:
        app.logger.info(
            "[flaggen_test] responding ok run_id=%s elapsed_ms=%s",
            run_id,
            int((time.time() - t0) * 1000),
        )
    except Exception:
        pass
    return jsonify({'ok': True, 'run_id': run_id, 'saved_uploads': saved_uploads})


@app.route('/flag_node_generators_test/run', methods=['POST'])
def flag_node_generators_test_run():
    """Start a node-generator test run."""
    t0 = time.time()
    generator_id = (request.form.get('generator_id') or '').strip()
    try:
        app.logger.info("[flagnodegen_test] POST /flag_node_generators_test/run generator_id=%s", generator_id)
    except Exception:
        pass

    gen = _find_enabled_node_generator_by_id(generator_id)
    if not gen:
        return jsonify({'ok': False, 'error': 'Generator not found (must be installed and enabled).'}), 404

    try:
        if isinstance(gen, dict) and _is_installed_generator_view(gen) and _is_installed_generator_disabled(kind='flag-node-generator', generator_id=generator_id):
            return jsonify({'ok': False, 'error': f'Node-generator {generator_id} is disabled.'}), 400
    except Exception:
        pass

    run_id = datetime.datetime.utcnow().strftime('%Y%m%d-%H%M%S') + '-' + uuid.uuid4().hex[:10]
    run_dir = os.path.join(_flag_node_generators_runs_dir(), run_id)
    inputs_dir = os.path.join(run_dir, 'inputs')
    os.makedirs(inputs_dir, exist_ok=True)
    log_path = os.path.join(run_dir, 'run.log')

    cfg: dict[str, Any] = {}
    saved_uploads: dict[str, dict[str, Any]] = {}
    inputs = gen.get('inputs') if isinstance(gen, dict) else None
    inputs_list = inputs if isinstance(inputs, list) else []
    for inp in inputs_list:
        if not isinstance(inp, dict):
            continue
        name = str(inp.get('name') or '').strip()
        if not name:
            continue
        f = request.files.get(name)
        if f and getattr(f, 'filename', ''):
            original_filename = str(getattr(f, 'filename', '') or '')
            safe_name = secure_filename(original_filename) or 'upload'
            stored = f"{name}__{safe_name}"
            dest = os.path.join(inputs_dir, stored)
            try:
                f.save(dest)
                cfg[name] = f"/inputs/{stored}"
                saved_uploads[name] = {
                    'original_filename': original_filename,
                    'requested_filename': None,
                    'stored_filename': stored,
                    'stored_path': f"inputs/{stored}",
                    'container_path': f"/inputs/{stored}",
                    'used_requested_filename': False,
                }
            except Exception:
                return jsonify({'ok': False, 'error': f"Failed saving file input: {name}"}), 400
            continue
        raw_val = request.form.get(name)
        if raw_val is not None:
            cfg[name] = raw_val

    missing: list[str] = []
    for inp in inputs_list:
        if not isinstance(inp, dict):
            continue
        try:
            if not inp.get('required'):
                continue
            name = str(inp.get('name') or '').strip()
            if not name:
                continue
            val = cfg.get(name)
            if val is None or (isinstance(val, str) and not val.strip()):
                missing.append(name)
        except Exception:
            continue
    if missing:
        return jsonify({'ok': False, 'error': f"Missing required input(s): {', '.join(missing)}"}), 400

    repo_root = _get_repo_root()
    runner_path = os.path.join(repo_root, 'scripts', 'run_flag_generator.py')
    cmd = [
        sys.executable or 'python',
        runner_path,
        '--kind',
        'flag-node-generator',
        '--generator-id',
        generator_id,
        '--out-dir',
        run_dir,
        '--config',
        json.dumps(cfg, ensure_ascii=False),
        '--repo-root',
        repo_root,
    ]

    try:
        with open(log_path, 'a', encoding='utf-8') as log_f:
            log_f.write(f"[flagnodegen] starting {generator_id}\n")
            _write_sse_marker(log_f, 'phase', {'phase': 'starting', 'generator_id': generator_id, 'run_id': run_id})
    except Exception:
        pass

    try:
        log_handle = open(log_path, 'a', encoding='utf-8')
        proc = subprocess.Popen(
            cmd,
            cwd=repo_root,
            stdout=log_handle,
            stderr=log_handle,
            text=True,
        )
    except Exception as exc:
        try:
            with open(log_path, 'a', encoding='utf-8') as log_f:
                log_f.write(f"[flagnodegen] failed to start: {exc}\n")
                _write_sse_marker(log_f, 'phase', {'phase': 'error', 'error': str(exc)})
        except Exception:
            pass
        return jsonify({'ok': False, 'error': f"Failed launching generator: {exc}"}), 500

    RUNS[run_id] = {
        'proc': proc,
        'log_path': log_path,
        'done': False,
        'returncode': None,
        'run_dir': run_dir,
        'kind': 'flag_node_generator_test',
        'generator_id': generator_id,
    }

    def _finalize(run_id_local: str, log_handle_local: Any):
        try:
            meta = RUNS.get(run_id_local)
            if not meta:
                return
            p = meta.get('proc')
            if not p:
                return
            rc = p.wait()
            meta['done'] = True
            meta['returncode'] = rc
            try:
                with open(meta.get('log_path'), 'a', encoding='utf-8') as log_f:
                    _write_sse_marker(log_f, 'phase', {'phase': 'done', 'returncode': rc})
            except Exception:
                pass
        finally:
            try:
                log_handle_local.close()
            except Exception:
                pass

    threading.Thread(
        target=_finalize,
        args=(run_id, log_handle),
        name=f'flagnodegen-{run_id[:8]}',
        daemon=True,
    ).start()

    try:
        app.logger.info(
            "[flagnodegen_test] spawned pid=%s run_id=%s run_dir=%s elapsed_ms=%s",
            getattr(proc, 'pid', None),
            run_id,
            run_dir,
            int((time.time() - t0) * 1000),
        )
    except Exception:
        pass
    return jsonify({'ok': True, 'run_id': run_id, 'saved_uploads': saved_uploads})


@app.route('/flag_node_generators_test/outputs/<run_id>')
def flag_node_generators_test_outputs(run_id: str):
    meta = RUNS.get(run_id)
    if meta and meta.get('kind') != 'flag_node_generator_test':
        return jsonify({'ok': False, 'error': 'not found'}), 404
    run_dir = meta.get('run_dir') if isinstance(meta, dict) else None
    if not isinstance(run_dir, str) or not run_dir:
        run_dir = _flagnodegen_run_dir_for_id(run_id)
    if not isinstance(run_dir, str) or not run_dir:
        return jsonify({'ok': False, 'error': 'missing run dir'}), 500
    abs_run_dir = os.path.abspath(run_dir)
    outputs_root = os.path.abspath(_outputs_dir())
    if not (abs_run_dir == outputs_root or abs_run_dir.startswith(outputs_root + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400
    if not os.path.isdir(abs_run_dir):
        done = bool(meta.get('done')) if isinstance(meta, dict) else False
        returncode = meta.get('returncode') if isinstance(meta, dict) else None
        return jsonify({'ok': True, 'files': [], 'done': done, 'returncode': returncode}), 200

    input_files: list[dict[str, Any]] = []
    output_files: list[dict[str, Any]] = []
    misc_files: list[dict[str, Any]] = []
    for root, _dirs, filenames in os.walk(abs_run_dir):
        rel_root = os.path.relpath(root, abs_run_dir).replace('\\', '/')
        for fn in filenames:
            abs_path = os.path.join(root, fn)
            try:
                st = os.stat(abs_path)
                rel = os.path.relpath(abs_path, abs_run_dir).replace('\\', '/')
                entry = {'path': rel, 'name': fn, 'size': st.st_size}
            except Exception:
                continue
            if rel_root == 'inputs' or rel_root.startswith('inputs/'):
                input_files.append(entry)
            elif rel == 'run.log':
                misc_files.append(entry)
            else:
                output_files.append(entry)

    input_files.sort(key=lambda x: x.get('path') or '')
    output_files.sort(key=lambda x: x.get('path') or '')
    misc_files.sort(key=lambda x: x.get('path') or '')
    done = bool(meta.get('done')) if isinstance(meta, dict) else True
    returncode = meta.get('returncode') if isinstance(meta, dict) else None
    return jsonify({'ok': True, 'inputs': input_files, 'outputs': output_files, 'misc': misc_files, 'done': done, 'returncode': returncode}), 200


@app.route('/flag_node_generators_test/download/<run_id>')
def flag_node_generators_test_download(run_id: str):
    meta = RUNS.get(run_id)
    if meta and meta.get('kind') != 'flag_node_generator_test':
        return jsonify({'ok': False, 'error': 'not found'}), 404
    run_dir = meta.get('run_dir') if isinstance(meta, dict) else None
    if not isinstance(run_dir, str) or not run_dir:
        run_dir = _flagnodegen_run_dir_for_id(run_id)
    if not isinstance(run_dir, str) or not run_dir:
        return jsonify({'ok': False, 'error': 'missing run dir'}), 500
    rel = (request.args.get('p') or '').strip().lstrip('/').replace('\\', '/')
    if not rel:
        return jsonify({'ok': False, 'error': 'invalid path'}), 400
    abs_run_dir = os.path.abspath(run_dir)
    outputs_root = os.path.abspath(_outputs_dir())
    if not (abs_run_dir == outputs_root or abs_run_dir.startswith(outputs_root + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400
    abs_path = os.path.abspath(os.path.join(abs_run_dir, rel))
    if not (abs_path == abs_run_dir or abs_path.startswith(abs_run_dir + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400
    if not os.path.exists(abs_path) or not os.path.isfile(abs_path):
        return jsonify({'ok': False, 'error': 'missing file'}), 404
    return send_file(abs_path, as_attachment=True, download_name=os.path.basename(abs_path))


@app.route('/flag_node_generators_test/cleanup/<run_id>', methods=['POST'])
def flag_node_generators_test_cleanup(run_id: str):
    """Delete all artifacts for a flag-node-generator test run (scoped to outputs/)."""
    meta = RUNS.get(run_id)
    if meta and meta.get('kind') != 'flag_node_generator_test':
        return jsonify({'ok': False, 'error': 'not found'}), 404
    run_dir = meta.get('run_dir') if isinstance(meta, dict) else None
    if not isinstance(run_dir, str) or not run_dir:
        run_dir = _flagnodegen_run_dir_for_id(run_id)
    if not isinstance(run_dir, str) or not run_dir:
        return jsonify({'ok': False, 'error': 'missing run dir'}), 500
    abs_run_dir = os.path.abspath(run_dir)
    outputs_root = os.path.abspath(_outputs_dir())
    if not (abs_run_dir == outputs_root or abs_run_dir.startswith(outputs_root + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400

    try:
        proc = meta.get('proc') if isinstance(meta, dict) else None
        if proc and hasattr(proc, 'poll') and proc.poll() is None:
            try:
                proc.terminate()
                proc.wait(timeout=5)
            except Exception:
                try:
                    proc.kill()
                except Exception:
                    pass
    except Exception:
        pass

    removed = False
    try:
        if os.path.isdir(abs_run_dir):
            shutil.rmtree(abs_run_dir, ignore_errors=True)
        removed = True
    except Exception:
        removed = False

    try:
        RUNS.pop(run_id, None)
    except Exception:
        pass
    return jsonify({'ok': True, 'removed': removed}), 200


@app.route('/flag_generators_test/outputs/<run_id>')
def flag_generators_test_outputs(run_id: str):
    meta = RUNS.get(run_id)
    if meta and meta.get('kind') != 'flag_generator_test':
        return jsonify({'ok': False, 'error': 'not found'}), 404

    run_dir = meta.get('run_dir') if isinstance(meta, dict) else None
    if not isinstance(run_dir, str) or not run_dir:
        run_dir = _flaggen_run_dir_for_id(run_id)
    if not isinstance(run_dir, str) or not run_dir:
        return jsonify({'ok': False, 'error': 'missing run dir'}), 500
    abs_run_dir = os.path.abspath(run_dir)
    outputs_root = os.path.abspath(_outputs_dir())
    if not (abs_run_dir == outputs_root or abs_run_dir.startswith(outputs_root + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400
    if not os.path.isdir(abs_run_dir):
        done = bool(meta.get('done')) if isinstance(meta, dict) else False
        returncode = meta.get('returncode') if isinstance(meta, dict) else None
        return jsonify({'ok': True, 'files': [], 'done': done, 'returncode': returncode}), 200

    input_files: list[dict[str, Any]] = []
    output_files: list[dict[str, Any]] = []
    misc_files: list[dict[str, Any]] = []

    for root, _dirs, filenames in os.walk(abs_run_dir):
        rel_root = os.path.relpath(root, abs_run_dir).replace('\\', '/')
        for fn in filenames:
            abs_path = os.path.join(root, fn)
            try:
                st = os.stat(abs_path)
                rel = os.path.relpath(abs_path, abs_run_dir).replace('\\', '/')
                entry = {'path': rel, 'name': fn, 'size': st.st_size}
            except Exception:
                continue

            if rel_root == 'inputs' or rel_root.startswith('inputs/'):
                input_files.append(entry)
            elif rel == 'run.log':
                misc_files.append(entry)
            else:
                output_files.append(entry)

    input_files.sort(key=lambda x: x.get('path') or '')
    output_files.sort(key=lambda x: x.get('path') or '')
    misc_files.sort(key=lambda x: x.get('path') or '')
    done = bool(meta.get('done')) if isinstance(meta, dict) else True
    returncode = meta.get('returncode') if isinstance(meta, dict) else None
    return jsonify({
        'ok': True,
        'inputs': input_files,
        'outputs': output_files,
        'misc': misc_files,
        'done': done,
        'returncode': returncode,
    }), 200


@app.route('/flag_generators_test/download/<run_id>')
def flag_generators_test_download(run_id: str):
    meta = RUNS.get(run_id)
    if meta and meta.get('kind') != 'flag_generator_test':
        return jsonify({'ok': False, 'error': 'not found'}), 404
    run_dir = meta.get('run_dir') if isinstance(meta, dict) else None
    if not isinstance(run_dir, str) or not run_dir:
        run_dir = _flaggen_run_dir_for_id(run_id)
    if not isinstance(run_dir, str) or not run_dir:
        return jsonify({'ok': False, 'error': 'missing run dir'}), 500
    rel = (request.args.get('p') or '').strip().lstrip('/').replace('\\', '/')
    if not rel:
        return jsonify({'ok': False, 'error': 'invalid path'}), 400
    abs_run_dir = os.path.abspath(run_dir)
    outputs_root = os.path.abspath(_outputs_dir())
    if not (abs_run_dir == outputs_root or abs_run_dir.startswith(outputs_root + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400
    abs_path = os.path.abspath(os.path.join(abs_run_dir, rel))
    if not (abs_path == abs_run_dir or abs_path.startswith(abs_run_dir + os.sep)):
        return jsonify({'ok': False, 'error': 'refusing'}), 400
    if not os.path.exists(abs_path) or not os.path.isfile(abs_path):
        return jsonify({'ok': False, 'error': 'missing file'}), 404
    return send_file(abs_path, as_attachment=True, download_name=os.path.basename(abs_path))


@app.route('/flag_generators_test/bundle/<run_id>')
def flag_generators_test_bundle(run_id: str):
    return jsonify({'ok': False, 'error': 'bundle.zip output disabled'}), 404


def _flag_resolve_path(raw_path: str) -> str:
    """Resolve a flag path that may be repo-relative."""
    p = (raw_path or '').strip()
    if not p:
        return ''
    try:
        if p.startswith('file://'):
            p = p[len('file://'):]
    except Exception:
        pass
    if os.path.isabs(p):
        return p
    try:
        return os.path.abspath(os.path.join(_get_repo_root(), p))
    except Exception:
        return os.path.abspath(p)


@app.route('/flag_compose/status', methods=['POST'])
def flag_compose_status():
    """Return status for a list of flag catalog items."""
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_flag_base_dir())
        os.makedirs(base_out, exist_ok=True)
        for it in items:
            name = (it.get('name') or it.get('Name') or '').strip()
            path_raw = (it.get('path') or it.get('Path') or '').strip()
            compose_name = (it.get('compose_name') or it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'flag')
            fdir = os.path.join(base_out, safe)
            gh = _parse_github_url(path_raw)
            compose_file = None
            base_dir = fdir
            exists = False

            # Local file path support (repo-relative)
            local_path = _flag_resolve_path(path_raw)
            if local_path and os.path.exists(local_path) and local_path.lower().endswith(('.yml', '.yaml')):
                compose_file = local_path
                base_dir = os.path.dirname(local_path)
                exists = True
                try:
                    logs.append(f"[status] {name}: local compose={compose_file}")
                except Exception:
                    pass
            elif gh.get('is_github'):
                repo_dir = os.path.join(fdir, _vuln_repo_subdir())
                sub = gh.get('subpath') or ''
                is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                if is_file_sub:
                    compose_file = os.path.join(repo_dir, sub)
                    base_dir = os.path.dirname(compose_file)
                    exists = os.path.exists(compose_file)
                else:
                    base_dir = os.path.join(repo_dir, sub) if sub else repo_dir
                    exists = os.path.isdir(base_dir)
                if exists and compose_name and not compose_file:
                    p = os.path.join(base_dir, compose_name)
                    if os.path.exists(p):
                        compose_file = p
                if not compose_file:
                    cand = _compose_candidates(base_dir)
                    compose_file = cand[0] if cand else None
                try:
                    logs.append(f"[status] {name}: github base={base_dir} exists={exists} compose={compose_name}")
                except Exception:
                    pass
            else:
                # Legacy direct download into outputs/flags/<safe>/compose_name
                compose_file = os.path.join(fdir, compose_name)
                exists = os.path.exists(compose_file)
                base_dir = fdir

            pulled = False
            if exists and compose_file and shutil.which('docker'):
                try:
                    proc = subprocess.run(['docker', 'compose', '-f', compose_file, 'config', '--images'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=30)
                    if proc.returncode == 0:
                        images = [ln.strip() for ln in (proc.stdout or '').splitlines() if ln.strip()]
                        if images:
                            present = []
                            for img in images:
                                p2 = subprocess.run(['docker', 'image', 'inspect', img], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                                present.append(p2.returncode == 0)
                            pulled = all(present)
                except Exception:
                    pulled = False
            out.append({
                'name': name,
                'path': path_raw,
                'compose_name': compose_name,
                'compose_path': compose_file,
                'exists': bool(exists),
                'pulled': bool(pulled),
                'dir': base_dir,
            })
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/flag_compose/download', methods=['POST'])
def flag_compose_download():
    """Download/clone docker-compose assets for the given flag catalog items."""
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_flag_base_dir())
        os.makedirs(base_out, exist_ok=True)

        try:
            from core_topo_gen.utils.vuln_process import _github_tree_to_raw as _to_raw
        except Exception:
            def _to_raw(base_url: str, filename: str) -> str | None:
                try:
                    from urllib.parse import urlparse
                    u = urlparse(base_url)
                    if u.netloc.lower() != 'github.com':
                        return None
                    parts = [p for p in u.path.strip('/').split('/') if p]
                    if len(parts) < 4 or parts[2] != 'tree':
                        return None
                    owner, repo, _tree, branch = parts[:4]
                    rest = '/'.join(parts[4:])
                    return f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{rest}/{filename}"
                except Exception:
                    return None

        import urllib.request
        import shlex
        for it in items:
            name = (it.get('name') or it.get('Name') or '').strip()
            path_raw = (it.get('path') or it.get('Path') or '').strip()
            compose_name = (it.get('compose_name') or it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'flag')
            fdir = os.path.join(base_out, safe)
            os.makedirs(fdir, exist_ok=True)

            # Local compose path support: nothing to download.
            local_path = _flag_resolve_path(path_raw)
            if local_path and os.path.exists(local_path) and local_path.lower().endswith(('.yml', '.yaml')):
                out.append({'name': name, 'path': path_raw, 'ok': True, 'dir': os.path.dirname(local_path), 'message': 'local compose file'})
                continue

            gh = _parse_github_url(path_raw)
            if gh.get('is_github'):
                if not shutil.which('git'):
                    logs.append(f"[download] {name}: git not available in PATH")
                    out.append({'name': name, 'path': path_raw, 'ok': False, 'dir': fdir, 'message': 'git not available'})
                    continue
                repo_dir = os.path.join(fdir, _vuln_repo_subdir())
                if os.path.isdir(os.path.join(repo_dir, '.git')):
                    base_dir = os.path.join(repo_dir, gh.get('subpath') or '') if gh.get('subpath') else repo_dir
                    out.append({'name': name, 'path': path_raw, 'ok': True, 'dir': base_dir, 'message': 'already downloaded'})
                    continue
                try:
                    if os.path.exists(repo_dir):
                        shutil.rmtree(repo_dir)
                except Exception:
                    pass
                cmd = ['git', 'clone', '--depth', '1']
                if gh.get('branch'):
                    cmd += ['--branch', gh.get('branch')]
                cmd += [gh.get('git_url'), repo_dir]
                try:
                    logs.append(f"[download] {name}: running: {' '.join(shlex.quote(c) for c in cmd)}")
                    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=120)
                    if proc.returncode == 0 and os.path.isdir(repo_dir):
                        base_dir = os.path.join(repo_dir, gh.get('subpath') or '') if gh.get('subpath') else repo_dir
                        out.append({'name': name, 'path': path_raw, 'ok': True, 'dir': base_dir, 'message': 'downloaded'})
                    else:
                        msg = (proc.stdout or '').strip()
                        out.append({'name': name, 'path': path_raw, 'ok': False, 'dir': fdir, 'message': msg[-1000:] if msg else 'git clone failed'})
                except Exception as e:
                    out.append({'name': name, 'path': path_raw, 'ok': False, 'dir': fdir, 'message': str(e)})
            else:
                raw = _to_raw(path_raw, compose_name) or (path_raw.rstrip('/') + '/' + compose_name)
                yml_path = os.path.join(fdir, compose_name)
                try:
                    logs.append(f"[download] {name}: GET {raw}")
                    with urllib.request.urlopen(raw, timeout=30) as resp:
                        data_bin = resp.read(1_000_000)
                    with open(yml_path, 'wb') as fh:
                        fh.write(data_bin)
                    out.append({'name': name, 'path': path_raw, 'ok': True, 'dir': fdir, 'message': 'downloaded', 'compose_name': compose_name})
                except Exception as e:
                    out.append({'name': name, 'path': path_raw, 'ok': False, 'dir': fdir, 'message': str(e), 'compose_name': compose_name})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/flag_compose/pull', methods=['POST'])
def flag_compose_pull():
    """Run docker compose pull for the given flag catalog items."""
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_flag_base_dir())
        for it in items:
            name = (it.get('name') or it.get('Name') or '').strip()
            path_raw = (it.get('path') or it.get('Path') or '').strip()
            compose_name = (it.get('compose_name') or it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'flag')
            fdir = os.path.join(base_out, safe)

            local_path = _flag_resolve_path(path_raw)
            if local_path and os.path.exists(local_path) and local_path.lower().endswith(('.yml', '.yaml')):
                yml_path = local_path
            else:
                gh = _parse_github_url(path_raw)
                if gh.get('is_github'):
                    repo_dir = os.path.join(fdir, _vuln_repo_subdir())
                    sub = gh.get('subpath') or ''
                    is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                    base_dir = os.path.join(repo_dir, os.path.dirname(sub)) if is_file_sub else (os.path.join(repo_dir, sub) if sub else repo_dir)
                    yml_path = os.path.join(repo_dir, sub) if is_file_sub else os.path.join(base_dir, compose_name)
                    if not os.path.exists(yml_path):
                        cand = _compose_candidates(base_dir)
                        yml_path = cand[0] if cand else None
                else:
                    yml_path = os.path.join(fdir, compose_name)

            if not yml_path or not os.path.exists(yml_path):
                out.append({'name': name, 'path': path_raw, 'ok': False, 'message': 'compose file missing', 'compose_name': compose_name})
                continue
            if not shutil.which('docker'):
                out.append({'name': name, 'path': path_raw, 'ok': False, 'message': 'docker not available', 'compose_name': compose_name})
                continue
            try:
                proc = subprocess.run(['docker', 'compose', '-f', yml_path, 'pull'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                logs.append(f"[pull] {name}: rc={proc.returncode} file={yml_path}")
                ok = proc.returncode == 0
                msg = 'ok' if ok else ((proc.stdout or '')[-1000:] if proc.stdout else 'failed')
                out.append({'name': name, 'path': path_raw, 'ok': ok, 'message': msg, 'compose_name': compose_name})
            except Exception as e:
                out.append({'name': name, 'path': path_raw, 'ok': False, 'message': str(e), 'compose_name': compose_name})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/flag_compose/remove', methods=['POST'])
def flag_compose_remove():
    """Remove compose assets and any local outputs for the given flag catalog items."""
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_flag_base_dir())
        for it in items:
            name = (it.get('name') or it.get('Name') or '').strip()
            path_raw = (it.get('path') or it.get('Path') or '').strip()
            compose_name = (it.get('compose_name') or it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'flag')
            fdir = os.path.join(base_out, safe)

            # Prefer local compose file if path points at one.
            local_path = _flag_resolve_path(path_raw)
            yml_path = None
            if local_path and os.path.exists(local_path) and local_path.lower().endswith(('.yml', '.yaml')):
                yml_path = local_path
            else:
                gh = _parse_github_url(path_raw)
                if gh.get('is_github'):
                    repo_dir = os.path.join(fdir, _vuln_repo_subdir())
                    sub = gh.get('subpath') or ''
                    is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                    base_dir = os.path.join(repo_dir, os.path.dirname(sub)) if is_file_sub else (os.path.join(repo_dir, sub) if sub else repo_dir)
                    yml_path = os.path.join(repo_dir, sub) if is_file_sub else os.path.join(base_dir, compose_name)
                    if not os.path.exists(yml_path):
                        cand = _compose_candidates(base_dir)
                        yml_path = cand[0] if cand else None
                else:
                    yml_path = os.path.join(fdir, compose_name)

            if yml_path and os.path.exists(yml_path) and shutil.which('docker'):
                try:
                    logs.append(f"[remove] {name}: docker compose down file={yml_path}")
                except Exception:
                    pass
                try:
                    subprocess.run(['docker', 'compose', '-f', yml_path, 'down', '--volumes', '--remove-orphans'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                except Exception:
                    pass

            # Remove downloaded dirs under outputs/flags only (never delete repo-local templates)
            try:
                gh = _parse_github_url(path_raw)
                if gh.get('is_github'):
                    repo_dir = os.path.join(fdir, _vuln_repo_subdir())
                    if os.path.isdir(repo_dir):
                        shutil.rmtree(repo_dir, ignore_errors=True)
                        logs.append(f"[remove] {name}: deleted {repo_dir}")
                else:
                    yml = os.path.join(fdir, compose_name)
                    if os.path.exists(yml):
                        try:
                            os.remove(yml)
                            logs.append(f"[remove] {name}: deleted {yml}")
                        except Exception:
                            pass
                try:
                    if os.path.isdir(fdir) and not os.listdir(fdir):
                        os.rmdir(fdir)
                except Exception:
                    pass
            except Exception as e:
                try:
                    logs.append(f"[remove] cleanup error: {e}")
                except Exception:
                    pass

            out.append({'name': name, 'path': path_raw, 'ok': True, 'message': 'removed', 'compose_name': compose_name})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/vuln_catalog')
def vuln_catalog():
    """Return vulnerability catalog.

    The Topology tab uses this for the "Select Vulnerability" modal.

     Source priority (strict for Web UI):
     1) Active installed Vulnerability Pack (Web UI state in outputs/installed_vuln_catalogs)
         - respects disabled/deleted items
         - uses UI-style display names (e.g. <parent>/<leaf>)

     Note: We intentionally do NOT fall back to repo defaults here. If the user
     has an empty/absent catalog pack, the Topology picker should show no items.
    """
    try:
        # Prefer the new vulnerability-pack-backed catalog when present.
        try:
            state = _load_vuln_catalogs_state()
            entry = _get_active_vuln_catalog_entry(state)
        except Exception:
            state = {}
            entry = None

        if entry and isinstance(entry, dict):
            cid = str(entry.get('id') or '').strip()
            norm_items = _normalize_vuln_catalog_items(entry)
            from_source = str(entry.get('from_source') or entry.get('label') or '').strip()

            def _display_name(it: dict[str, Any]) -> str:
                base = str(it.get('name') or '').strip() or 'root'
                rel_dir = str(it.get('rel_dir') or it.get('dir_rel') or '').strip()
                if not rel_dir or rel_dir in ('', '.', 'root'):
                    return base
                parts = [p for p in rel_dir.replace('\\', '/').split('/') if p]
                if len(parts) >= 2:
                    return f"{parts[-2]}/{parts[-1]}"
                return parts[-1] if parts else base

            items: list[dict[str, str]] = []
            for it in norm_items:
                if bool(it.get('disabled', False)):
                    continue
                try:
                    abs_compose = _vuln_catalog_item_abs_compose_path(catalog_id=cid, item=it)
                except Exception:
                    continue
                rel_dir = str(it.get('rel_dir') or it.get('dir_rel') or '').strip().replace('\\', '/')
                files_api_url = ''
                try:
                    files_api_url = url_for('vuln_catalog_pack_item_files', catalog_id=cid, item_id=int(it.get('id') or 0))
                except Exception:
                    files_api_url = ''
                items.append({
                    'Name': _display_name(it),
                    'Path': os.path.abspath(abs_compose),
                    'Type': 'docker-compose',
                    'Vector': '',
                    'Startup': '',
                    'CVE': '',
                    'Description': '',
                    'References': '',
                    'id': str(it.get('id') or '').strip(),
                    'from_source': from_source,
                    'files_api_url': files_api_url,
                    'validated_ok': bool(it.get('validated_ok')) if it.get('validated_ok') is not None else None,
                    'validated_at': str(it.get('validated_at') or '').strip() or None,
                })
        else:
            items = []

        types = sorted({str(it.get('Type') or '').strip() for it in items if str(it.get('Type') or '').strip()})
        vectors = sorted({str(it.get('Vector') or '').strip() for it in items if str(it.get('Vector') or '').strip()})
        return jsonify({'types': types, 'vectors': vectors, 'items': items})
    except Exception as e:
        return jsonify({'error': str(e), 'types': [], 'vectors': [], 'items': []}), 500


# ------------ Vulnerability compose helpers (GitHub-aware) ---------------
def _safe_name(s: str) -> str:
    try:
        return re.sub(r"[^a-z0-9_.-]+", "-", (s or '').strip().lower())[:80] or 'vuln'
    except Exception:
        return 'vuln'


def _parse_github_url(url: str):
    """Parse a GitHub URL. Supports formats:
    - https://github.com/owner/repo/tree/<branch>/<subpath>
    - https://github.com/owner/repo/blob/<branch>/<file_or_subpath>
    - https://github.com/owner/repo (no branch; default branch)

    Returns dict with keys:
      { 'is_github': bool, 'git_url': str|None, 'branch': str|None, 'subpath': str|None, 'mode': 'tree'|'blob'|'root' }
    """
    try:
        from urllib.parse import urlparse
        u = urlparse(url)
        if u.netloc.lower() != 'github.com':
            return {'is_github': False}
        parts = [p for p in u.path.strip('/').split('/') if p]
        if len(parts) < 2:
            return {'is_github': False}
        owner, repo = parts[0], parts[1]
        git_url = f"https://github.com/{owner}/{repo}.git"
        if len(parts) == 2:
            return {'is_github': True, 'git_url': git_url, 'branch': None, 'subpath': '', 'mode': 'root'}
        mode = parts[2]
        if mode not in ('tree', 'blob') or len(parts) < 4:
            # Unknown path mode; treat as root
            return {'is_github': True, 'git_url': git_url, 'branch': None, 'subpath': '', 'mode': 'root'}
        branch = parts[3]
        rest = '/'.join(parts[4:])
        return {'is_github': True, 'git_url': git_url, 'branch': branch, 'subpath': rest, 'mode': mode}
    except Exception:
        return {'is_github': False}


def _compose_candidates(base_dir: str):
    """Return possible compose file paths under base_dir in priority order."""
    cands = ['docker-compose.yml', 'docker-compose.yaml', 'compose.yml', 'compose.yaml']
    out = []
    try:
        for name in cands:
            p = os.path.join(base_dir, name)
            if os.path.exists(p):
                out.append(p)
    except Exception:
        pass
    return out


def _vuln_resolve_local_path(path_raw: str) -> str | None:
    """Resolve a local vuln compose path (file or directory).

    Safety: only allow paths under the repo root.
    """
    try:
        p = str(path_raw or '').strip()
        if not p:
            return None
        abs_p = os.path.abspath(os.path.expanduser(p))
        if not os.path.exists(abs_p):
            return None
        repo_root = os.path.abspath(_get_repo_root())
        try:
            if os.path.commonpath([repo_root, abs_p]) != repo_root:
                return None
        except Exception:
            return None
        return abs_p
    except Exception:
        return None

@app.route('/vuln_compose/status', methods=['POST'])
def vuln_compose_status():
    """Return status for a list of catalog items: whether compose file is downloaded and images pulled.

    Payload: { items: [{Name, Path}] }
    Returns: { items: [{Name, Path, exists: bool, pulled: bool, dir: str}] }
    """
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_vuln_base_dir())
        os.makedirs(base_out, exist_ok=True)
        for it in items:
            name = (it.get('Name') or '').strip()
            path = (it.get('Path') or '').strip()
            compose_name = (it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'vuln')
            vdir = os.path.join(base_out, safe)
            base_dir = vdir
            compose_file = None
            exists = False

            local = _vuln_resolve_local_path(path)
            if local:
                if os.path.isdir(local):
                    base_dir = local
                    pref = os.path.join(base_dir, compose_name)
                    if os.path.exists(pref):
                        compose_file = pref
                    else:
                        cand = _compose_candidates(base_dir)
                        compose_file = cand[0] if cand else None
                    exists = bool(compose_file and os.path.exists(compose_file))
                else:
                    compose_file = local
                    base_dir = os.path.dirname(local)
                    exists = os.path.exists(compose_file)
            else:
                gh = _parse_github_url(path)
                if gh.get('is_github'):
                    try:
                        logs.append(f"[status] {name}: Path={path}")
                        logs.append(f"[status] {name}: git_url={gh.get('git_url')} branch={gh.get('branch')} subpath={gh.get('subpath')} mode={gh.get('mode')}")
                    except Exception:
                        pass
                    repo_dir = os.path.join(vdir, _vuln_repo_subdir())
                    sub = gh.get('subpath') or ''
                    # If subpath looks like a compose file, resolve directly
                    is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                    if is_file_sub:
                        compose_file = os.path.join(repo_dir, sub)
                        base_dir = os.path.dirname(compose_file)
                        exists = os.path.exists(compose_file)
                    else:
                        base_dir = os.path.join(repo_dir, sub) if sub else repo_dir
                        exists = os.path.isdir(base_dir)
                        # prefer provided compose name
                        if exists and compose_name:
                            p = os.path.join(base_dir, compose_name)
                            if os.path.exists(p):
                                compose_file = p
                        if not compose_file and exists:
                            cand = _compose_candidates(base_dir)
                            compose_file = cand[0] if cand else None
                        exists = bool(compose_file and os.path.exists(compose_file)) if compose_file else bool(exists)
                    try:
                        logs.append(f"[status] {name}: base={base_dir} exists={exists} compose={compose_name}")
                    except Exception:
                        pass
                    # log compose candidates
                    try:
                        cands = _compose_candidates(base_dir) if os.path.isdir(base_dir) else []
                        logs.append(f"[status] {name}: compose candidates={cands[:4]}")
                    except Exception:
                        pass
                else:
                    # legacy / direct download case: vdir/docker-compose.yml
                    compose_file = os.path.join(vdir, compose_name or 'docker-compose.yml')
                    base_dir = vdir
                    exists = os.path.exists(compose_file)
                    try:
                        logs.append(f"[status] {name}: non-github Path={path} compose_path={compose_file} exists={exists}")
                    except Exception:
                        pass
            pulled = False
            if exists and compose_file and shutil.which('docker'):
                try:
                    proc = subprocess.run(['docker', 'compose', '-f', compose_file, 'config', '--images'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=30)
                    try:
                        logs.append(f"[status] docker compose config --images rc={proc.returncode}")
                    except Exception:
                        pass
                    if proc.returncode == 0:
                        images = [ln.strip() for ln in (proc.stdout or '').splitlines() if ln.strip()]
                        try:
                            logs.append(f"[status] images discovered: {len(images)}")
                            logs.append(f"[status] images sample: {images[:4]}")
                        except Exception:
                            pass
                        if images:
                            present = []
                            for img in images:
                                p2 = subprocess.run(['docker', 'image', 'inspect', img], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                                try:
                                    logs.append(f"[status] image inspect {img} rc={p2.returncode}")
                                except Exception:
                                    pass
                                present.append(p2.returncode == 0)
                            pulled = all(present)
                except Exception:
                    pulled = False
            out.append({'Name': name, 'Path': path, 'compose': compose_name, 'compose_path': compose_file, 'exists': bool(exists), 'pulled': bool(pulled), 'dir': base_dir})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/vuln_compose/status_images', methods=['POST'])
def vuln_compose_status_images():
    """Fast-ish status for a list of catalog items: whether compose is downloaded and images exist locally.

    This endpoint is optimized for UI gating (vuln picker):
    - Resolve the compose file path similarly to `/vuln_compose/status`.
    - Prefer parsing docker-compose YAML (`services.*.image`) to avoid invoking `docker compose config`.
    - Check local images via `docker image inspect`.
    - Fallback to `docker compose -f <file> config --images` only when YAML parsing yields no images.

    Payload: { items: [{Name, Path}] }
    Returns: { items: [{Name, Path, exists: bool, pulled: bool, compose_path: str|None, images: [str], missing_images: [str]}] }
    """
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out: list[dict] = []

        base_out = os.path.abspath(_vuln_base_dir())
        docker_ok = bool(shutil.which('docker'))

        # Optional dependency (pinned in requirements, but keep best-effort).
        try:
            import yaml  # type: ignore
        except Exception:  # pragma: no cover
            yaml = None  # type: ignore

        def _resolve_compose_path(name: str, path: str, compose_name: str) -> tuple[bool, str | None]:
            safe = _safe_name(name or 'vuln')
            vdir = os.path.join(base_out, safe)
            compose_file: str | None = None
            exists = False

            local = _vuln_resolve_local_path(path)
            if local:
                if os.path.isdir(local):
                    pref = os.path.join(local, compose_name)
                    if os.path.exists(pref):
                        compose_file = pref
                    else:
                        cand = _compose_candidates(local)
                        compose_file = cand[0] if cand else None
                    return bool(compose_file and os.path.exists(compose_file)), compose_file
                return bool(os.path.exists(local)), local

            gh = _parse_github_url(path)

            if gh.get('is_github'):
                repo_dir = os.path.join(vdir, _vuln_repo_subdir())
                sub = gh.get('subpath') or ''
                is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                if is_file_sub:
                    compose_file = os.path.join(repo_dir, sub)
                    exists = os.path.exists(compose_file)
                else:
                    base_dir = os.path.join(repo_dir, sub) if sub else repo_dir
                    exists = os.path.isdir(base_dir)
                    if exists:
                        preferred = os.path.join(base_dir, compose_name)
                        if os.path.exists(preferred):
                            compose_file = preferred
                        else:
                            cand = _compose_candidates(base_dir)
                            compose_file = cand[0] if cand else None
                return bool(exists and compose_file and os.path.exists(compose_file)), compose_file

            # Non-github: legacy direct download
            compose_file = os.path.join(vdir, compose_name)
            exists = os.path.exists(compose_file)
            return bool(exists), compose_file if exists else compose_file

        def _images_from_compose_yaml(compose_path: str) -> list[str]:
            if not yaml:
                return []
            try:
                with open(compose_path, 'r', encoding='utf-8', errors='ignore') as f:
                    doc = yaml.safe_load(f) or {}
                if not isinstance(doc, dict):
                    return []
                services = doc.get('services')
                if not isinstance(services, dict):
                    return []
                images: list[str] = []
                for _svc_name, svc in services.items():
                    if not isinstance(svc, dict):
                        continue
                    img = svc.get('image')
                    if isinstance(img, str) and img.strip():
                        images.append(img.strip())
                # preserve stable order but dedupe
                seen: set[str] = set()
                out_imgs: list[str] = []
                for img in images:
                    if img in seen:
                        continue
                    seen.add(img)
                    out_imgs.append(img)
                return out_imgs
            except Exception:
                return []

        for it in items:
            name = (it.get('Name') or '').strip()
            path = (it.get('Path') or '').strip()
            compose_name = (it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'

            exists, compose_path = _resolve_compose_path(name, path, compose_name)
            images: list[str] = []
            missing: list[str] = []
            pulled = False

            if exists and compose_path and docker_ok:
                images = _images_from_compose_yaml(compose_path)

                # Fallback: ask docker compose to render images list (slower).
                if not images:
                    try:
                        proc = subprocess.run(
                            ['docker', 'compose', '-f', compose_path, 'config', '--images'],
                            stdout=subprocess.PIPE,
                            stderr=subprocess.STDOUT,
                            text=True,
                            timeout=20,
                        )
                        if proc.returncode == 0:
                            images = [ln.strip() for ln in (proc.stdout or '').splitlines() if ln.strip()]
                    except Exception:
                        images = []

                if images:
                    for img in images:
                        try:
                            p2 = subprocess.run(
                                ['docker', 'image', 'inspect', img],
                                stdout=subprocess.DEVNULL,
                                stderr=subprocess.DEVNULL,
                            )
                            if p2.returncode != 0:
                                missing.append(img)
                        except Exception:
                            missing.append(img)
                    pulled = len(missing) == 0
                else:
                    # No images declared (common with build-only compose files). Treat as ready
                    # so the UI doesn't perpetually show Download/Pull.
                    pulled = True

            out.append({
                'Name': name,
                'Path': path,
                'compose': compose_name,
                'compose_path': compose_path,
                'exists': bool(exists),
                'pulled': bool(pulled),
                'images': images,
                'missing_images': missing,
                'docker_available': docker_ok,
            })

        return jsonify({'items': out})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/vuln_compose/download', methods=['POST'])
def vuln_compose_download():
    """Download docker-compose.yml for the given catalog items.

    Payload: { items: [{Name, Path}] }
    Returns: { items: [{Name, Path, ok: bool, dir: str, message: str}] }
    """
    try:
        try:
            from core_topo_gen.utils.vuln_process import _github_tree_to_raw as _to_raw
        except Exception as _imp_err:
            # Fallback: minimal tree->raw converter for GitHub tree URLs
            def _to_raw(base_url: str, filename: str) -> str | None:
                try:
                    from urllib.parse import urlparse
                    u = urlparse(base_url)
                    if u.netloc.lower() != 'github.com':
                        return None
                    parts = [p for p in u.path.strip('/').split('/') if p]
                    if len(parts) < 4 or parts[2] != 'tree':
                        return None
                    owner, repo, _tree, branch = parts[:4]
                    rest = '/'.join(parts[4:])
                    return f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{rest}/{filename}"
                except Exception:
                    return None
            try:
                app.logger.warning("[download] fallback _to_raw used due to import error: %s", _imp_err)
            except Exception:
                pass
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_vuln_base_dir())
        os.makedirs(base_out, exist_ok=True)
        import urllib.request
        import shlex
        for it in items:
            name = (it.get('Name') or '').strip()
            path = (it.get('Path') or '').strip()
            compose_name = (it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'vuln')
            vdir = os.path.join(base_out, safe)
            os.makedirs(vdir, exist_ok=True)

            local = _vuln_resolve_local_path(path)
            if local:
                if os.path.isdir(local):
                    pref = os.path.join(local, compose_name)
                    if os.path.exists(pref):
                        out.append({'Name': name, 'Path': path, 'ok': True, 'dir': local, 'message': 'local compose directory'})
                    else:
                        out.append({'Name': name, 'Path': path, 'ok': False, 'dir': local, 'message': 'compose file missing in local directory'})
                else:
                    out.append({'Name': name, 'Path': path, 'ok': True, 'dir': os.path.dirname(local), 'message': 'local compose file'})
                continue

            gh = _parse_github_url(path)
            if gh.get('is_github'):
                # Clone the repo; use branch if provided
                if not shutil.which('git'):
                    try:
                        logs.append(f"[download] {name}: git not available in PATH")
                    except Exception:
                        pass
                    out.append({'Name': name, 'Path': path, 'ok': False, 'dir': vdir, 'message': 'git not available'})
                    continue
                repo_dir = os.path.join(vdir, _vuln_repo_subdir())
                try:
                    logs.append(f"[download] {name}: Path={path}")
                    logs.append(f"[download] {name}: git_url={gh.get('git_url')} branch={gh.get('branch')} subpath={gh.get('subpath')} -> repo_dir={repo_dir}")
                except Exception:
                    pass
                # If already cloned and looks valid, skip re-clone
                if os.path.isdir(os.path.join(repo_dir, '.git')):
                    try:
                        logs.append(f"[download] {name}: repo exists {repo_dir}")
                    except Exception:
                        pass
                    base_dir = os.path.join(repo_dir, gh.get('subpath') or '') if gh.get('subpath') else repo_dir
                    try:
                        logs.append(f"[download] {name}: base_dir={base_dir}")
                        # limited directory listing
                        if os.path.isdir(base_dir):
                            entries = []
                            for nm in os.listdir(base_dir)[:10]:
                                p = os.path.join(base_dir, nm)
                                kind = 'dir' if os.path.isdir(p) else 'file'
                                entries.append(f"{nm}({kind})")
                            logs.append(f"[download] {name}: base_dir entries: {entries}")
                    except Exception:
                        pass
                    out.append({'Name': name, 'Path': path, 'ok': True, 'dir': base_dir, 'message': 'already downloaded'})
                    continue
                # Ensure empty directory
                try:
                    if os.path.exists(repo_dir):
                        shutil.rmtree(repo_dir)
                except Exception:
                    pass
                cmd = ['git', 'clone', '--depth', '1']
                if gh.get('branch'):
                    cmd += ['--branch', gh.get('branch')]
                cmd += [gh.get('git_url'), repo_dir]
                try:
                    try:
                        logs.append(f"[download] {name}: running: {' '.join(shlex.quote(c) for c in cmd)}")
                    except Exception:
                        pass
                    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=120)
                    try:
                        logs.append(f"[download] git clone rc={proc.returncode} dir={repo_dir}")
                        if proc.stdout:
                            for ln in proc.stdout.splitlines()[:100]:
                                logs.append(f"[git] {ln}")
                    except Exception:
                        pass
                    if proc.returncode == 0 and os.path.isdir(repo_dir):
                        base_dir = os.path.join(repo_dir, gh.get('subpath') or '') if gh.get('subpath') else repo_dir
                        try:
                            logs.append(f"[download] {name}: base_dir={base_dir}")
                            # limited directory listing
                            if os.path.isdir(base_dir):
                                entries = []
                                for nm in os.listdir(base_dir)[:10]:
                                    p = os.path.join(base_dir, nm)
                                    kind = 'dir' if os.path.isdir(p) else 'file'
                                    entries.append(f"{nm}({kind})")
                                logs.append(f"[download] {name}: base_dir entries: {entries}")
                        except Exception:
                            pass
                        out.append({'Name': name, 'Path': path, 'ok': True, 'dir': base_dir, 'message': 'downloaded'})
                    else:
                        msg = (proc.stdout or '').strip()
                        out.append({'Name': name, 'Path': path, 'ok': False, 'dir': vdir, 'message': msg[-1000:] if msg else 'git clone failed'})
                except Exception as e:
                    out.append({'Name': name, 'Path': path, 'ok': False, 'dir': vdir, 'message': str(e)})
            else:
                # Legacy: direct download of compose file (use provided compose name)
                raw = _to_raw(path, compose_name) or (path.rstrip('/') + '/' + compose_name)
                yml_path = os.path.join(vdir, compose_name)
                try:
                    try:
                        logs.append(f"[download] {name}: Path={path}")
                        logs.append(f"[download] {name}: GET {raw}")
                    except Exception:
                        pass
                    with urllib.request.urlopen(raw, timeout=30) as resp:
                        status = getattr(resp, 'status', None) or getattr(resp, 'code', None)
                        data_bin = resp.read(1_000_000)
                        try:
                            logs.append(f"[download] {name}: HTTP {status} bytes={len(data_bin) if data_bin else 0}")
                        except Exception:
                            pass
                    with open(yml_path, 'wb') as f:
                        f.write(data_bin)
                    out.append({'Name': name, 'Path': path, 'ok': True, 'dir': vdir, 'message': 'downloaded', 'compose': compose_name})
                except Exception as e:
                    out.append({'Name': name, 'Path': path, 'ok': False, 'dir': vdir, 'message': str(e), 'compose': compose_name})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/vuln_compose/pull', methods=['POST'])
def vuln_compose_pull():
    """Run docker compose pull for the given catalog items (assumes docker-compose.yml is present).

    Payload: { items: [{Name, Path}] }
    Returns: { items: [{Name, Path, ok: bool, message: str}] }
    """
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_vuln_base_dir())
        for it in items:
            name = (it.get('Name') or '').strip()
            path = (it.get('Path') or '').strip()
            compose_name = (it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'vuln')
            vdir = os.path.join(base_out, safe)

            local = _vuln_resolve_local_path(path)
            if local:
                if os.path.isdir(local):
                    pref = os.path.join(local, compose_name)
                    if os.path.exists(pref):
                        yml_path = pref
                    else:
                        cand = _compose_candidates(local)
                        yml_path = cand[0] if cand else None
                else:
                    yml_path = local
            else:
                gh = _parse_github_url(path)
                if gh.get('is_github'):
                    repo_dir = os.path.join(vdir, _vuln_repo_subdir())
                    sub = gh.get('subpath') or ''
                    # blob file path -> direct compose path
                    is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                    base_dir = os.path.join(repo_dir, os.path.dirname(sub)) if is_file_sub else (os.path.join(repo_dir, sub) if sub else repo_dir)
                    try:
                        logs.append(
                            f"[pull] {name}: git_url={gh.get('git_url')} branch={gh.get('branch')} subpath={gh.get('subpath')} base_dir={base_dir}"
                        )
                    except Exception:
                        pass
                    # prefer provided compose name
                    yml_path = os.path.join(repo_dir, sub) if is_file_sub else os.path.join(base_dir, compose_name)
                    if not os.path.exists(yml_path):
                        cand = _compose_candidates(base_dir)
                        yml_path = cand[0] if cand else None
                    try:
                        logs.append(f"[pull] {name}: yml_path={yml_path}")
                    except Exception:
                        pass
                else:
                    yml_path = os.path.join(vdir, compose_name)
                    try:
                        logs.append(f"[pull] {name}: non-github base_dir={vdir}")
                    except Exception:
                        pass
            if not yml_path or not os.path.exists(yml_path):
                out.append({'Name': name, 'Path': path, 'ok': False, 'message': 'compose file missing', 'compose': compose_name})
                continue
            if not shutil.which('docker'):
                out.append({'Name': name, 'Path': path, 'ok': False, 'message': 'docker not available', 'compose': compose_name})
                continue
            try:
                proc = subprocess.run(['docker', 'compose', '-f', yml_path, 'pull'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                try:
                    logs.append(f"[pull] {name}: docker compose pull rc={proc.returncode} file={yml_path}")
                    if proc.stdout:
                        for ln in proc.stdout.splitlines()[:200]:
                            logs.append(f"[docker] {ln}")
                except Exception:
                    pass
                ok = proc.returncode == 0
                msg = 'ok' if ok else ((proc.stdout or '')[-1000:] if proc.stdout else 'failed')
                out.append({'Name': name, 'Path': path, 'ok': ok, 'message': msg, 'compose': compose_name})
            except Exception as e:
                out.append({'Name': name, 'Path': path, 'ok': False, 'message': str(e), 'compose': compose_name})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/vuln_compose/remove', methods=['POST'])
def vuln_compose_remove():
    """Remove docker-compose assets and containers/images for the given catalog items.

    Steps per item:
    - Resolve compose file path (like status/pull)
    - docker compose down --volumes --remove-orphans
    - Optionally remove images referenced by compose (best-effort)
    - Remove downloaded directories (repo dir or compose file directory) under outputs

    Payload: { items: [{Name, Path}] }
    Returns: { items: [{Name, Path, ok: bool, message: str}] }
    """
    try:
        data = request.get_json(silent=True) or {}
        items = data.get('items') or []
        out = []
        logs: list[str] = []
        base_out = os.path.abspath(_vuln_base_dir())
        for it in items:
            name = (it.get('Name') or '').strip()
            path = (it.get('Path') or '').strip()
            compose_name = (it.get('compose') or 'docker-compose.yml').strip() or 'docker-compose.yml'
            safe = _safe_name(name or 'vuln')
            vdir = os.path.join(base_out, safe)
            yml_path = None
            base_dir = vdir
            is_local = False
            try:
                logs.append(f"[remove] {name}: Path={path}")
            except Exception:
                pass

            local = _vuln_resolve_local_path(path)
            if local:
                is_local = True
                if os.path.isdir(local):
                    base_dir = local
                    pref = os.path.join(base_dir, compose_name)
                    if os.path.exists(pref):
                        yml_path = pref
                    else:
                        cand = _compose_candidates(base_dir)
                        yml_path = cand[0] if cand else None
                else:
                    yml_path = local
                    base_dir = os.path.dirname(local)
            else:
                gh = _parse_github_url(path)
                if gh.get('is_github'):
                    repo_dir = os.path.join(vdir, _vuln_repo_subdir())
                    sub = gh.get('subpath') or ''
                    is_file_sub = bool(sub) and sub.lower().endswith(('.yml', '.yaml'))
                    base_dir = os.path.join(repo_dir, os.path.dirname(sub)) if is_file_sub else (os.path.join(repo_dir, sub) if sub else repo_dir)
                    yml_path = os.path.join(repo_dir, sub) if is_file_sub else os.path.join(base_dir, compose_name)
                    if not os.path.exists(yml_path):
                        cand = _compose_candidates(base_dir)
                        yml_path = cand[0] if cand else None
                else:
                    yml_path = os.path.join(vdir, compose_name)
            # Bring down compose stack
            if yml_path and os.path.exists(yml_path) and shutil.which('docker'):
                try:
                    logs.append(f"[remove] {name}: docker compose down file={yml_path}")
                except Exception:
                    pass
                try:
                    proc = subprocess.run(['docker', 'compose', '-f', yml_path, 'down', '--volumes', '--remove-orphans'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                    try:
                        logs.append(f"[remove] docker compose down rc={proc.returncode}")
                        if proc.stdout:
                            for ln in proc.stdout.splitlines()[:200]:
                                logs.append(f"[docker] {ln}")
                    except Exception:
                        pass
                except Exception as e:
                    try: logs.append(f"[remove] compose down error: {e}")
                    except Exception: pass
                # Attempt to remove images referenced by compose (best-effort)
                try:
                    proc2 = subprocess.run(['docker', 'compose', '-f', yml_path, 'config', '--images'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                    if proc2.returncode == 0:
                        images = [ln.strip() for ln in (proc2.stdout or '').splitlines() if ln.strip()]
                        for img in images:
                            p3 = subprocess.run(['docker', 'image', 'rm', '-f', img], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                            try: logs.append(f"[remove] image rm {img} rc={p3.returncode}")
                            except Exception: pass
                except Exception:
                    pass
            # Remove downloaded files/dirs under outputs for this item
            try:
                if is_local:
                    # Never delete repo-local installed templates.
                    pass
                else:
                    gh = _parse_github_url(path)
                    if gh.get('is_github'):
                        repo_dir = os.path.join(vdir, _vuln_repo_subdir())
                        if os.path.isdir(repo_dir):
                            shutil.rmtree(repo_dir, ignore_errors=True)
                            logs.append(f"[remove] {name}: deleted {repo_dir}")
                    else:
                        # legacy direct compose path
                        yml = os.path.join(vdir, compose_name)
                        if os.path.exists(yml):
                            try:
                                os.remove(yml)
                                logs.append(f"[remove] {name}: deleted {yml}")
                            except Exception:
                                pass
                    # Remove vdir if empty
                    try:
                        if os.path.isdir(vdir) and not os.listdir(vdir):
                            os.rmdir(vdir)
                            logs.append(f"[remove] {name}: cleaned empty {vdir}")
                    except Exception:
                        pass
            except Exception as e:
                try: logs.append(f"[remove] cleanup error: {e}")
                except Exception: pass
            out.append({'Name': name, 'Path': path, 'ok': True, 'message': 'removed', 'compose': compose_name})
        return jsonify({'items': out, 'log': logs})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def _purge_run_history_for_scenario(scenario_name: str, delete_artifacts: bool = True) -> int:
    """Remove any run history entries whose scenario_names contains scenario_name.
    Optionally delete associated artifact files (xml/report/pre-session xml) under outputs/.
    Returns number of entries removed.
    """
    try:
        if not os.path.exists(RUN_HISTORY_PATH):
            return 0
        with open(RUN_HISTORY_PATH, 'r', encoding='utf-8') as f:
            hist = json.load(f)
        if not isinstance(hist, list):
            return 0
        kept = []
        removed = 0
        target_key = ''
        try:
            target_key = _scenario_match_key(scenario_name)
        except Exception:
            target_key = ''
        for entry in hist:
            scen_list = []
            try:
                if 'scenario_names' in entry:
                    scen_list = entry.get('scenario_names') or []
                else:
                    scen_list = _scenario_names_from_xml(entry.get('xml_path'))
            except Exception:
                scen_list = []
            match = False
            if target_key and scen_list:
                try:
                    for nm in scen_list:
                        if _scenario_match_key(nm) == target_key:
                            match = True
                            break
                except Exception:
                    match = False
            else:
                match = bool(scenario_name in (scen_list or []))

            if match:
                removed += 1
                if delete_artifacts:
                    for key in ('xml_path','report_path','pre_xml_path','post_xml_path','scenario_xml_path'):
                        p = entry.get(key)
                        if p and isinstance(p,str) and os.path.exists(p):
                            # Only delete if inside outputs directory for safety
                            try:
                                out_abs = os.path.abspath(_outputs_dir())
                                p_abs = os.path.abspath(p)
                                if p_abs.startswith(out_abs):
                                    try: os.remove(p_abs)
                                    except Exception: pass
                                    # Attempt to remove directory if empty afterwards
                                    try:
                                        parent = os.path.dirname(p_abs)
                                        if parent.startswith(out_abs) and os.path.isdir(parent) and not os.listdir(parent):
                                            os.rmdir(parent)
                                    except Exception:
                                        pass
                            except Exception:
                                pass
                continue
            kept.append(entry)
        if removed:
            tmp = RUN_HISTORY_PATH + '.tmp'
            with open(tmp, 'w', encoding='utf-8') as f:
                json.dump(kept, f, indent=2)
            os.replace(tmp, RUN_HISTORY_PATH)
        return removed
    except Exception:
        return 0

@app.route('/purge_history_for_scenario', methods=['POST'])
def purge_history_for_scenario():
    try:
        data = request.get_json(silent=True) or {}
        name = (data.get('name') or '').strip()
        if not name:
            return jsonify({'removed': 0}), 200
        removed = _purge_run_history_for_scenario(name, delete_artifacts=True)
        planner_removed = _purge_planner_state_for_scenarios([name])
        plans_removed = _purge_plan_artifacts_for_scenarios([name])
        return jsonify({
            'removed': removed,
            'planner_removed': planner_removed,
            'plans_removed': plans_removed,
        })
    except Exception as e:
        return jsonify({'removed': 0, 'error': str(e)}), 200


@app.route('/delete_scenarios', methods=['POST'])
def delete_scenarios():
    """Persist scenario deletions across refresh.

    This updates outputs/scenario_catalog.json, which seeds the Scenario Editor list
    on initial page load.
    """
    try:
        data = request.get_json(silent=True) or {}
        raw_names = data.get('names')
        if isinstance(raw_names, str):
            names = [raw_names]
        elif isinstance(raw_names, list):
            names = [n for n in raw_names if isinstance(n, (str, int, float))]
        else:
            names = []
        names = [str(n).strip() for n in names if str(n).strip()]
        if not names:
            return jsonify({'ok': False, 'error': 'Missing scenario names'}), 400
        result = _remove_scenarios_from_catalog(names)
        # Also delete any saved Scenario_*.xml artifacts and remove the scenario(s)
        # from all editor snapshots, otherwise they will be re-discovered on refresh.
        artifacts = _delete_saved_scenario_xml_artifacts(names)
        snapshots = _remove_scenarios_from_all_editor_snapshots(names)
        planner_removed = _purge_planner_state_for_scenarios(names)
        plans_removed = _purge_plan_artifacts_for_scenarios(names)
        history_removed = 0
        try:
            for nm in names:
                try:
                    history_removed += _purge_run_history_for_scenario(str(nm), delete_artifacts=True)
                except Exception:
                    continue
        except Exception:
            history_removed = history_removed
        return jsonify({
            'ok': True,
            **result,
            **artifacts,
            **snapshots,
            'history_removed': history_removed,
            'planner_removed': planner_removed,
            'plans_removed': plans_removed,
        })
    except Exception as e:
        try:
            app.logger.exception('[delete_scenarios] failed: %s', e)
        except Exception:
            pass
        return jsonify({'ok': False, 'error': str(e)}), 500

if __name__ == '__main__':
    try:
        port = int(os.environ.get('PORT') or os.environ.get('WEBAPP_PORT') or '9090')
    except Exception:
        port = 9090
    host = os.environ.get('CORETG_HOST') or '0.0.0.0'
    debug = _env_flag('CORETG_DEBUG', False) or _env_flag('FLASK_DEBUG', False)
    use_reloader = _env_flag('CORETG_USE_RELOADER', debug)
    try:
        did_scrub = _scrub_hitl_validation_usernames_in_scenario_catalog()
        if did_scrub:
            app.logger.info('[hitl_validation] scrubbed usernames from scenario_catalog.json')
    except Exception:
        pass
    try:
        did_backfill = _backfill_hitl_config_from_editor_snapshots()
        if did_backfill:
            app.logger.info('[hitl_config] backfilled hitl_config from editor snapshots')
    except Exception:
        pass
    try:
        did_scrub_cfg = _scrub_unverified_hitl_config_in_scenario_catalog()
        if did_scrub_cfg:
            app.logger.info('[hitl_config] scrubbed unverified hitl_config entries from scenario_catalog.json')
    except Exception:
        pass
    app.run(host=host, port=port, debug=debug, use_reloader=use_reloader)
